[{"git_group": "static-frame", "git_name": "static-frame", "version": "v2.6.0", "language": "Python", "project_name": "static-frame-v2.6.0.zip", "file_path": "/static-frame-v2.6.0/static-frame-2.6.0/static_frame/core/loc_map.py", "file_name": "loc_map.py", "focal_class": "HierarchicalLocMap", "focal_name": "build_offsets_and_overflow", "focal_parameter": [], "solution": "    def build_offsets_and_overflow(\n            num_unique_elements_per_depth: tp.List[int],\n            ) -> tp.Tuple[TNDArrayAny, bool]:\n        # `bit_sizes` is an array that shows how many bits are needed to contain the max indexer per depth\n        #\n        # For example, lets say there are 3 levels, and number of unique elements per depth is [71, 5, 13].\n        # `bit_sizes` will become: [7, 3, 4], which mean that we can fit the indexer for each depth into 7, 3, and 4 bits, respectively.\n        #    (i.e.) all valid indexers at depth N can be represented with bit_sizes[N] bits!\n        #\n        # We see this: [2**power for power in [7,3,4]] = [128, 8, 16]\n        # We can prove this: num_unique_elements_per_depth <= [2**bit_size for bit_size in bit_sizes]\n        bit_sizes = np.floor(np.log2(num_unique_elements_per_depth)) + 1\n\n        # Based on bit_sizes, we cumsum to determine the successive number of total bits needed for each depth\n        # Using the previous example, this would look like: [7, 10, 14]\n        # This means:\n        #  - depth 0 ends at bit offset 7.\n        #  - depth 1 ends at bit offset 10. (depth 1 needs 3 bits!)\n        #  - depth 2 ends at bit offset 14. (depth 2 needs 4 bits!)\n        bit_end_positions = np.cumsum(bit_sizes, dtype=DTYPE_UINT_DEFAULT)\n\n        # However, since we ultimately need these values to bitshift, we want them to offset based on start position, not end.\n        # This means:\n        #  - depth 0 starts at bit offset 0.\n        #  - depth 1 starts at bit offset 7. (depth 0 needed 7 bits!)\n        #  - depth 2 starts at bit offset 10. (depth 1 needed 3 bits!)\n        bit_start_positions = np.zeros(\n                len(bit_end_positions),\n                dtype=DTYPE_UINT_DEFAULT)\n        bit_start_positions[1:] = bit_end_positions[:-1]\n        bit_start_positions.flags.writeable = False\n\n        # We now return these offsets, and whether or not we have overflow.\n        # If the last end bit is greater than 64, then it means we cannot encode a label's indexer into a uint64.\n        return bit_start_positions, bit_end_positions[-1] > 64", "function_signature": "def build_offsets_and_overflow(\n            num_unique_elements_per_depth: tp.List[int],\n            ) -> tp.Tuple[TNDArrayAny, bool] :", "left_context": "from __future__ import annotations\n\nimport itertools\nimport sys\nfrom copy import deepcopy\nfrom functools import reduce\n\nimport numpy as np\nimport typing_extensions as tp\nfrom arraykit import array_deepcopy\nfrom arraykit import first_true_1d\nfrom arraymap import FrozenAutoMap  # pylint: disable = E0611\nfrom arraymap import NonUniqueError  # pylint: disable=E0611\n\nfrom static_frame.core.exception import ErrorInitIndexNonUnique\nfrom static_frame.core.exception import LocEmpty\nfrom static_frame.core.exception import LocInvalid\nfrom static_frame.core.util import DTYPE_BOOL\nfrom static_frame.core.util import DTYPE_DATETIME_KIND\nfrom static_frame.core.util import DTYPE_OBJECT\nfrom static_frame.core.util import DTYPE_OBJECTABLE_DT64_UNITS\nfrom static_frame.core.util import DTYPE_UINT_DEFAULT\nfrom static_frame.core.util import EMPTY_ARRAY_INT\nfrom static_frame.core.util import EMPTY_FROZEN_AUTOMAP\nfrom static_frame.core.util import EMPTY_SLICE\nfrom static_frame.core.util import INT_TYPES\nfrom static_frame.core.util import NULL_SLICE\nfrom static_frame.core.util import OPERATORS\nfrom static_frame.core.util import SLICE_ATTRS\nfrom static_frame.core.util import SLICE_START_ATTR\nfrom static_frame.core.util import SLICE_STEP_ATTR\nfrom static_frame.core.util import SLICE_STOP_ATTR\nfrom static_frame.core.util import TILocSelector\nfrom static_frame.core.util import TLabel\nfrom static_frame.core.util import TLocSelector\n\nif tp.TYPE_CHECKING:\n    from static_frame.core.index import Index  # pylint: disable=W0611,C0412 # pragma: no cover\n\nTNDArrayAny = np.ndarray[tp.Any, tp.Any]\nTDtypeAny = np.dtype[tp.Any]\n\nHierarchicalLocMapKey = tp.Union[TNDArrayAny, tp.Tuple[tp.Union[tp.Sequence[TLabel], TLabel], ...]]\n_HLMap = tp.TypeVar('_HLMap', bound='HierarchicalLocMap')\nTypePos = tp.Optional[int]\nLocEmptyInstance = LocEmpty()\n\n\nclass FirstDuplicatePosition(KeyError):\n    def __init__(self, first_dup: int) -> None:\n        self.first_dup = first_dup\n\n\nclass LocMap:\n\n    @staticmethod\n    def map_slice_args(\n            label_to_pos: tp.Callable[[tp.Iterable[TLabel]], int],\n            key: slice,\n            labels: tp.Optional[TNDArrayAny] = None,\n            ) -> tp.Iterator[tp.Union[int, None]]:\n        '''Given a slice ``key`` and a label-to-position mapping, yield each integer argument necessary to create a new iloc slice. If the ``key`` defines a region with no constituents, raise ``LocEmpty``\n\n        Args:\n            label_to_pos: callable into mapping (can be a get() method from a dictionary)\n        '''\n        # NOTE: it is expected that NULL_SLICE is already identified\n        labels_astype: tp.Optional[TNDArrayAny] = None\n\n        for field in SLICE_ATTRS:\n            attr = getattr(key, field)\n            if attr is None:\n                yield None\n\n            # NOTE: We can do `is` checks on field since `SLICE_ATTRS` only contains some certain symbolic constants\n\n            elif attr.__class__ is np.datetime64:\n                if field is SLICE_STEP_ATTR:\n                    raise RuntimeError(f'Step cannot be {attr}')\n                # if we match the same dt64 unit, simply use label_to_pos, increment stop\n                if attr.dtype == labels.dtype: # type: ignore\n                    pos: TypePos = label_to_pos(attr)\n                    if pos is None:\n                        # if same type, and that atter is not in labels, we fail, just as we do in then non-datetime64 case. Only when datetimes are given in a different unit are we \"loose\" about matching.\n                        raise LocInvalid('Invalid loc given in a slice', attr, field)\n                    if field is SLICE_STOP_ATTR:\n                        pos += 1 # stop is inclusive\n                elif field is SLICE_START_ATTR:\n                    # NOTE: as an optimization only for the start attr, we can try to convert attr to labels unit and see if there is a match; this avoids astyping the entire labels array\n                    pos: TypePos = label_to_pos(attr.astype(labels.dtype)) #type: ignore\n                    if pos is None: # we did not find a start position\n                        labels_astype = labels.astype(attr.dtype) #type: ignore\n                        matches = np.nonzero(labels_astype == attr)[0]\n                        if len(matches):\n                            pos = matches[0]\n                        else:\n                            raise LocEmptyInstance\n                elif field is SLICE_STOP_ATTR:\n                    # NOTE: we do not want to convert attr to labels dtype and take the match as we want to get the last of all possible matches of labels at the attr unit\n                    # NOTE: try to re-use labels_astype if possible\n                    if labels_astype is None or labels_astype.dtype != attr.dtype:\n                        labels_astype = labels.astype(attr.dtype) #type: ignore\n                    matches = np.nonzero(labels_astype == attr)[0]\n                    if len(matches):\n                        pos = matches[-1] + 1\n                    else:\n                        raise LocEmptyInstance\n\n                yield pos\n\n            else:\n                if field is not SLICE_STEP_ATTR:\n                    pos = label_to_pos(attr)\n                    if pos is None:\n                        # NOTE: could raise LocEmpty() to silently handle this\n                        raise LocInvalid('Invalid loc given in a slice', attr, field)\n                else: # step\n                    pos = attr # should be an integer\n                    if not isinstance(pos, INT_TYPES):\n                        raise TypeError(f'Step must be an integer, not {pos}')\n                if field is SLICE_STOP_ATTR:\n                    # loc selections are inclusive, so iloc gets one more\n                    pos += 1\n                yield pos\n\n    @classmethod\n    def loc_to_iloc(cls, *,\n            label_to_pos: FrozenAutoMap,\n            labels: TNDArrayAny,\n            positions: TNDArrayAny,\n            key: TLocSelector,\n            partial_selection: bool = False,\n            ) -> TILocSelector:\n        '''\n        Note: all SF objects (Series, Index) need to be converted to basic types before being passed as `key` to this function.\n\n        Args:\n            partial_selection: if True and key is an iterable of labels that includes labels not in the mapping, available matches will be returned rather than raising.\n        Returns:\n            An integer mapped slice, or GetItemKey type that is based on integers, compatible with TypeBlocks\n        '''\n        # NOTE: ILoc is handled prior to this call, in the Index._loc_to_iloc method\n\n        if key.__class__ is slice:\n            if key == NULL_SLICE:\n                return NULL_SLICE\n            try:\n                return slice(*cls.map_slice_args(\n                        label_to_pos.get,\n                        key, # type: ignore\n                        labels)\n                        )\n            except LocEmpty:\n                return EMPTY_SLICE\n\n        labels_is_dt64 = labels.dtype.kind == DTYPE_DATETIME_KIND\n\n        if key.__class__ is np.datetime64:\n            # if we have a single dt64, convert this to the key's unit and do a Boolean selection if the key is a less-granular unit\n            if (labels.dtype == DTYPE_OBJECT\n                    and np.datetime_data(key.dtype)[0] in DTYPE_OBJECTABLE_DT64_UNITS): #type: ignore\n                key = key.astype(DTYPE_OBJECT) #type: ignore\n            elif labels_is_dt64 and key.dtype < labels.dtype: #type: ignore\n                key = labels.astype(key.dtype) == key #type: ignore\n            # if not different type, keep it the same so as to do a direct, single element selection\n\n        is_array = key.__class__ is np.ndarray\n        is_list = isinstance(key, list)\n\n        # can be an iterable of labels (keys) or an iterable of Booleans\n        if is_array or is_list:\n            if len(key) == 0: # type: ignore\n                return EMPTY_ARRAY_INT\n\n            if is_array and key.dtype.kind == DTYPE_DATETIME_KIND: #type: ignore\n                dt64_unit = np.datetime_data(key.dtype)[0] #type: ignore\n                # NOTE: only in the conditions of an empty array, the unit might be generic\n                if (labels.dtype == DTYPE_OBJECT and dt64_unit in DTYPE_OBJECTABLE_DT64_UNITS):\n                    # if key is dt64 and labels are object, then for objectable units we can convert key to object to permit matching in the AutoMap\n                    # NOTE: tolist() is expected to be faster than astype object for smaller collections\n                    key = key.tolist() #type: ignore\n                    is_array = False\n                    is_list = True\n                elif labels_is_dt64 and key.dtype < labels.dtype: #type: ignore\n                    # NOTE: change the labels to the dt64 dtype, i.e., if the key is years, recast the labels as years, and do a Boolean selection of everything that matches each key\n                    labels_ref = labels.astype(key.dtype) # type: ignore\n                    # NOTE: this is only correct if both key and labels are dt64, and key is a less granular unit, as the order in the key and will not be used\n                    # let Boolean key advance to next branch\n                    key = reduce(OPERATORS['__or__'], (labels_ref == k for k in key)) # type: ignore\n\n            if is_array and key.dtype == DTYPE_BOOL: #type: ignore\n                return positions[key] # type: ignore\n\n            # map labels to integer positions, return a list of integer positions\n            # NOTE: we may miss the opportunity to identify contiguous keys and extract a slice\n            if partial_selection:\n                return label_to_pos.get_any(key) # type: ignore\n            return label_to_pos.get_all(key) # type: ignore\n\n        # if a single element (an integer, string, or date, we just get the integer out of the map\n        return label_to_pos[key] # type: ignore\n\n\nclass HierarchicalLocMap:\n    '''\n    A utility utilized by IndexHierarchy in order to quickly map keys to ilocs.\n    '''\n\n    __slots__ = (\n            'bit_offset_encoders',\n            'encoding_can_overflow',\n            'encoded_indexer_map',\n            )\n\n    bit_offset_encoders: TNDArrayAny\n    encoding_can_overflow: bool\n    encoded_indexer_map: FrozenAutoMap\n\n    def __init__(self: _HLMap,\n            *,\n            indices: tp.List[Index[tp.Any]],\n            indexers: TNDArrayAny,\n            ) -> None:\n\n        if not len(indexers[0]):\n            self.bit_offset_encoders = np.full(len(indices), 0, dtype=DTYPE_UINT_DEFAULT)\n            self.encoding_can_overflow = False\n            self.encoded_indexer_map = EMPTY_FROZEN_AUTOMAP\n            return\n\n        self.bit_offset_encoders, self.encoding_can_overflow = self.build_offsets_and_overflow(\n                num_unique_elements_per_depth=list(map(len, indices))\n                )\n        try:\n            self.encoded_indexer_map = self.build_encoded_indexers_map(\n                    encoding_can_overflow=self.encoding_can_overflow,\n                    bit_offset_encoders=self.bit_offset_encoders,\n                    indexers=indexers,\n                    )\n        except FirstDuplicatePosition as e:\n            duplicate_labels = tuple(\n                    index[indexer[e.first_dup]]\n                    for (index, indexer) in zip(indices, indexers)\n                    )\n            raise ErrorInitIndexNonUnique(duplicate_labels) from None\n\n    def __deepcopy__(self: _HLMap,\n            memo: tp.Dict[int, tp.Any],\n            ) -> _HLMap:\n        '''\n        Return a deep copy of this IndexHierarchy.\n        '''\n        obj: _HLMap = self.__class__.__new__(self.__class__)\n        obj.bit_offset_encoders = array_deepcopy(self.bit_offset_encoders, memo)\n        obj.encoding_can_overflow = self.encoding_can_overflow\n        obj.encoded_indexer_map = deepcopy(self.encoded_indexer_map, memo)\n\n        memo[id(self)] = obj\n        return obj\n\n    def __setstate__(self, state: tp.Tuple[None, tp.Dict[str, tp.Any]]) -> None:\n        '''\n        Ensure that reanimated NP arrays are set not writeable.\n        '''\n        for key, value in state[1].items():\n            setattr(self, key, value)\n        self.bit_offset_encoders.flags.writeable = False\n\n    @property\n    def nbytes(self: _HLMap) -> int:\n        return (\n                sys.getsizeof(self.encoding_can_overflow) +\n                self.bit_offset_encoders.nbytes +\n                sys.getsizeof(self.encoded_indexer_map)\n        )\n\n    @staticmethod", "right_context": "\n    @staticmethod\n    def build_encoded_indexers_map(\n            *,\n            encoding_can_overflow: bool,\n            bit_offset_encoders: TNDArrayAny,\n            indexers: TNDArrayAny,\n            ) -> FrozenAutoMap:\n        '''\n        Builds up a mapping from indexers to iloc positions using their encoded values\n        '''\n        # We previously determined we cannot encode indexers into uint64. Cast to object to rely on Python's bigint\n        if encoding_can_overflow:\n            indexers = indexers.astype(DTYPE_OBJECT).T\n        else:\n            indexers = indexers.astype(DTYPE_UINT_DEFAULT).T\n\n        # Encode indexers into uint64\n        # indexers: (n, m), offsets: (m,)\n        # This bitshifts all indexers by the offset, resulting in numbers that are ready to be bitwise OR'd\n        # We need to reverse in order to have depth 0\n        # Example:\n        #  indexers:      bitshift         (Bit representation)\n        #                                    d0, d1 d0, d2 d1 d0  (d = depth)\n        #    [0, 1, 2] => [0, 4, 32]       ([00, 01 00, 10 00 00])\n        #    [0, 2, 0] => [0, 8,  0]       ([00, 10 00, 00 00 00])\n        #    [2, 2, 0] => [2, 8,  0]       ([10, 10 00, 00 00 00])\n        #    [1, 0, 1] => [1, 0, 16]       ([01, 00 00, 01 00 00])\n        encoded_indexers = indexers << bit_offset_encoders\n\n        # Finally, we bitwise OR all them together to encode them into a single, unique uint64 for each iloc\n        #  encoded_indexers   bitwise OR   (Bit representation)\n        #                                    d0 d1 d2  (d = depth)\n        #    [2, 4,  0]    => [36]         ([10 01 00])\n        #    [0, 8,  0]    => [ 8]         ([00 10 00])\n        #    [0, 8, 32]    => [10]         ([00 10 10])\n        #    [1, 0, 16]    => [17]         ([01 00 01])\n        encoded_indexers = np.bitwise_or.reduce(encoded_indexers, axis=1)\n\n        # Success! We have now successfully encoded each indexer into a single, unique uint64.\n        #    [0, 1, 2] => [36]\n        #    [0, 2, 0] => [ 8]\n        #    [2, 2, 0] => [10]\n        #    [1, 0, 1] => [17]\n\n        # Finally, we create a mapping from encoded indexers to ilocs.\n        #    [0, 1, 2] => [36] => [0]\n        #    [0, 2, 0] => [ 8] => [1]\n        #    [2, 2, 0] => [10] => [2]\n        #    [1, 0, 1] => [17] => [3]\n        # len(encoded_indexers) == len(self)!\n        try:\n            return FrozenAutoMap(encoded_indexers.tolist()) # Automap is faster with Python lists :(\n        except NonUniqueError as e:\n            # nonzero returns arrays of indices per dimension. We are 1D, so we\n            # will receive an array containing one other array. Of that inner\n            # array, we only need the first occurrence\n            first_duplicate = first_true_1d(encoded_indexers == e.args[0], forward=True)\n            raise FirstDuplicatePosition(first_duplicate) from None\n\n    @staticmethod\n    def is_single_element(element: tp.Any) -> bool:\n        # By definition, all index labels are hashable. If it's not, then it\n        # means this must be a container of labels.\n        try:\n            hash(element)\n        except TypeError:\n            return False\n        return True\n\n    def build_key_indexers(self: _HLMap,\n            key: HierarchicalLocMapKey,\n            indices: tp.List[Index[tp.Any]],\n            ) -> TNDArrayAny:\n        key_indexers: tp.List[tp.Sequence[int]] = []\n\n        is_single_key = True\n\n        subkey_indexers: tp.List[int]\n\n        # 1. Perform label resolution\n        for key_at_depth, index_at_depth in zip(key, indices):\n            if self.is_single_element(key_at_depth):\n                key_indexers.append((index_at_depth._loc_to_iloc(key_at_depth),)) # type: ignore\n            else:\n                is_single_key = False\n                subkey_indexers = []\n                for sub_key in key_at_depth:\n                    subkey_indexers.append(index_at_depth._loc_to_iloc(sub_key)) # type: ignore\n                key_indexers.append(subkey_indexers)\n\n        # 2. Convert to numpy array\n        combinations = np.array(list(itertools.product(*key_indexers)), dtype=DTYPE_UINT_DEFAULT)\n        if is_single_key and len(combinations) == 1:\n            [combinations] = combinations\n\n        if self.encoding_can_overflow:\n            return combinations.astype(object)\n\n        return combinations\n\n    def loc_to_iloc(self: _HLMap,\n            key: HierarchicalLocMapKey,\n            indices: tp.List[Index[tp.Any]],\n            ) -> tp.Union[int, tp.List[int]]:\n        key_indexers = self.build_key_indexers(key=key, indices=indices)\n\n        # 2. Encode the indexers. See `build_encoded_indexers_map` for detailed comments.\n        key_indexers <<= self.bit_offset_encoders\n\n        if key_indexers.ndim == 2:\n            key_indexers = np.bitwise_or.reduce(key_indexers, axis=1)\n            return list(map(self.encoded_indexer_map.__getitem__, key_indexers))\n\n        key_indexers = np.bitwise_or.reduce(key_indexers)\n        return self.encoded_indexer_map[key_indexers] # type: ignore\n\n    def indexers_to_iloc(self: _HLMap,\n            indexers: TNDArrayAny,\n            ) -> tp.List[int]:\n        '''\n        Encodes indexers, and then remaps them to ilocs using the encoded_indexer_map\n        '''\n        indexers = self.encode(indexers, self.bit_offset_encoders)\n        return list(map(self.encoded_indexer_map.__getitem__, indexers))\n\n    @staticmethod\n    def encode(indexers: TNDArrayAny, bit_offset_encoders: TNDArrayAny) -> TNDArrayAny:\n        '''\n        Encode indexers into a 1-dim array of uint64\n        '''\n        # Validate input requirements\n        assert indexers.ndim == 2\n        assert indexers.shape[1] == len(bit_offset_encoders)\n        assert indexers.dtype == DTYPE_UINT_DEFAULT\n\n        array: TNDArrayAny = np.bitwise_or.reduce(indexers << bit_offset_encoders, axis=1)\n        return array\n\n    @staticmethod\n    def unpack_encoding(\n            encoded_arr: TNDArrayAny,\n            bit_offset_encoders: TNDArrayAny,\n            encoding_can_overflow: bool,\n            ) -> TNDArrayAny:\n        '''\n        Given an encoding, unpack it into its constituent parts\n\n        Ex:\n            bit_offset_encoders = [0, 2, 4]\n\n            Encodings:\n                36  => [0, 4, 32] => [0, 1, 2]\n                 8  => [0, 8,  0] => [0, 2, 0]\n                10  => [2, 8,  0] => [2, 2, 0]\n                17  => [1, 0, 16] => [1, 0, 1]\n\n            Step 1:\n            Expand bit_offset_encoders into something more helpful -> masks\n\n            [0, 2, 4] is where the bit offsets start. They end one bit before the next offset.\n            Thus, the bit offset ends are:\n            [1, 3, 64] # since 64 is the max bit offset\n\n            From here, we build up a list of masks that each have the correct number of up bits\n\n            0 => [11] # 2 bit mask\n            2 => [11] # 2 bit mask\n            4 => [11] # 2 bit mask\n\n            Now, for each component (i.e. the number of bit_offset_encoders), we\n            apply to corresponding mask to the values AFTER they have been shifted backwards\n\n            36 == [10 01 00]\n             8 == [00 10 00]\n            10 == [00 10 10]\n            17 == [01 00 01]\n\n            Depth 0:\n                offset = bit_offset_encoders[0] = 0\n                36 => ([10 01 00] << 0) => [10 01 00] & [11] => [00 00 00] => 0\n                 8 => ([00 10 00] << 0) => [00 10 00] & [11] => [00 00 00] => 0\n                10 => ([00 10 10] << 0) => [00 10 10] & [11] => [00 00 10] => 2\n                17 => ([01 00 01] << 0) => [01 00 01] & [11] => [00 00 01] => 1\n\n            Depth 1:\n                offset = bit_offset_encoders[1] = 2\n                36 => ([10 01 00] << 2) => [10 01] & [11] => [00 01] => 1\n                 8 => ([00 10 00] << 2) => [00 10] & [11] => [00 10] => 2\n                10 => ([00 10 10] << 2) => [00 10] & [11] => [00 10] => 2\n                17 => ([01 00 01] << 2) => [01 00] & [11] => [00 00] => 0\n\n            Depth 2:\n                offset = bit_offset_encoders[2] = 4\n                36 => ([10 01 00] << 4) => [10] & [11] => [10] => 2\n                 8 => ([00 10 00] << 4) => [00] & [11] => [00] => 0\n                10 => ([00 10 10] << 4) => [00] & [11] => [00] => 0\n                17 => ([01 00 01] << 4) => [01] & [11] => [01] => 1\n\n            Result:\n                36 => [0, 1, 2]\n                 8 => [0, 2, 0]\n                10 => [2, 2, 0]\n                17 => [1, 0, 1]\n\n            NOTE: This is the inverse of the documentation in `build_encoded_indexers_map`\n        '''\n        assert bit_offset_encoders.dtype == DTYPE_UINT_DEFAULT\n        assert bit_offset_encoders[0] == 0 # By definition, the first offset starts at 0!\n        assert encoded_arr.ndim == 1 # Encodings are always 1D\n\n        dtype = DTYPE_OBJECT if encoding_can_overflow else DTYPE_UINT_DEFAULT\n\n        starts = bit_offset_encoders\n        stops: TNDArrayAny = np.empty(len(starts), dtype=dtype)\n        stops[:-1] = starts[1:]\n        stops[-1] = 64\n\n        lens = stops - starts\n        masks = [x for x in (1 << lens) - 1]\n\n        target = np.empty((len(bit_offset_encoders), len(encoded_arr)), dtype=DTYPE_UINT_DEFAULT)\n\n        for depth in range(len(bit_offset_encoders)):\n            target[depth] = (encoded_arr >> starts[depth]) & masks[depth]\n\n        target.flags.writeable = False\n        return target\n", "import_text": ["itertools", "sys", "copy.deepcopy", "functools.reduce", "numpy", "typing_extensions", "arraykit.array_deepcopy", "arraykit.first_true_1d", "arraymap.FrozenAutoMap", "arraymap.NonUniqueError", "static_frame.core.exception.ErrorInitIndexNonUnique", "static_frame.core.exception.LocEmpty", "static_frame.core.exception.LocInvalid", "static_frame.core.util.DTYPE_BOOL", "static_frame.core.util.DTYPE_DATETIME_KIND", "static_frame.core.util.DTYPE_OBJECT", "static_frame.core.util.DTYPE_OBJECTABLE_DT64_UNITS", "static_frame.core.util.DTYPE_UINT_DEFAULT", "static_frame.core.util.EMPTY_ARRAY_INT", "static_frame.core.util.EMPTY_FROZEN_AUTOMAP", "static_frame.core.util.EMPTY_SLICE", "static_frame.core.util.INT_TYPES", "static_frame.core.util.NULL_SLICE", "static_frame.core.util.OPERATORS", "static_frame.core.util.SLICE_ATTRS", "static_frame.core.util.SLICE_START_ATTR", "static_frame.core.util.SLICE_STEP_ATTR", "static_frame.core.util.SLICE_STOP_ATTR", "static_frame.core.util.TILocSelector", "static_frame.core.util.TLabel", "static_frame.core.util.TLocSelector"], "prompt": "\"\"\"\nDescription: This function builds offsets and checks for overflow in a hierarchical structure.\n\nArgs:\n    num_unique_elements_per_depth (List[int]): A list of integers representing the number of unique elements per depth in the hierarchical structure.\n\nReturns:\n    Tuple[TNDArrayAny, bool]: A tuple containing a numpy array of bit start positions and a boolean indicating whether there is an overflow.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        '''\n        Derive the offsets and the overflow flag from the number of unique values per depth\n        '''", "function_dependencies": ["numpy.floor", "numpy.log2", "numpy.cumsum", "numpy.zeros"], "project_create_time": "2018-01-03T15:07:52+00:00", "project_update_time": "2024-04-17T17:48:14+00:00", "file_create_time": "2021-12-17T18:25:27Z", "file_update_time": "2024-03-20T17:56:08Z", "function_update_time": "2021-12-17T18:25:27Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["numpy.floor"], "test_function": [{"file_path": "/static-frame-v2.6.0/static-frame-2.6.0/static_frame/test/unit/test_loc_map.py", "class_name": "TestHierarchicalLocMapUnit", "function_name": "test_build_offsets_and_overflow_a", "code": "\n    def test_build_offsets_and_overflow_a(self) -> None:\n        def check(sizes: tp.List[int], offsets: tp.List[int], overflow: bool) -> None:\n            actual_offset, actual_overflow = HierarchicalLocMap.build_offsets_and_overflow(sizes)\n            self.assertListEqual(actual_offset.tolist(), offsets)\n            self.assertEqual(actual_overflow, overflow)\n\n        check([17, 99], [0, 5], False)\n        check([1, 1], [0, 1], False)\n        check([1, 2, 4, 8, 16, 32], [0, 1, 3, 6, 10, 15], False)\n        check([2**30, 2, 3, 4], [0, 31, 33, 35], False)\n        check([2**40, 2**18, 15], [0, 41, 60], False)\n        check([2**40, 2**18, 16], [0, 41, 60], True)"}, {"file_path": "/static-frame-v2.6.0/static-frame-2.6.0/static_frame/test/unit/test_loc_map.py", "class_name": "TestHierarchicalLocMapUnit", "function_name": "test_build_encoded_indexers_map_a", "code": "\n    def test_build_encoded_indexers_map_a(self) -> None:\n        sizes = [188, 5, 77]\n        indexers = build_indexers_from_product(sizes)\n\n        bit_offsets, overflow = HierarchicalLocMap.build_offsets_and_overflow(sizes)\n\n        self.assertListEqual(bit_offsets.tolist(), [0, 8, 11])\n        self.assertFalse(overflow)\n\n        result = HierarchicalLocMap.build_encoded_indexers_map(\n                encoding_can_overflow=overflow,\n                bit_offset_encoders=bit_offsets,\n                indexers=indexers,\n                )\n        self.assertEqual(len(result), len(indexers[0]))\n\n        self.assertEqual(min(result), 0)\n        self.assertEqual(max(result), 156859)\n\n        # Manually check every element to ensure it encodes to the same value\n        for i, row in enumerate(np.array(indexers).T):\n            encoded = np.bitwise_or.reduce(row.astype(np.uint64) << bit_offsets)\n            self.assertEqual(i, result[encoded])"}, {"file_path": "/static-frame-v2.6.0/static-frame-2.6.0/static_frame/test/unit/test_loc_map.py", "class_name": "TestHierarchicalLocMapUnit", "function_name": "test_build_encoded_indexers_map_b", "code": "\n    def test_build_encoded_indexers_map_b(self) -> None:\n        size = 2**20\n        sizes = [size for _ in range(4)]\n\n        arr = PositionsAllocator.get(size)\n        indexers = np.array([arr for _ in range(4)])\n\n        bit_offsets, overflow = HierarchicalLocMap.build_offsets_and_overflow(sizes)\n\n        self.assertListEqual(bit_offsets.tolist(), [0, 21, 42, 63])\n        self.assertTrue(overflow)\n\n        result = HierarchicalLocMap.build_encoded_indexers_map(\n                encoding_can_overflow=overflow,\n                bit_offset_encoders=bit_offsets,\n                indexers=indexers,\n                )\n        self.assertEqual(len(result), len(indexers[0]))\n\n        self.assertEqual(min(result), 0)\n        self.assertEqual(max(result), 9671401945228815945957375)\n\n        # Manually encode the last row to ensure it matches!\n        indexer = np.array([size - 1 for _ in range(4)], dtype=object)\n        encoded = np.bitwise_or.reduce(indexer << bit_offsets)\n        self.assertEqual(max(result), encoded)"}, {"file_path": "/static-frame-v2.6.0/static-frame-2.6.0/static_frame/test/unit/test_loc_map.py", "class_name": "TestHierarchicalLocMapUnit", "function_name": "test_encoding_roundtrip", "code": "\n    def test_encoding_roundtrip(self) -> None:\n        indexers = np.array(\n            [\n                [0, 2, 1, 1, 4, 5, 6, 7, 8, 9, 0],\n                [1, 0, 2, 0, 5, 4, 5, 8, 9, 0, 7],\n                [0, 9, 4, 2, 1, 3, 1, 4, 5, 6, 7],\n            ],\n            dtype=np.uint64,\n        )\n        indexers.flags.writeable = False\n\n        bit_offset_encoders, can_overflow = HierarchicalLocMap.build_offsets_and_overflow([10, 10, 10])\n        assert not can_overflow\n\n        encodings = HierarchicalLocMap.build_encoded_indexers_map(\n                encoding_can_overflow=can_overflow,\n                bit_offset_encoders=bit_offset_encoders,\n                indexers=indexers,\n                )\n        encoded_arr = np.array(list(encodings), dtype=np.uint64)\n\n        unpacked_indexers = HierarchicalLocMap.unpack_encoding(\n                encoded_arr=encoded_arr,\n                bit_offset_encoders=bit_offset_encoders,\n                encoding_can_overflow=can_overflow,\n                )\n\n        assert unpacked_indexers is not indexers\n        assert id(unpacked_indexers) != id(indexers)\n        assert (unpacked_indexers == indexers).all().all()"}]}, {"git_group": "peeringdb", "git_name": "peeringdb", "version": "2.57.0", "language": "Python", "project_name": "peeringdb-2.57.0.zip", "file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/peeringdb_server/ixf.py", "file_name": "ixf.py", "focal_class": "Importer", "focal_name": "notify_proposals", "focal_parameter": [], "solution": "    def notify_proposals(self, error_handler=None):\n\n        if not self.save:\n            return\n\n        # consolidate proposals into net,ix and ix,net\n        # groupings\n\n        try:\n            consolidated = self.consolidate_proposals()\n        except Exception as exc:\n            if error_handler:\n                error_handler(exc, ixf_member_data=self.current_proposal)\n                return\n            else:\n                raise\n\n        ticket_days = settings.IXF_IMPORTER_DAYS_UNTIL_TICKET\n\n        template = loader.get_template(\"email/notify-ixf-consolidated.txt\")\n\n        for recipient in [\"ix\", \"net\"]:\n            for other_entity, data in consolidated[recipient].items():\n                try:\n                    self._notify_proposal(recipient, data, ticket_days, template)\n                except Exception as exc:\n                    if error_handler:\n                        error_handler(exc)\n                    else:\n                        raise\n\n        try:\n            self.ticket_consolidated_proposals(consolidated[\"ac\"])\n        except Exception as exc:\n            if error_handler:\n                error_handler(exc)\n            else:\n                raise", "function_signature": "def notify_proposals(self, error_handler=None) :", "left_context": "\"\"\"\nIX-F importer implementation.\n\nHandles import of IX-F feeds, creation of suggestions for networks and exchanges\nto follow.\n\nHandles notifications of networks and exchanges as part of that process.\n\nA substantial part of the import logic is handled through models.py::IXFMemberData\n\"\"\"\n\nimport datetime\nimport ipaddress\nimport json\nfrom smtplib import SMTPException\n\nimport django\nimport requests\nimport reversion\nfrom django.conf import settings\nfrom django.core.cache import cache\nfrom django.core.exceptions import ValidationError\nfrom django.core.mail.message import EmailMultiAlternatives\nfrom django.db import transaction\nfrom django.db.models import Q\nfrom django.template import loader\nfrom django.utils.html import strip_tags\nfrom django.utils.translation import gettext_lazy as _\n\nimport peeringdb_server.deskpro as deskpro\nfrom peeringdb_server.models import (\n    DataChangeNotificationQueue,\n    DeskProTicket,\n    IXFImportEmail,\n    IXFMemberData,\n    IXLanIXFMemberImportAttempt,\n    IXLanIXFMemberImportLog,\n    IXLanIXFMemberImportLogEntry,\n    Network,\n    NetworkIXLan,\n    NetworkProtocolsDisabled,\n    User,\n    ValidationErrorEncoder,\n)\n\nREASON_ENTRY_GONE_FROM_REMOTE = _(\n    \"The entry for (asn and IPv4 and IPv6) does not exist \"\n    \"in the exchange's IX-F data as a singular member connection\"\n)\n\nREASON_NEW_ENTRY = _(\n    \"The entry for (asn and IPv4 and IPv6) does not exist \"\n    \"in PeeringDB as a singular network -> ix connection\"\n)\n\nREASON_VALUES_CHANGED = _(\n    \"Data differences between PeeringDB and the exchange's IX-F data\"\n)\n\n\nclass MultipleVlansInPrefix(ValueError):\n\n    \"\"\"\n    This error is raised when an IX-F export contains\n    multiple vlan ids for the prefixes defined in the processed ixlan.\n\n    Since peeringdb treats each vlan as it's own exchange this currently\n    is not a compatible setup for import (see #889).\n    \"\"\"\n\n    def __init__(self, importer, *args, **kwargs):\n        importer.ixlan.ixf_ixp_member_list_url\n        importer.ixlan.ix.name\n        support_email = settings.DEFAULT_FROM_EMAIL\n        super().__init__(\n            _(\n                f\"We found that your IX-F output \"\n                f\"contained multiple VLANs for the prefixes defined in the PeeringDB entry for your exchange.\"\n                \"\\n\"\n                f\"This setup is not compatible as PeeringDB regards each VLAN as its own exchange.\"\n                \"\\n\"\n                f\"Please contact {support_email} if you need assistance with resolving this issue.\"\n            )\n        )\n\n\nclass Importer:\n    allowed_states = [\n        \"\",\n        None,\n        \"active\",\n        \"inactive\",\n        \"connected\",\n        \"operational\",\n    ]\n\n    @property\n    def ticket_user(self):\n        \"\"\"\n        Return the User instance for the user\n        to create DeskPRO tickets.\n        \"\"\"\n        if not hasattr(self, \"_ticket_user\"):\n            self._ticket_user = User.objects.get(username=\"ixf_importer\")\n        return self._ticket_user\n\n    @property\n    def deskpro_client(self):\n        if not hasattr(self, \"_deskpro_client\"):\n            if settings.IXF_SEND_TICKETS:\n                cls = deskpro.APIClient\n            else:\n                cls = deskpro.MockAPIClient\n\n            self._deskpro_client = cls(settings.DESKPRO_URL, settings.DESKPRO_KEY)\n        return self._deskpro_client\n\n    @property\n    def tickets_enabled(self):\n        \"\"\"\n        Return whether deskpr ticket creation for IX-F\n        conflicts are enabled or not.\n\n        This can be controlled by the IXF_TICKET_ON_CONFLICT\n        setting.\n        \"\"\"\n\n        return getattr(settings, \"IXF_TICKET_ON_CONFLICT\", True)\n\n    @property\n    def notify_ix_enabled(self):\n        \"\"\"\n        Return whether notifications to the exchange\n        are enabled or not.\n\n        This can be controlled by the IXF_NOTIFY_IX_ON_CONFLICT\n        setting.\n        \"\"\"\n\n        return getattr(settings, \"IXF_NOTIFY_IX_ON_CONFLICT\", False)\n\n    @property\n    def notify_net_enabled(self):\n        \"\"\"\n        Return whether notifications to the network\n        are enabled or not.\n\n        This can be controlled by the IXF_NOTIFY_NET_ON_CONFLICT\n        setting.\n        \"\"\"\n\n        return getattr(settings, \"IXF_NOTIFY_NET_ON_CONFLICT\", False)\n\n    def __init__(self):\n        self.cache_only = False\n        self.skip_import = False\n        self.reset()\n\n    def reset(self, ixlan=None, save=False, asn=None):\n        self.reset_log()\n        self.ixf_ids = []\n        self.actions_taken = {\n            \"add\": [],\n            \"delete\": [],\n            \"modify\": [],\n            \"noop\": [],\n        }\n        self.pending_save = []\n        self.deletions = {}\n        self.asns = []\n        self.ixlan = ixlan\n        self.vlan = None\n        self.save = save\n        self.asn = asn\n        self.now = datetime.datetime.now(datetime.timezone.utc)\n        self.invalid_ip_errors = []\n        self.notifications = []\n        self.protocol_conflict = 0\n        self.emails = 0\n\n    def fetch(self, url, timeout=5):\n        \"\"\"\n        Retrieve ixf member export data from the url.\n\n        Will do a quick sanity check on the data.\n\n        Return dict containing the parsed data.\n\n        Arguments:\n            - url <str>\n\n        Keyword arguments:\n            - timeout <float>: max time to spend on request\n        \"\"\"\n\n        if not url:\n            return {\"pdb_error\": _(\"IX-F import url not specified\")}\n\n        try:\n            result = requests.get(url, timeout=timeout)\n        except Exception as exc:\n            return {\"pdb_error\": exc}\n\n        if result.status_code != 200:\n            return {\"pdb_error\": f\"Got HTTP status {result.status_code}\"}\n\n        try:\n            data = result.json()\n        except Exception:\n            data = {\"pdb_error\": _(\"No JSON could be parsed\")}\n            return data\n\n        data = self.sanitize(data)\n\n        # locally cache result\n\n        if data and not data.get(\"pdb_error\"):\n            cache.set(self.cache_key(url), data, timeout=None)\n\n        return data\n\n    def cache_key(self, url):\n        \"\"\"\n        Return the django cache key to use for caching IX-F data.\n\n        Argument:\n\n            url <str>\n        \"\"\"\n\n        return f\"IXF-CACHE-{url}\"\n\n    def fetch_cached(self, url):\n        \"\"\"\n        Return locally cached IX-F data.\n\n        Arguments:\n\n            url <str>\n        \"\"\"\n\n        if not url:\n            return {\"pdb_error\": _(\"IX-F import url not specified\")}\n\n        data = cache.get(self.cache_key(url))\n\n        if data is None:\n            return {\n                \"pdb_error\": _(\"IX-F data not locally cached for this resource yet.\")\n            }\n\n        return data\n\n    def find_vlan_needing_pair(self, connection):\n        vlans_needing_pair = []\n        vlan_list = connection.get(\"vlan_list\", [])\n\n        for vlan in vlan_list:\n            if vlan.get(\"ipv4\") and not vlan.get(\"ipv6\"):\n                vlans_needing_pair.append(vlan)\n            elif vlan.get(\"ipv6\") and not vlan.get(\"ipv4\"):\n                vlans_needing_pair.append(vlan)\n\n        if len(vlans_needing_pair) == 0:\n            return None\n\n        return vlans_needing_pair\n\n    def get_if_speed_list(self, connection):\n        if_speed_list = []\n        for if_entry in connection.get(\"if_list\", []):\n            if if_entry.get(\"if_speed\"):\n                if_speed_list.append(if_entry.get(\"if_speed\"))\n\n        if_speed_list.sort()\n        return if_speed_list\n\n    def connections_match(self, connection1, connection2):\n        # Check that both connections have a 'state' set\n        state_match = connection1.get(\"state\", \"undefined\") == connection2.get(\n            \"state\", \"undefined\"\n        )\n        if_list_1 = self.get_if_speed_list(connection1)\n        if_list_2 = self.get_if_speed_list(connection2)\n        if_list_match = if_list_1 == if_list_2\n        return state_match and if_list_match\n\n    def find_connections_that_match(self, connection, connection_list):\n        cxns_that_match = []\n        for connection2 in connection_list:\n            if self.connections_match(connection, connection2):\n                cxns_that_match.append(connection2)\n\n        if len(cxns_that_match) == 0:\n            return None\n\n        return cxns_that_match\n\n    def match_vlans_across_connections(self, connection_list):\n        modified_connection_list = []\n\n        for i, connection in enumerate(connection_list):\n            \"\"\"\n            If there aren't any vlans in the connection at\n            all, skip it. (This might happen in the course\n            of matching vlans).\n            \"\"\"\n            if len(connection.get(\"vlan_list\", [])) == 0:\n                continue\n\n            remaining_connections = connection_list[i + 1 :]\n            vlans_needing_pair = self.find_vlan_needing_pair(connection)\n            # If there aren't any vlans that need to be paired,\n            # we're done looking at this connection\n            if vlans_needing_pair is None:\n                modified_connection_list.append(connection)\n                continue\n\n            cxns_that_match = self.find_connections_that_match(\n                connection, remaining_connections\n            )\n            # If there aren't any connections we could match up,\n            # we're done looking at this connection\n            if cxns_that_match is None:\n                modified_connection_list.append(connection)\n                continue\n\n            # If we have vlans we want to match, and\n            # at least one connection we can look for\n            # start looking\n            for lone_vlan in vlans_needing_pair:\n                matching_vlan = self.find_matching_vlan(lone_vlan, cxns_that_match)\n                # if we found one we need to transfer it\n                if matching_vlan is not None:\n                    # matching vlan goes into connection vlan_list\n                    connection[\"vlan_list\"].append(matching_vlan)\n\n            modified_connection_list.append(connection)\n        return modified_connection_list\n\n    def find_matching_vlan(self, lone_vlan, connections_that_match):\n        for connection in connections_that_match:\n            for j, potential_vlan in enumerate(connection[\"vlan_list\"]):\n                if self.vlan_matches(lone_vlan, potential_vlan):\n                    # Matching vlan gets deleted from other connection\n                    return connection[\"vlan_list\"].pop(j)\n\n        return None\n\n    def vlan_matches(self, vlan1, vlan2):\n        vlan_ids_match = vlan1.get(\"vlan_id\", 0) == vlan2.get(\"vlan_id\", 0)\n\n        if vlan_ids_match is False:\n            return False\n        if vlan1.get(\"ipv4\") and vlan2.get(\"ipv4\"):\n            return False\n        if vlan1.get(\"ipv6\") and vlan2.get(\"ipv6\"):\n            return False\n\n        return True\n\n    def sanitize_vlans(self, vlans):\n        \"\"\"\n        Sanitize vlan lists where ip 4 and 6 addresses\n        for the same vlan (determined by vlan id) exist\n        in separate entries by combining those\n        list entries to one.\n        \"\"\"\n\n        _vlans = {}\n        sanitized = []\n\n        for vlan in vlans:\n            # if the vlan_id is not specified we want\n            # to default to 0 so we can still group based\n            # on that\n\n            id = vlan.get(\"vlan_id\", 0)\n\n            # the IX-F schema allows setting ipv4 and ipv6 to\n            # null, in which case remove the property\n\n            if \"ipv4\" in vlan and not vlan.get(\"ipv4\"):\n                del vlan[\"ipv4\"]\n\n            if \"ipv6\" in vlan and not vlan.get(\"ipv6\"):\n                del vlan[\"ipv6\"]\n\n            # neither ipv4 nor ipv6 is specified, there is\n            # nothing to sanitize here, so skip\n\n            if \"ipv4\" not in vlan and \"ipv6\" not in vlan:\n                continue\n\n            if id not in _vlans:\n                # first occurance of vlan id gets appended\n                # as is\n\n                _vlans[id] = [vlan]\n            else:\n                # additional occurances of vlan id get checked\n                # on whether or not they will fill in a missing\n                # ipv4 or ipv6 address, and if so will update\n                # the existing vlan entry.\n                #\n                # otherwise append as a new entry for that vlan id\n\n                current = _vlans[id][-1]\n\n                update = None\n\n                if \"ipv4\" in vlan and \"ipv4\" not in current:\n                    update = \"ipv4\"\n                elif \"ipv6\" in vlan and \"ipv6\" not in current:\n                    update = \"ipv6\"\n\n                if update:\n                    current[update] = vlan[update]\n                else:\n                    _vlans[id].append(vlan)\n\n        for vlan_id, entries in _vlans.items():\n            sanitized.extend(entries)\n\n        return sanitized\n\n    def sanitize(self, data):\n        \"\"\"\n        Take ixf data dict and run sanitization on it.\n        \"\"\"\n\n        invalid = None\n        ipv4_addresses = {}\n        ipv6_addresses = {}\n\n        # dedupe identical entries in member list\n        member_list = [json.dumps(m) for m in data.get(\"member_list\", [])]\n        member_list = [json.loads(m) for m in set(member_list)]\n\n        # This fixes instances where ixps provide two separate entries for\n        # vlans in vlan_list for ipv4 and ipv6 (AMS-IX for example)\n        for member in member_list:\n            # handle `null` properties inside connection list\n\n            for conn in member.get(\"connection_list\", []):\n                # handle case where IX-F feed has set `vlan_list` to `null` (#1244)\n                # treat same as if it wasn't set at all\n\n                if conn.get(\"vlan_list\", []) is None:\n                    conn.pop(\"vlan_list\")\n\n                # handle case where IX-F feed has set `if_list` to `null` (#1244)\n                # treat same as if it wasn't set at all\n\n                if conn.get(\"if_list\", []) is None:\n                    conn.pop(\"if_list\")\n\n            asn = member.get(\"asnum\")\n\n            connection_list = self.match_vlans_across_connections(\n                member.get(\"connection_list\", [])\n            )\n\n            for conn in connection_list:\n                conn[\"vlan_list\"] = self.sanitize_vlans(conn.get(\"vlan_list\", []))\n                vlans = conn[\"vlan_list\"]\n\n                if not vlans:\n                    continue\n\n                # de-dupe reoccurring ipv4 / ipv6 addresses\n\n                ipv4 = vlans[0].get(\"ipv4\", {}).get(\"address\")\n                ipv6 = vlans[0].get(\"ipv6\", {}).get(\"address\")\n\n                ixf_id = (asn, ipv4, ipv6)\n\n                if ipv4 and ipv4 in ipv4_addresses:\n                    invalid = _(\n                        \"Address {} assigned to more than one distinct connection\"\n                    ).format(ipv4)\n                    break\n\n                ipv4_addresses[ipv4] = ixf_id\n\n                if ipv6 and ipv6 in ipv6_addresses:\n                    invalid = _(\n                        \"Address {} assigned to more than one distinct connection\"\n                    ).format(ipv6)\n                    break\n\n                ipv6_addresses[ipv6] = ixf_id\n        data[\"pdb_error\"] = invalid\n\n        # set member_list to the sanitized copy\n        data[\"member_list\"] = member_list\n\n        return data\n\n    def update(self, ixlan, save=True, data=None, timeout=5, asn=None):\n        \"\"\"\n        Sync netixlans under this ixlan from ixf member export json data (specs\n        can be found at https://github.com/euro-ix/json-schemas).\n\n        Arguments:\n            - ixlan (IXLan): ixlan object to update from ixf\n\n        Keyword Arguments:\n            - save (bool): commit changes to db\n            - asn (int): only process changes for this ASN\n\n        Returns:\n            - Tuple(success<bool>, netixlans<list>, log<list>)\n        \"\"\"\n\n        self.reset(ixlan=ixlan, save=save, asn=asn)\n\n        # if data is not provided, retrieve it either from cache or\n        # from the remote resource\n        if data is None:\n            if self.cache_only:\n                data = self.fetch_cached(ixlan.ixf_ixp_member_list_url)\n            else:\n                data = self.fetch(ixlan.ixf_ixp_member_list_url, timeout=timeout)\n\n        # bail if there has been any errors during sanitize() or fetch()\n        if data.get(\"pdb_error\"):\n            self.notify_error(data.get(\"pdb_error\"))\n            self.log_error(data.get(\"pdb_error\"), save=save)\n            return False\n\n        # bail if there are no active prefixes on the ixlan\n        if ixlan.ixpfx_set_active.count() == 0:\n            self.log_error(_(\"No prefixes defined on ixlan\"), save=save)\n            return False\n\n        if self.skip_import:\n            return True\n\n        try:\n            # parse the ixf data\n            self.parse(data)\n        except MultipleVlansInPrefix as exc:\n            # multiple vlans found for prefixes specified on the ixlan\n            # we fail the import and notify the ix\n            #\n            # since the import hard fails here we want to remove all\n            # other queued notifications\n            #\n            # transactions are atomic and will be rolled back\n            self.notify_error(f\"{exc}\")\n            self.log_error(f\"{exc}\", save=save)\n            self.notifications = []\n            return False\n        except KeyError as exc:\n            # any key erros mean that the data is invalid, log the error and\n            # bail (transactions are atomic and will be rolled back)\n            self.log_error(f\"Internal Error 'KeyError': {exc}\", save=save)\n            return False\n\n        # null IX-F error note on ixlan if it had error'd before\n        if self.ixlan.ixf_ixp_import_error:\n            with transaction.atomic():\n                with reversion.create_revision():\n                    reversion.set_user(self.ticket_user)\n                    self.ixlan.ixf_ixp_import_error = None\n                    self.ixlan.ixf_ixp_import_error_notified = None\n                    self.ixlan.save()\n\n        with transaction.atomic():\n            # process any netixlans that need to be deleted\n            self.process_deletions()\n\n            # process creation of new netixlans and updates\n            # of existing netixlans. This needs to happen\n            # after process_deletions in order to avoid potential\n            # ip conflicts\n            self.process_saves()\n\n        self.cleanup_ixf_member_data()\n\n        self.notify_stale_netixlans()\n\n        self.cleanup_aged_proposals()\n\n        # create tickets for unresolved proposals\n        # This function is currently disabled as per issue #860\n        self.ticket_aged_proposals()\n\n        # archive the import so we can roll it back later if needed\n        self.archive()\n\n        if self.invalid_ip_errors:\n            self.notify_error(\"\\n\".join(self.invalid_ip_errors))\n\n        if save:\n            # update exchange's ixf fields\n            self.update_ix()\n\n            if (\n                not self.protocol_conflict\n                and self.ixlan.ixf_ixp_import_protocol_conflict\n            ):\n                self.ixlan.ixf_ixp_import_protocol_conflict = 0\n                self.ixlan.save()\n\n            self.save_log()\n\n        return True\n\n    def update_ix(self):\n        \"\"\"\n        Determine if any data was changed during this import\n        and update the exchange's ixf_last_import timestamp\n        if so.\n\n        Set the ixf_net_count value if it has changed\n        from before.\n        \"\"\"\n\n        ix = self.ixlan.ix\n\n        # ixf_member_data_changed = IXFMemberData.objects.filter(\n        #    updated__gte=self.now, ixlan=self.ixlan\n        # ).exists()\n\n        # netixlan_data_changed = NetworkIXLan.objects.filter(\n        #    updated__gte=self.now, ixlan=self.ixlan\n        # ).exists()\n\n        ix.ixf_last_import = self.now\n\n        ixf_net_count = len(self.pending_save)\n        if ixf_net_count != ix.ixf_net_count:\n            ix.ixf_net_count = ixf_net_count\n\n        # we do not want these updates to affect the\n        # exchanges `updated` timestamp as per #812\n        # so we temporarily disable auto_now\n\n        ix._meta.get_field(\"updated\").auto_now = False\n        try:\n            with transaction.atomic():\n                with reversion.create_revision():\n                    reversion.set_user(self.ticket_user)\n                    ix.save()\n        finally:\n            # always turn auto_now back on afterwards\n\n            ix._meta.get_field(\"updated\").auto_now = True\n\n    def fix_consolidated_modify(self, ixf_member_data):\n        \"\"\"\n        Fix consolidated modify (#770) to retain value\n        for speed and is_rs_peer (#793).\n        \"\"\"\n\n        for other in self.pending_save:\n            if other.asn == ixf_member_data.asn:\n                if (\n                    other.init_ipaddr4\n                    and other.init_ipaddr4 == ixf_member_data.init_ipaddr4\n                ) or (\n                    other.init_ipaddr6\n                    and other.init_ipaddr6 == ixf_member_data.init_ipaddr6\n                ):\n                    if not other.modify_speed:\n                        other.speed = ixf_member_data.speed\n\n                    if not other.modify_is_rs_peer:\n                        other.is_rs_peer = ixf_member_data.is_rs_peer\n\n                    break\n\n    @reversion.create_revision()\n    @transaction.atomic()\n    def process_saves(self):\n        reversion.set_user(self.ticket_user)\n\n        for ixf_member in self.pending_save:\n            self.apply_add_or_update(ixf_member)\n\n    @reversion.create_revision()\n    @transaction.atomic()\n    def process_deletions(self):\n        \"\"\"\n           Cycle all netixlans on the ixlan targeted by the importer and\n        remove any that are no longer found in the ixf data by\n           their ip addresses.\n\n           In order for a netixlan to be removed, both its ipv4 and ipv6 address\n           or its asn need to be gone from the ixf data after validation.\n        \"\"\"\n\n        reversion.set_user(self.ticket_user)\n\n        netixlan_qset = self.ixlan.netixlan_set_active\n\n        # if we are only processing a specific asn ignore\n        # all that don't match\n\n        if self.asn:\n            netixlan_qset = netixlan_qset.filter(asn=self.asn)\n\n        for netixlan in netixlan_qset:\n            if netixlan.ixf_id not in self.ixf_ids:\n                ixf_member_data = IXFMemberData.instantiate(\n                    netixlan.asn,\n                    netixlan.ipaddr4,\n                    netixlan.ipaddr6,\n                    netixlan.ixlan,\n                    speed=netixlan.speed,\n                    operational=netixlan.operational,\n                    is_rs_peer=netixlan.is_rs_peer,\n                    delete=True,\n                    data={},\n                )\n\n                # fix consolidated modify (#770) to retain values\n                # for speed and is_rs_peer (#793)\n                self.fix_consolidated_modify(ixf_member_data)\n\n                self.deletions[ixf_member_data.ixf_id] = ixf_member_data\n                if netixlan.network.allow_ixp_update:\n                    self.log_apply(\n                        ixf_member_data.apply(save=self.save),\n                        reason=REASON_ENTRY_GONE_FROM_REMOTE,\n                    )\n                else:\n                    notify = ixf_member_data.set_remove(\n                        save=self.save, reason=REASON_ENTRY_GONE_FROM_REMOTE\n                    )\n                    if notify:\n                        self.queue_notification(ixf_member_data, \"remove\")\n                    self.log_ixf_member_data(ixf_member_data)\n\n    def cleanup_ixf_member_data(self):\n        if not self.save:\n            \"\"\"\n            Do not run a cleanup process in some cases.\n            For example, when the importer runs in preview mode\n            triggered by a network admin.\n            \"\"\"\n\n            return\n\n        qset = IXFMemberData.objects.filter(ixlan=self.ixlan)\n\n        if self.asn:\n            # if we are only processing for a specified asn\n            # we only clean up member data for that asn\n\n            qset = qset.filter(asn=self.asn)\n\n        # clean up old IX-F memeber data objects\n\n        for ixf_member in qset:\n            # proposed deletion got fulfilled\n\n            if ixf_member.action == \"delete\":\n                if ixf_member.netixlan.status == \"deleted\":\n                    if ixf_member.set_resolved(save=self.save):\n                        self.queue_notification(ixf_member, \"resolved\")\n\n            # noop means the ask has been fulfilled but the\n            # ixf member data entry has not been set to resolved yet\n\n            elif ixf_member.action == \"noop\":\n                if (\n                    ixf_member.set_resolved(save=self.save)\n                    and not ixf_member.requirement_of_id\n                ):\n                    self.queue_notification(ixf_member, \"resolved\")\n\n            # proposed change / addition is now gone from\n            # IX-F data\n\n            elif not self.skip_import and ixf_member.ixf_id not in self.ixf_ids:\n                if ixf_member.action in [\"add\", \"modify\"]:\n                    if ixf_member.set_resolved(save=self.save):\n                        self.queue_notification(ixf_member, \"resolved\")\n\n    @transaction.atomic()\n    def archive(self):\n        \"\"\"\n        Create the IXLanIXFMemberImportLog for this import.\n        \"\"\"\n\n        if not self.save:\n            return\n\n        persist_log = IXLanIXFMemberImportLog.objects.create(ixlan=self.ixlan)\n        for action in [\"delete\", \"modify\", \"add\"]:\n            for info in self.actions_taken[action]:\n                netixlan = info[\"netixlan\"]\n                version_before = info[\"version\"]\n\n                versions = reversion.models.Version.objects.get_for_object(netixlan)\n\n                if version_before:\n                    versions = versions.filter(id__gt=version_before.id)\n                    version_after = versions.last()\n                else:\n                    version_after = versions.first()\n\n                if not version_after:\n                    continue\n\n                # push for data change notification (#403)\n                DataChangeNotificationQueue.push(\n                    \"ixf\", action, netixlan, version_before, version_after, **info\n                )\n\n                persist_log.entries.create(\n                    netixlan=netixlan,\n                    version_before=version_before,\n                    action=action,\n                    reason=info.get(\"reason\"),\n                    version_after=version_after,\n                )\n\n    def parse(self, data):\n        \"\"\"\n        Parse ixf data.\n\n        Arguments:\n            - data <dict>: result from fetch()\n        \"\"\"\n        with transaction.atomic():\n            self.parse_members(data.get(\"member_list\", []))\n\n    def parse_members(self, member_list):\n        \"\"\"\n        Parse the `member_list` section of the ixf schema.\n\n        Arguments:\n            - member_list <list>\n        \"\"\"\n        for member in member_list:\n            asn = member[\"asnum\"]\n\n            # if we are only processing a specific asn, ignore all\n            # that don't match\n            if self.asn and asn != self.asn:\n                continue\n\n            # keep track of asns we find in the IX-F data\n            if asn not in self.asns:\n                self.asns.append(asn)\n\n            if Network.objects.filter(asn=asn).exists():\n                network = Network.objects.get(asn=asn)\n                if network.status != \"ok\":\n                    self.log_peer(\n                        asn,\n                        \"ignore\",\n                        _(\"Network status is '{}'\").format(network.status),\n                    )\n                    continue\n\n                self.parse_connections(\n                    member.get(\"connection_list\", []), network, member\n                )\n            else:\n                self.log_peer(asn, \"ignore\", _(\"Network does not exist in peeringdb\"))\n\n    def parse_connections(self, connection_list, network, member):\n        \"\"\"\n        Parse the 'connection_list' section of the ixf schema.\n\n        Arguments:\n            - connection_list <list>\n            - network <Network>: pdb network instance\n            - member <dict>: row from ixf member_list\n        \"\"\"\n\n        asn = member[\"asnum\"]\n        for connection in connection_list:\n            self.connection_errors = {}\n            state = connection.get(\"state\", \"active\").lower()\n            if state in self.allowed_states:\n                speed = self.parse_speed(connection.get(\"if_list\", []))\n\n                self.parse_vlans(\n                    connection.get(\"vlan_list\", []), network, member, connection, speed\n                )\n            else:\n                self.log_peer(\n                    asn, \"ignore\", _(\"Invalid connection state: {}\").format(state)\n                )\n\n    def parse_vlans(self, vlan_list, network, member, connection, speed):\n        \"\"\"\n        Parse the 'vlan_list' section of the ixf_schema.\n\n        Arguments:\n            - vlan_list <list>\n            - network <Network>: pdb network instance\n            - member <dict>: row from ixf member_list\n            - connection <dict>: row from ixf connection_list\n            - speed <int>: interface speed\n        \"\"\"\n\n        asn = member[\"asnum\"]\n        for lan in vlan_list:\n            ipv4 = lan.get(\"ipv4\", {})\n            ipv6 = lan.get(\"ipv6\", {})\n\n            # vlan entry has no ipaddresses set, log and ignore\n            if not ipv4 and not ipv6:\n                self.log_error(\n                    _(\n                        \"Could not find ipv4 or 6 address in \"\n                        \"vlan_list entry for vlan_id {} (AS{})\"\n                    ).format(lan.get(\"vlan_id\"), asn)\n                )\n                continue\n\n            ipv4_addr = ipv4.get(\"address\")\n            ipv6_addr = ipv6.get(\"address\")\n\n            ipv4_support = network.ipv4_support\n            ipv6_support = network.ipv6_support\n\n            # parse and validate the ipaddresses attached to the vlan\n            # append a unqiue ixf identifier to self.ixf_ids\n            #\n            # identifier is a tuple of (asn, ip4, ip6)\n            #\n            # we will later check them to see which netixlans need to be\n            # dropped during `process_deletions`\n            try:\n                ixf_id = [asn]\n\n                if ipv4_addr:\n                    ipv4_addr = ipaddress.ip_address(f\"{ipv4_addr}\")\n                    ixf_id.append(ipv4_addr)\n                else:\n                    ixf_id.append(None)\n\n                if ipv6_addr:\n                    ipv6_addr = ipaddress.ip_address(f\"{ipv6_addr}\")\n                    ixf_id.append(ipv6_addr)\n                else:\n                    ixf_id.append(None)\n\n                ixf_id = tuple(ixf_id)\n\n            except (ipaddress.AddressValueError, ValueError) as exc:\n                self.invalid_ip_errors.append(f\"{exc}\")\n                self.log_error(\n                    _(\"Ip address error '{}' in vlan_list entry for vlan_id {}\").format(\n                        exc, lan.get(\"vlan_id\")\n                    )\n                )\n                continue\n\n            ipv4_valid_for_ixlan = self.ixlan.test_ipv4_address(ipv4_addr)\n            ipv6_valid_for_ixlan = self.ixlan.test_ipv6_address(ipv6_addr)\n\n            if (\n                ipv4_addr\n                and not ipv4_valid_for_ixlan\n                and ipv6_addr\n                and not ipv6_valid_for_ixlan\n            ):\n                # neither ipaddress falls into address space\n                # for this ixlan, ignore\n\n                continue\n\n            elif not ipv4_valid_for_ixlan and not ipv6_addr:\n                # ipv4 address does not fall into address space\n                # and ipv6 is not provided, ignore\n\n                continue\n\n            elif not ipv6_valid_for_ixlan and not ipv4_addr:\n                # ipv6 address does not fall into address space\n                # and ipv4 is not provided, ignore\n\n                continue\n\n            vlan = lan.get(\"vlan_id\")\n\n            if self.vlan is not None and vlan != self.vlan:\n                # prefixes spread over multiple vlans and\n                # cannot be represented properly at one ixlan\n                # fail the import\n                raise MultipleVlansInPrefix(self)\n\n            self.vlan = vlan\n\n            protocol_conflict = 0\n\n            # keep track of conflicts between ix/net in terms of ip\n            # protocols supported.\n\n            if ipv4_addr and not ipv4_support:\n                protocol_conflict = 4\n            elif ipv6_addr and not ipv6_support:\n                protocol_conflict = 6\n\n            if protocol_conflict and not self.protocol_conflict:\n                self.protocol_conflict = protocol_conflict\n\n            if protocol_conflict and not self.ixlan.ixf_ixp_import_protocol_conflict:\n                self.ixlan.ixf_ixp_import_protocol_conflict = protocol_conflict\n\n                if self.save:\n                    self.ixlan.save()\n\n                self.queue_notification(\n                    IXFMemberData.instantiate(\n                        asn,\n                        ipv4_addr,\n                        ipv6_addr,\n                        ixlan=self.ixlan,\n                        save=False,\n                        validate_network_protocols=False,\n                    ),\n                    \"protocol-conflict\",\n                    ac=False,\n                    net=True,\n                    ix=True,\n                    ipaddr4=ipv4_addr,\n                    ipaddr6=ipv6_addr,\n                )\n\n            self.ixf_ids.append(ixf_id)\n\n            if not network.ipv6_support:\n                self.ixf_ids.append((asn, ixf_id[1], None))\n                netixlan = NetworkIXLan.objects.filter(\n                    status=\"ok\", ipaddr4=ixf_id[1]\n                ).first()\n                if netixlan:\n                    self.ixf_ids.append((asn, ixf_id[1], netixlan.ipaddr6))\n\n            if not network.ipv4_support:\n                self.ixf_ids.append((asn, None, ixf_id[2]))\n                netixlan = NetworkIXLan.objects.filter(\n                    status=\"ok\", ipaddr6=ixf_id[2]\n                ).first()\n                if netixlan:\n                    self.ixf_ids.append((asn, netixlan.ipaddr4, ixf_id[2]))\n\n            if connection.get(\"state\", \"active\") == \"inactive\":\n                operational = False\n            else:\n                operational = True\n\n            if \"routeserver\" not in ipv4 and \"routeserver\" not in ipv6:\n                is_rs_peer = None\n            else:\n                is_rs_peer = ipv4.get(\"routeserver\", ipv6.get(\"routeserver\"))\n\n            try:\n                ixf_member_data = IXFMemberData.instantiate(\n                    asn,\n                    ipv4_addr,\n                    ipv6_addr,\n                    speed=speed,\n                    operational=operational,\n                    is_rs_peer=is_rs_peer,\n                    data=json.dumps(member),\n                    ixlan=self.ixlan,\n                    save=self.save,\n                )\n\n                if not ixf_member_data.ipaddr4 and not ixf_member_data.ipaddr6:\n                    continue\n\n            except NetworkProtocolsDisabled as exc:\n                self.log_error(f\"{exc}\")\n                continue\n\n            if self.connection_errors:\n                ixf_member_data.error = json.dumps(\n                    self.connection_errors, cls=ValidationErrorEncoder\n                )\n            else:\n                ixf_member_data.error = ixf_member_data.previous_error\n\n            self.pending_save.append(ixf_member_data)\n\n    def parse_speed(self, if_list):\n        \"\"\"\n        Parse speed from the 'if_list' section in the ixf data.\n\n        Arguments:\n            - if_list <list>\n\n        Returns:\n            - speed <int>\n        \"\"\"\n        speed = 0\n        for iface in if_list:\n            try:\n                speed += int(iface.get(\"if_speed\", 0))\n            except (ValueError, AttributeError):\n                try:\n                    log_msg = _(\"Invalid speed value: {}\").format(iface.get(\"if_speed\"))\n                except AttributeError:\n                    log_msg = _(\"Invalid speed value: could not be parsed\")\n                self.log_error(log_msg)\n                if \"speed\" not in self.connection_errors:\n                    self.connection_errors[\"speed\"] = []\n                self.connection_errors[\"speed\"].append(log_msg)\n        return speed\n\n    def apply_add_or_update(self, ixf_member_data):\n        if ixf_member_data.netixlan_exists:\n            # importer-protocol: netixlan exists\n\n            if not ixf_member_data.changes:\n                # importer-protocol: no changes\n\n                self.resolve(ixf_member_data)\n\n            else:\n                # importer-protocol: data changes\n\n                self.apply_update(ixf_member_data)\n\n        else:\n            # importer-protocol: netixlan does not exist\n\n            self.apply_add(ixf_member_data)\n\n    def queue_notification(\n        self, ixf_member_data, typ, ac=True, ix=True, net=True, **context\n    ):\n        self.notifications.append(\n            {\n                \"ixf_member_data\": ixf_member_data,\n                \"ac\": ac,\n                \"ix\": ix,\n                \"net\": net,\n                \"typ\": typ,\n                \"action\": ixf_member_data.action,\n                \"context\": context,\n            }\n        )\n\n    def resolve(self, ixf_member_data):\n        if ixf_member_data.set_resolved(save=self.save):\n            self.queue_notification(ixf_member_data, \"resolved\")\n\n    def apply_update(self, ixf_member_data):\n        changed_fields = \", \".join(ixf_member_data.changes.keys())\n        reason = f\"{REASON_VALUES_CHANGED}: {changed_fields}\"\n\n        if ixf_member_data.net.allow_ixp_update:\n            try:\n                self.log_apply(ixf_member_data.apply(save=self.save), reason=reason)\n            except ValidationError as exc:\n                if ixf_member_data.set_conflict(error=exc, save=self.save):\n                    self.queue_notification(ixf_member_data, ixf_member_data.action)\n        else:\n            notify = ixf_member_data.set_update(\n                save=self.save,\n                reason=reason,\n            )\n            if notify:\n                self.queue_notification(ixf_member_data, \"modify\")\n            self.log_ixf_member_data(ixf_member_data)\n\n    def apply_add(self, ixf_member_data):\n        if ixf_member_data.net.allow_ixp_update:\n            try:\n                self.log_apply(\n                    ixf_member_data.apply(save=self.save), reason=REASON_NEW_ENTRY\n                )\n                if not self.save:\n                    self.consolidate_delete_add(ixf_member_data)\n            except ValidationError as exc:\n                if ixf_member_data.set_conflict(error=exc, save=self.save):\n                    self.queue_notification(ixf_member_data, ixf_member_data.action)\n\n        else:\n            notify = ixf_member_data.set_add(save=self.save, reason=REASON_NEW_ENTRY)\n\n            self.log_ixf_member_data(ixf_member_data)\n            self.consolidate_delete_add(ixf_member_data)\n\n            if notify and ixf_member_data.net_present_at_ix:\n                self.queue_notification(ixf_member_data, ixf_member_data.action)\n            elif notify:\n                self.queue_notification(\n                    ixf_member_data, ixf_member_data.action, ix=False, ac=False\n                )\n\n    def consolidate_delete_add(self, ixf_member_data):\n        ip4_deletion = None\n        ip6_deletion = None\n\n        for ixf_id, deletion in self.deletions.items():\n            if deletion.asn == ixf_member_data.asn:\n                if (\n                    deletion.ipaddr4\n                    and deletion.ipaddr4 == ixf_member_data.init_ipaddr4\n                ):\n                    ip4_deletion = deletion\n                if (\n                    deletion.ipaddr6\n                    and deletion.ipaddr6 == ixf_member_data.init_ipaddr6\n                ):\n                    ip6_deletion = deletion\n\n            if ip4_deletion and ip6_deletion:\n                break\n\n        if not ip4_deletion and not ip6_deletion:\n            return\n\n        ip4_req = ixf_member_data.set_requirement(ip4_deletion, save=self.save)\n        ip6_req = ixf_member_data.set_requirement(ip6_deletion, save=self.save)\n\n        if not ip4_req and not ip6_req:\n            return\n\n        if not ixf_member_data.has_requirements:\n            return\n\n        if ip4_deletion:\n            try:\n                self.log[\"data\"].remove(ip4_deletion.ixf_log_entry)\n            except (ValueError, AttributeError):\n                pass\n        if ip6_deletion:\n            try:\n                self.log[\"data\"].remove(ip6_deletion.ixf_log_entry)\n            except (ValueError, AttributeError):\n                pass\n\n        changed_fields = \", \".join(\n            ixf_member_data._changes(\n                getattr(ip4_deletion, \"netixlan\", None)\n                or getattr(ip6_deletion, \"netixlan\", None)\n            ).keys()\n        )\n\n        ipaddr_info = \"\"\n\n        if ip4_deletion and ip6_deletion:\n            ipaddr_info = _(\"IP addresses moved to same entry\")\n        elif ip4_deletion:\n            ipaddr_info = _(\"IPv6 not set\")\n        elif ip6_deletion:\n            ipaddr_info = _(\"IPv4 not set\")\n\n        reason = f\"{REASON_VALUES_CHANGED}: {changed_fields} {ipaddr_info}\"\n\n        ixf_member_data.reason = reason\n        ixf_member_data.error = None\n        if self.save:\n            if ixf_member_data.updated:\n                ixf_member_data.save_without_update()\n            else:\n                ixf_member_data.save()\n\n        # update import log\n        # in the case of chained consolidated-add-del, log entry here will\n        # not be defined as only the last item in the chain needs to appear in\n        # the log (#889)\n        log_entry = getattr(ixf_member_data, \"ixf_log_entry\", None)\n        if log_entry:\n            log_entry[\"action\"] = log_entry[\"action\"].replace(\"add\", \"modify\")\n            log_entry[\"reason\"] = reason\n\n    def save_log(self):\n        \"\"\"\n        Save the attempt log.\n        \"\"\"\n        IXLanIXFMemberImportAttempt.objects.update_or_create(\n            ixlan=self.ixlan, defaults={\"info\": \"\\n\".join(json.dumps(self.log))}\n        )\n\n    def reset_log(self):\n        \"\"\"\n        Reset the attempt log.\n        \"\"\"\n        self.log = {\"data\": [], \"errors\": []}\n\n    def log_apply(self, apply_result, reason=\"\"):\n        netixlan = apply_result[\"netixlan\"]\n        self.actions_taken[apply_result[\"action\"]].append(\n            {\n                \"netixlan\": netixlan,\n                \"version\": reversion.models.Version.objects.get_for_object(\n                    netixlan\n                ).first(),\n                \"reason\": reason,\n            }\n        )\n\n        result = self.log_peer(\n            netixlan.asn, apply_result[\"action\"], reason, netixlan=netixlan\n        )\n\n        apply_result[\"ixf_member_data\"].ixf_log_entry = netixlan.ixf_log_entry\n\n        return result\n\n    def log_ixf_member_data(self, ixf_member_data):\n        return self.log_peer(\n            ixf_member_data.net.asn,\n            f\"suggest-{ixf_member_data.action}\",\n            ixf_member_data.reason,\n            netixlan=ixf_member_data,\n        )\n\n    def log_peer(self, asn, action, reason, netixlan=None):\n        \"\"\"\n        Log peer action in attempt log.\n\n        Arguments:\n            - asn <int>\n            - action <str>: add | modify | delete | noop | ignore\n            - reason <str>\n\n        Keyrword Arguments:\n            - netixlan <Netixlan>: if set, extra data will be added\n                to the log.\n        \"\"\"\n        peer = {\n            \"ixlan_id\": self.ixlan.id,\n            \"ix_id\": self.ixlan.ix.id,\n            \"ix_name\": self.ixlan.ix.name,\n            \"asn\": asn,\n        }\n\n        if netixlan:\n            # ixf-memberdata actions that are consolidated\n            # requirements of other actions should be kept\n            # out of the log as they are already implied by\n            # the log entry of the requirement (#824)\n\n            if getattr(netixlan, \"requirement_of_id\", None):\n                return\n\n            if hasattr(netixlan, \"network_id\"):\n                net_id = netixlan.network_id\n            else:\n                net_id = netixlan.net.id\n\n            peer.update(\n                {\n                    \"net_id\": net_id,\n                    \"ipaddr4\": \"{}\".format(netixlan.ipaddr4 or \"\"),\n                    \"ipaddr6\": \"{}\".format(netixlan.ipaddr6 or \"\"),\n                    \"speed\": netixlan.speed,\n                    \"is_rs_peer\": netixlan.is_rs_peer,\n                    \"operational\": netixlan.operational,\n                }\n            )\n        entry = {\n            \"peer\": peer,\n            \"action\": action,\n            \"reason\": f\"{reason}\",\n        }\n\n        self.log[\"data\"].append(entry)\n\n        if netixlan:\n            netixlan.ixf_log_entry = entry\n\n    def _email(self, subject, message, recipients, net=None, ix=None):\n        \"\"\"\n        Send email.\n\n        Honors the MAIL_DEBUG setting.\n\n        Will create IXFImportEmail entry.\n        \"\"\"\n\n        if not recipients:\n            return\n\n        email_log = None\n\n        logged_subject = f\"{settings.EMAIL_SUBJECT_PREFIX}[IX-F] {subject}\"\n\n        if net:\n            email_log = IXFImportEmail.objects.create(\n                subject=logged_subject,\n                message=strip_tags(message),\n                recipients=\",\".join(recipients),\n                net=net,\n            )\n\n            if not self.notify_net_enabled:\n                return\n\n        if ix:\n            email_log = IXFImportEmail.objects.create(\n                subject=logged_subject,\n                message=strip_tags(message),\n                recipients=\",\".join(recipients),\n                ix=ix,\n            )\n            if not self.notify_ix_enabled:\n                return\n\n        self.emails += 1\n\n        prod_mail_mode = not getattr(settings, \"MAIL_DEBUG\", True)\n        if prod_mail_mode:\n            self._send_email(subject, message, recipients)\n            if email_log:\n                email_log.sent = datetime.datetime.now(datetime.timezone.utc)\n\n        if email_log:\n            email_log.save()\n\n    def _send_email(self, subject, message, recipients):\n        mail = EmailMultiAlternatives(\n            subject,\n            strip_tags(message),\n            settings.DEFAULT_FROM_EMAIL,\n            recipients,\n        )\n\n        # Do not strip_tags for the HTML attachment\n        mail.attach_alternative(message.replace(\"\\n\", \"<br />\\n\"), \"text/html\")\n\n        mail.send(fail_silently=False)\n\n    def _ticket(self, ixf_member_data, subject, message, ix=False, net=False):\n        \"\"\"\n        Create and send a deskpro ticket.\n\n        Return the DeskPROTicket instance.\n\n        Argument(s):\n\n        - ixf_member_data (`IXFMemberData`)\n        - subject (`str`)\n        - message (`str`)\n\n        Keyword Argument(s):\n\n        - ix ('bool'): cc ix contacts\n        - net ('bool'): cc net contacts\n\n        \"\"\"\n\n        subject = f\"{settings.EMAIL_SUBJECT_PREFIX}[IX-F] {subject}\"\n\n        client = self.deskpro_client\n\n        if not ixf_member_data.deskpro_id:\n            old_ticket = DeskProTicket.objects.filter(\n                subject=subject, deskpro_id__isnull=False\n            ).first()\n            if old_ticket:\n                ixf_member_data.deskpro_id = old_ticket.deskpro_id\n                ixf_member_data.deskpro_ref = old_ticket.deskpro_ref\n\n        ticket = DeskProTicket.objects.create(\n            subject=subject,\n            body=message,\n            user=self.ticket_user,\n            deskpro_id=ixf_member_data.deskpro_id,\n            deskpro_ref=ixf_member_data.deskpro_ref,\n        )\n\n        cc = []\n\n        if ix:\n            # if ix to be notified, cc suitable contacts\n            cc += ixf_member_data.ix_contacts\n\n        if net:\n            # if net is to be notified, cc suitable contacts\n            cc += ixf_member_data.net_contacts\n\n        cc = list(set(cc))\n\n        for email in cc:\n            # we need to relate a name to the emails\n            # since deskpro will make a person for the email address\n            # we should attempt to supply the best possibly option\n\n            ticket.cc_set.create(\n                email=email,\n            )\n\n        try:\n            client.create_ticket(ticket)\n            ticket.published = datetime.datetime.now(datetime.timezone.utc)\n            ticket.save()\n        except Exception as exc:\n            ticket.subject = f\"[FAILED]{ticket.subject}\"\n            if hasattr(exc, \"data\"):\n                # api error returned with validation error data\n                ticket.body = f\"{ticket.body}\\n\\n{exc.data}\"\n            else:\n                # api error configuration issue\n                ticket.body = f\"{ticket.body}\\n\\n{exc}\"\n            ticket.save()\n        return ticket\n\n    def _ticket_consolidated(self, ixf_member_data_list, subject, message_list):\n        \"\"\"\n        Create and send a consolidated deskpro ticket.\n\n        Return the DeskPROTicket instance.\n\n        Argument(s):\n\n        - ixf_member_data_list (`List[IXFMemberData]`)\n        - subject (`str`)\n        - message (`List[str]`)\n        \"\"\"\n\n        subject = f\"{settings.EMAIL_SUBJECT_PREFIX}[IX-F] {subject}\"\n\n        client = self.deskpro_client\n\n        message = (\"-\" * 80 + \"\\n\").join(message_list)\n\n        ticket = DeskProTicket.objects.create(\n            subject=subject,\n            body=message,\n            user=self.ticket_user,\n            deskpro_id=None,\n            deskpro_ref=None,\n        )\n\n        try:\n            client.create_ticket(ticket)\n            ticket.published = datetime.datetime.now(datetime.timezone.utc)\n            ticket.save()\n        except Exception as exc:\n            ticket.subject = f\"[FAILED]{ticket.subject}\"\n            if hasattr(exc, \"data\"):\n                # api error returned with validation error data\n                ticket.body = f\"{ticket.body}\\n\\n{exc.data}\"\n            else:\n                # api error configuration issue\n                ticket.body = f\"{ticket.body}\\n\\n{exc}\"\n            ticket.save()\n        return ticket\n\n    def consolidate_proposals(self):\n        \"\"\"\n        Render and consolidate all proposals for each net and ix.\n        (#772)\n\n        Returns a dict.\n\n        {\n            \"net\": {\n                Network : {\n                    \"proposals\": {\n                        InternetExchange {\n                            \"add\" : [<str>, ...],\n                            \"modify\" : [<str>, ...],\n                            \"delete\" : [<str>, ...],\n                        },\n                    },\n                    \"count\": <int>\n                    \"entity\": Network,\n                    \"contacts\": [<str>, ...]\n                }\n            },\n            \"ix\": {\n                InternetExchange : {\n                    \"proposals\": {\n                        Network : {\n                            \"add\" : [<str>, ...],\n                            \"modify\" : [<str>, ...],\n                            \"delete\" : [<str>, ...],\n                        },\n                    },\n                    \"count\": <int>\n                    \"entity\": InternetExchange,\n                    \"contacts\": [<str>, ...]\n                }\n            }\n        }\n        \"\"\"\n\n        net_notifications = {}\n        ix_notifications = {}\n        deskpro_notifications = {}\n\n        self.current_proposal = None\n\n        for notification in self.notifications:\n            ixf_member_data = notification[\"ixf_member_data\"]\n            action = ixf_member_data.action\n            typ = notification[\"typ\"]\n            notify_ix = notification[\"ix\"]\n            notify_net = notification[\"net\"]\n            context = notification[\"context\"]\n            self.current_proposal = ixf_member_data\n\n            # we don't care about resolved proposals\n\n            if typ == \"resolved\":\n                if ixf_member_data.deskpro_id:\n                    self.ticket_proposal(**notification)\n                continue\n\n            if typ == \"protocol-conflict\":\n                action = \"protocol_conflict\"\n\n            # noop proposals are not actionable (#965)\n            if action == \"noop\":\n                continue\n\n            # in some edge cases (ip4 set on netixlan, network indicating\n            # only ipv6 support) we can get empty modify notifications\n            # that we need to throw out. (#771)\n            if typ == \"modify\":\n                if not ixf_member_data.actionable_changes:\n                    continue\n\n            # we don't care about proposals that are hidden\n            # requirements of other proposals\n\n            if ixf_member_data.requirement_of_id:\n                continue\n\n            asn = ixf_member_data.net\n            ix = ixf_member_data.ix\n            ix_contacts = ixf_member_data.ix_contacts\n            net_contacts = ixf_member_data.net_contacts\n\n            # no suitable contact points found for\n            # ix, immediately make a ticket\n            if not ix_contacts:\n                if typ != \"protocol-conflict\":\n                    self.ticket_proposal(**notification)\n\n            # Issue 883: if no suitable contact point\n            # for network, consolidate tickets\n            if not net_contacts:\n                if typ != \"protocol-conflict\" and notification[\"ac\"]:\n                    # Issue #883: consolidate tickets\n                    if asn not in deskpro_notifications:\n                        deskpro_notifications[asn] = []\n                    deskpro_notifications[asn].append(notification)\n\n            template_file = f\"email/notify-ixf-{typ}-inline.txt\"\n\n            # prepare consolidation rocketship\n\n            if notify_net and asn not in net_notifications:\n                net_notifications[asn] = {\n                    \"proposals\": {},\n                    \"count\": 0,\n                    \"entity\": ixf_member_data.net,\n                    \"contacts\": ixf_member_data.net_contacts,\n                }\n\n            if notify_net and ix not in net_notifications[asn][\"proposals\"]:\n                net_notifications[asn][\"proposals\"][ix] = {\n                    \"add\": [],\n                    \"modify\": [],\n                    \"delete\": [],\n                    \"protocol_conflict\": None,\n                }\n\n            if notify_ix and ix not in ix_notifications:\n                ix_notifications[ix] = {\n                    \"proposals\": {},\n                    \"count\": 0,\n                    \"entity\": ixf_member_data.ix,\n                    \"contacts\": ixf_member_data.ix_contacts,\n                }\n\n            if notify_ix and asn not in ix_notifications[ix][\"proposals\"]:\n                ix_notifications[ix][\"proposals\"][asn] = {\n                    \"add\": [],\n                    \"modify\": [],\n                    \"delete\": [],\n                    \"protocol_conflict\": None,\n                }\n\n            # render and push proposal text for network\n\n            if notify_net and (\n                ixf_member_data.actionable_for_network or action == \"protocol_conflict\"\n            ):\n                proposals = net_notifications[asn][\"proposals\"][ix]\n                message = ixf_member_data.render_notification(\n                    template_file,\n                    recipient=\"net\",\n                    context=context,\n                )\n\n                if action == \"protocol_conflict\" and not proposals[action]:\n                    proposals[action] = message\n                    net_notifications[asn][\"count\"] += 1\n                else:\n                    proposals[action].append(message)\n                    net_notifications[asn][\"count\"] += 1\n\n            # render and push proposal text for exchange\n\n            if notify_ix:\n                proposals = ix_notifications[ix][\"proposals\"][asn]\n                message = ixf_member_data.render_notification(\n                    template_file,\n                    recipient=\"ix\",\n                    context=context,\n                )\n\n                if action == \"protocol_conflict\" and not proposals[action]:\n                    proposals[action] = message\n                    ix_notifications[ix][\"count\"] += 1\n                else:\n                    proposals[action].append(message)\n                    ix_notifications[ix][\"count\"] += 1\n\n        return {\n            \"net\": net_notifications,\n            \"ix\": ix_notifications,\n            \"ac\": deskpro_notifications,\n        }\n", "right_context": "\n    def _notify_proposal(self, recipient, data, ticket_days, template):\n        contacts = data[\"contacts\"]\n\n        # we did not find any suitable contact points\n        # skip\n\n        if not contacts:\n            return\n\n        # no messages\n\n        if not data[\"count\"]:\n            return\n\n        # render the consolidated message\n\n        message = template.render(\n            {\n                \"recipient\": recipient,\n                \"entity\": data[\"entity\"],\n                \"count\": data[\"count\"],\n                \"ticket_days\": ticket_days,\n                \"proposals\": data[\"proposals\"],\n            }\n        )\n\n        if recipient == \"net\":\n            subject = _(\n                \"PeeringDB: Action May Be Needed: IX-F Importer \"\n                \"data mismatch between AS{} and one or more IXPs\"\n            ).format(data[\"entity\"].asn)\n            self._email(subject, message, contacts, net=data[\"entity\"])\n        else:\n            subject = _(\n                \"PeeringDB: Action May Be Needed: IX-F Importer \"\n                \"data mismatch between {} and one or more networks\"\n            ).format(data[\"entity\"].name)\n            self._email(subject, message, contacts, ix=data[\"entity\"])\n\n    def ticket_aged_proposals(self):\n        \"\"\"\n        This function is currently disabled as per issue #860.\n\n        Cycle through all IXFMemberData objects that\n        and create tickets for those that are older\n        than the period specified in IXF_IMPORTER_DAYS_UNTIL_TICKET\n        and that don't have any ticket associated with\n        them yet.\n        \"\"\"\n        \"\"\"\n        if not self.save:\n            return\n\n        qset = IXFMemberData.objects.filter(\n            deskpro_id__isnull=True, requirement_of__isnull=True\n        )\n\n        # get ticket days period\n        ticket_days = EnvironmentSetting.get_setting_value(\n            \"IXF_IMPORTER_DAYS_UNTIL_TICKET\"\n        )\n\n        if ticket_days > 0:\n\n            # we adjust the query to only get proposals\n            # that are older than the specified period\n\n            now = datetime.datetime.now(datetime.timezone.utc)\n            max_age = now - datetime.timedelta(days=ticket_days)\n            qset = qset.filter(created__lte=max_age)\n\n        for ixf_member_data in qset:\n\n            action = ixf_member_data.action\n            if action == \"delete\":\n                action = \"remove\"\n            elif action == \"noop\":\n                continue\n\n            typ = action\n\n            # create the ticket\n            # and also notify the net and ix with\n            # a reference to the ticket in the subject\n\n            self.ticket_proposal(\n                ixf_member_data, typ, True, True, True, {}, ixf_member_data.action\n            )\n        \"\"\"\n        return\n\n    def notify_stale_netixlans(self):\n        \"\"\"\n        Will send a repeat notification to the network about stale netixlans\n        according to settings specified in\n\n        * IXF_REMOVE_STALE_NETIXLAN_NOTIFY_PERIOD - days to wait between re-notification\n        * IXF_REMOVE_STALE_NETIXLAN_NOTIFY_COUNT - limits number of total re-notifications\n        \"\"\"\n\n        # global off switch\n\n        if not settings.IXF_REMOVE_STALE_NETIXLAN:\n            return\n\n        # currently running in preview mode, so no notifications should be sent\n\n        if not self.save:\n            return\n\n        # ix has import disabled, so no notifications should be sent (#1360)\n\n        if not self.ixlan.ready_for_ixf_import:\n            return\n\n        notify_period = settings.IXF_REMOVE_STALE_NETIXLAN_NOTIFY_PERIOD\n        notify_max = settings.IXF_REMOVE_STALE_NETIXLAN_NOTIFY_COUNT\n        now = django.utils.timezone.now()\n        max_age = now - datetime.timedelta(days=notify_period)\n\n        qset = IXFMemberData.objects.filter(\n            Q(extra_notifications_net_date__lte=max_age)\n            | Q(extra_notifications_net_date__isnull=True),\n            created__lte=max_age,\n        )\n        qset = qset.exclude(extra_notifications_net_num__gte=notify_max)\n\n        for ixf_member_data in qset:\n            # stale networks are only indicated by `delete` proposals\n\n            if ixf_member_data.action != \"delete\":\n                continue\n\n            # a delete proposal can be part of a modify proposal\n            # when the proposal is to combine ipv4 + ipv6 to the same\n            # netixlan object. in this case we do not want to remove\n            # as part of stale removal, since the same asn still owns\n            # both ips and is present at the ix.\n\n            if ixf_member_data.requirement_of_id:\n                continue\n\n            # check the date of when the IX-F information that suggests\n            # the netixlan removal was last seen in the IX-F data.\n            # ignore any that were not seen the last time the importer ran\n            #\n            # this is to catch edge cases where the ix has had i turned off\n            # for a while, turned it back on and now needs to process\n            # current vs old data. (#1360)\n\n            if ixf_member_data.fetched <= self.now:\n                continue\n\n            ixf_member_data.extra_notifications_net_num += 1\n            ixf_member_data.extra_notifications_net_date = now\n            ixf_member_data.save()\n            self.queue_notification(\n                ixf_member_data, \"remove\", ac=False, ix=False, net=True\n            )\n\n    @reversion.create_revision()\n    def cleanup_aged_proposals(self):\n        \"\"\"\n        Remove IXFMemberData objects that are older\n        than the period specified in IXF_REMOVE_STALE_NETIXLAN_PERIOD\n        and also remove associated netixlan object.\n        \"\"\"\n\n        if not self.save:\n            # Do not run a cleanup process in some cases.\n            # For example, when the importer runs in preview mode\n            # triggered by a network admin.\n\n            return\n\n        # global off switch\n\n        if not settings.IXF_REMOVE_STALE_NETIXLAN:\n            return\n\n        # ix has import disabled, so no notifications should be sent (#1360)\n\n        if not self.ixlan.ready_for_ixf_import:\n            return\n\n        notify_max = settings.IXF_REMOVE_STALE_NETIXLAN_NOTIFY_COUNT\n        cleanup_days = settings.IXF_REMOVE_STALE_NETIXLAN_PERIOD\n\n        # stale netixlan removal only happens after the network\n        # has been notified a number of times\n\n        qset = IXFMemberData.objects.filter(extra_notifications_net_num__gte=notify_max)\n\n        if cleanup_days > 0:\n            now = django.utils.timezone.now()\n            max_age = now - datetime.timedelta(days=cleanup_days)\n            qset = qset.filter(created__lte=max_age)\n\n        for ixf_member_data in qset:\n            action = ixf_member_data.action\n\n            # stale networks are only indicated by delete proposals\n\n            if action != \"delete\":\n                continue\n\n            # a delete proposal can be part of a modify proposal\n            # when the proposal is to combine ipv4 + ipv6 to the same\n            # netixlan object. in this case we do not want to remove\n            # as part of stale removal, since the same asn still owns\n            # both ips and is present at the ix.\n\n            if ixf_member_data.requirement_of_id:\n                continue\n\n            # check the date of when the IX-F information that suggests\n            # the netixlan removal was last seen in the IX-F data.\n            # ignore any that were not seen the last time the importer ran\n            #\n            # this is to catch edge cases where the ix has had i turned off\n            # for a while, turned it back on and now needs to process\n            # current vs old data. (#1360)\n\n            if ixf_member_data.fetched <= self.now:\n                continue\n\n            if ixf_member_data.netixlan is not None and ixf_member_data.netixlan.id:\n                self.actions_taken[\"delete\"].append(\n                    {\n                        \"netixlan\": ixf_member_data.netixlan,\n                        \"version\": reversion.models.Version.objects.get_for_object(\n                            ixf_member_data.netixlan\n                        ).first(),\n                        \"reason\": \"Stale netixlan removed due to long-standing unsolved data conflict (github#1271)\",\n                    }\n                )\n                ixf_member_data.netixlan.delete()\n            ixf_member_data.delete(hard=True)\n\n    def ticket_proposal(self, ixf_member_data, typ, ac, ix, net, context, action):\n        \"\"\"\n        Create a deskpro ticket and contexts net and ix with\n        ticket reference in the subject.\n\n        Argument(s)\n\n        - ixf_member_data (IXFMemberData)\n        - typ (str): proposal type 'add','delete','modify','resolve','conflict'\n        - ac (bool): If true DeskProTicket will be created\n        - ix (bool): If true email will be sent to ix\n        - net (bool): If true email will be sent to net\n        - context (dict): extra template context\n        \"\"\"\n\n        if typ == \"add\" and ixf_member_data.requirements:\n            typ = ixf_member_data.action\n            subject = f\"{ixf_member_data.primary_requirement}\"\n        else:\n            subject = f\"{ixf_member_data}\"\n\n        subject = f\"{subject} IX-F Conflict Resolution\"\n\n        template_file = f\"email/notify-ixf-{typ}.txt\"\n\n        # DeskPRO ticket\n\n        if ac and self.tickets_enabled:\n            message = ixf_member_data.render_notification(\n                template_file, recipient=\"ac\", context=context\n            )\n\n            ticket = self._ticket(ixf_member_data, subject, message, ix=ix, net=net)\n            ixf_member_data.deskpro_id = ticket.deskpro_id\n            ixf_member_data.deskpro_ref = ticket.deskpro_ref\n            if ixf_member_data.id:\n                ixf_member_data.save()\n\n    def ticket_consolidated_proposals(self, consolidated_proposals):\n        \"\"\"\n        Create a single ticket for each network that is missing a\n        network contact.\n\n        Argument\n        - consolidated_proposals (Dict[List]): a dictionary keyed on network name\n        and ASN, containing a list of notifications for that network\n        \"\"\"\n\n        if not self.tickets_enabled:\n            return False\n\n        for network, notification_list in consolidated_proposals.items():\n            asn = network.asn\n            name = network.name\n\n            count = len(consolidated_proposals)\n            index = 1\n            consolidated_messages = []\n            ixf_member_data_list = []\n\n            # Check again empty list\n            if len(notification_list) == 0:\n                continue\n\n            for notification in notification_list:\n                ixf_member_data = notification[\"ixf_member_data\"]\n                ixf_member_data_list.append(ixf_member_data)\n                typ = notification[\"typ\"]\n                context = notification[\"context\"]\n\n                if typ == \"add\" and ixf_member_data.requirements:\n                    typ = ixf_member_data.action\n                    subheading = f\"{ixf_member_data.primary_requirement}\"\n                else:\n                    subheading = f\"{ixf_member_data}\"\n\n                # Begin message with the subheading\n                message = f\"{subheading} IX-F Conflict Resolution ({index}/{count}) \\n\"\n                index += 1\n\n                template_file = f\"email/notify-ixf-{typ}.txt\"\n\n                # Add actual message body\n                message += ixf_member_data.render_notification(\n                    template_file, recipient=\"ac\", context=context\n                )\n                consolidated_messages.append(message)\n\n            consolidated_subject = (\n                f\"Several Actions May Be Needed for Network {name} AS{asn}\"\n            )\n\n            ticket = self._ticket_consolidated(\n                ixf_member_data_list, consolidated_subject, consolidated_messages\n            )\n\n            # Resave all ixf_member_data for deskpro attributes\n            for ixf_m_d in ixf_member_data_list:\n                ixf_m_d.deskpro_id = ticket.deskpro_id\n                ixf_m_d.deskpro_ref = ticket.deskpro_ref\n                if ixf_m_d.id:\n                    ixf_m_d.save()\n\n    @reversion.create_revision()\n    @transaction.atomic()\n    def notify_error(self, error):\n        \"\"\"\n        Notifie the exchange and AC of any errors that\n        were encountered when the IX-F data was\n        parsed.\n        \"\"\"\n\n        if not self.save:\n            return\n\n        reversion.set_user(self.ticket_user)\n\n        now = datetime.datetime.now(datetime.timezone.utc)\n        notified = self.ixlan.ixf_ixp_import_error_notified\n        self.ixlan.ixf_ixp_import_error\n\n        if notified:\n            diff = (now - notified).total_seconds() / 3600\n            if diff < settings.IXF_PARSE_ERROR_NOTIFICATION_PERIOD:\n                return\n\n        self.ixlan.ixf_ixp_import_error_notified = now\n        self.ixlan.ixf_ixp_import_error = error\n        self.ixlan.save()\n\n        ixf_member_data = IXFMemberData(ixlan=self.ixlan, asn=0)\n\n        subject = (\n            f\"Could not process IX-F Data - {self.ixlan.ix.name} ({self.ixlan.ix.id})\"\n        )\n        template = loader.get_template(\"email/notify-ixf-source-error.txt\")\n        message = template.render(\n            {\"error\": error, \"dt\": now, \"instance\": ixf_member_data}\n        )\n\n        # AC does not want ticket here as per #794\n        # self._ticket(ixf_member_data, subject, message)\n\n        if ixf_member_data.ix_contacts:\n            self._email(\n                subject, message, ixf_member_data.ix_contacts, ix=ixf_member_data.ix\n            )\n\n    def log_error(self, error, save=False):\n        \"\"\"\n        Append error to the attempt log.\n        \"\"\"\n        self.log[\"errors\"].append(f\"{error}\")\n        if save:\n            self.save_log()\n\n    def resend_emails(self):\n        \"\"\"\n        Resend emails that weren't sent.\n        \"\"\"\n\n        resent_emails = []\n        for email in self.emails_to_resend:\n            try:\n                resent_email = self._resend_email(email)\n                if resent_email:\n                    resent_emails.append(resent_email)\n            except SMTPException:\n                pass\n\n        return resent_emails\n\n    @property\n    def emails_to_resend(self):\n        return IXFImportEmail.objects.filter(sent__isnull=True).all()\n\n    def _resend_email(self, email):\n        subject = email.subject\n        message = email.message\n        resend_str = \"This email could not be delivered initially and may contain stale information. \\n\"\n        if not message.startswith(resend_str):\n            message = resend_str + message\n\n        recipients = email.recipients.split(\",\")\n\n        if email.net and not self.notify_net_enabled:\n            return False\n\n        if email.ix and not self.notify_ix_enabled:\n            return False\n\n        prod_mail_mode = not getattr(settings, \"MAIL_DEBUG\", True)\n        prod_resend_mode = getattr(settings, \"IXF_RESEND_FAILED_EMAILS\", False)\n\n        if prod_mail_mode and prod_resend_mode:\n            self._send_email(subject, message, recipients)\n            email.sent = datetime.datetime.now(datetime.timezone.utc)\n            email.message = strip_tags(message)\n            email.save()\n        else:\n            return False\n\n        return email\n\n\nclass PostMortem:\n\n    \"\"\"\n    Generate postmortem report for IX-F import.\n    \"\"\"\n\n    def reset(self, asn, **kwargs):\n        \"\"\"\n        Reset for a fresh run.\n\n        Argument(s):\n\n            - asn <int>: asn of the network to run postormem\n              report for\n\n        Keyword Argument(s):\n\n            - limit <int=100>: limit amount of import logs to process\n              max limit is defined by server config `IXF_POSTMORTEM_LIMIT`\n\n        \"\"\"\n\n        self.asn = asn\n        self.limit = kwargs.get(\"limit\", 100)\n        self.post_mortem = []\n\n    def generate(self, asn, **kwargs):\n        \"\"\"\n        Generate and return a new postmortem report.\n\n        Argument(s):\n\n            - asn <int>: asn of the network to run postmortem\n              report for\n\n        Keyword Argument(s):\n\n            - limit <int=100>: limit amount of import logs to process\n              max limit is defined by server config `IXF_POSTMORTEM_LIMIT`\n\n        Returns:\n\n            - dict: postmortem report\n        \"\"\"\n\n        self.reset(asn, **kwargs)\n        self._process_logs(limit=self.limit)\n        return self.post_mortem\n\n    def _process_logs(self, limit=100):\n        \"\"\"\n        Process IX-F import logs.\n\n        KeywordArgument(s):\n\n             - limit <int=100>: limit amount of import logs to process\n              max limit is defined by server config `IXF_POSTMORTEM_LIMIT`\n        \"\"\"\n\n        # we only want import logs that actually touched the specified\n        # asn\n\n        qset = IXLanIXFMemberImportLogEntry.objects.filter(netixlan__asn=self.asn)\n        qset = qset.exclude(action__isnull=True)\n        qset = qset.order_by(\"-log__created\", \"-id\")\n        qset = qset.select_related(\"log\", \"netixlan\", \"log__ixlan\", \"log__ixlan__ix\")\n\n        for entry in qset[:limit]:\n            self._process_log_entry(entry.log, entry)\n\n    def _process_log_entry(self, log, entry):\n        \"\"\"\n        Process a single IX-F import log entry.\n\n        Argument(s):\n\n            - log <IXLanIXFMemberImportLog>\n            - entry <IXLanIXFMemberImportLogEntry>\n\n        \"\"\"\n\n        if entry.netixlan.asn != self.asn:\n            return\n\n        data = entry.version_after.field_dict\n        if data.get(\"asn\") != self.asn:\n            return\n\n        if data.get(\"ipaddr4\"):\n            ipaddr4 = \"{}\".format(data.get(\"ipaddr4\"))\n        else:\n            ipaddr4 = None\n\n        if data.get(\"ipaddr6\"):\n            ipaddr6 = \"{}\".format(data.get(\"ipaddr6\"))\n        else:\n            ipaddr6 = None\n\n        self.post_mortem.append(\n            {\n                \"ix_id\": log.ixlan.ix.id,\n                \"ix_name\": log.ixlan.ix.name,\n                \"ixlan_id\": log.ixlan.id,\n                \"changes\": entry.changes,\n                \"reason\": entry.reason,\n                \"action\": entry.action,\n                \"asn\": data.get(\"asn\"),\n                \"ipaddr4\": ipaddr4,\n                \"ipaddr6\": ipaddr6,\n                \"speed\": data.get(\"speed\"),\n                \"is_rs_peer\": data.get(\"is_rs_peer\"),\n                \"created\": log.created.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            }\n        )\n", "import_text": ["datetime", "ipaddress", "json", "smtplib.SMTPException", "django", "requests", "reversion", "django.conf.settings", "django.core.cache.cache", "django.core.exceptions.ValidationError", "django.core.mail.message.EmailMultiAlternatives", "django.db.transaction", "django.db.models.Q", "django.template.loader", "django.utils.html.strip_tags", "django.utils.translation.gettext_lazy", "peeringdb_server.deskpro", "peeringdb_server.models.DataChangeNotificationQueue", "peeringdb_server.models.DeskProTicket", "peeringdb_server.models.IXFImportEmail", "peeringdb_server.models.IXFMemberData", "peeringdb_server.models.IXLanIXFMemberImportAttempt", "peeringdb_server.models.IXLanIXFMemberImportLog", "peeringdb_server.models.IXLanIXFMemberImportLogEntry", "peeringdb_server.models.Network", "peeringdb_server.models.NetworkIXLan", "peeringdb_server.models.NetworkProtocolsDisabled", "peeringdb_server.models.User", "peeringdb_server.models.ValidationErrorEncoder"], "prompt": "\"\"\"\nDescription: This function is used to notify proposals for consolidation and ticket creation.\n\nArgs:\n    self: The instance of the class that this function is called on.\n    error_handler (function, optional): A function that handles errors. It should accept an exception as its first argument and optionally additional arguments.\n\nReturns:\n    None\n\nRaises:\n    Exception: If an error occurs and no error handler is provided, the exception is raised.\n\nNotes:\n    This function uses the Django template loader to get a template for email notifications.\n    It consolidates proposals into 'net,ix' and 'ix,net' groupings and notifies the respective recipients.\n    If an error occurs during this process, it either handles the error with the provided error handler or re-raises the exception if no error handler is provided.\n    It also creates tickets for consolidated proposals.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"\n        Send all collected notification proposals.\n        \"\"\"", "function_dependencies": ["django.template.loader.get_template"], "project_create_time": "2016-06-06T21:49:25+00:00", "project_update_time": "2024-04-17T22:56:51+00:00", "file_create_time": "2019-02-15T17:46:40Z", "file_update_time": "2024-04-15T15:42:10Z", "function_update_time": "2019-02-15T17:46:40Z", "license": {"key": "bsd-2-clause", "name": "BSD 2-Clause \"Simplified\" License", "spdx_id": "BSD-2-Clause", "url": "https://api.github.com/licenses/bsd-2-clause", "node_id": "MDc6TGljZW5zZTQ="}, "reference_api": ["django.template.loader.get_template"], "test_function": [{"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_add_deleted_netixlan", "code": "def test_add_deleted_netixlan(entities, use_ip, save):\n\n    data = setup_test_data(\"ixf.member.speed.0\")\n    network = entities[\"net\"][\"UPDATE_ENABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    netixlan = NetworkIXLan.objects.create(\n        network=network,\n        ixlan=ixlan,\n        asn=network.asn,\n        speed=1,\n        ipaddr4=use_ip(4, \"195.69.147.250\"),\n        ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n        status=\"ok\",\n        is_rs_peer=True,\n        operational=False,\n    )\n\n    netixlan.delete()\n\n    assert NetworkIXLan.objects.filter(status=\"ok\").count() == 0\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    for email in IXFImportEmail.objects.all():\n        print(email.message)\n\n    assert_no_emails(network, ixlan.ix)\n\n    netixlan = NetworkIXLan.objects.filter(status=\"ok\").first()\n    # Assert data values are updated\n    assert netixlan.is_rs_peer == True\n    assert netixlan.operational == True\n    assert netixlan.speed == 0"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_resolve_local_ixf", "code": "def test_resolve_local_ixf(entities, use_ip, save):\n    data = setup_test_data(\"ixf.member.0\")\n    network = entities[\"net\"][\"UPDATE_ENABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=use_ip(4, \"195.69.147.250\"),\n            ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    # Create a local IXF that matches remote details\n    IXFMemberData.objects.create(\n        asn=network.asn,\n        ipaddr4=use_ip(4, \"195.69.147.250\"),\n        ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n        ixlan=ixlan,\n        speed=10000,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 0\n\n    # We do not email upon resolve\n    assert_no_emails(network, ixlan.ix)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_resolve_local_ixf_stale_netixlan", "code": "def test_resolve_local_ixf_stale_netixlan(entities, use_ip, save):\n    data = setup_test_data(\"ixf.member.7\")\n    network = entities[\"net\"][\"UPDATE_DISABLED_2\"]\n    network_o = entities[\"net\"][\"UPDATE_ENABLED\"]\n\n    ixlan = entities[\"ixlan\"][0]\n    ixlan.ixf_ixp_import_enabled = True\n\n    netixlan = NetworkIXLan.objects.create(\n        network=network,\n        ixlan=ixlan,\n        asn=network.asn,\n        speed=10000,\n        ipaddr4=use_ip(4, \"195.69.147.250\"),\n        ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n        status=\"ok\",\n        is_rs_peer=True,\n        operational=True,\n    )\n\n    entities[\"netixlan\"].append(netixlan)\n\n    # Create a local IXF that matches remote details\n    ixm = IXFMemberData.objects.create(\n        asn=network.asn,\n        ipaddr4=use_ip(4, \"195.69.147.250\"),\n        ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n        ixlan=ixlan,\n        speed=10000,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n        data=\"{}\",\n        extra_notifications_net_num=0,\n        extra_notifications_net_date=None,\n    )\n\n    assert IXFMemberData.objects.count() == 1\n\n    ixm.created = ixm.created - datetime.timedelta(\n        days=settings.IXF_REMOVE_STALE_NETIXLAN_PERIOD + 1\n    )\n    ixm.save()\n\n    assert ixm.action == \"delete\"\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    # first update should not remove the netixlan or IX-F entry\n    # since the notification count requirement is not met\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 1\n\n    netixlan.refresh_from_db()\n    assert netixlan.status == \"ok\"\n\n    # ixf member data entry should have logged one notification\n    # to the network.\n\n    ixm.refresh_from_db()\n    assert ixm.extra_notifications_net_num == 1\n    assert ixm.extra_notifications_net_date is not None\n\n    if not network_o.ipv4_support or not network_o.ipv6_support:\n        # this initial ixf import will send two additional emails (one to the net, one to the ix)\n        # due to protocol mismatch which we need to account for here - this is due to the test\n        # data which is not otherwise relevant to this test, but\n        # needs to be there for the IX-F import to be valid\n        #\n        # then we also expect one re-notificaiton email about the stale network\n\n        assert IXFImportEmail.objects.count() == 3\n    else:\n        # otherwise expect one re-notification email about the stale network\n\n        assert IXFImportEmail.objects.count() == 1\n\n    # now notification count rquirement is set to the required amount\n    # stale netixlan and IX-F entry should be removed\n\n    ixm.extra_notifications_net_num = settings.IXF_REMOVE_STALE_NETIXLAN_NOTIFY_COUNT\n    ixm.save()\n\n    # reset emails...\n    IXFImportEmail.objects.all().delete()\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 0\n    assert IXFImportEmail.objects.count() == 0\n\n    netixlan.refresh_from_db()\n    assert netixlan.status == \"deleted\"\n\n    # Test idempotent\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_resolve_local_ixf_stale_netixlan_import_disabled", "code": "def test_resolve_local_ixf_stale_netixlan_import_disabled(entities, use_ip, save):\n    data = setup_test_data(\"ixf.member.7\")\n    network = entities[\"net\"][\"UPDATE_DISABLED_2\"]\n    network_o = entities[\"net\"][\"UPDATE_ENABLED\"]\n\n    ixlan = entities[\"ixlan\"][0]\n    ixlan.ixf_ixp_import_enabled = True\n\n    netixlan = NetworkIXLan.objects.create(\n        network=network,\n        ixlan=ixlan,\n        asn=network.asn,\n        speed=10000,\n        ipaddr4=use_ip(4, \"195.69.147.250\"),\n        ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n        status=\"ok\",\n        is_rs_peer=True,\n        operational=True,\n    )\n\n    entities[\"netixlan\"].append(netixlan)\n\n    # Create a local IXF that matches remote details\n    ixm = IXFMemberData.objects.create(\n        asn=network.asn,\n        ipaddr4=use_ip(4, \"195.69.147.250\"),\n        ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n        ixlan=ixlan,\n        speed=10000,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n        data=\"{}\",\n        extra_notifications_net_num=0,\n        extra_notifications_net_date=None,\n    )\n\n    assert IXFMemberData.objects.count() == 1\n\n    ixm.created = ixm.created - datetime.timedelta(\n        days=settings.IXF_REMOVE_STALE_NETIXLAN_PERIOD + 1\n    )\n    ixm.save()\n\n    assert ixm.action == \"delete\"\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    # first update should not remove the netixlan or IX-F entry\n    # since the notification count requirement is not met\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 1\n\n    netixlan.refresh_from_db()\n    assert netixlan.status == \"ok\"\n\n    # ixf member data entry should have logged one notification\n    # to the network.\n\n    ixm.refresh_from_db()\n    assert ixm.extra_notifications_net_num == 1\n    assert ixm.extra_notifications_net_date is not None\n\n    if not network_o.ipv4_support or not network_o.ipv6_support:\n        # this initial ixf import will send two additional emails (one to the net, one to the ix)\n        # due to protocol mismatch which we need to account for here - this is due to the test\n        # data which is not otherwise relevant to this test, but\n        # needs to be there for the IX-F import to be valid\n        #\n        # then we also expect one re-notificaiton email about the stale network\n\n        assert IXFImportEmail.objects.count() == 3\n    else:\n        # otherwise expect one re-notification email about the stale network\n\n        assert IXFImportEmail.objects.count() == 1\n\n    # now notification count rquirement is set to the required amount\n    # however internet exchange has disabled import, so nothing should get deleted\n\n    ixm.extra_notifications_net_num = settings.IXF_REMOVE_STALE_NETIXLAN_NOTIFY_COUNT\n    ixm.save()\n\n    # reset emails...\n    IXFImportEmail.objects.all().delete()\n\n    # disable ixf import\n    ixlan.ixf_ixp_import_enabled = False\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 1\n    assert IXFImportEmail.objects.count() == 0\n\n    netixlan.refresh_from_db()\n    assert netixlan.status == \"ok\"\n\n    # Test idempotent\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_resolve_local_ixf_stale_netixlan_removal_disabled", "code": "def test_resolve_local_ixf_stale_netixlan_removal_disabled(entities, save):\n\n    data = setup_test_data(\"ixf.member.7\")\n    network = entities[\"net\"][\"UPDATE_DISABLED_2\"]\n    network_o = entities[\"net\"][\"UPDATE_ENABLED\"]\n\n    ixlan = entities[\"ixlan\"][0]\n    ixlan.ixf_ixp_import_enabled = True\n\n    netixlan = NetworkIXLan.objects.create(\n        network=network,\n        ixlan=ixlan,\n        asn=network.asn,\n        speed=10000,\n        ipaddr4=\"195.69.147.250\",\n        ipaddr6=\"2001:7f8:1::a500:2906:1\",\n        status=\"ok\",\n        is_rs_peer=True,\n        operational=True,\n    )\n\n    entities[\"netixlan\"].append(netixlan)\n\n    # Create a local IXF that matches remote details\n    ixm = IXFMemberData.objects.create(\n        asn=network.asn,\n        ipaddr4=\"195.69.147.250\",\n        ipaddr6=\"2001:7f8:1::a500:2906:1\",\n        ixlan=ixlan,\n        speed=10000,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n        data=\"{}\",\n        extra_notifications_net_num=settings.IXF_REMOVE_STALE_NETIXLAN_NOTIFY_COUNT,\n    )\n\n    assert IXFMemberData.objects.count() == 1\n\n    ixm.created = ixm.created - datetime.timedelta(\n        days=settings.IXF_REMOVE_STALE_NETIXLAN_PERIOD + 1\n    )\n    ixm.save()\n\n    assert ixm.action == \"delete\"\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    # first update should not remove the netixlan or IX-F entry\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 1\n\n    netixlan.refresh_from_db()\n    assert netixlan.status == \"ok\""}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_resolve_local_ixf_stale_netixlan_renotification", "code": "def test_resolve_local_ixf_stale_netixlan_renotification(entities, save):\n\n    data = setup_test_data(\"ixf.member.7\")\n    network = entities[\"net\"][\"UPDATE_DISABLED_2\"]\n    ixlan = entities[\"ixlan\"][0]\n    ixlan.ixf_ixp_import_enabled = True\n\n    netixlan = NetworkIXLan.objects.create(\n        network=network,\n        ixlan=ixlan,\n        asn=network.asn,\n        speed=10000,\n        ipaddr4=\"195.69.147.250\",\n        ipaddr6=\"2001:7f8:1::a500:2906:1\",\n        status=\"ok\",\n        is_rs_peer=True,\n        operational=True,\n    )\n\n    entities[\"netixlan\"].append(netixlan)\n\n    # Create a local IXF that matches remote details\n    ixm = IXFMemberData.objects.create(\n        asn=network.asn,\n        ipaddr4=\"195.69.147.250\",\n        ipaddr6=\"2001:7f8:1::a500:2906:1\",\n        ixlan=ixlan,\n        speed=10000,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n        data=\"{}\",\n    )\n\n    assert IXFMemberData.objects.count() == 1\n\n    importer = ixf.Importer()\n\n    # first importer run, no re-notification\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    ixm.refresh_from_db()\n    assert ixm.extra_notifications_net_num == 0\n\n    # second importer run, firstre-notification after moving the date back\n\n    ixm.created = ixm.created - datetime.timedelta(\n        days=settings.IXF_REMOVE_STALE_NETIXLAN_NOTIFY_PERIOD + 1\n    )\n    print(\"CREATED\", ixm.created)\n    ixm.save()\n\n    IXFImportEmail.objects.all().delete()\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    ixm.refresh_from_db()\n    assert ixm.extra_notifications_net_num == 1\n\n    # third importer run, no re-notification, not enough time has passed\n    # between last re-notification and now\n\n    IXFImportEmail.objects.all().delete()\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    ixm.refresh_from_db()\n    assert ixm.extra_notifications_net_num == 1\n\n    # fourth importer run, set the notification date back to the past, causing\n    # the next notification to be sent\n\n    ixm.extra_notifications_net_date = (\n        ixm.extra_notifications_net_date\n        - datetime.timedelta(days=settings.IXF_REMOVE_STALE_NETIXLAN_NOTIFY_PERIOD + 1)\n    )\n    ixm.save()\n\n    IXFImportEmail.objects.all().delete()\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    ixm.refresh_from_db()\n    assert ixm.extra_notifications_net_num == 2\n\n    # fifth importer run, set the notification date back to the past, but\n    # since max notification limit has been reached, no notification is sent\n\n    ixm.extra_notifications_net_date = (\n        ixm.extra_notifications_net_date\n        - datetime.timedelta(days=settings.IXF_REMOVE_STALE_NETIXLAN_NOTIFY_PERIOD + 1)\n    )\n    ixm.save()\n\n    IXFImportEmail.objects.all().delete()\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    ixm.refresh_from_db()\n    assert ixm.extra_notifications_net_num == 2"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_resolve_local_ixf_stale_netixlan_renotification_import_disabled", "code": "def test_resolve_local_ixf_stale_netixlan_renotification_import_disabled(\n    entities, save\n):\n\n    data = setup_test_data(\"ixf.member.7\")\n    network = entities[\"net\"][\"UPDATE_DISABLED_2\"]\n    ixlan = entities[\"ixlan\"][0]\n    ixlan.ixf_ixp_import_enabled = False\n\n    netixlan = NetworkIXLan.objects.create(\n        network=network,\n        ixlan=ixlan,\n        asn=network.asn,\n        speed=10000,\n        ipaddr4=\"195.69.147.250\",\n        ipaddr6=\"2001:7f8:1::a500:2906:1\",\n        status=\"ok\",\n        is_rs_peer=True,\n        operational=True,\n    )\n\n    entities[\"netixlan\"].append(netixlan)\n\n    # Create a local IXF that matches remote details\n    ixm = IXFMemberData.objects.create(\n        asn=network.asn,\n        ipaddr4=\"195.69.147.250\",\n        ipaddr6=\"2001:7f8:1::a500:2906:1\",\n        ixlan=ixlan,\n        speed=10000,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n        data=\"{}\",\n    )\n\n    assert IXFMemberData.objects.count() == 1\n\n    importer = ixf.Importer()\n\n    # first importer run, no re-notification\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    ixm.refresh_from_db()\n    assert ixm.extra_notifications_net_num == 0\n\n    # second importer run, first re-notification after moving the date back\n    # however import is disabled, so no notification should be sent\n\n    ixm.created = ixm.created - datetime.timedelta(\n        days=settings.IXF_REMOVE_STALE_NETIXLAN_NOTIFY_PERIOD + 1\n    )\n    ixm.save()\n\n    IXFImportEmail.objects.all().delete()\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    ixm.refresh_from_db()\n    assert ixm.extra_notifications_net_num == 0\n\n    # third importer run, no re-notification, not enough time has passed\n    # between last re-notification and now\n\n    IXFImportEmail.objects.all().delete()\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    ixm.refresh_from_db()\n    assert ixm.extra_notifications_net_num == 0"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_update_data_attributes", "code": "def test_update_data_attributes(entities, use_ip, save):\n    data = setup_test_data(\"ixf.member.0\")\n    network = entities[\"net\"][\"UPDATE_ENABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n    ix_updated = ixlan.ix.updated\n\n    with reversion.create_revision():\n        entities[\"netixlan\"].append(\n            NetworkIXLan.objects.create(\n                network=network,\n                ixlan=ixlan,\n                asn=network.asn,\n                speed=20000,\n                ipaddr4=use_ip(4, \"195.69.147.250\"),\n                ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n                status=\"ok\",\n                is_rs_peer=False,\n                operational=False,\n            )\n        )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    # assert that the exchange's `updated` field was not\n    # altered by the import (#812)\n    ixlan.ix.refresh_from_db()\n    assert ixlan.ix.updated == ix_updated\n\n    assert IXFMemberData.objects.count() == 0\n\n    if (network.ipv4_support and not use_ip(4)) or (\n        network.ipv6_support and not use_ip(6)\n    ):\n        # test (delete+add) consolidation (#770)\n\n        assert len(importer.log[\"data\"]) == 2\n        log_delete = importer.log[\"data\"][0]\n        log_add = importer.log[\"data\"][1]\n        assert log_delete[\"action\"] == \"delete\"\n        assert log_add[\"action\"] == \"add\"\n\n    else:\n        # test modify\n        assert len(importer.log[\"data\"]) == 1\n        log = importer.log[\"data\"][0]\n\n        assert log[\"action\"] == \"modify\"\n        assert \"operational\" in log[\"reason\"]\n\n        # #793 we are currently ignoring is_rs_peer\n        # and speed for modifies\n        assert \"is_rs_peer\" not in log[\"reason\"]\n        assert \"speed\" not in log[\"reason\"]\n\n    netixlan = NetworkIXLan.objects.filter(status=\"ok\").first()\n    assert netixlan.operational == True\n\n    # #793 we are currently ignoring is_rs_peer\n    # and speed for modifies\n    assert netixlan.is_rs_peer == False\n    assert netixlan.speed == 20000\n\n    # Assert idempotent\n    assert_idempotent(importer, ixlan, data)\n\n    # Assert no emails\n    assert_no_emails(network, ixlan.ix)\n\n    # test revision user\n    version = reversion.models.Version.objects.get_for_object(netixlan)\n    assert version.first().revision.user == importer.ticket_user\n\n    # test rollback\n    import_log = IXLanIXFMemberImportLog.objects.first()\n    import_log.rollback()\n    netixlan = NetworkIXLan.objects.filter(status=\"ok\").first()\n    assert netixlan.operational == False\n    assert netixlan.is_rs_peer == False\n    assert netixlan.speed == 20000\n    assert netixlan.ipaddr4 == use_ip(4, \"195.69.147.250\")\n    assert netixlan.ipaddr6 == use_ip(6, \"2001:7f8:1::a500:2906:1\")"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_update_data_attributes_no_routeserver", "code": "def test_update_data_attributes_no_routeserver(entities, save):\n    data = setup_test_data(\"ixf.member.4\")\n    network = entities[\"net\"][\"UPDATE_ENABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    with reversion.create_revision():\n        entities[\"netixlan\"].append(\n            NetworkIXLan.objects.create(\n                network=network,\n                ixlan=ixlan,\n                asn=network.asn,\n                speed=20000,\n                ipaddr4=\"195.69.147.250\",\n                ipaddr6=\"2001:7f8:1::a500:2906:1\",\n                status=\"ok\",\n                is_rs_peer=True,\n                operational=False,\n            )\n        )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 0\n\n    netixlan = entities[\"netixlan\"][-1]\n    netixlan.refresh_from_db()\n    assert netixlan.is_rs_peer == True"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_suggest_modify_local_ixf", "code": "def test_suggest_modify_local_ixf(entities, use_ip, save):\n    data = setup_test_data(\"ixf.member.1\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=20000,\n            ipaddr4=use_ip(4, \"195.69.147.250\"),\n            ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n            status=\"ok\",\n            is_rs_peer=False,\n            operational=False,\n        )\n    )\n\n    # Matches the json data, doesn't match the existing netixlan.\n    preexisting_ixfmember_data = IXFMemberData.objects.create(\n        asn=network.asn,\n        ipaddr4=use_ip(4, \"195.69.147.250\"),\n        ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n        ixlan=ixlan,\n        speed=10000,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n        data={\"foo\": \"bar\"},\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    for email in IXFImportEmail.objects.all():\n        print(email.message)\n\n    if (network.ipv4_support and not network.ipv6_support and not use_ip(4)) or (\n        network.ipv6_support and not network.ipv4_support and not use_ip(6)\n    ):\n        # edge case where network has the one ip set that\n        # its not supporting and the other ip nto set at all\n        # (see #771 and #770) and the existing suggestion was for\n        # a different combination protocols supported and signature\n        #\n        # this will generate a new proposal notification for the entry\n        # and there is nothing we can do about it at this point\n        #\n        # should only happen very rarely\n\n        email_info = [\n            (\n                \"MODIFY\",\n                network.asn,\n                use_ip(4, \"195.69.147.250\"),\n                use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n            )\n        ]\n\n        assert_ix_email(ixlan.ix, email_info)\n        assert_network_email(network, email_info)\n\n        assert IXFMemberData.objects.count() == 2\n        ixf_member_data_delete = IXFMemberData.objects.all()[0]\n        ixf_member_data_modify = IXFMemberData.objects.all()[1]\n        assert ixf_member_data_modify.action == \"modify\"\n        assert ixf_member_data_delete.action == \"delete\"\n\n        assert ixf_member_data_delete.requirement_of == ixf_member_data_modify\n\n    elif (network.ipv4_support and network.ipv6_support and not use_ip(4)) or (\n        network.ipv6_support and network.ipv4_support and not use_ip(6)\n    ):\n        # network supports both protocols, old IX-F data only has one\n        # of the ips set, suggest adding the other\n        # #770 #771\n\n        email_info = [\n            (\n                \"MODIFY\",\n                network.asn,\n                use_ip(4, \"195.69.147.250\"),\n                use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n            )\n        ]\n\n        assert_ix_email(ixlan.ix, email_info)\n        assert_network_email(network, email_info)\n\n        assert IXFMemberData.objects.count() == 2\n        ixf_member_data_delete = IXFMemberData.objects.all()[0]\n        ixf_member_data_modify = IXFMemberData.objects.all()[1]\n        assert ixf_member_data_modify.action == \"modify\"\n        assert ixf_member_data_delete.action == \"delete\"\n        assert ixf_member_data_delete.requirement_of == ixf_member_data_modify\n\n    else:\n        assert_no_emails(network, ixlan.ix)\n        assert IXFMemberData.objects.count() == 1\n        assert preexisting_ixfmember_data == IXFMemberData.objects.first()\n\n    # Assert idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_suggest_modify", "code": "def test_suggest_modify(entities, use_ip, save):\n    data = setup_test_data(\"ixf.member.1\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=20000,\n            ipaddr4=use_ip(4, \"195.69.147.250\"),\n            ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n            status=\"ok\",\n            is_rs_peer=False,\n            operational=False,\n        )\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    print(importer.log)\n\n    # Create local ixf\n    if (not network.ipv4_support and not use_ip(6)) or (\n        not network.ipv6_support and not use_ip(4)\n    ):\n        # data changes and signature (ip) change with\n        # partial ip protocol support\n        # #770 and #771\n\n        assert IXFMemberData.objects.count() == 2\n    elif (network.ipv4_support and network.ipv6_support) and (\n        not use_ip(6) or not use_ip(4)\n    ):\n        # data changes and signature (ip) change with\n        # full ip protocol support\n        # #770 and #771\n\n        assert IXFMemberData.objects.count() == 2\n    else:\n        assert IXFMemberData.objects.count() == 1\n    assert len(importer.log[\"data\"]) == 1\n    log = importer.log[\"data\"][0]\n    assert log[\"action\"] == \"suggest-modify\"\n\n    # NetIXLAN is unchanged\n    assert NetworkIXLan.objects.first().speed == 20000\n    assert NetworkIXLan.objects.first().is_rs_peer == False\n    assert NetworkIXLan.objects.first().operational == False\n\n    # Consolidated email is sent to the Network and the IX\n    email_info = [\n        (\n            \"MODIFY\",\n            network.asn,\n            use_ip(4, \"195.69.147.250\"),\n            use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n        )\n    ]\n    assert_ix_email(ixlan.ix, email_info)\n    assert_network_email(network, email_info)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data, save=save)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_suggest_modify_no_routeserver", "code": "def test_suggest_modify_no_routeserver(entities, save):\n    data = setup_test_data(\"ixf.member.5\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=20000,\n            ipaddr4=\"195.69.147.250\",\n            ipaddr6=\"2001:7f8:1::a500:2906:1\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=False,\n        )\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert NetworkIXLan.objects.last().is_rs_peer == True\n    assert IXFMemberData.objects.first().is_rs_peer == None\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data, save=save)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_add_netixlan", "code": "def test_add_netixlan(entities, use_ip, save):\n    data = setup_test_data(\"ixf.member.0\")\n    network = entities[\"net\"][\"UPDATE_ENABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    ix_updated = ixlan.ix.updated\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    # assert that the exchange's `updated` field was not\n    # altered by the import (#812)\n    ixlan.ix.refresh_from_db()\n    assert ixlan.ix.updated == ix_updated\n\n    log = importer.log[\"data\"][0]\n    assert log[\"action\"] == \"add\"\n    assert NetworkIXLan.objects.count() == 1\n\n    assert_data_change_notification([(\"netixlan\", \"add\")])\n\n    # Test idempotent\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 0\n    assert NetworkIXLan.objects.count() == 1\n\n    assert_no_emails(network, ixlan.ix)\n    assert_no_data_change_notification()\n\n    # test rollback\n    import_log = IXLanIXFMemberImportLog.objects.first()\n    import_log.rollback()\n    assert NetworkIXLan.objects.first().status == \"deleted\"\n    assert NetworkIXLan.objects.first().ipaddr4 == None\n    assert NetworkIXLan.objects.first().ipaddr6 == None"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_add_netixlan_no_routeserver", "code": "def test_add_netixlan_no_routeserver(entities, use_ip, save):\n    data = setup_test_data(\"ixf.member.4\")\n    network = entities[\"net\"][\"UPDATE_ENABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 0\n    assert NetworkIXLan.objects.count() == 1\n    assert NetworkIXLan.objects.first().is_rs_peer == False\n\n    assert_data_change_notification([(\"netixlan\", \"add\")])"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_add_netixlan_conflict_local_ixf", "code": "def test_add_netixlan_conflict_local_ixf(entities, use_ip, save):\n\n    data = setup_test_data(\"ixf.member.0\")\n    network = entities[\"net\"][\"UPDATE_ENABLED\"]\n\n    ixlan = entities[\"ixlan\"][1]  # So we have conflicts with IPAddresses\n\n    # invalid prefix space error will only be raised if\n    # the other ipaddress (v6 in this case) matches\n    # so we move that prefix over\n\n    if use_ip(4):\n        ixpfx = entities[\"ixlan\"][0].ixpfx_set.filter(protocol=\"IPv6\").first()\n        invalid_ip = 4\n    else:\n        ixpfx = entities[\"ixlan\"][0].ixpfx_set.filter(protocol=\"IPv4\").first()\n        invalid_ip = 6\n    ixpfx.ixlan = entities[\"ixlan\"][1]\n    ixpfx.save()\n\n    preexisting_ixfmember_data = IXFMemberData.objects.create(\n        asn=network.asn,\n        # Matches remote-ixf, but conflicts with IXLan\n        ipaddr4=use_ip(4, \"195.69.147.250\"),\n        # Matches remote-ixf, but conflicts with IXLan\n        ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n        ixlan=ixlan,\n        speed=10000,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n        data=json.dumps({\"foo\": \"bar\"}),\n        error=json.dumps(\n            {\"ipaddr4\": [\"IPv4 195.69.147.250 does not match any prefix on this ixlan\"]}\n        ),\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    ixfmemberdata = IXFMemberData.objects.first()\n\n    for email in IXFImportEmail.objects.all():\n        print(email.message)\n\n    if (not network.ipv4_support and invalid_ip == 4) or (\n        not network.ipv6_support and invalid_ip == 6\n    ):\n        # edge case, signature changed, and invalid ip\n        # is on unsupported protocol, making the proposal\n        # irrelevant\n\n        assert IXFMemberData.objects.count() == 0\n        assert_no_emails(network, ixlan.ix)\n        assert_idempotent(importer, ixlan, data, save=save)\n\n    elif (network.ipv4_support and not use_ip(4)) or (\n        network.ipv6_support and not use_ip(6)\n    ):\n        # edge case, signature changed, and invalid and\n        # conflicting ip changed causing a drop of the original\n        # erorring proposal, and a creation of a new one\n        # on the next one\n\n        email_info = [\n            (\n                \"CREATE\",\n                network.asn,\n                \"195.69.147.250\",\n                \"2001:7f8:1::a500:2906:1\",\n            )\n        ]\n        assert_no_emails(network, ixlan.ix)\n\n        assert IXFMemberData.objects.count() == 0\n        assert NetworkIXLan.objects.count() == 0\n\n        importer.update(ixlan, data=data)\n        importer.notify_proposals()\n\n        assert IXFMemberData.objects.count() == 1\n        assert NetworkIXLan.objects.count() == 0\n\n        assert_ix_email(ixlan.ix, email_info)\n        assert_no_network_email(network)\n\n        assert_idempotent(importer, ixlan, data, save=save)\n\n    else:\n        assert IXFMemberData.objects.count() == 1\n        assert NetworkIXLan.objects.count() == 0\n\n        assert_no_emails(network, ixlan.ix)\n        assert_idempotent(importer, ixlan, data, save=save)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_add_netixlan_conflict", "code": "def test_add_netixlan_conflict(entities, save):\n\n    data = setup_test_data(\"ixf.member.0\")\n    network = entities[\"net\"][\"UPDATE_ENABLED\"]\n    ixlan = entities[\"ixlan\"][1]  # So we have conflicts with IPAddresses\n\n    # invalid prefix space error will only be raised if\n    # the other ipaddress (v6 in this case) matches\n    # so we move that prefix over\n\n    if network.ipv6_support:\n        ixpfx = entities[\"ixlan\"][0].ixpfx_set.filter(protocol=\"IPv6\").first()\n    else:\n        ixpfx = entities[\"ixlan\"][0].ixpfx_set.filter(protocol=\"IPv4\").first()\n\n    ixpfx.ixlan = entities[\"ixlan\"][1]\n    ixpfx.save()\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    ixfmemberdata = IXFMemberData.objects.first()\n\n    if network.ipv4_support and network.ipv6_support:\n        assert IXFMemberData.objects.count() == 1\n        email_info = [\n            (\"CREATE\", network.asn, \"195.69.147.250\", \"2001:7f8:1::a500:2906:1\")\n        ]\n        assert_ix_email(ixlan.ix, email_info)\n        assert_no_network_email(network)\n\n        assert \"does not match any prefix on this ixlan\" in ixfmemberdata.error\n\n        # Assert that message to IX also includes the error\n        assert (\n            \"A validation error was raised when the IX-F importer attempted to process this change.\"\n            in IXFImportEmail.objects.filter(ix=ixlan.ix.id).first().message\n        )\n\n    else:\n        # invalid ip is on unsupported protocol, so it was ignored\n        # #771\n\n        assert IXFMemberData.objects.count() == 0\n        assert_no_emails(network, ixlan.ix)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data, save=save)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_suggest_add_local_ixf", "code": "def test_suggest_add_local_ixf(entities, use_ip, save):\n    data = setup_test_data(\"ixf.member.3\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n    ix_updated = ixlan.ix.updated\n\n    # This appears in the remote-ixf data so should not\n    # create a IXFMemberData instance\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=use_ip(4, \"195.69.147.251\"),\n            ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:3\"),\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    preexisting_ixfmember_data = IXFMemberData.objects.create(\n        # Matches remote-ixf data\n        asn=1001,\n        # Matches remote-ixf data\n        ipaddr4=use_ip(4, \"195.69.147.250\"),\n        # Matches remote-ixf data\n        ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n        ixlan=ixlan,\n        speed=10000,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        data=json.dumps({\"foo\": \"bar\"}),\n        status=\"ok\",\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    # assert that the exchange's `updated` field was not\n    # altered by the import (#812)\n    ixlan.ix.refresh_from_db()\n    assert ixlan.ix.updated == ix_updated\n\n    if (not network.ipv4_support and use_ip(4) and not use_ip(6)) or (\n        not network.ipv6_support and use_ip(6) and not use_ip(4)\n    ):\n        # edge case, supported protocols changed\n        # one of the ips on an unsupported protocol\n        # effectively changing the signature, send\n        # out create notifications for both\n\n        assert IXFMemberData.objects.count() == 3\n        assert NetworkIXLan.objects.count() == 1\n\n        email_info = [\n            (\n                \"CREATE\",\n                network.asn,\n                use_ip(6, \"195.69.147.251\"),\n                use_ip(4, \"2001:7f8:1::a500:2906:3\"),\n            )\n        ]\n\n        assert_ix_email(ixlan.ix, email_info)\n        assert_network_email(network, email_info)\n\n    elif (network.ipv4_support and network.ipv6_support and not use_ip(4)) or (\n        network.ipv4_support and network.ipv6_support and not use_ip(6)\n    ):\n        # edge case, supported protocols changed\n        # effectively changing the signature, send\n        # out modify to the existing netixlan and re-create\n        # for the existing ixfmemberdata\n\n        assert IXFMemberData.objects.count() == 3\n        assert NetworkIXLan.objects.count() == 1\n\n        email_info = [\n            (\"CREATE\", network.asn, \"195.69.147.250\", \"2001:7f8:1::a500:2906:1\"),\n            (\n                \"MODIFY\",\n                network.asn,\n                use_ip(4, \"195.69.147.251\"),\n                use_ip(6, \"2001:7f8:1::a500:2906:3\"),\n            ),\n        ]\n\n        assert_ix_email(ixlan.ix, email_info)\n        assert_network_email(network, email_info)\n\n    else:\n        assert IXFMemberData.objects.count() == 1\n        assert NetworkIXLan.objects.count() == 1\n\n        assert_no_emails(network, ixlan.ix)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data, save=save)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_suggest_add", "code": "def test_suggest_add(entities, use_ip, save):\n\n    data = setup_test_data(\"ixf.member.3\")  # asn1001\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]  # asn1001\n    ixlan = entities[\"ixlan\"][0]\n\n    # This appears in the remote-ixf data so should not\n    # create a IXFMemberData instance\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=use_ip(4, \"195.69.147.251\"),\n            ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:3\"),\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    print(importer.log)\n\n    if (not network.ipv4_support and use_ip(4) and not use_ip(6)) or (\n        not network.ipv6_support and use_ip(6) and not use_ip(4)\n    ):\n        # edge case, supported protocols changed\n        # one of the ips on an unsupported protocol\n        # effectively changing the signature, send\n        # out create with the apprp\n\n        assert IXFMemberData.objects.count() == 3\n        assert NetworkIXLan.objects.count() == 1\n\n        email_info = [\n            (\n                \"CREATE\",\n                network.asn,\n                use_ip(6, \"195.69.147.250\"),\n                use_ip(4, \"2001:7f8:1::a500:2906:1\"),\n            )\n        ]\n\n        log_250 = importer.log[\"data\"][0]\n        log_251 = importer.log[\"data\"][1]\n\n        assert log_250[\"action\"] == \"suggest-add\"\n        assert log_251[\"action\"] == \"suggest-modify\"\n\n        if use_ip(4):\n            assert log_250[\"peer\"][\"ipaddr4\"] == \"\"\n            assert log_251[\"peer\"][\"ipaddr4\"] == \"\"\n            assert log_250[\"peer\"][\"ipaddr6\"] == \"2001:7f8:1::a500:2906:1\"\n            assert log_251[\"peer\"][\"ipaddr6\"] == \"2001:7f8:1::a500:2906:3\"\n        elif use_ip(6):\n            assert log_250[\"peer\"][\"ipaddr4\"] == \"195.69.147.250\"\n            assert log_251[\"peer\"][\"ipaddr4\"] == \"195.69.147.251\"\n            assert log_250[\"peer\"][\"ipaddr6\"] == \"\"\n            assert log_251[\"peer\"][\"ipaddr6\"] == \"\"\n\n        assert_ix_email(ixlan.ix, email_info)\n        assert_network_email(network, email_info)\n\n    elif (network.ipv4_support and network.ipv6_support and not use_ip(4)) or (\n        network.ipv4_support and network.ipv6_support and not use_ip(6)\n    ):\n        # edge case, supported protocols changed\n        # effectively changing the signature, send\n        # out modify to the existing netixlan and re-create\n        # for the existing ixfmemberdata\n\n        assert IXFMemberData.objects.count() == 3\n        assert NetworkIXLan.objects.count() == 1\n\n        email_info = [\n            (\"CREATE\", network.asn, \"195.69.147.250\", \"2001:7f8:1::a500:2906:1\"),\n            (\n                \"MODIFY\",\n                network.asn,\n                use_ip(4, \"195.69.147.251\"),\n                use_ip(6, \"2001:7f8:1::a500:2906:3\"),\n            ),\n        ]\n\n        assert_ix_email(ixlan.ix, email_info)\n        assert_network_email(network, email_info)\n\n    else:\n        assert IXFMemberData.objects.count() == 1\n        assert NetworkIXLan.objects.count() == 1\n\n        log = importer.log[\"data\"][0]\n        assert log[\"action\"] == \"suggest-add\"\n\n        if network.ipv4_support and network.ipv6_support:\n            email_info = [\n                (\"CREATE\", network.asn, \"195.69.147.250\", \"2001:7f8:1::a500:2906:1\")\n            ]\n        elif network.ipv4_support:\n            email_info = [(\"CREATE\", network.asn, \"195.69.147.250\", None)]\n        elif network.ipv6_support:\n            email_info = [(\"CREATE\", network.asn, None, \"2001:7f8:1::a500:2906:1\")]\n\n        assert_ix_email(ixlan.ix, email_info)\n        assert_network_email(network, email_info)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_suggest_add_delete", "code": "def test_suggest_add_delete(entities, use_ip_alt, save):\n\n    data = setup_test_data(\"ixf.member.3\")  # asn1001\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]  # asn1001\n    ixlan = entities[\"ixlan\"][0]\n\n    # remove ip from IX-F data as per use_ip_alt fixture\n    if not use_ip_alt(4):\n        del data[\"member_list\"][0][\"connection_list\"][0][\"vlan_list\"][0][\"ipv4\"]\n    elif not use_ip_alt(6):\n        del data[\"member_list\"][0][\"connection_list\"][0][\"vlan_list\"][0][\"ipv6\"]\n\n    # we don't want the extra IX-F entry for this test\n    del data[\"member_list\"][0][\"connection_list\"][1]\n\n    # This appears in the remote-ixf data so should not\n    # create a IXFMemberData instance\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=use_ip_alt(4, \"195.69.147.252\"),\n            ipaddr6=use_ip_alt(6, \"2001:7f8:1::a500:2906:2\"),\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    if (not network.ipv6_support and not use_ip_alt(4)) or (\n        not network.ipv4_support and not use_ip_alt(6)\n    ):\n        # edge case: network not supporting the only provided ip\n        # do nothing\n        assert IXFMemberData.objects.all().count() == 0\n\n        assert_no_emails(network, ixlan.ix)\n\n    else:\n        assert IXFMemberData.objects.all().count() == 2\n\n        email_info = [\n            (\n                \"REMOVE\",\n                network.asn,\n                use_ip_alt(4, \"195.69.147.252\"),\n                use_ip_alt(6, \"2001:7f8:1::a500:2906:2\"),\n            ),\n            (\n                \"CREATE\",\n                network.asn,\n                use_ip_alt(4, \"195.69.147.250\"),\n                use_ip_alt(6, \"2001:7f8:1::a500:2906:1\"),\n            ),\n        ]\n\n        assert_ix_email(ixlan.ix, email_info)\n        assert_network_email(network, email_info)\n\n        assert (\n            IXFMemberData.objects.get(\n                ipaddr4=use_ip_alt(4, \"195.69.147.252\"),\n                ipaddr6=use_ip_alt(6, \"2001:7f8:1::a500:2906:2\"),\n            ).action\n            == \"delete\"\n        )\n\n        assert (\n            IXFMemberData.objects.get(\n                ipaddr4=use_ip_alt(4, \"195.69.147.250\"),\n                ipaddr6=use_ip_alt(6, \"2001:7f8:1::a500:2906:1\"),\n            ).action\n            == \"add\"\n        )\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_suggest_add_no_netixlan_local_ixf", "code": "def test_suggest_add_no_netixlan_local_ixf(entities, use_ip, save):\n    data = setup_test_data(\"ixf.member.1\")  # asn1001\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]  # asn1001\n    ixlan = entities[\"ixlan\"][0]\n\n    preexisting_ixfmember_data = IXFMemberData.objects.create(\n        # Matches remote-ixf data\n        asn=1001,\n        # Matches remote-ixf data\n        ipaddr4=use_ip(4, \"195.69.147.250\"),\n        # Matches remote-ixf data\n        ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n        ixlan=ixlan,\n        speed=10000,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 1\n    assert NetworkIXLan.objects.count() == 0\n\n    if (not network.ipv4_support and use_ip(4) and not use_ip(6)) or (\n        not network.ipv6_support and use_ip(6) and not use_ip(4)\n    ):\n        # edge case where the network has only one ip\n        # set and its on an unsupported protocol\n        # we re-create the ixfmemberdata and re notify the\n        # network\n\n        email_info = [\n            (\n                \"CREATE\",\n                network.asn,\n                use_ip(6, \"195.69.147.250\"),\n                use_ip(4, \"2001:7f8:1::a500:2906:1\"),\n            )\n        ]\n\n        assert_network_email(network, email_info)\n\n    elif (network.ipv4_support and network.ipv6_support and not use_ip(4)) or (\n        network.ipv4_support and network.ipv6_support and not use_ip(6)\n    ):\n        # edge case, supported protocols changed\n        # effectively changing the signature, send\n        # we re-create the ixfmemberdata and re notify the\n        # network\n\n        email_info = [\n            (\"CREATE\", network.asn, \"195.69.147.250\", \"2001:7f8:1::a500:2906:1\"),\n        ]\n\n        assert_network_email(network, email_info)\n\n    else:\n        assert_no_emails(network, ixlan.ix)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_suggest_add_no_netixlan", "code": "def test_suggest_add_no_netixlan(entities, use_ip, save):\n    data = setup_test_data(\"ixf.member.1\")  # asn1001\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]  # asn1001\n    ixlan = entities[\"ixlan\"][0]\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 1\n    assert NetworkIXLan.objects.count() == 0\n\n    log = importer.log[\"data\"][0]\n    assert log[\"action\"] == \"suggest-add\"\n\n    if network.ipv4_support and network.ipv6_support:\n        email_info = [\n            (\"CREATE\", network.asn, \"195.69.147.250\", \"2001:7f8:1::a500:2906:1\")\n        ]\n    elif network.ipv4_support:\n        email_info = [(\"CREATE\", network.asn, \"195.69.147.250\", None)]\n    elif network.ipv6_support:\n        email_info = [(\"CREATE\", network.asn, None, \"2001:7f8:1::a500:2906:1\")]\n\n    assert_network_email(network, email_info)\n\n    if network.ipv4_support and network.ipv6_support:\n        assert_no_ix_email(ixlan.ix)\n    else:\n        assert_protocol_conflict_email(network, ix=ixlan.ix)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_single_ipaddr_matches", "code": "def test_single_ipaddr_matches(entities, save):\n\n    data = setup_test_data(\"ixf.member.0\")\n    network = entities[\"net\"][\"UPDATE_ENABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.250\",\n            ipaddr6=None,\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=None,\n            ipaddr6=\"2001:7f8:1::a500:2906:1\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    if not network.ipv4_support or not network.ipv6_support:\n        # edge case\n        #\n        # one protocol is turned off, in this case we actually\n        # delete the  netixlan on the unsupported protocol and keep\n        # and keep the one on the supported protocol (since we\n        # cant update it with the unsupported ip either)\n\n        assert len(importer.log[\"data\"]) == 1\n        assert importer.log[\"data\"][0][\"action\"] == \"delete\"\n\n        assert_data_change_notification([(\"netixlan\", \"delete\")])\n    else:\n        assert len(importer.log[\"data\"]) == 3\n\n        assert NetworkIXLan.objects.filter(status=\"ok\").count() == 1\n\n        assert importer.log[\"data\"][0][\"action\"] == \"delete\"\n        assert importer.log[\"data\"][1][\"action\"] == \"delete\"\n        assert importer.log[\"data\"][2][\"action\"] == \"add\"\n\n        assert_data_change_notification(\n            [(\"netixlan\", \"delete\"), (\"netixlan\", \"delete\"), (\"netixlan\", \"add\")]\n        )\n\n    assert_no_emails(network, ixlan.ix)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_single_ipaddr_matches_no_auto_update", "code": "def test_single_ipaddr_matches_no_auto_update(entities, use_ip, save):\n\n    data = setup_test_data(\"ixf.member.1\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=use_ip(4, \"195.69.147.250\"),\n            ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    if use_ip(4) and use_ip(6):\n        assert_no_emails(network, ixlan.ix)\n        assert IXFMemberData.objects.count() == 0\n        assert NetworkIXLan.objects.count() == 1\n\n    elif (\n        (not network.ipv6_support or not network.ipv4_support)\n        and not (network.ipv4_support and use_ip(6) and not use_ip(4))\n        and not (network.ipv6_support and use_ip(4) and not use_ip(6))\n    ):\n        assert len(importer.log[\"data\"]) == 0\n        assert_no_emails(network, ixlan.ix)\n\n    else:\n        # Assert NetworkIXLan is unchanged\n        assert NetworkIXLan.objects.filter(status=\"ok\").count() == 1\n\n        # We consolidate notifications into a single MODIFY\n        assert len(importer.log[\"data\"]) == 1\n        assert importer.log[\"data\"][0][\"action\"] == \"suggest-modify\"\n\n        ixf_member_del = IXFMemberData.objects.filter(\n            requirement_of__isnull=False\n        ).first()\n        ixf_member_add = IXFMemberData.objects.filter(\n            requirement_of__isnull=True\n        ).first()\n\n        assert ixf_member_del.requirement_of == ixf_member_add\n        assert ixf_member_add.action == \"modify\"\n\n        netixlan = NetworkIXLan.objects.filter(status=\"ok\").first()\n\n        email_info = [(\"MODIFY\", network.asn, netixlan.ipaddr4, netixlan.ipaddr6)]\n        assert_ix_email(ixlan.ix, email_info)\n        assert_network_email(network, email_info)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_816_edge_case", "code": "def test_816_edge_case(entities, use_ip, save):\n\n    data = setup_test_data(\"ixf.member.1\")\n    network = entities[\"net\"][\"UPDATE_DISABLED_2\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=use_ip(4, \"195.69.147.250\"),\n            ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:1\"),\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 2\n    assert IXFMemberData.objects.get(asn=1001).action == \"add\"\n\n    assert IXFImportEmail.objects.filter(\n        net__asn=1001, message__contains=\"CREATE\"\n    ).exists()\n    assert not IXFImportEmail.objects.filter(\n        net__asn=1001, message__contains=\"MODIFY\"\n    ).exists()\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_two_missing_ipaddrs_no_auto_update", "code": "def test_two_missing_ipaddrs_no_auto_update(entities, save):\n\n    data = setup_test_data(\"ixf.member.1\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.250\",\n            ipaddr6=None,\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=None,\n            ipaddr6=\"2001:7f8:1::a500:2906:1\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n    # Assert NetworkIXLans are unchanged\n    assert NetworkIXLan.objects.filter(status=\"ok\").count() == 2\n\n    if not network.ipv4_support or not network.ipv6_support:\n        # only one of the protocols is supported by the network\n        # suggest deletion of the other ip address\n\n        assert IXFMemberData.objects.count() == 1\n\n        assert importer.log[\"data\"][0][\"action\"] == \"suggest-delete\"\n\n        if not network.ipv4_support:\n            ipaddr4 = \"195.69.147.250\"\n        else:\n            ipaddr4 = None\n\n        if not network.ipv6_support:\n            ipaddr6 = \"2001:7f8:1::a500:2906:1\"\n        else:\n            ipaddr6 = None\n\n        email_info = [(\"REMOVE\", network.asn, ipaddr4, ipaddr6)]\n\n        assert_network_email(network, email_info)\n        assert_ix_email(ixlan.ix, email_info)\n\n    else:\n        # On the IXFMemberData side, we create instances\n        # for two deletions and one addition.\n        # The deletions will be the requirement of the addition.\n\n        assert IXFMemberData.objects.count() == 3\n        ixfmdata_d4 = IXFMemberData.objects.filter(\n            ipaddr4=\"195.69.147.250\", ipaddr6=None\n        ).first()\n        ixfmdata_d6 = IXFMemberData.objects.filter(\n            ipaddr4=None, ipaddr6=\"2001:7f8:1::a500:2906:1\"\n        ).first()\n        ixfmdata_m = IXFMemberData.objects.filter(\n            ipaddr4=\"195.69.147.250\", ipaddr6=\"2001:7f8:1::a500:2906:1\"\n        ).first()\n\n        assert ixfmdata_d4.action == \"delete\"\n        assert ixfmdata_d6.action == \"delete\"\n        assert ixfmdata_m.action == \"modify\"\n        assert ixfmdata_d4.requirement_of == ixfmdata_m\n        assert ixfmdata_d6.requirement_of == ixfmdata_m\n\n        assert ixfmdata_m.primary_requirement == ixfmdata_d4\n        assert ixfmdata_m.secondary_requirements == [ixfmdata_d6]\n\n        # We consolidate notifications into a single MODIFY\n        assert len(importer.log[\"data\"]) == 1\n        assert importer.log[\"data\"][0][\"action\"] == \"suggest-modify\"\n\n        # We only create an email for the primary requirement\n        email_info_4 = [(\"MODIFY\", network.asn, \"195.69.147.250\", \"IPv6 not set\")]\n        assert IXFImportEmail.objects.count() == 2\n        assert_ix_email(ixlan.ix, email_info_4)\n        assert_network_email(network, email_info_4)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_delete", "code": "def test_delete(entities, save):\n    data = setup_test_data(\"ixf.member.0\")\n    network = entities[\"net\"][\"UPDATE_ENABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n    ix_updated = ixlan.ix.updated\n\n    with reversion.create_revision():\n        entities[\"netixlan\"].append(\n            NetworkIXLan.objects.create(\n                network=network,\n                ixlan=ixlan,\n                asn=network.asn,\n                speed=10000,\n                ipaddr4=\"195.69.147.250\",\n                ipaddr6=\"2001:7f8:1::a500:2906:1\",\n                status=\"ok\",\n                is_rs_peer=True,\n                operational=True,\n            )\n        )\n\n        entities[\"netixlan\"].append(\n            NetworkIXLan.objects.create(\n                network=network,\n                ixlan=ixlan,\n                asn=network.asn,\n                speed=20000,\n                ipaddr4=\"195.69.147.251\",\n                ipaddr6=None,\n                status=\"ok\",\n                is_rs_peer=False,\n                operational=False,\n            )\n        )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    # assert that the exchange's `updated` field was not\n    # altered by the import (#812)\n    ixlan.ix.refresh_from_db()\n    assert ixlan.ix.updated == ix_updated\n\n    assert len(importer.log[\"data\"]) == 1\n    log = importer.log[\"data\"][0]\n\n    assert log[\"action\"] == \"delete\"\n    assert NetworkIXLan.objects.filter(status=\"ok\").count() == 1\n    assert_data_change_notification([(\"netixlan\", \"delete\")])\n    assert_no_emails(network, ixlan.ix)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)\n\n    # test rollback\n    import_log = IXLanIXFMemberImportLog.objects.first()\n    import_log.rollback()\n    assert NetworkIXLan.objects.filter(status=\"ok\").count() == 2"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_suggest_delete_local_ixf_has_flag", "code": "def test_suggest_delete_local_ixf_has_flag(entities, save):\n    data = setup_test_data(\"ixf.member.1\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.250\",\n            ipaddr6=\"2001:7f8:1::a500:2906:1\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=20000,\n            ipaddr4=\"195.69.147.251\",\n            ipaddr6=None,\n            status=\"ok\",\n            is_rs_peer=False,\n            operational=False,\n        )\n    )\n\n    preexisting_ixfmember_data = IXFMemberData.objects.create(\n        asn=1001,\n        ipaddr4=\"195.69.147.251\",\n        ipaddr6=None,\n        ixlan=ixlan,\n        speed=10000,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n        data={},  # Makes self.remote_data_missing and self.marked_for_removal True\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert NetworkIXLan.objects.count() == 2\n    assert IXFMemberData.objects.count() == 1\n\n    assert_no_emails(network, ixlan.ix)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_suggest_delete_local_ixf_no_flag", "code": "def test_suggest_delete_local_ixf_no_flag(entities, save):\n    data = setup_test_data(\"ixf.member.1\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.250\",\n            ipaddr6=\"2001:7f8:1::a500:2906:1\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=20000,\n            ipaddr4=\"195.69.147.251\",\n            ipaddr6=None,\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    ixf_member_data_field = {\n        \"ixp_id\": 42,\n        \"connected_since\": \"2009-02-04T00:00:00Z\",\n        \"state\": \"connected\",\n        \"if_list\": [{\"switch_id\": 1, \"if_speed\": 20000, \"if_type\": \"LR4\"}],\n        \"vlan_list\": [\n            {\n                \"vlan_id\": 0,\n                \"ipv4\": {\n                    \"address\": \"195.69.147.251\",\n                    \"routeserver\": True,\n                    \"max_prefix\": 42,\n                    \"as_macro\": \"AS-NFLX-V4\",\n                    \"mac_address\": [\"00:0a:95:9d:68:16\"],\n                },\n            }\n        ],\n    }\n\n    preexisting_ixfmember_data = IXFMemberData.objects.create(\n        asn=1001,\n        ipaddr4=\"195.69.147.251\",\n        ipaddr6=None,\n        ixlan=ixlan,\n        speed=20000,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n        data=json.dumps(ixf_member_data_field),\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert importer.log[\"data\"][0][\"action\"] == \"suggest-delete\"\n    assert NetworkIXLan.objects.count() == 2\n\n    assert IXFMemberData.objects.count() == 1\n\n    email_info = [(\"REMOVE\", 1001, \"195.69.147.251\", \"IPv6 not set\")]\n    assert_ix_email(ixlan.ix, email_info)\n    assert_network_email(network, email_info)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_suggest_delete_no_local_ixf", "code": "def test_suggest_delete_no_local_ixf(entities, save):\n\n    data = setup_test_data(\"ixf.member.1\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.250\",\n            ipaddr6=\"2001:7f8:1::a500:2906:1\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=20000,\n            ipaddr4=\"195.69.147.251\",\n            ipaddr6=None,\n            status=\"ok\",\n            is_rs_peer=False,\n            operational=False,\n        )\n    )\n\n    importer = ixf.Importer()\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert importer.log[\"data\"][0][\"action\"] == \"suggest-delete\"\n    assert NetworkIXLan.objects.count() == 2\n    assert IXFMemberData.objects.count() == 1\n\n    email_info = [(\"REMOVE\", 1001, \"195.69.147.251\", \"IPv6 not set\")]\n    assert_ix_email(ixlan.ix, email_info)\n    assert_network_email(network, email_info)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_mark_invalid_remote_w_local_ixf_auto_update", "code": "def test_mark_invalid_remote_w_local_ixf_auto_update(entities, save):\n    data = setup_test_data(\"ixf.member.invalid.0\")\n    network = entities[\"net\"][\"UPDATE_ENABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    # Just to create a connection between the network and ix\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.200\",\n            ipaddr6=\"2001:7f8:1::a500:2906:2\",\n            status=\"ok\",\n            is_rs_peer=False,\n            operational=True,\n        )\n    )\n\n    preexisting_ixfmember_data = IXFMemberData.objects.create(\n        asn=2906,\n        ipaddr4=\"195.69.147.200\",\n        ipaddr6=\"2001:7f8:1::a500:2906:2\",\n        ixlan=ixlan,\n        speed=0,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n        error=json.dumps({\"speed\": \"Invalid speed value: this is not valid\"}),\n    )\n\n    preexisting_ixfmember_data = IXFMemberData.objects.create(\n        asn=2906,\n        ipaddr4=\"195.69.147.100\",\n        ipaddr6=\"2001:7f8:1::a500:2906:4\",\n        ixlan=ixlan,\n        speed=0,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n        error=json.dumps({\"speed\": \"Invalid speed value: this is not valid\"}),\n    )\n\n    importer = ixf.Importer()\n    data = importer.sanitize(data)\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    # #793 count should be 2 if we were not ignoring changes\n    # to is_rs_peer and speed, but because we currently are\n    # one of the pre-existing ixfmemberdata entries gets resolved\n    assert IXFMemberData.objects.count() == 1\n\n    assert_no_emails(network, ixlan.ix)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_mark_invalid_remote_auto_update", "code": "def test_mark_invalid_remote_auto_update(entities, save):\n\n    data = setup_test_data(\"ixf.member.invalid.0\")\n    network = entities[\"net\"][\"UPDATE_ENABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    # Just to create a connection between the network and ix\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.200\",\n            ipaddr6=\"2001:7f8:1::a500:2906:2\",\n            status=\"ok\",\n            is_rs_peer=False,\n            operational=True,\n        )\n    )\n\n    importer = ixf.Importer()\n    data = importer.sanitize(data)\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert NetworkIXLan.objects.count() == 1\n    assert IXFMemberData.objects.count() == 1\n\n    # We email to say there is invalid data\n    if network.ipv4_support and network.ipv6_support:\n        email_info = [\n            (\"CREATE\", network.asn, \"195.69.147.100\", \"2001:7f8:1::a500:2906:4\"),\n            # #793 no modifies to speed or is_rs_peer for now\n            # (\"MODIFY\", network.asn, \"195.69.147.200\", \"2001:7f8:1::a500:2906:2\"),\n        ]\n    elif network.ipv4_support:\n        email_info = [\n            (\"CREATE\", network.asn, \"195.69.147.100\", None),\n            # #793 no modifies to speed or is_rs_peer for now\n            # (\"MODIFY\", network.asn, \"195.69.147.200\", \"2001:7f8:1::a500:2906:2\"),\n        ]\n    elif network.ipv6_support:\n        email_info = [\n            (\"CREATE\", network.asn, None, \"2001:7f8:1::a500:2906:4\"),\n            # #793 no modifies to speed or is_rs_peer for now\n            # (\"MODIFY\", network.asn, \"195.69.147.200\", \"2001:7f8:1::a500:2906:2\"),\n        ]\n\n    assert_ix_email(ixlan.ix, email_info)\n    assert (\n        \"Invalid speed value: This is invalid\"\n        in IXFImportEmail.objects.filter(ix=ixlan.ix.id).first().message\n    )\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_mark_invalid_remote_w_local_ixf_no_auto_update", "code": "def test_mark_invalid_remote_w_local_ixf_no_auto_update(entities, save):\n\n    data = setup_test_data(\"ixf.member.invalid.1\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    # Just to create a connection between the network and ix\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.200\",\n            ipaddr6=\"2001:7f8:1::a500:2906:2\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    # this will get resolved since invalid speed means no changes\n    # to the existing netixlan, thus it becomes noop (#792)\n\n    preexisting_ixfmember_data = IXFMemberData.objects.create(\n        asn=1001,\n        ipaddr4=\"195.69.147.200\",\n        ipaddr6=\"2001:7f8:1::a500:2906:2\",\n        ixlan=ixlan,\n        speed=0,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n        error=json.dumps({\"speed\": \"Invalid speed value: this is not valid\"}),\n    )\n\n    # this suggests adding a new netixlan, and will be made\n    # but with an error note attached that the speed could\n    # not be parsed (#792)\n\n    preexisting_ixfmember_data = IXFMemberData.objects.create(\n        asn=1001,\n        ipaddr4=\"195.69.147.100\",\n        ipaddr6=\"2001:7f8:1::a500:2906:4\",\n        ixlan=ixlan,\n        speed=0,\n        fetched=datetime.datetime.now(datetime.timezone.utc),\n        operational=True,\n        is_rs_peer=True,\n        status=\"ok\",\n        error=json.dumps({\"speed\": \"Invalid speed value: this is not valid\"}),\n    )\n\n    importer = ixf.Importer()\n    data = importer.sanitize(data)\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    # for email in IXFImportEmail.objects.all():\n    #   print(email.message)\n\n    for ixf_member in IXFMemberData.objects.all():\n        print(ixf_member, ixf_member.id)\n\n    assert IXFMemberData.objects.count() == 1\n    assert_no_emails(network, ixlan.ix)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_mark_invalid_remote_no_auto_update", "code": "def test_mark_invalid_remote_no_auto_update(entities, save):\n\n    data = setup_test_data(\"ixf.member.invalid.2\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    # Just to create a connection between the network and ix\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.200\",\n            ipaddr6=\"2001:7f8:1::a500:2906:2\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    importer = ixf.Importer()\n    data = importer.sanitize(data)\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 1\n\n    # We send an email about the updates\n    # But it also contains information about the invalid speed\n    if network.ipv4_support and network.ipv6_support:\n        email_info = [\n            (\"CREATE\", network.asn, \"195.69.147.100\", \"2001:7f8:1::a500:2906:4\"),\n            # #793 no modifies to speed or is_rs_peer for now\n            # (\"MODIFY\", network.asn, \"195.69.147.200\", \"2001:7f8:1::a500:2906:2\"),\n        ]\n    elif network.ipv4_support:\n        email_info = [\n            (\"CREATE\", network.asn, \"195.69.147.100\", None),\n            # #793 no modifies to speed or is_rs_peer for now\n            # (\"MODIFY\", network.asn, \"195.69.147.200\", \"2001:7f8:1::a500:2906:2\"),\n        ]\n    elif network.ipv6_support:\n        email_info = [\n            (\"CREATE\", network.asn, None, \"2001:7f8:1::a500:2906:4\"),\n            # #793 no modifies to speed or is_rs_peer for now\n            # (\"MODIFY\", network.asn, \"195.69.147.200\", \"2001:7f8:1::a500:2906:2\"),\n        ]\n\n    assert_ix_email(ixlan.ix, email_info)\n\n    assert (\n        \"Invalid speed value: This is invalid\" in IXFImportEmail.objects.first().message\n    )\n    assert_no_network_email(network)\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_notify_duplicate_ip", "code": "\ndef test_notify_duplicate_ip(entities, save):\n    data = setup_test_data(\"ixf.member.dupe.ip\")\n    ixlan = entities[\"ixlan\"][0]\n    start = datetime.datetime.now(datetime.timezone.utc)\n\n    importer = ixf.Importer()\n    data = importer.sanitize(data)\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    assert importer.update(ixlan, data=data) == False\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 0\n    assert IXFImportEmail.objects.count() == 1\n    ERROR_MESSAGE = data.get(\"pdb_error\")\n    assert \"assigned to more than one distinct connection\" in ERROR_MESSAGE\n    assert importer.ixlan.ixf_ixp_import_error_notified > start\n    assert ERROR_MESSAGE in importer.ixlan.ixf_ixp_import_error\n    assert (\n        ERROR_MESSAGE in IXFImportEmail.objects.filter(ix=ixlan.ix.id).first().message\n    )\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_mark_invalid_multiple_vlans", "code": "def test_mark_invalid_multiple_vlans(entities, save):\n\n    data = setup_test_data(\"ixf.member.invalid.vlan\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n    start = datetime.datetime.now(datetime.timezone.utc)\n\n    importer = ixf.Importer()\n    data = importer.sanitize(data)\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    assert importer.update(ixlan, data=data) == False\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 0\n    assert IXFImportEmail.objects.filter(ix=ixlan.ix.id).count() == 1\n    ERROR_MESSAGE = \"We found that your IX-F output contained multiple VLANs\"\n    assert importer.ixlan.ixf_ixp_import_error_notified > start  # This sets the lock\n    assert ERROR_MESSAGE in importer.ixlan.ixf_ixp_import_error\n    assert (\n        ERROR_MESSAGE in IXFImportEmail.objects.filter(ix=ixlan.ix.id).first().message\n    )\n\n    # Assert idempotent / lock\n    importer.update(ixlan, data=data)\n\n    assert ERROR_MESSAGE in importer.ixlan.ixf_ixp_import_error\n\n    for email in IXFImportEmail.objects.filter(ix=ixlan.ix.id):\n        print(email.message)\n\n    assert IXFImportEmail.objects.filter(ix=ixlan.ix.id).count() == 1\n\n    # Test idempotent\n    assert_idempotent(importer, ixlan, data)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_vlan_null_ips", "code": "def test_vlan_null_ips(entities, save):\n\n    data = setup_test_data(\"ixf.member.null.ip.vlan\")\n    ixlan = entities[\"ixlan\"][0]\n\n    importer = ixf.Importer()\n    importer.sanitize(data)\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert importer.ixlan.ixf_ixp_import_error_notified is None\n    assert importer.ixlan.ixf_ixp_import_error is None\n    assert_no_emails(ix=ixlan.ix)\n\n    # Assert idempotent / lock\n    importer.sanitize(data)\n    importer.update(ixlan, data=data)\n\n    assert importer.ixlan.ixf_ixp_import_error_notified is None\n    assert importer.ixlan.ixf_ixp_import_error is None\n    assert_no_emails(ix=ixlan.ix)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_vlan_null_vlan_list_and_if_list", "code": "def test_vlan_null_vlan_list_and_if_list(entities, save):\n\n    data = setup_test_data(\"ixf.member.null.vlan_list_and_if_list\")\n    ixlan = entities[\"ixlan\"][0]\n\n    importer = ixf.Importer()\n    importer.sanitize(data)\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert importer.ixlan.ixf_ixp_import_error_notified is None\n    assert importer.ixlan.ixf_ixp_import_error is None\n    assert_no_emails(ix=ixlan.ix)\n\n    # Assert idempotent / lock\n    importer.sanitize(data)\n    importer.update(ixlan, data=data)\n\n    assert importer.ixlan.ixf_ixp_import_error_notified is None\n    assert importer.ixlan.ixf_ixp_import_error is None\n    assert_no_emails(ix=ixlan.ix)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_vlan_list_empty", "code": "def test_vlan_list_empty(entities, save):\n    data = setup_test_data(\"ixf.member.vlan_list_empty\")\n    ixlan = entities[\"ixlan\"][0]\n\n    importer = ixf.Importer()\n    importer.sanitize(data)\n\n    if not save:\n        return assert_idempotent(importer, ixlan, data, save=False)\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert importer.ixlan.ixf_ixp_import_error_notified is None\n    assert importer.ixlan.ixf_ixp_import_error is None\n    assert_no_emails(ix=ixlan.ix)\n\n    # Assert idempotent / lock\n    importer.sanitize(data)\n    importer.update(ixlan, data=data)\n\n    assert importer.ixlan.ixf_ixp_import_error_notified is None\n    assert importer.ixlan.ixf_ixp_import_error is None\n    assert_no_emails(ix=ixlan.ix)"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_create_deskpro_tickets_after_x_days", "code": "\ndef test_create_deskpro_tickets_after_x_days(entities):\n    data = setup_test_data(\"ixf.member.2\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    # disable while #793 is active\n    \"\"\"\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.252\",\n            ipaddr6=\"2001:7f8:1::a500:2906:2\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n    \"\"\"\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.240\",\n            ipaddr6=\"2001:7f8:1::a500:2905:1\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n    importer = ixf.Importer()\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    for ixfmd in IXFMemberData.objects.all():\n        # Edit so that they've been created two weeks ago\n        ixfmd.created = datetime.datetime.now(\n            datetime.timezone.utc\n        ) - datetime.timedelta(days=14)\n        ixfmd.save()\n        print(ixfmd.ixf_id, ixfmd.action)\n\n    importer.update(ixlan, data=data)\n\n    # Assert IXFMemberData still the same\n    assert IXFMemberData.objects.count() == 4\n\n    \"\"\"\n    # Assert DeskProTickets are created\n    assert DeskProTicket.objects.count() == 4\n\n    # Assert emails go to IX and Network for each Ticket\n    deskpro_refs = [dpt.deskpro_ref for dpt in DeskProTicket.objects.all()]\n    for dpt in deskpro_refs:\n        assert IXFImportEmail.objects.filter(\n            subject__contains=dpt, ix=ixlan.ix.id\n        ).exists()\n        assert IXFImportEmail.objects.filter(\n            subject__contains=dpt, net=network.id\n        ).exists()\n    \"\"\"\n\n    # Per issue #860, we no longer create the DeskProTickets\n    # after x days\n    assert DeskProTicket.objects.count() == 0"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_create_deskpro_tickets_no_contacts", "code": "def test_create_deskpro_tickets_no_contacts(entities):\n    data = setup_test_data(\"ixf.member.6\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    network2 = entities[\"net\"][\"UPDATE_DISABLED_2\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    # Delete contacts\n    for netcontact in entities[\"netcontact\"]:\n        netcontact.delete()\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.251\",\n            ipaddr6=\"2001:7f8:1::a500:2906:2\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.240\",\n            ipaddr6=\"2001:7f8:1::a500:2905:1\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network2,\n            ixlan=ixlan,\n            asn=network2.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.239\",\n            ipaddr6=\"2001:7f8:1::a500:2100:1\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    importer = ixf.Importer()\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    # Issue 883: Assert a single consolidated ticket is created\n    assert DeskProTicket.objects.count() == 2\n    for ticket in DeskProTicket.objects.all():\n        assert ticket.cc_set.count() == 0\n\n    assert DeskProTicket.objects.filter(subject__contains=str(network.asn)).exists()\n    assert DeskProTicket.objects.filter(subject__contains=str(network2.asn)).exists()"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_email_with_partial_contacts", "code": "\ndef test_email_with_partial_contacts(entities):\n    data = setup_test_data(\"ixf.member.2\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    # Delete network contact but keep ix contact\n    for netcontact in entities[\"netcontact\"]:\n        netcontact.delete()\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.251\",\n            ipaddr6=\"2001:7f8:1::a500:2906:2\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.240\",\n            ipaddr6=\"2001:7f8:1::a500:2905:1\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n    importer = ixf.Importer()\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    # Issue 883: Assert a single consolidated ticket is created\n    assert DeskProTicket.objects.count() == 1\n    for ticket in DeskProTicket.objects.all():\n        assert ticket.cc_set.count() == 0"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_no_email_if_deskpro_fails", "code": "def test_no_email_if_deskpro_fails(entities, use_ip, save):\n\n    data = setup_test_data(\"ixf.member.2\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    # Delete network contacts\n    for netcontact in entities[\"netcontact\"]:\n        netcontact.delete()\n\n    # Keep IX contacts. Ordinarily this would trigger an email to the IX\n    # However since the deskPRO API response will fail,\n    # no emails should get sent.\n\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.251\",\n            ipaddr6=\"2001:7f8:1::a500:2906:2\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.240\",\n            ipaddr6=\"2001:7f8:1::a500:2905:1\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n    importer = ixf.Importer()\n    importer._deskpro_client = FailingMockAPIClient\n\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    # Issue 883: Assert a single consolidated ticket is created\n    assert DeskProTicket.objects.count() == 1\n    for ticket in DeskProTicket.objects.all():\n        assert ticket.cc_set.count() == 0\n\n    # This is the single consolidated email\n    assert IXFImportEmail.objects.count() == 1"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_resolve_deskpro_ticket", "code": "\ndef test_resolve_deskpro_ticket(entities):\n    data = setup_test_data(\"ixf.member.1\")\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]\n    ixlan = entities[\"ixlan\"][0]\n\n    importer = ixf.Importer()\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    assert IXFMemberData.objects.count() == 1\n    ixf_member_data = IXFMemberData.objects.first()\n\n    assert not ixf_member_data.deskpro_id\n    assert not ixf_member_data.deskpro_ref\n\n    # Edit so that they've been created two weeks ago\n    ixf_member_data.created = datetime.datetime.now(\n        datetime.timezone.utc\n    ) - datetime.timedelta(days=14)\n    ixf_member_data.save_without_update()\n\n    # re run importer to create tickets\n    importer.notifications = []\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    # Per issue #860 we no longer create tickets for conflict resolution\n    # just based on age\n    assert DeskProTicket.objects.count() == 0\n\n    # Commented out bc of issue #860\n    \"\"\"\n    ticket = DeskProTicket.objects.first()\n    assert ticket.deskpro_id\n    assert ticket.deskpro_ref\n\n    # 1 member data instance\n    assert IXFMemberData.objects.count() == 1\n    ixf_member_data = IXFMemberData.objects.first()\n    assert ixf_member_data.deskpro_id == ticket.deskpro_id\n    assert ixf_member_data.deskpro_ref == ticket.deskpro_ref\n\n    # 4 emails total\n    # 2 emails for initial consolidated notification\n    # 2 emails for ticket\n    if network.ipv4_support and network.ipv6_support:\n        assert IXFImportEmail.objects.count() == 3\n    else:\n        assert IXFImportEmail.objects.count() == 4\n    conflict_emails = IXFImportEmail.objects.filter(subject__icontains=\"conflict\")\n    assert conflict_emails.count() == 2\n    \"\"\"\n\n    consolid_emails = IXFImportEmail.objects.exclude(subject__icontains=\"conflict\")\n    for email in consolid_emails:\n        # if network is only supporting one ip protocol\n        # since the ix is sending both it should be mentioned\n        if not network.ipv4_support:\n            assert \"IX-F data provides IPv4 addresses\" in email.message\n        if not network.ipv6_support:\n            assert \"IX-F data provides IPv6 addresses\" in email.message\n\n    # for email in conflict_emails:\n    #     assert ticket.deskpro_ref in email.subject\n\n    # Resolve issue\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=\"195.69.147.250\",\n            ipaddr6=\"2001:7f8:1::a500:2906:1\",\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    # Re run import to notify resolution\n    importer.notifications = []\n    importer.update(ixlan, data=data)\n    importer.notify_proposals()\n\n    # resolved\n    assert IXFMemberData.objects.count() == 0\n\n    # Commented out bc of issue #860\n    \"\"\"\n    # assert DeskProTicket.objects.count() == 2\n\n    ticket_r = DeskProTicket.objects.last()\n    assert ticket_r.deskpro_id == ticket.deskpro_id\n    assert ticket_r.deskpro_ref == ticket.deskpro_ref\n    assert \"resolved\" in ticket_r.body\n\n\n    conflict_emails = IXFImportEmail.objects.filter(subject__icontains=\"conflict\")\n    assert conflict_emails.count() == 4\n\n    for email in conflict_emails.order_by(\"-id\")[:2]:\n        assert \"resolved\" in email.message\n        assert ticket.deskpro_ref in email.subject\n    \"\"\""}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_ixf_member_import_protocol.py", "class_name": null, "function_name": "test_send_email", "code": "\ndef test_send_email(entities, use_ip):\n    # Setup is from test_suggest_add()\n    print(f\"Debug mode for mail: {settings.MAIL_DEBUG}\")\n    data = setup_test_data(\"ixf.member.3\")  # asn1001\n    network = entities[\"net\"][\"UPDATE_DISABLED\"]  # asn1001\n    ixlan = entities[\"ixlan\"][0]\n\n    # This appears in the remote-ixf data so should not\n    # create a IXFMemberData instance\n    entities[\"netixlan\"].append(\n        NetworkIXLan.objects.create(\n            network=network,\n            ixlan=ixlan,\n            asn=network.asn,\n            speed=10000,\n            ipaddr4=use_ip(4, \"195.69.147.251\"),\n            ipaddr6=use_ip(6, \"2001:7f8:1::a500:2906:3\"),\n            status=\"ok\",\n            is_rs_peer=True,\n            operational=True,\n        )\n    )\n\n    importer = ixf.Importer()\n    importer.update(ixlan, data=data)\n\n    # This should actually send an email\n    importer.notify_proposals()\n    assert importer.emails == 2"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_cmd_ixf_ixp_member_import.py", "class_name": null, "function_name": "test_reset_email", "code": "\ndef test_reset_email(entities, data_cmd_ixf_email):\n    ixf_import_data = json.loads(data_cmd_ixf_email.json)\n    importer = ixf.Importer()\n    ixlan = entities[\"ixlan\"]\n    # Create IXFMemberData\n    importer.update(ixlan, data=ixf_import_data)\n    importer.notify_proposals()\n    assert IXFImportEmail.objects.count() == 1\n\n    call_command(\"pdb_ixf_ixp_member_import\", reset_email=True, commit=True)\n\n    assert IXFImportEmail.objects.count() == 0\n    assert DeskProTicket.objects.filter(body__contains=\"reset_email\").count() == 1"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_cmd_ixf_ixp_member_import.py", "class_name": null, "function_name": "test_reset_all", "code": "\ndef test_reset_all(entities, deskprotickets, data_cmd_ixf_reset):\n    ixf_import_data = json.loads(data_cmd_ixf_reset.json)\n    importer = ixf.Importer()\n    ixlan = entities[\"ixlan\"]\n    # Create IXFMemberData\n    importer.update(ixlan, data=ixf_import_data)\n    importer.notify_proposals()\n\n    assert DeskProTicket.objects.count() == 5\n    assert IXFMemberData.objects.count() == 4\n    assert IXFImportEmail.objects.count() == 1\n\n    call_command(\"pdb_ixf_ixp_member_import\", reset=True, commit=True)\n\n    assert DeskProTicket.objects.count() == 2\n    assert DeskProTicket.objects.filter(body__contains=\"reset\").count() == 1\n    assert IXFMemberData.objects.count() == 0\n    assert IXFImportEmail.objects.count() == 0"}, {"file_path": "/peeringdb-2.57.0/peeringdb-2.57.0/tests/test_cmd_ixf_ixp_member_import.py", "class_name": null, "function_name": "test_reset_all", "code": "\ndef test_reset_all(entities, deskprotickets, data_cmd_ixf_reset):\n    ixf_import_data = json.loads(data_cmd_ixf_reset.json)\n    importer = ixf.Importer()\n    ixlan = entities[\"ixlan\"]\n    # Create IXFMemberData\n    importer.update(ixlan, data=ixf_import_data)\n    importer.notify_proposals()\n\n    assert DeskProTicket.objects.count() == 5\n    assert IXFMemberData.objects.count() == 4\n    assert IXFImportEmail.objects.count() == 1\n\n    call_command(\"pdb_ixf_ixp_member_import\", reset=True, commit=True)\n\n    assert DeskProTicket.objects.count() == 2\n    assert DeskProTicket.objects.filter(body__contains=\"reset\").count() == 1\n    assert IXFMemberData.objects.count() == 0\n    assert IXFImportEmail.objects.count() == 0"}]}, {"git_group": "SeldonIO", "git_name": "alibi-detect", "version": "v0.12.0", "language": "Python", "project_name": "alibi-detect-v0.12.0.zip", "file_path": "/alibi-detect-v0.12.0/alibi-detect-0.12.0/alibi_detect/datasets.py", "file_name": "datasets.py", "focal_class": null, "focal_name": "fetch_nab", "focal_parameter": [], "solution": "def fetch_nab(ts: str,\n              return_X_y: bool = False\n              ) -> Union[Bunch, Tuple[pd.DataFrame, pd.DataFrame]]:\n    url_labels = 'https://raw.githubusercontent.com/numenta/NAB/master/labels/combined_labels.json'\n    try:\n        resp = requests.get(url_labels, timeout=TIMEOUT)\n        resp.raise_for_status()\n    except RequestException:\n        logger.exception(\"Could not connect, URL may be out of service\")\n        raise\n    labels_json = resp.json()\n    outliers = labels_json[ts + '.csv']\n    if not outliers:\n        logger.warning('The dataset does not contain any outliers.')\n    url = 'https://raw.githubusercontent.com/numenta/NAB/master/data/' + ts + '.csv'\n    df = pd.read_csv(url, header=0, index_col=0)\n    labels = np.zeros(df.shape[0])\n    for outlier in outliers:\n        outlier_id = np.where(df.index == outlier)[0][0]\n        labels[outlier_id] = 1\n    df.index = pd.to_datetime(df.index)\n    df_labels = pd.DataFrame(data={'is_outlier': labels}, index=df.index)\n\n    if return_X_y:\n        return df, df_labels\n\n    return Bunch(data=df,\n                 target=df_labels,\n                 target_names=['normal', 'outlier'])", "function_signature": "def fetch_nab(ts: str,\n              return_X_y: bool = False\n              ) -> Union[Bunch, Tuple[pd.DataFrame, pd.DataFrame]] :", "left_context": "import io\nimport logging\nfrom io import BytesIO\nfrom typing import List, Tuple, Type, Union\nfrom xml.etree import ElementTree\n\nimport dill\nimport numpy as np\nimport pandas as pd\nimport requests\nfrom alibi_detect.utils.data import Bunch\nfrom alibi_detect.utils.url import _join_url\nfrom requests import RequestException\nfrom urllib.error import URLError\nfrom scipy.io import arff\nfrom sklearn.datasets import fetch_kddcup99\n\n# do not extend pickle dispatch table so as not to change pickle behaviour\ndill.extend(use_dill=False)\n\npd.options.mode.chained_assignment = None  # default='warn'\n\nlogger = logging.getLogger(__name__)\n\n\"\"\"Number of seconds to wait for URL requests before raising an error.\"\"\"\nTIMEOUT = 10\n\n\ndef fetch_kdd(target: list = ['dos', 'r2l', 'u2r', 'probe'],\n              keep_cols: list = ['srv_count', 'serror_rate', 'srv_serror_rate',\n                                 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n                                 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n                                 'dst_host_srv_count', 'dst_host_same_srv_rate',\n                                 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n                                 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n                                 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n                                 'dst_host_srv_rerror_rate'],\n              percent10: bool = True,\n              return_X_y: bool = False) -> Union[Bunch, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    KDD Cup '99 dataset. Detect computer network intrusions.\n\n    Parameters\n    ----------\n    target\n        List with attack types to detect.\n    keep_cols\n        List with columns to keep. Defaults to continuous features.\n    percent10\n        Bool, whether to only return 10% of the data.\n    return_X_y\n        Bool, whether to only return the data and target values or a Bunch object.\n\n    Returns\n    -------\n    Bunch\n        Dataset and outlier labels (0 means 'normal' and 1 means 'outlier').\n    (data, target)\n        Tuple if 'return_X_y' equals True.\n    \"\"\"\n\n    # fetch raw data\n    try:\n        data_raw = fetch_kddcup99(subset=None, data_home=None, percent10=percent10)\n    except URLError:\n        logger.exception(\"Could not connect, URL may be out of service\")\n        raise\n\n    # specify columns\n    cols = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n            'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n            'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n            'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n            'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n            'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n            'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n            'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n            'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n\n    # create dataframe\n    data = pd.DataFrame(data=data_raw['data'], columns=cols)\n\n    # add target to dataframe\n    data['attack_type'] = data_raw['target']\n\n    # specify and map attack types\n    attack_list = np.unique(data['attack_type'])\n    attack_category = ['dos', 'u2r', 'r2l', 'r2l', 'r2l', 'probe', 'dos', 'u2r',\n                       'r2l', 'dos', 'probe', 'normal', 'u2r', 'r2l', 'dos', 'probe',\n                       'u2r', 'probe', 'dos', 'r2l', 'dos', 'r2l', 'r2l']\n\n    attack_types = {}\n    for i, j in zip(attack_list, attack_category):\n        attack_types[i] = j\n\n    data['attack_category'] = 'normal'\n    for k, v in attack_types.items():\n        data['attack_category'][data['attack_type'] == k] = v\n\n    # define target\n    data['target'] = 0\n    for t in target:\n        data['target'][data['attack_category'] == t] = 1\n    is_outlier = data['target'].values\n\n    # define columns to be dropped\n    drop_cols = []\n    for col in data.columns.values:\n        if col not in keep_cols:\n            drop_cols.append(col)\n\n    if drop_cols != []:\n        data.drop(columns=drop_cols, inplace=True)\n\n    if return_X_y:\n        return data.values, is_outlier\n\n    return Bunch(data=data.values,\n                 target=is_outlier,\n                 target_names=['normal', 'outlier'],\n                 feature_names=keep_cols)\n\n\ndef load_url_arff(url: str, dtype: Type[np.generic] = np.float32) -> np.ndarray:\n    \"\"\"\n    Load arff files from url.\n\n    Parameters\n    ----------\n    url\n        Address of arff file.\n\n    Returns\n    -------\n    Arrays with data and labels.\n    \"\"\"\n    try:\n        resp = requests.get(url, timeout=TIMEOUT)\n        resp.raise_for_status()\n    except RequestException:\n        logger.exception(\"Could not connect, URL may be out of service\")\n        raise\n    data = arff.loadarff(io.StringIO(resp.text))[0]\n    return np.array(data.tolist(), dtype=dtype)\n\n\ndef fetch_ecg(return_X_y: bool = False) \\\n        -> Union[Bunch, Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]]:\n    \"\"\"\n    Fetch ECG5000 data. The dataset contains 5000 ECG's, originally obtained from\n    Physionet (https://archive.physionet.org/cgi-bin/atm/ATM) under the name\n    \"BIDMC Congestive Heart Failure Database(chfdb)\", record \"chf07\".\n\n    Parameters\n    ----------\n    return_X_y\n        Bool, whether to only return the data and target values or a Bunch object.\n\n    Returns\n    -------\n    Bunch\n        Train and test datasets with labels.\n    (train data, train target), (test data, test target)\n        Tuple of tuples if 'return_X_y' equals True.\n    \"\"\"\n    Xy_train = load_url_arff('https://storage.googleapis.com/seldon-datasets/ecg/ECG5000_TRAIN.arff')\n    X_train, y_train = Xy_train[:, :-1], Xy_train[:, -1]\n    Xy_test = load_url_arff('https://storage.googleapis.com/seldon-datasets/ecg/ECG5000_TEST.arff')\n    X_test, y_test = Xy_test[:, :-1], Xy_test[:, -1]\n    if return_X_y:\n        return (X_train, y_train), (X_test, y_test)\n    else:\n        return Bunch(data_train=X_train,\n                     data_test=X_test,\n                     target_train=y_train,\n                     target_test=y_test)\n\n\ndef fetch_cifar10c(corruption: Union[str, List[str]], severity: int, return_X_y: bool = False) \\\n        -> Union[Bunch, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Fetch CIFAR-10-C data. Originally obtained from https://zenodo.org/record/2535967#.XkKh2XX7Qts and\n    introduced in \"Hendrycks, D and Dietterich, T.G. Benchmarking Neural Network Robustness to Common Corruptions\n    and Perturbations. In 7th International Conference on Learning Represenations, 2019.\".\n\n    Parameters\n    ----------\n    corruption\n        Corruption type. Options can be checked with `get_corruption_cifar10c()`.\n        Alternatively, specify 'all' for all corruptions at a severity level.\n    severity\n        Severity level of corruption (1-5).\n    return_X_y\n        Bool, whether to only return the data and target values or a Bunch object.\n\n    Returns\n    -------\n    Bunch\n        Corrupted dataset with labels.\n    (corrupted data, target)\n        Tuple if 'return_X_y' equals True.\n    \"\"\"\n    url = 'https://storage.googleapis.com/seldon-datasets/cifar10c/'\n    n = 10000  # instances per corrupted test set\n    istart, iend = (severity - 1) * n, severity * n  # idx for the relevant severity level\n    corruption_list = corruption_types_cifar10c()  # get all possible corruption types\n    # convert input to list\n    if isinstance(corruption, str) and corruption != 'all':\n        corruption = [corruption]\n    elif corruption == 'all':\n        corruption = corruption_list\n    for corr in corruption:  # check values in corruptions\n        if corr not in corruption_list:\n            raise ValueError(f'{corr} is not a valid corruption type.')\n    # get corrupted data\n    shape = ((len(corruption)) * n, 32, 32, 3)\n    X = np.zeros(shape)\n    for i, corr in enumerate(corruption):\n        url_corruption = _join_url(url, corr + '.npy')\n        try:\n            resp = requests.get(url_corruption, timeout=TIMEOUT)\n            resp.raise_for_status()\n        except RequestException:\n            logger.exception(\"Could not connect, URL may be out of service\")\n            raise\n        X_corr = np.load(BytesIO(resp.content))[istart:iend].astype('float32')\n        X[i * n:(i + 1) * n] = X_corr\n\n    # get labels\n    url_labels = _join_url(url, 'labels.npy')\n    try:\n        resp = requests.get(url_labels, timeout=TIMEOUT)\n        resp.raise_for_status()\n    except RequestException:\n        logger.exception(\"Could not connect, URL may be out of service\")\n        raise\n    y = np.load(BytesIO(resp.content))[istart:iend].astype('int64')\n    if X.shape[0] != y.shape[0]:\n        repeat = X.shape[0] // y.shape[0]\n        y = np.tile(y, (repeat,))\n\n    if return_X_y:\n        return (X, y)\n    else:\n        return Bunch(data=X, target=y)\n\n\ndef google_bucket_list(url: str, folder: str, filetype: str = None, full_path: bool = False) -> List[str]:\n    \"\"\"\n    Retrieve list with items in google bucket folder.\n\n    Parameters\n    ----------\n    url\n        Bucket directory.\n    folder\n        Folder to retrieve list of items from.\n    filetype\n        File extension, e.g. `npy` for saved numpy arrays.\n\n    Returns\n    -------\n    List with items in the folder of the google bucket.\n    \"\"\"\n    try:\n        resp = requests.get(url, timeout=TIMEOUT)\n        resp.raise_for_status()\n    except RequestException:\n        logger.exception(\"Could not connect, URL may be out of service\")\n        raise\n    root = ElementTree.fromstring(resp.content)\n    bucket_list = []\n    for r in root:\n        if list(r):\n            filepath = r[0].text\n            if filetype is not None:\n                if filepath.startswith(folder) and filepath.endswith(filetype):\n                    istart, istop = filepath.find('/') + 1, filepath.find('.')\n                    bucket_list.append(filepath[istart:istop])\n            else:\n                if filepath.startswith(folder):\n                    istart, istop = filepath.find('/') + 1, filepath.find('.')\n                    bucket_list.append(filepath[istart:istop])\n    return bucket_list\n\n\ndef corruption_types_cifar10c() -> List[str]:\n    \"\"\"\n    Retrieve list with corruption types used in CIFAR-10-C.\n\n    Returns\n    -------\n    List with corruption types.\n    \"\"\"\n    url = 'https://storage.googleapis.com/seldon-datasets/'\n    folder = 'cifar10c'\n    filetype = 'npy'\n    corruption_types = google_bucket_list(url, folder, filetype)\n    corruption_types.remove('labels')\n    return corruption_types\n\n\ndef fetch_attack(dataset: str, model: str, attack: str, return_X_y: bool = False) \\\n        -> Union[Bunch, Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]]:\n    \"\"\"\n    Load adversarial instances for a given dataset, model and attack type.\n\n    Parameters\n    ----------\n    dataset\n        Dataset under attack.\n    model\n        Model under attack.\n    attack\n        Attack name.\n    return_X_y\n        Bool, whether to only return the data and target values or a Bunch object.\n\n    Returns\n    -------\n    Bunch\n        Adversarial instances with original labels.\n    (train data, train target), (test data, test target)\n        Tuple of tuples if 'return_X_y' equals True.\n    \"\"\"\n    # define paths\n    url = 'https://storage.googleapis.com/seldon-datasets/'\n    path_attack = _join_url(url, [dataset, 'attacks', model, attack])\n    path_data = path_attack + '.npz'\n    path_meta = path_attack + '_meta.pickle'\n    # get adversarial instances and labels\n    try:\n        resp = requests.get(path_data, timeout=TIMEOUT)\n        resp.raise_for_status()\n    except RequestException:\n        logger.exception(\"Could not connect, URL may be out of service\")\n        raise\n    data = np.load(BytesIO(resp.content))\n    X_train, X_test = data['X_train_adv'], data['X_test_adv']\n    y_train, y_test = data['y_train'], data['y_test']\n\n    if return_X_y:\n        return (X_train, y_train), (X_test, y_test)\n\n    # get metadata\n    try:\n        resp = requests.get(path_meta, timeout=TIMEOUT)\n        resp.raise_for_status()\n    except RequestException:\n        logger.exception(\"Could not connect, URL may be out of service\")\n        raise\n    meta = dill.load(BytesIO(resp.content))\n    return Bunch(data_train=X_train,\n                 data_test=X_test,\n                 target_train=y_train,\n                 target_test=y_test,\n                 meta=meta)\n\n", "right_context": "\n\ndef get_list_nab() -> list:\n    \"\"\"\n    Get list of possible time series to retrieve from the Numenta Anomaly Benchmark: https://github.com/numenta/NAB.\n\n    Returns\n    -------\n    List with time series names.\n    \"\"\"\n    url_labels = 'https://raw.githubusercontent.com/numenta/NAB/master/labels/combined_labels.json'\n    try:\n        resp = requests.get(url_labels, timeout=TIMEOUT)\n        resp.raise_for_status()\n    except RequestException:\n        logger.exception(\"Could not connect, URL may be out of service\")\n        raise\n    labels_json = resp.json()\n    files = [k[:-4] for k, v in labels_json.items()]\n    return files\n\n\ndef load_genome_npz(fold: str, return_labels: bool = False) \\\n        -> Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n    url = 'https://storage.googleapis.com/seldon-datasets/genome/'\n    path_data = _join_url(url, fold + '.npz')\n    try:\n        resp = requests.get(path_data, timeout=TIMEOUT)\n        resp.raise_for_status()\n    except RequestException:\n        logger.exception(\"Could not connect, URL may be out of service\")\n        raise\n    data = np.load(BytesIO(resp.content))\n    if return_labels:\n        return data['x'], data['is_outlier'], data['y']\n    else:\n        return data['x'], data['is_outlier']\n\n\ndef fetch_genome(return_X_y: bool = False, return_labels: bool = False) -> Union[Bunch, tuple]:\n    \"\"\"\n    Load genome data including their labels and whether they are outliers or not. More details about the data can be\n    found in the readme on https://console.cloud.google.com/storage/browser/seldon-datasets/genome/.\n    The original data can be found here: https://drive.google.com/drive/folders/1Ht9xmzyYPbDouUTl_KQdLTJQYX2CuclR.\n\n    Parameters\n    ----------\n    return_X_y\n        Bool, whether to only return the data and target values or a Bunch object.\n    return_labels\n        Whether to return the genome labels which are detailed in the `label_json` dict\n        of the returned Bunch object.\n\n    Returns\n    -------\n    Bunch\n        Training, validation and test data, whether they are outliers and optionally including the\n        genome labels which are specified in the `label_json` key as a dictionary.\n    (data, outlier) or (data, outlier, target)\n        Tuple for the train, validation and test set with either the data and whether they\n        are outliers or the data, outlier flag and labels for the genomes if 'return_X_y' equals True.\n    \"\"\"\n    data_train = load_genome_npz('train_in', return_labels=return_labels)\n    data_val_in = load_genome_npz('val_in', return_labels=return_labels)\n    data_val_ood = load_genome_npz('val_ood', return_labels=return_labels)\n    data_val = (\n        np.concatenate([data_val_in[0], data_val_ood[0]]),\n        np.concatenate([data_val_in[1], data_val_ood[1]])\n    )\n    data_test_in = load_genome_npz('test_in', return_labels=return_labels)\n    data_test_ood = load_genome_npz('test_ood', return_labels=return_labels)\n    data_test = (\n        np.concatenate([data_test_in[0], data_test_ood[0]]),\n        np.concatenate([data_test_in[1], data_test_ood[1]])\n    )\n    if return_labels:\n        data_val += (np.concatenate([data_val_in[2], data_val_ood[2]]),)  # type: ignore\n        data_test += (np.concatenate([data_test_in[2], data_test_ood[2]]),)  # type: ignore\n    if return_X_y:\n        return data_train, data_val, data_test\n    try:\n        resp = requests.get('https://storage.googleapis.com/seldon-datasets/genome/label_dict.json', timeout=TIMEOUT)\n        resp.raise_for_status()\n    except RequestException:\n        logger.exception(\"Could not connect, URL may be out of service\")\n        raise\n\n    label_dict = resp.json()\n    bunch = Bunch(\n        data_train=data_train[0],\n        data_val=data_val[0],\n        data_test=data_test[0],\n        outlier_train=data_train[1],\n        outlier_val=data_val[1],\n        outlier_test=data_test[1],\n        label_dict=label_dict\n    )\n    if not return_labels:\n        return bunch\n    else:\n        bunch['target_train'] = data_train[2]  # type: ignore\n        bunch['target_val'] = data_val[2]  # type: ignore\n        bunch['target_test'] = data_test[2]  # type: ignore\n        return bunch\n", "import_text": ["io", "logging", "io.BytesIO", "typing.List", "typing.Tuple", "typing.Type", "typing.Union", "xml.etree.ElementTree", "dill", "numpy", "pandas", "requests", "alibi_detect.utils.data.Bunch", "alibi_detect.utils.url._join_url", "requests.RequestException", "urllib.error.URLError", "scipy.io.arff", "sklearn.datasets.fetch_kddcup99"], "prompt": "\"\"\"\nDescription: This function fetches a dataset from the Numenta Anomaly Benchmark (NAB) and labels it with outliers.\n\nArgs:\n    ts (str): The name of the dataset to fetch.\n    return_X_y (bool): If True, returns a tuple of (data, target) instead of a Bunch object. Defaults to False.\n\nReturns:\n    Union[Bunch, Tuple[pd.DataFrame, pd.DataFrame]]: If return_X_y is True, returns a tuple of (data, target). Otherwise, returns a Bunch object with data, target, and target_names.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Get time series in a DataFrame from the Numenta Anomaly Benchmark: https://github.com/numenta/NAB.\n\n    Parameters\n    ----------\n    ts\n\n    return_X_y\n        Bool, whether to only return the data and target values or a Bunch object.\n\n    Returns\n    -------\n    Bunch\n        Dataset and outlier labels (0 means 'normal' and 1 means 'outlier') in DataFrames with timestamps.\n    (data, target)\n        Tuple if 'return_X_y' equals True.\n    \"\"\"", "function_dependencies": ["requests.get", "requests.get.raise_for_status", "requests.get.json", "pandas.read_csv", "numpy.zeros", "numpy.where", "pandas.to_datetime", "pandas.DataFrame", "alibi_detect.utils.data.Bunch"], "project_create_time": "2019-10-07T13:29:13+00:00", "project_update_time": "2024-04-18T01:33:48+00:00", "file_create_time": "2019-11-14T17:10:10Z", "file_update_time": "2022-09-09T16:52:01Z", "function_update_time": "2019-11-21T18:14:10Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["pandas.read_csv", "pandas.to_datetime"], "test_function": [{"file_path": "/alibi-detect-v0.12.0/alibi-detect-0.12.0/alibi_detect/tests/test_datasets.py", "class_name": null, "function_name": "test_fetch_nab", "code": "\ndef test_fetch_nab(return_X_y):\n    idx = np.random.choice(len(files))\n    try:\n        data = fetch_nab(files[idx], return_X_y=return_X_y)\n    except RequestException:\n        pytest.skip('NAB dataset URL down')\n    if return_X_y:\n        assert isinstance(data, tuple)\n        assert isinstance(data[0], pd.DataFrame) and isinstance(data[1], pd.DataFrame)\n    else:\n        assert isinstance(data, Bunch)\n        assert isinstance(data.data, pd.DataFrame) and isinstance(data.target, pd.DataFrame)"}]}, {"git_group": "marshmallow-code", "git_name": "apispec", "version": "6.6.1", "language": "Python", "project_name": "apispec-6.6.1.zip", "file_path": "/apispec-6.6.1/apispec-6.6.1/src/apispec/yaml_utils.py", "file_name": "yaml_utils.py", "focal_class": null, "focal_name": "load_yaml_from_docstring", "focal_parameter": [], "solution": "def load_yaml_from_docstring(docstring: str) -> dict:\n    split_lines = trim_docstring(docstring).split(\"\\n\")\n\n    # Cut YAML from rest of docstring\n    for index, line in enumerate(split_lines):\n        line = line.strip()\n        if line.startswith(\"---\"):\n            cut_from = index\n            break\n    else:\n        return {}\n\n    yaml_string = \"\\n\".join(split_lines[cut_from:])\n    yaml_string = dedent(yaml_string)\n    return yaml.safe_load(yaml_string) or {}", "function_signature": "def load_yaml_from_docstring(docstring: str) -> dict :", "left_context": "\"\"\"YAML utilities\"\"\"\n\nfrom __future__ import annotations\n\nimport typing\n\nimport yaml\n\nfrom apispec.utils import dedent, trim_docstring\n\n\ndef dict_to_yaml(dic: dict, yaml_dump_kwargs: typing.Any | None = None) -> str:\n    \"\"\"Serializes a dictionary to YAML.\"\"\"\n    yaml_dump_kwargs = yaml_dump_kwargs or {}\n\n    # By default, don't sort alphabetically to respect schema field ordering\n    yaml_dump_kwargs.setdefault(\"sort_keys\", False)\n    return yaml.dump(dic, **yaml_dump_kwargs)\n\n", "right_context": "\n\nPATH_KEYS = {\"get\", \"put\", \"post\", \"delete\", \"options\", \"head\", \"patch\"}\n\n\ndef load_operations_from_docstring(docstring: str) -> dict:\n    \"\"\"Return a dictionary of OpenAPI operations parsed from a\n    a docstring.\n    \"\"\"\n    doc_data = load_yaml_from_docstring(docstring)\n    return {\n        key: val\n        for key, val in doc_data.items()\n        if key in PATH_KEYS or key.startswith(\"x-\")\n    }\n", "import_text": ["typing", "yaml", "apispec.utils.dedent", "apispec.utils.trim_docstring"], "prompt": "\"\"\"\nLoads YAML from a docstring using the yaml.safe_load API.\n\nArgs:\n    docstring (str): The docstring from which to extract the YAML.\n\nReturns:\n    dict: The YAML content as a dictionary, or an empty dictionary if no YAML is found.\n\"\"\"", "comment": "    \"\"\"Loads YAML from docstring.\"\"\"", "prompt_is_gen_from_api": true, "function_dependencies": ["apispec.utils.trim_docstring", "apispec.utils.trim_docstring.split", "apispec.utils.dedent", "yaml.safe_load"], "project_create_time": "2014-10-18T23:48:49+00:00", "project_update_time": "2024-04-16T20:20:44+00:00", "file_create_time": "2019-03-10T22:28:19Z", "file_update_time": "2024-01-19T05:25:49Z", "function_update_time": "2022-02-04T13:42:31Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["yaml.safe_load"], "test_function": [{"file_path": "/apispec-6.6.1/apispec-6.6.1/tests/test_yaml_utils.py", "class_name": null, "function_name": "test_load_yaml_from_docstring", "code": "\ndef test_load_yaml_from_docstring():\n    def f():\n        \"\"\"\n        Foo\n            bar\n            baz quux\n\n        ---\n        herp: 1\n        derp: 2\n        \"\"\"\n\n    result = yaml_utils.load_yaml_from_docstring(f.__doc__)\n    assert result == {\"herp\": 1, \"derp\": 2}"}]}, {"git_group": "WZBSocialScienceCenter", "git_name": "tmtoolkit", "version": "v0.11.2", "language": "Python", "project_name": "tmtoolkit-v0.11.2.zip", "file_path": "/tmtoolkit-v0.11.2/tmtoolkit-0.11.2/tmtoolkit/topicmod/model_io.py", "file_name": "model_io.py", "focal_class": null, "focal_name": "save_ldamodel_summary_to_excel", "focal_parameter": ["excel_file", "topic_word_distrib", "doc_topic_distrib", "doc_labels", "vocab"], "solution": "def save_ldamodel_summary_to_excel(excel_file, topic_word_distrib, doc_topic_distrib, doc_labels, vocab,\n                                   top_n_topics=10, top_n_words=10, dtm=None,\n                                   rank_label_fmt=None, topic_labels=None):\n\n    rank_label_fmt = rank_label_fmt or DEFAULT_RANK_NAME_FMT\n    if topic_labels is None:\n        topic_labels = DEFAULT_TOPIC_NAME_FMT\n    sheets = OrderedDict()\n\n    # must convert NumPy string array to lists of Python strings, because OpenPyXL can't handle them\n    if isinstance(doc_labels, np.ndarray):\n        doc_labels = list(map(str, doc_labels))\n\n    if isinstance(vocab, np.ndarray):\n        vocab = list(map(str, vocab))\n\n    if isinstance(topic_labels, np.ndarray):\n        topic_labels = list(map(str, topic_labels))\n\n    # doc-topic distribution sheets\n    logger.info(f'generating document-topic distribution sheets for top {top_n_topics} topics')\n    sheets['top_doc_topics_vals'] = top_n_from_distribution(doc_topic_distrib, top_n=top_n_topics,\n                                                            row_labels=doc_labels,\n                                                            col_labels=rank_label_fmt)\n    sheets['top_doc_topics_labels'] = top_n_from_distribution(doc_topic_distrib, top_n=top_n_topics,\n                                                              row_labels=doc_labels,\n                                                              col_labels=rank_label_fmt,\n                                                              val_labels=topic_labels)\n    sheets['top_doc_topics_labelled_vals'] = ldamodel_top_doc_topics(doc_topic_distrib, doc_labels,\n                                                                     topic_labels=topic_labels,\n                                                                     top_n=top_n_topics)\n\n    # topic-word distribution sheets\n    logger.info(f'generating topic-word distribution sheets for top {top_n_words} words')\n    sheets['top_topic_word_vals'] = top_n_from_distribution(topic_word_distrib, top_n=top_n_words,\n                                                            row_labels=topic_labels,\n                                                            col_labels=rank_label_fmt)\n    sheets['top_topic_word_labels'] = top_n_from_distribution(topic_word_distrib, top_n=top_n_words,\n                                                              row_labels=topic_labels,\n                                                              col_labels=rank_label_fmt,\n                                                              val_labels=vocab)\n    sheets['top_topic_words_labelled_vals'] = ldamodel_top_topic_words(topic_word_distrib, vocab,\n                                                                       row_labels=topic_labels,\n                                                                       top_n=top_n_words)\n\n    if dtm is not None:\n        logger.info('generating marginal topic distribution')\n        doc_len = doc_lengths(dtm)\n        marg_topic_distr = marginal_topic_distrib(doc_topic_distrib, doc_len)\n        if isinstance(topic_labels, str):\n            row_names = [DEFAULT_TOPIC_NAME_FMT.format(i0=i, i1=i + 1) for i in range(len(marg_topic_distr))]\n        elif isinstance(topic_labels, list):\n            row_names = topic_labels\n        else:\n            raise ValueError('unexpected type of `topic_labels`: %s. must be string or list' % type(topic_labels))\n        sheets['marginal_topic_distrib'] = pd.DataFrame(marg_topic_distr, columns=['marginal_topic_distrib'],\n                                                        index=row_names)\n\n    logger.info(f'generating Excel file \"{excel_file}\"')\n    excel_writer = pd.ExcelWriter(excel_file)\n\n    for sh_name, sh_data in sheets.items():\n        sh_data.to_excel(excel_writer, sh_name)\n\n    excel_writer.save()\n\n    return sheets", "function_signature": "def save_ldamodel_summary_to_excel(excel_file, topic_word_distrib, doc_topic_distrib, doc_labels, vocab,\n                                   top_n_topics=10, top_n_words=10, dtm=None,\n                                   rank_label_fmt=None, topic_labels=None) :", "left_context": "\"\"\"\nFunctions for printing/exporting topic model results.\n\n.. codeauthor:: Markus Konrad <markus.konrad@wzb.eu>\n\"\"\"\nimport logging\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\n\nfrom ._common import DEFAULT_RANK_NAME_FMT, DEFAULT_TOPIC_NAME_FMT\nfrom .model_stats import marginal_topic_distrib, top_n_from_distribution, _join_value_and_label_dfs\nfrom ..bow.bow_stats import doc_lengths\nfrom ..utils import pickle_data, unpickle_file\n\nlogger = logging.getLogger('tmtoolkit')\n\n\ndef ldamodel_top_topic_words(topic_word_distrib, vocab, top_n=10, val_fmt=None, row_labels=DEFAULT_TOPIC_NAME_FMT,\n                             col_labels=None, index_name='topic'):\n    \"\"\"\n    Retrieve the top (i.e. most probable) `top_n` words for each topic in the topic-word distribution\n    `topic_word_distrib` as pandas DataFrame.\n\n    .. seealso:: :func:`~tmtoolkit.topicmod.model_io.ldamodel_full_topic_words` to retrieve the full distribution as\n                 formatted pandas DataFrame;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_word_topics` to retrieve the top topics per word from\n                 a topic-word distribution;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_doc_topics` to retrieve\n                 the top topics per document from a document-topic distribution;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_topic_docs` to retrieve\n                 the top documents per topic;\n\n    :param topic_word_distrib: topic-word distribution; shape KxM, where K is number of topics, M is vocabulary size\n    :param vocab: vocabulary list/array of length K\n    :param top_n: number of most probable words per topic to select\n    :param val_fmt: format string for table cells where ``{lbl}`` is replaced by the respective word from `vocab` and\n                    ``{val}`` is replaced by the word's probability given the topic\n    :param row_labels: format string for each row index where ``{i0}`` or ``{i1}`` are replaced by the respective\n                       zero- or one-indexed topic numbers or an array with individual row labels\n    :param col_labels: format string for the columns where ``{i0}`` or ``{i1}`` are replaced by the respective zero-\n                       or one-indexed rank\n    :param index_name: name of the table index\n    :return: pandas DataFrame\n    \"\"\"\n    df_values = top_n_from_distribution(topic_word_distrib, top_n=top_n,\n                                        row_labels=row_labels, val_labels=None)\n    df_labels = top_n_from_distribution(topic_word_distrib, top_n=top_n,\n                                        row_labels=row_labels, val_labels=vocab)\n    return _join_value_and_label_dfs(df_values, df_labels, top_n, row_labels=row_labels,\n                                     val_fmt=val_fmt, col_labels=col_labels, index_name=index_name)\n\n\ndef ldamodel_top_word_topics(topic_word_distrib, vocab, top_n=10, val_fmt=None, topic_labels=DEFAULT_TOPIC_NAME_FMT,\n                             col_labels=None, index_name='token'):\n    \"\"\"\n    Retrieve the top (i.e. most probable) `top_n` topics for each word in the topic-word distribution\n    `topic_word_distrib` as pandas DataFrame.\n\n    .. seealso:: :func:`~tmtoolkit.topicmod.model_io.ldamodel_full_topic_words` to retrieve the full distribution as\n                 formatted pandas DataFrame;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_topic_words` to retrieve the top words per topic from\n                 a topic-word distribution;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_doc_topics` to retrieve\n                 the top topics per document from a document-topic distribution;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_topic_docs` to retrieve\n                 the top documents per topic;\n\n    :param topic_word_distrib: topic-word distribution; shape KxM, where K is number of topics, M is vocabulary size\n    :param vocab: vocabulary list/array of length K\n    :param top_n: number of most probable words per topic to select\n    :param val_fmt: format string for table cells where ``{lbl}`` is replaced by the respective topic label from\n                    `topic_labels` and ``{val}`` is replaced by the word's probability given the topic\n    :param topic_labels: format string for each row index where ``{i0}`` or ``{i1}`` are replaced by the respective\n                         zero- or one-indexed topic numbers or an array with individual topic labels\n    :param col_labels: format string for the columns where ``{i0}`` or ``{i1}`` are replaced by the respective zero-\n                       or one-indexed rank\n    :param index_name: name of the table index\n    :return: pandas DataFrame\n    \"\"\"\n    word_topic_distrib = topic_word_distrib.T\n    df_values = top_n_from_distribution(word_topic_distrib, top_n=top_n,\n                                        row_labels=vocab, val_labels=None)\n    df_labels = top_n_from_distribution(word_topic_distrib, top_n=top_n,\n                                        row_labels=vocab, val_labels=topic_labels)\n    return _join_value_and_label_dfs(df_values, df_labels, top_n, row_labels=vocab,\n                                     val_fmt=val_fmt, col_labels=col_labels, index_name=index_name)\n\n\ndef ldamodel_top_doc_topics(doc_topic_distrib, doc_labels, top_n=3, val_fmt=None, topic_labels=DEFAULT_TOPIC_NAME_FMT,\n                            col_labels=None, index_name='document'):\n    \"\"\"\n    Retrieve the top (i.e. most probable) `top_n` topics for each document in the document-topic distribution\n    `doc_topic_distrib` as pandas DataFrame.\n\n    .. seealso:: :func:`~tmtoolkit.topicmod.model_io.ldamodel_full_doc_topics` to retrieve the full distribution as\n                 formatted pandas DataFrame;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_topic_docs` to retrieve\n                 the top documents per topic;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_topic_words` to retrieve\n                 the top words per topic from a topic-word distribution;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_word_topics` to retrieve the top topics per word from\n                 a topic-word distribution\n\n    :param doc_topic_distrib: document-topic distribution; shape NxK, where N is the number of documents, K is the\n                              number of topics\n    :param doc_labels: list/array of length N with a string label for each document\n    :param top_n: number of most probable topics per document to select\n    :param val_fmt: format string for table cells where ``{lbl}`` is replaced by the respective topic name and\n                    ``{val}`` is replaced by the topic's probability given the document\n    :param topic_labels: format string for each row index where ``{i0}`` or ``{i1}`` are replaced by the respective\n                         zero- or one-indexed topic numbers or an array with individual topic labels\n    :param col_labels: format string for the columns where ``{i0}`` or ``{i1}`` are replaced by the respective zero-\n                       or one-indexed rank\n    :param index_name: name of the table index\n    :return: pandas DataFrame\n    \"\"\"\n    df_values = top_n_from_distribution(doc_topic_distrib, top_n=top_n,\n                                        row_labels=doc_labels, val_labels=None)\n    df_labels = top_n_from_distribution(doc_topic_distrib, top_n=top_n,\n                                        row_labels=doc_labels, val_labels=topic_labels)\n    return _join_value_and_label_dfs(df_values, df_labels, top_n, row_labels=doc_labels,\n                                     val_fmt=val_fmt, col_labels=col_labels, index_name=index_name)\n\n\ndef ldamodel_top_topic_docs(doc_topic_distrib, doc_labels, top_n=3, val_fmt=None, topic_labels=DEFAULT_TOPIC_NAME_FMT,\n                            col_labels=None, index_name='topic'):\n    \"\"\"\n    Retrieve the top (i.e. most probable) `top_n` documents for each topic in the document-topic distribution\n    `doc_topic_distrib` as pandas DataFrame.\n\n    .. seealso:: :func:`~tmtoolkit.topicmod.model_io.ldamodel_full_doc_topics` to retrieve the full distribution as\n                 formatted pandas DataFrame;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_doc_topics` to retrieve\n                 the top topics per document;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_topic_words` to retrieve\n                 the top words per topic from a topic-word distribution;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_word_topics` to retrieve the top topics per word from\n                 a topic-word distribution\n\n    :param doc_topic_distrib: document-topic distribution; shape NxK, where N is the number of documents, K is the\n                              number of topics\n    :param doc_labels: list/array of length N with a string label for each document\n    :param top_n: number of most probable documents per topic to select\n    :param val_fmt: format string for table cells where ``{lbl}`` is replaced by the respective document label and\n                    ``{val}`` is replaced by the topic's probability given the document\n    :param topic_labels: format string for each row index where ``{i0}`` or ``{i1}`` are replaced by the respective\n                         zero- or one-indexed topic numbers or an array with individual topic labels\n    :param col_labels: format string for the columns where ``{i0}`` or ``{i1}`` are replaced by the respective zero-\n                       or one-indexed rank\n    :param index_name: name of the table index\n    :return: pandas DataFrame\n    \"\"\"\n    topic_doc_distrib = doc_topic_distrib.T\n    df_values = top_n_from_distribution(topic_doc_distrib, top_n=top_n,\n                                        row_labels=topic_labels, val_labels=None)\n    df_labels = top_n_from_distribution(topic_doc_distrib, top_n=top_n,\n                                        row_labels=topic_labels, val_labels=doc_labels)\n    return _join_value_and_label_dfs(df_values, df_labels, top_n, row_labels=topic_labels,\n                                     val_fmt=val_fmt, col_labels=col_labels, index_name=index_name)\n\n\ndef ldamodel_full_topic_words(topic_word_distrib, vocab, colname_rowindex='_topic',\n                              row_labels=DEFAULT_TOPIC_NAME_FMT):\n    \"\"\"\n    Generate a pandas DataFrame for the full topic-word distribution `topic_word_distrib`.\n\n    .. seealso:: :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_topic_words` to retrieve only the most probable words\n                 in the distribution as formatted pandas DataFrame;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_full_doc_topics` to retrieve the full document-topic\n                 distribution as dataframe\n\n    :param topic_word_distrib: topic-word distribution; shape KxM, where K is number of topics, M is vocabulary size\n    :param vocab: vocabulary list/array of length K\n    :param colname_rowindex: column name for the \"row index\", i.e. the column that identifies each row\n    :param row_labels: format string for each row index where ``{i0}`` or ``{i1}`` are replaced by the respective\n                       zero- or one-indexed topic numbers or an array with individual row labels\n    :return: pandas DataFrame\n    \"\"\"\n    if isinstance(row_labels, str):\n        rownames = [row_labels.format(i0=i, i1=i + 1) for i in range(topic_word_distrib.shape[0])]\n    else:\n        rownames = row_labels\n\n    return pd.concat((pd.DataFrame({colname_rowindex: rownames}),\n                      pd.DataFrame(topic_word_distrib, columns=list(vocab))),\n                     axis=1)\n\n\ndef ldamodel_full_doc_topics(doc_topic_distrib, doc_labels, colname_rowindex='_doc',\n                             topic_labels=DEFAULT_TOPIC_NAME_FMT):\n    \"\"\"\n    Generate a pandas DataFrame for the full doc-topic distribution `doc_topic_distrib`.\n\n    .. seealso:: :func:`~tmtoolkit.topicmod.model_io.ldamodel_top_doc_topics` to retrieve only the most probable topics\n                 in the distribution as formatted pandas DataFrame;\n                 :func:`~tmtoolkit.topicmod.model_io.ldamodel_full_topic_words` to retrieve the full topic-word\n                 distribution as dataframe\n\n    :param doc_topic_distrib: document-topic distribution; shape NxK, where N is the number of documents, K is the\n                              number of topics\n    :param doc_labels: list/array of length N with a string label for each document\n    :param colname_rowindex: column name for the \"row index\", i.e. the column that identifies each row\n    :param topic_labels: format string for each row index where ``{i0}`` or ``{i1}`` are replaced by the respective\n                         zero- or one-indexed topic numbers or an array with individual topic labels\n    :return: pandas DataFrame\n    \"\"\"\n    if isinstance(topic_labels, str):\n        colnames = [topic_labels.format(i0=i, i1=i+1) for i in range(doc_topic_distrib.shape[1])]\n    else:\n        colnames = topic_labels\n\n    return pd.concat((pd.DataFrame({colname_rowindex: doc_labels}),\n                      pd.DataFrame(doc_topic_distrib, columns=list(colnames))),\n                     axis=1)\n\n\ndef print_ldamodel_distribution(distrib, row_labels, val_labels, top_n=10):\n    \"\"\"\n    Print `top_n` top values from a LDA model's distribution `distrib`. This is a general function to print top values\n    of any multivariate distribution given as matrix `distrib` with H rows and I columns, each identified by\n    H `row_labels` and I `val_labels`.\n\n    .. seealso:: :func:`~tmtoolkit.topicmod.model_io.print_ldamodel_topic_words` to print the top values of a\n                 topic-word distribution or :func:`~tmtoolkit.topicmod.model_io.print_ldamodel_doc_topics`\n                 to print the top values of a document-topic distribution.\n\n    :param distrib: either a topic-word or a document-topic distribution of shape HxI\n    :param row_labels: list/array of length H with label string for each row of `distrib` or format string\n    :param val_labels: list/array of length I with label string for each column of `distrib` or format string\n    :param top_n: number of top values to print\n    \"\"\"\n\n    df_values = top_n_from_distribution(distrib, top_n=top_n, row_labels=row_labels, val_labels=None)\n    df_labels = top_n_from_distribution(distrib, top_n=top_n, row_labels=row_labels, val_labels=val_labels)\n\n    for i, (ind, row) in enumerate(df_labels.iterrows()):\n        print(ind)\n        for j, label in enumerate(row):\n            val = df_values.iloc[i, j]\n            print('> #%d. %s (%f)' % (j + 1, label, val))\n\n\ndef print_ldamodel_topic_words(topic_word_distrib, vocab, top_n=10, row_labels=DEFAULT_TOPIC_NAME_FMT):\n    \"\"\"\n    Print `top_n` values from an LDA model's topic-word distribution `topic_word_distrib`.\n\n    .. seealso:: :func:`~tmtoolkit.topicmod.model_io.print_ldamodel_doc_topics`\n                 to print the top values of a document-topic distribution.\n\n    :param topic_word_distrib: topic-word distribution; shape KxM, where K is number of topics, M is vocabulary size\n    :param vocab: vocabulary list/array of length K\n    :param top_n: number of top values to print\n    :param row_labels: format string for each row index where ``{i0}`` or ``{i1}`` are replaced by the respective\n                       zero- or one-indexed topic numbers or an array with individual row labels\n    \"\"\"\n    print_ldamodel_distribution(topic_word_distrib, row_labels=row_labels, val_labels=vocab,\n                                top_n=top_n)\n\n\ndef print_ldamodel_doc_topics(doc_topic_distrib, doc_labels, top_n=3, val_labels=DEFAULT_TOPIC_NAME_FMT):\n    \"\"\"\n    Print `top_n` values from an LDA model's document-topic distribution `doc_topic_distrib`.\n\n    .. seealso:: :func:`~tmtoolkit.topicmod.model_io.print_ldamodel_topic_words`\n                 to print the top values of a topic-word distribution.\n\n    :param doc_topic_distrib: document-topic distribution; shape NxK, where N is the number of documents, K is the\n                              number of topics\n    :param doc_labels: list/array of length N with a string label for each document\n    :param top_n: number of top values to print\n    :param val_labels: format string for each value where ``{i0}`` or ``{i1}`` are replaced by the respective\n                       zero- or one-indexed topic numbers or an array with individual value labels\n    \"\"\"\n    print_ldamodel_distribution(doc_topic_distrib, row_labels=doc_labels, val_labels=val_labels,\n                                top_n=top_n)\n\n", "right_context": "\n\ndef save_ldamodel_to_pickle(picklefile, model, vocab, doc_labels, dtm=None, **kwargs):\n    \"\"\"\n    Save an LDA model object `model` as pickle file to `picklefile`.\n\n    .. seealso:: :func:`~tmtoolkit.topicmod.model_io.load_ldamodel_from_pickle` to load the saved model.\n\n    :param picklefile: target file\n    :param model: LDA model instance\n    :param vocab: vocabulary list/array of length M\n    :param doc_labels: document labels list/array of length N\n    :param dtm: optional document-term matrix of shape NxM\n    :param kwargs: additional options for :func:`tmtoolkit.utils.pickle_data`\n    \"\"\"\n    pickle_data({'model': model, 'vocab': vocab, 'doc_labels': doc_labels, 'dtm': dtm}, picklefile, **kwargs)\n\n\ndef load_ldamodel_from_pickle(picklefile, **kwargs):\n    \"\"\"\n    Load an LDA model object from a pickle file `picklefile`.\n\n    .. seealso:: :func:`~tmtoolkit.topicmod.model_io.save_ldamodel_to_pickle` to save a model.\n\n    .. warning:: Python pickle files may contain malicious code. You should only load pickle files from trusted sources.\n\n    :param picklefile: target file\n    :param kwargs: additional options for :func:`tmtoolkit.utils.unpickle_file`\n    :return: dict with keys: ``'model'`` \u2013 model instance; ``'vocab'`` \u2013 vocabulary; ``'doc_labels'`` \u2013 document labels;\n                             ``'dtm'`` \u2013 optional document-term matrix;\n    \"\"\"\n    return unpickle_file(picklefile, **kwargs)\n", "import_text": ["logging", "collections.OrderedDict", "numpy", "pandas"], "prompt": "\"\"\"\nDescription: This function saves the LDA model summary to an Excel file.\n\nArgs:\n    excel_file (str): The name of the Excel file to save the model summary to.\n    topic_word_distrib (array-like): The topic-word distribution of the LDA model.\n    doc_topic_distrib (array-like): The document-topic distribution of the LDA model.\n    doc_labels (list): The labels of the documents.\n    vocab (list): The vocabulary of the LDA model.\n    top_n_topics (int, optional): The number of top topics to include in the summary. Defaults to 10.\n    top_n_words (int, optional): The number of top words to include in the summary. Defaults to 10.\n    dtm (array-like, optional): The document-term matrix of the LDA model. Defaults to None.\n    rank_label_fmt (str, optional): The format of the rank labels. Defaults to None.\n    topic_labels (list, optional): The labels of the topics. Defaults to None.\n\nReturns:\n    dict: A dictionary of pandas DataFrames, where each DataFrame is a sheet in the Excel file.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Save a summary derived from an LDA model's topic-word and document-topic distributions (`topic_word_distrib` and\n    `doc_topic_distrib` to an Excel file `excel_file`. Return the generated Excel sheets as dict of pandas DataFrames.\n\n    The resulting Excel file will consist of 6 or optional 7 sheets:\n\n    - ``top_doc_topics_vals``: document-topic distribution with probabilities of top topics per document\n    - ``top_doc_topics_labels``: document-topic distribution with labels (e.g. ``\"topic_12\"``) of top topics per\n      document\n    - ``top_doc_topics_labelled_vals``: document-topic distribution combining probabilities and labels of top topics per\n      document (e.g. ``\"topic_12 (0.21)\"``)\n    - ``top_topic_word_vals``: topic-word distribution with probabilities of top words per topic\n    - ``top_topic_word_labels``: topic-word distribution with top words per (e.g. ``\"politics\"``) topic\n    - ``top_topic_words_labelled_vals``: topic-word distribution combining probabilities and top words per topic\n      (e.g. ``\"politics (0.08)\"``)\n    - optional if `dtm` is given \u2013 ``marginal_topic_distrib``: marginal topic distribution\n\n    :param excel_file: target Excel file\n    :param topic_word_distrib: topic-word distribution; shape KxM, where K is number of topics, M is vocabulary size\n    :param doc_topic_distrib: document-topic distribution; shape NxK, where N is the number of documents, K is the\n                              number of topics\n    :param doc_labels: list/array of length N with a string label for each document\n    :param vocab: vocabulary list/array of length K\n    :param top_n_topics: number of most probable topics per document to include in the summary\n    :param top_n_words: number of most probable words per topic to include in the summary\n    :param dtm: document-term matrix; shape NxM; if this is given, a sheet for the marginal topic distribution will\n                be included\n    :param rank_label_fmt: format string for the rank labels where ``{i0}`` or ``{i1}`` are replaced by the respective\n                       zero- or one-indexed rank numbers (leave to None for default)\n    :param topic_labels: format string for each row index where ``{i0}`` or ``{i1}`` are replaced by the respective\n                         zero- or one-indexed topic numbers or an array with individual topic labels\n    :return: dict mapping sheet name to pandas DataFrame\n    \"\"\"", "function_dependencies": ["collections.OrderedDict", "pandas.DataFrame", "pandas.ExcelWriter", "collections.OrderedDict.items", "pandas.ExcelWriter.save"], "project_create_time": "2017-11-07T09:11:54+00:00", "project_update_time": "2024-03-24T04:04:16+00:00", "file_create_time": "2018-04-23T16:14:25Z", "file_update_time": "2022-01-04T12:15:56Z", "function_update_time": "2018-04-23T16:14:25Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["pandas.ExcelWriter"], "test_function": [{"file_path": "/tmtoolkit-v0.11.2/tmtoolkit-0.11.2/tests/test_topicmod_model_io.py", "class_name": null, "function_name": "test_save_ldamodel_summary_to_excel", "code": "\ndef test_save_ldamodel_summary_to_excel(n_docs, n_topics, size_vocab, top_n_topics, top_n_words, create_dtm):\n    try:\n        import openpyxl\n    except ImportError:\n        pytest.skip('openpyxl not installed')\n\n    topic_word = np.random.uniform(size=n_topics * size_vocab).reshape((n_topics, size_vocab))\n    doc_topic = np.random.uniform(size=n_docs * n_topics).reshape((n_docs, n_topics))\n    doc_labels = np.array(['doc%d' % i for i in range(doc_topic.shape[0])])\n    vocab = np.array(['t%d' % i for i in range(topic_word.shape[1])])\n    _, excelfile = tempfile.mkstemp(suffix='.xlsx')\n\n    if create_dtm:\n        dtm = np.random.randint(0, 10, size=n_docs*size_vocab).reshape(n_docs, size_vocab)\n    else:\n        dtm = None\n\n    if top_n_words < 1 or top_n_words > topic_word.shape[1] or top_n_topics < 1 or top_n_topics > topic_word.shape[0]\\\n            or n_docs < 1:\n        with pytest.raises(ValueError):\n            model_io.save_ldamodel_summary_to_excel(excelfile, topic_word, doc_topic, doc_labels, vocab,\n                                                    top_n_topics=top_n_topics, top_n_words=top_n_words)\n    else:\n        excelsheets = model_io.save_ldamodel_summary_to_excel(excelfile, topic_word, doc_topic, doc_labels, vocab,\n                                                              top_n_topics=top_n_topics, top_n_words=top_n_words,\n                                                              dtm=dtm)\n        assert isinstance(excelsheets, OrderedDict)\n\n        sheetnames = ['top_doc_topics_vals', 'top_doc_topics_labels', 'top_doc_topics_labelled_vals',\n                      'top_topic_word_vals', 'top_topic_word_labels', 'top_topic_words_labelled_vals']\n\n        if dtm is not None:\n            sheetnames.append('marginal_topic_distrib')\n\n        assert list(excelsheets.keys()) == sheetnames\n\n        for sheetn in sheetnames:\n            assert isinstance(excelsheets[sheetn], pd.DataFrame)"}]}, {"git_group": "facebook", "git_name": "Ax", "version": "v0.1.20", "language": "Python", "project_name": "Ax-v0.1.20.zip", "file_path": "/Ax-v0.1.20/Ax-0.1.20/ax/models/torch/botorch_moo_defaults.py", "file_name": "botorch_moo_defaults.py", "focal_class": null, "focal_name": "get_EHVI", "focal_parameter": [], "solution": "def get_EHVI(\n    model: Model,\n    objective_weights: Tensor,\n    objective_thresholds: Tensor,\n    outcome_constraints: Optional[Tuple[Tensor, Tensor]] = None,\n    X_observed: Optional[Tensor] = None,\n    X_pending: Optional[Tensor] = None,\n    **kwargs: Any,\n) -> AcquisitionFunction:\n    if X_observed is None:\n        raise ValueError(\"There are no feasible observed points.\")\n    # construct Objective module\n    (\n        objective,\n        objective_thresholds,\n    ) = get_weighted_mc_objective_and_objective_thresholds(\n        objective_weights=objective_weights, objective_thresholds=objective_thresholds\n    )\n    if \"Ys\" not in kwargs:\n        raise ValueError(\"Expected Hypervolume Improvement requires Ys argument\")\n    Y_tensor = torch.stack(kwargs.get(\"Ys\")).transpose(0, 1).squeeze(-1)\n    # For EHVI acquisition functions we pass the constraint transform directly.\n    if outcome_constraints is None:\n        cons_tfs = None\n    else:\n        cons_tfs = get_outcome_constraint_transforms(outcome_constraints)\n    num_objectives = objective_thresholds.shape[0]\n    return get_acquisition_function(\n        acquisition_function_name=\"qEHVI\",\n        model=model,\n        # TODO (jej): Fix pyre error below by restructuring class hierarchy.\n        # pyre-fixme[6]: Expected `botorch.acquisition.objective.\n        #  MCAcquisitionObjective` for 3rd parameter `objective` to call\n        #  `get_acquisition_function` but got `IdentityMCMultiOutputObjective`.\n        objective=objective,\n        X_observed=X_observed,\n        X_pending=X_pending,\n        constraints=cons_tfs,\n        mc_samples=kwargs.get(\"mc_samples\", DEFAULT_EHVI_MC_SAMPLES),\n        qmc=kwargs.get(\"qmc\", True),\n        alpha=kwargs.get(\n            \"alpha\", get_default_partitioning_alpha(num_objectives=num_objectives)\n        ),\n        seed=torch.randint(1, 10000, (1,)).item(),\n        ref_point=objective_thresholds.tolist(),\n        Y=Y_tensor,\n    )", "function_signature": "def get_EHVI(\n    model: Model,\n    objective_weights: Tensor,\n    objective_thresholds: Tensor,\n    outcome_constraints: Optional[Tuple[Tensor, Tensor]] = None,\n    X_observed: Optional[Tensor] = None,\n    X_pending: Optional[Tensor] = None,\n    **kwargs: Any,\n) -> AcquisitionFunction :", "left_context": "#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nReferences\n\n.. [Daulton2020qehvi]\n    S. Daulton, M. Balandat, and E. Bakshy. Differentiable Expected Hypervolume\n    Improvement for Parallel Multi-Objective Bayesian Optimization. Advances in Neural\n    Information Processing Systems 33, 2020.\n\n\"\"\"\n\nimport warnings\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nimport torch\nfrom ax.exceptions.core import AxWarning\nfrom ax.models.torch.utils import (  # noqa F40\n    _to_inequality_constraints,\n    get_outcome_constraint_transforms,\n    predict_from_model,\n)\nfrom ax.models.torch_base import TorchModel\nfrom botorch.acquisition.acquisition import AcquisitionFunction\nfrom botorch.acquisition.multi_objective.objective import WeightedMCMultiOutputObjective\nfrom botorch.acquisition.utils import get_acquisition_function\nfrom botorch.models.model import Model\nfrom botorch.optim.optimize import optimize_acqf_list\nfrom botorch.utils.multi_objective.pareto import is_non_dominated\nfrom torch import Tensor\n\n\nDEFAULT_EHVI_MC_SAMPLES = 128\n\n\n# Callable that takes tensors of observations and model parameters,\n# then returns means of observations that make up a pareto frontier.\nTFrontierEvaluator = Callable[\n    [\n        TorchModel,\n        Tensor,\n        Optional[Tensor],\n        Optional[Tensor],\n        Optional[Tensor],\n        Optional[Tensor],\n        Optional[Tuple[Tensor, Tensor]],\n    ],\n    Tuple[Tensor, Tensor],\n]\n\n\ndef get_default_frontier_evaluator() -> TFrontierEvaluator:\n    return pareto_frontier_evaluator\n\n\ndef get_weighted_mc_objective_and_objective_thresholds(\n    objective_weights: Tensor, objective_thresholds: Tensor\n) -> Tuple[WeightedMCMultiOutputObjective, Tensor]:\n    r\"\"\"Construct weighted objective and apply the weights to objective thresholds.\n\n    Args:\n        objective_weights: The objective is to maximize a weighted sum of\n            the columns of f(x). These are the weights.\n        objective_thresholds: A tensor containing thresholds forming a reference point\n            from which to calculate pareto frontier hypervolume. Points that do not\n            dominate the objective_thresholds contribute nothing to hypervolume.\n\n    Returns:\n        A two-element tuple with the objective and objective thresholds:\n\n            - The objective\n            - The objective thresholds\n\n    \"\"\"\n    # pyre-ignore [16]\n    nonzero_idcs = objective_weights.nonzero(as_tuple=False).view(-1)\n    objective_weights = objective_weights[nonzero_idcs]\n    objective = WeightedMCMultiOutputObjective(\n        weights=objective_weights, outcomes=nonzero_idcs.tolist()\n    )\n    objective_thresholds = torch.mul(objective_thresholds, objective_weights)\n    return objective, objective_thresholds\n\n", "right_context": "\n\n# TODO (jej): rewrite optimize_acqf wrappers to avoid duplicate code.\ndef scipy_optimizer_list(\n    acq_function_list: List[AcquisitionFunction],\n    bounds: Tensor,\n    inequality_constraints: Optional[List[Tuple[Tensor, Tensor, float]]] = None,\n    fixed_features: Optional[Dict[int, float]] = None,\n    rounding_func: Optional[Callable[[Tensor], Tensor]] = None,\n    **kwargs: Any,\n) -> Tuple[Tensor, Tensor]:\n    r\"\"\"Sequential optimizer using scipy's minimize module on a numpy-adaptor.\n\n    The ith acquisition in the sequence uses the ith given acquisition_function.\n\n    Args:\n        acq_function_list: A list of botorch AcquisitionFunctions,\n            optimized sequentially.\n        bounds: A `2 x d`-dim tensor, where `bounds[0]` (`bounds[1]`) are the\n            lower (upper) bounds of the feasible hyperrectangle.\n        n: The number of candidates to generate.\n        inequality constraints: A list of tuples (indices, coefficients, rhs),\n            with each tuple encoding an inequality constraint of the form\n            `\\sum_i (X[indices[i]] * coefficients[i]) >= rhs`\n        fixed_features: A map {feature_index: value} for features that should\n            be fixed to a particular value during generation.\n        rounding_func: A function that rounds an optimization result\n            appropriately (i.e., according to `round-trip` transformations).\n\n    Returns:\n        2-element tuple containing\n\n        - A `n x d`-dim tensor of generated candidates.\n        - A `n`-dim tensor of conditional acquisition\n          values, where `i`-th element is the expected acquisition value\n          conditional on having observed candidates `0,1,...,i-1`.\n    \"\"\"\n    num_restarts: int = kwargs.get(\"num_restarts\", 20)\n    raw_samples: int = kwargs.get(\"num_raw_samples\", 50 * num_restarts)\n\n    # use SLSQP by default for small problems since it yields faster wall times\n    if \"method\" not in kwargs:\n        kwargs[\"method\"] = \"SLSQP\"\n    X, expected_acquisition_value = optimize_acqf_list(\n        acq_function_list=acq_function_list,\n        bounds=bounds,\n        num_restarts=num_restarts,\n        raw_samples=raw_samples,\n        options=kwargs,\n        inequality_constraints=inequality_constraints,\n        fixed_features=fixed_features,\n        post_processing_func=rounding_func,\n    )\n    return X, expected_acquisition_value\n\n\ndef pareto_frontier_evaluator(\n    model: TorchModel,\n    objective_weights: Tensor,\n    objective_thresholds: Optional[Tensor] = None,\n    X: Optional[Tensor] = None,\n    Y: Optional[Tensor] = None,\n    Yvar: Optional[Tensor] = None,\n    outcome_constraints: Optional[Tuple[Tensor, Tensor]] = None,\n) -> Tuple[Tensor, Tensor]:\n    \"\"\"Return outcomes predicted to lie on a pareto frontier.\n\n    Given a model and a points to evaluate use the model to predict which points\n    lie on the pareto frontier.\n\n    Args:\n        model: Model used to predict outcomes.\n        objective_weights: A `m` tensor of values indicating the weight to put\n            on different outcomes. For pareto frontiers only the sign matters.\n        objective_thresholds:  A tensor containing thresholds forming a reference point\n            from which to calculate pareto frontier hypervolume. Points that do not\n            dominate the objective_thresholds contribute nothing to hypervolume.\n        X: A `n x d` tensor of features to evaluate.\n        Y: A `n x m` tensor of outcomes to use instead of predictions.\n        Yvar: A `n x m` tensor of input variances (NaN if unobserved).\n        outcome_constraints: A tuple of (A, b). For k outcome constraints\n            and m outputs at f(x), A is (k x m) and b is (k x 1) such that\n            A f(x) <= b.\n\n    Returns:\n        2-element tuple containing\n\n        - A `j x m` tensor of outcome on the pareto frontier. j is the number\n            of frontier points.\n        - A `j x m x m` tensor of predictive covariances.\n            cov[j, m1, m2] is Cov[m1@j, m2@j].\n    \"\"\"\n    if X is not None:\n        Y, Yvar = model.predict(X)\n    elif Y is None or Yvar is None:\n        raise ValueError(\n            \"Requires `X` to predict or both `Y` and `Yvar` to select a subset of \"\n            \"points on the pareto frontier.\"\n        )\n\n    # Apply objective_weights to outcomes and objective_thresholds.\n    # If objective_thresholds is not None use a dummy tensor of zeros.\n    (\n        obj,\n        weighted_objective_thresholds,\n    ) = get_weighted_mc_objective_and_objective_thresholds(\n        objective_weights=objective_weights,\n        objective_thresholds=(\n            objective_thresholds\n            if objective_thresholds is not None\n            else torch.zeros(objective_weights.shape)\n        ),\n    )\n    Y_obj = obj(Y)\n\n    # Filter Y, Yvar, Y_obj to items that dominate all objective thresholds\n    if objective_thresholds is not None:\n        objective_thresholds_mask = (Y_obj >= weighted_objective_thresholds).all(dim=1)\n        Y = Y[objective_thresholds_mask]\n        Yvar = Yvar[objective_thresholds_mask]\n        Y_obj = Y_obj[objective_thresholds_mask]\n\n    # Get feasible points that do not violate outcome_constraints\n    if outcome_constraints is not None:\n        cons_tfs = get_outcome_constraint_transforms(outcome_constraints)\n        # pyre-ignore [16]\n        feas = torch.stack([c(Y) <= 0 for c in cons_tfs], dim=-1).all(dim=-1)\n        Y = Y[feas]\n        Yvar = Yvar[feas]\n        Y_obj = Y_obj[feas]\n\n    # calculate pareto front with only objective outcomes:\n    frontier_mask = is_non_dominated(Y_obj)\n\n    # Apply masks\n    Y_frontier = Y[frontier_mask]\n    Yvar_frontier = Yvar[frontier_mask]\n    return Y_frontier, Yvar_frontier\n\n\ndef get_default_partitioning_alpha(num_objectives: int) -> float:\n    \"\"\"Adaptively selects a reasonable partitioning based on the number of objectives.\n\n    This strategy is derived from the results in [Daulton2020qehvi]_, which suggest\n    that this heuristic provides a reasonable trade-off between the closed-loop\n    performance and the wall time required for the partitioning.\n    \"\"\"\n    if num_objectives == 2:\n        return 0.0\n    elif num_objectives > 6:\n        warnings.warn(\"EHVI works best for less than 7 objectives.\", AxWarning)\n    return 10 ** (-8 + num_objectives)\n", "import_text": ["warnings", "typing.Any", "typing.Callable", "typing.Dict", "typing.List", "typing.Optional", "typing.Tuple", "torch", "ax.exceptions.core.AxWarning", "ax.models.torch.utils._to_inequality_constraints", "ax.models.torch.utils.get_outcome_constraint_transforms", "ax.models.torch.utils.predict_from_model", "ax.models.torch_base.TorchModel", "botorch.acquisition.acquisition.AcquisitionFunction", "botorch.acquisition.multi_objective.objective.WeightedMCMultiOutputObjective", "botorch.acquisition.utils.get_acquisition_function", "botorch.models.model.Model", "botorch.optim.optimize.optimize_acqf_list", "botorch.utils.multi_objective.pareto.is_non_dominated", "torch.Tensor"], "prompt": "\"\"\"\nDescription: This function is used to get the Expected Hypervolume Improvement (EHVI) acquisition function.\n\nArgs:\n    model (Model): The model used for the acquisition function.\n    objective_weights (Tensor): The weights of the objectives.\n    objective_thresholds (Tensor): The thresholds of the objectives.\n    outcome_constraints (Optional[Tuple[Tensor, Tensor]]): The constraints on the outcomes.\n    X_observed (Optional[Tensor]): The observed points.\n    X_pending (Optional[Tensor]): The pending points.\n    **kwargs (Any): Additional arguments.\n\nReturns:\n    AcquisitionFunction: The EHVI acquisition function.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    r\"\"\"Instantiates a qExpectedHyperVolumeImprovement acquisition function.\n\n    Args:\n        model: The underlying model which the acqusition function uses\n            to estimate acquisition values of candidates.\n        objective_weights: The objective is to maximize a weighted sum of\n            the columns of f(x). These are the weights.\n        objective_thresholds:  A tensor containing thresholds forming a reference point\n            from which to calculate pareto frontier hypervolume. Points that do not\n            dominate the objective_thresholds contribute nothing to hypervolume.\n        outcome_constraints: A tuple of (A, b). For k outcome constraints\n            and m outputs at f(x), A is (k x m) and b is (k x 1) such that\n            A f(x) <= b. (Not used by single task models)\n        X_observed: A tensor containing points observed for all objective\n            outcomes and outcomes that appear in the outcome constraints (if\n            there are any).\n        X_pending: A tensor containing points whose evaluation is pending (i.e.\n            that have been submitted for evaluation) present for all objective\n            outcomes and outcomes that appear in the outcome constraints (if\n            there are any).\n        mc_samples: The number of MC samples to use (default: 512).\n        qmc: If True, use qMC instead of MC (default: True).\n\n    Returns:\n        qExpectedHypervolumeImprovement: The instantiated acquisition function.\n    \"\"\"", "function_dependencies": ["torch.stack", "torch.stack.transpose", "torch.stack.transpose.squeeze", "ax.models.torch.utils.get_outcome_constraint_transforms", "botorch.acquisition.utils.get_acquisition_function", "torch.randint", "torch.randint.item"], "project_create_time": "2019-02-09T15:23:44+00:00", "project_update_time": "2024-04-15T08:03:49+00:00", "file_create_time": "2020-08-25T20:57:05Z", "file_update_time": "2021-02-05T20:11:08Z", "function_update_time": "2020-08-25T20:57:05Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["torch.randint"], "test_function": [{"file_path": "/Ax-v0.1.20/Ax-0.1.20/ax/models/tests/test_botorch_moo_defaults.py", "class_name": "BotorchMOODefaultsTest", "function_name": "test_get_EHVI_input_validation_errors", "code": "\n    def test_get_EHVI_input_validation_errors(self):\n        model = MultiObjectiveBotorchModel()\n        x = torch.zeros(2, 2)\n        weights = torch.ones(2)\n        objective_thresholds = torch.zeros(2)\n        with self.assertRaisesRegex(\n            ValueError, \"There are no feasible observed points.\"\n        ):\n            get_EHVI(\n                model=model,\n                objective_weights=weights,\n                objective_thresholds=objective_thresholds,\n            )\n        with self.assertRaisesRegex(\n            ValueError, \"Expected Hypervolume Improvement requires Ys argument\"\n        ):\n            get_EHVI(\n                model=model,\n                X_observed=x,\n                objective_weights=weights,\n                objective_thresholds=objective_thresholds,\n            )"}]}, {"git_group": "ARM-DOE", "git_name": "ACT", "version": "v2.0.5", "language": "Python", "project_name": "ACT-v2.0.5.zip", "file_path": "/ACT-v2.0.5/ACT-2.0.5/act/qc/add_supplemental_qc.py", "file_name": "add_supplemental_qc.py", "focal_class": null, "focal_name": "read_yaml_supplemental_qc", "focal_parameter": ["ds", "fullpath"], "solution": "def read_yaml_supplemental_qc(\n    ds,\n    fullpath,\n    variables=None,\n    assessments=None,\n    datetime64=True,\n    time_delim=(';', ',', '|', r'\\t'),\n    none_if_empty=True,\n    quiet=False,\n):\n\n    flag_file = []\n    if Path(fullpath).is_file():\n        flag_file = [Path(fullpath)]\n    else:\n        try:\n            datastream = ds.attrs['_datastream']\n        except KeyError:\n            raise RuntimeError(\n                'Unable to determine datastream name from Dataset. Need to set global attribute '\n                '_datastream in Dataset or provided full path to flag file.'\n            )\n\n        flag_file = list(Path(fullpath).glob(f'{datastream}.yml'))\n        flag_file.extend(list(Path(fullpath).glob(f'{datastream}.yaml')))\n\n    if len(flag_file) > 0:\n        flag_file = flag_file[0]\n    else:\n        if not quiet:\n            print(f'Could not find supplemental QC file for {datastream} in {fullpath}')\n\n        return None\n\n    # Ensure keywords are lists\n    if isinstance(variables, str):\n        variables = [variables]\n\n    if isinstance(assessments, str):\n        assessments = [assessments]\n\n    if isinstance(time_delim, str):\n        time_delim = [time_delim]\n\n    # Ensure the assessments are capitalized for matching\n    if assessments is not None:\n        assessments = [ii.capitalize() for ii in assessments]\n\n    # Read YAML file\n    with open(flag_file) as fp:\n        try:\n            data_dict = yaml.load(fp, Loader=yaml.FullLoader)\n        except AttributeError:\n            data_dict = yaml.load(fp)\n\n    # If variable names are provided only keep those names.\n    if variables is not None:\n        variables.append('_all')\n        del_vars = set(data_dict.keys()) - set(variables)\n        for var_name in del_vars:\n            del data_dict[var_name]\n\n    # If assessments are given only keep those assessments.\n    if assessments is not None:\n        for var_name in data_dict.keys():\n            for asses_name in data_dict[var_name].keys():\n                # Check if yaml file assessments are capitalized. If not fix.\n                if not asses_name == asses_name.capitalize():\n                    data_dict[var_name][asses_name.capitalize()] = data_dict[var_name][asses_name]\n                    del data_dict[var_name][asses_name]\n\n            # Delete assessments if not in provided list.\n            del_asses = set(data_dict[var_name].keys()) - set(assessments)\n            for asses_name in del_asses:\n                del data_dict[var_name][asses_name]\n\n    # Convert from string to numpy datetime64 2D array\n    if datetime64:\n        for var_name in data_dict.keys():\n            for asses_name in data_dict[var_name].keys():\n                for description in data_dict[var_name][asses_name].keys():\n                    try:\n                        num_times = len(data_dict[var_name][asses_name][description])\n                        new_times = np.empty((num_times, 2), dtype='datetime64[ms]')\n                    except TypeError:\n                        # Set to single value Not A Time numpy array if times\n                        # from yaml file are empty list.\n                        new_times = np.full([], np.datetime64('NaT'), dtype='datetime64[ms]')\n\n                    for ii, tm in enumerate(data_dict[var_name][asses_name][description]):\n                        # Split the times on multiple different types of delimiters.\n                        for delim in time_delim:\n                            split_tm = tm.split(delim)\n                            if len(split_tm) > 1:\n                                break\n\n                        new_times[ii, 0] = np.datetime64(parser.parse(split_tm[0]))\n                        new_times[ii, 1] = np.datetime64(parser.parse(split_tm[1]))\n\n                    data_dict[var_name][asses_name][description] = new_times\n\n    # If the return dictinary is empty convert to None\n    if none_if_empty and len(data_dict) == 0:\n        data_dict = None\n\n    return data_dict", "function_signature": "def read_yaml_supplemental_qc(\n    ds,\n    fullpath,\n    variables=None,\n    assessments=None,\n    datetime64=True,\n    time_delim=(';', ',', '|', r'\\t'),\n    none_if_empty=True,\n    quiet=False,\n) :", "left_context": "import yaml\nimport numpy as np\nfrom pathlib import Path\nfrom dateutil import parser\n\n#   Example of the YAML file and how to construct.\n#   The times are set as inclusive start to inclusive end time.\n#   Different formats are acceptable as displayed with temp_mean\n#   Good example below.\n#\n#   The general format is to use the variable name as initial key\n#   followed by an assessment key, followed by a 'description' key which\n#   is a description of that test. Each test listed will insert a new test\n#   into the ancillary quality control variable. After the 'description'\n#   list all time ranges as a YAML array.\n#   If a test is to be applied to all variables list under the specail\n#   variable name '_all'.\n#\n#   The file should be named same as datastream name with standard\n#   YAML file extension when only providing the directoy.\n#   For example the file below would be named sgpmetE13.b1.yaml\n#\n#\n#  _all:\n#    Bad:\n#      Values are bad for all:\n#        - 2020-01-21 01:01:02, 2020-01-21 01:03:13\n#        - 2020-01-21 02:01:02, 2020-01-21 04:03:13\n#   Suspect:\n#     Values are suspect for all: []\n#\n# temp_mean:\n#   Bad:\n#     Values are bad:\n#       - 2020-01-01 00:01:02, 2020-01-01 00:03:44\n#       - 2020-01-02 01:01:02, 2020-01-02 01:03:13\n#       - 2020-02-01 00:01:02, 2020-02-01 00:03:44\n#       - 2020-03-02 01:01:02, 2020-03-02 01:03:13\n#       - 2020-01-21 01:01:02, 2020-01-21 01:03:13\n#   Suspect:\n#     Values are suspect:\n#       - 2020-01-01 02:04:02, 2020-01-01 02:05:44\n#   Good:\n#     Values are good:\n#       - 2020-01-01 00:08:02, 2020-01-01 00:09:44\n#       - Jan 1, 2020 00:08:02 ; January 1, 2020 00:09:44 AM\n#       - 2020-01-01 00:08 | 2020-01-01 00:09\n#       - 2020-01-01T00:08:02 ; 2020-01-01T00:09:44\n#\n# rh_mean:\n#   Bad:\n#     Values are bad:\n#       - 2020-01-01 00:01:02, 2020-01-01 00:03:44\n#       - 2020-01-02 00:01:02, 2020-01-02 00:03:44\n#   Suspect:\n#     Values are suspect:\n#       - 2020-01-01 00:04:02, 2020-01-01 00:05:44\n\n", "right_context": "\n\ndef apply_supplemental_qc(\n    ds,\n    fullpath,\n    variables=None,\n    assessments=None,\n    apply_all=True,\n    exclude_all_variables=None,\n    quiet=False,\n):\n    \"\"\"\n    Apply flagging from supplemental QC file by adding new QC tests.\n\n    Parameters\n    ----------\n    ds : xarray.Dataset\n        Xarray dataset containing data. QC variables should be converted to CF\n        format prior to adding new tests.\n    fullpath : str or `pathlib.Path`\n        Fullpath to file or directory with supplemental QC files.\n    variables : str, list of str or None\n        Variables to apply to the dataset from supplemental QC flag file.\n        If not set will apply all variables in the file.\n    assessments : str, list of str or None\n        Assessments to apply. If not not set will apply all assesments in the flag file.\n    apply_all : boolean\n        If a \"_all\" variable exists in the supplemental QC flag file will apply to all variables\n        in the Dataset.\n    exclude_all_variables : str, list of str or None\n        Variables to skip when applying \"_all\" variables.\n    quiet : boolean\n        Suppress information about not finding a supplemental QC file to read.\n\n    Examples\n    --------\n    This example will load the example sounding data used for unit testing.\n\n    .. code-block:: python\n\n        from act.tests import EXAPLE_MET_YAML, EXAMPLE_MET1\n        from act.io.arm import read_arm_netcdf\n        from act.qc.add_supplemental_qc import apply_supplemental_qc\n        ds = read_arm_netcdf(EXAMPLE_MET1, cleanup_qc=True)\n        apply_supplemental_qc(ds, EXAPLE_MET_YAML, apply_all=False)\n        print(ds['qc_temp_mean'].attrs['flag_meanings'])\n\n        ['Value is equal to missing_value.', 'Value is less than the fail_min.',\n         'Value is greater than the fail_max.',\n         'Difference between current and previous values exceeds fail_delta.',\n         'Values are bad', 'Values are super bad', 'Values are suspect', 'Values are good']\n\n    \"\"\"\n\n    exclude_vars = ['time', 'base_time', 'time_offset']\n    if exclude_all_variables is not None:\n        if isinstance(exclude_all_variables, str):\n            exclude_all_variables = [exclude_all_variables]\n\n        exclude_vars.extend(exclude_all_variables)\n\n    flag_dict = read_yaml_supplemental_qc(\n        ds, fullpath, variables=variables, assessments=assessments, quiet=quiet\n    )\n\n    if flag_dict is None:\n        return\n\n    for var_name in list(ds.variables):\n        if var_name in flag_dict.keys():\n            for asses_name in flag_dict[var_name].keys():\n                for description in flag_dict[var_name][asses_name]:\n                    times = flag_dict[var_name][asses_name][description]\n\n                    if np.all(np.isnat(times)):\n                        continue\n\n                    indexes = np.array([], dtype=np.int32)\n                    for vals in times:\n                        ind = np.argwhere(\n                            (ds['time'].values >= vals[0]) & (ds['time'].values <= vals[1])\n                        )\n\n                        if len(ind) > 0:\n                            indexes = np.append(indexes, ind)\n\n                    if indexes.size > 0:\n                        ds.qcfilter.add_test(\n                            var_name,\n                            index=indexes,\n                            test_meaning=description,\n                            test_assessment=asses_name,\n                        )\n\n    var_name = '_all'\n    if apply_all and var_name in flag_dict.keys():\n        for asses_name in flag_dict[var_name].keys():\n            for description in flag_dict[var_name][asses_name]:\n                times = flag_dict[var_name][asses_name][description]\n\n                if np.all(np.isnat(times)):\n                    continue\n\n                indexes = np.array([], dtype=np.int32)\n                for vals in times:\n                    ind = np.argwhere(\n                        (ds['time'].values >= vals[0]) & (ds['time'].values <= vals[1])\n                    )\n                    if ind.size > 0:\n                        indexes = np.append(indexes, np.ndarray.flatten(ind))\n\n                if indexes.size > 0:\n                    for all_var_name in list(ds.data_vars):\n                        if all_var_name in exclude_vars:\n                            continue\n\n                        if 'time' not in ds[all_var_name].dims:\n                            continue\n\n                        try:\n                            if ds[all_var_name].attrs['standard_name'] == 'quality_flag':\n                                continue\n                        except KeyError:\n                            pass\n\n                        ds.qcfilter.add_test(\n                            all_var_name,\n                            index=indexes,\n                            test_meaning=description,\n                            test_assessment=asses_name,\n                        )\n", "import_text": ["yaml", "numpy", "pathlib.Path", "dateutil.parser"], "prompt": "\"\"\"\nDescription: This function reads supplemental quality control (QC) data from a YAML file and processes it into a dictionary.\n\nArgs:\n    ds (Dataset): The xarray Dataset object.\n    fullpath (str): The full path to the directory containing the YAML file.\n    variables (list or str, optional): A list or string of variable names to keep. Defaults to None.\n    assessments (list or str, optional): A list or string of assessment names to keep. Defaults to None.\n    datetime64 (bool, optional): If True, convert time strings to numpy datetime64 objects. Defaults to True.\n    time_delim (list or str, optional): A list or string of delimiters to split time strings. Defaults to (';', ',', '|', r'\\t').\n    none_if_empty (bool, optional): If True, return None if the processed dictionary is empty. Defaults to True.\n    quiet (bool, optional): If True, do not print a message if the supplemental QC file is not found. Defaults to False.\n\nReturns:\n    dict or None: A dictionary containing the processed QC data, or None if the dictionary is empty and none_if_empty is True.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Returns a dictionary converstion of YAML file for flagging data. The dictionary\n    will contain variable names as first key, assessents as second keys containing\n    test description as third key with time as last key. Multiple descriptions\n    are allowed.\n\n    Parameters\n    ----------\n    ds : xarray.Dataset\n        Xarray dataset containing data.\n    fullpath : str or `pathlib.Path`\n        Full path to file to read or directory containing YAML files to read. If providing\n        a directory a file with the datastream name ending in .yaml or .yml must exists.\n    variables : str, list of str or None\n        Optional, variable names to keep in dictionary. All others will be removed\n        before returning dictionary. If no variables are left will return None.\n    assessments : str, list of str or None\n        Optional, assessments categories to keep in dictionary. All others will be removed\n        before returning dictionary.\n    datetime64 : boolean\n        Convert the string value list to 2D numpy datetime64 array with first value\n        the start time and second value end time. If the time list in the YAML file\n        is empty, the 'time' numpy array will be a single value instead of 2D array\n        and set to numpy datetime64 NaT.\n    time_delim : str or list of str or tuple of str\n        Optional, character delimiter to use when parsing times.\n    none_if_empty : boolean\n        Return None instead of empty dictionary\n    quiet : boolean\n        Suppress information about not finding a YAML file to read.\n\n    Returns\n    -------\n        Dictionary of [variable names][assessments][description] and time values\n        or if the dictionary is empty after processing options and none_if_empty set\n        to True will return None.\n\n    Examples\n    --------\n    This example will load the example MET data used for unit testing.\n\n    .. code-block:: python\n\n        from act.tests import EXAPLE_MET_YAML, EXAMPLE_MET1\n        from act.io.arm import read_arm_netcdf\n        from act.qc.add_supplemental_qc import read_yaml_supplemental_qc\n        ds = read_arm_netcdf(EXAMPLE_MET1, cleanup_qc=True)\n        result = read_yaml_supplemental_qc(ds, EXAPLE_MET_YAML,\n                                     variables=['rh_mean'], assessments='Bad')\n        print(result)\n\n        {'rh_mean': {'Bad': {'description': 'Values are bad',\n            'time': array([['2020-01-01T00:01:02.000', '2020-01-01T00:03:44.000'],\n                           ['2020-01-02T00:01:02.000', '2020-01-02T00:03:44.000']],\n                           dtype='datetime64[ms]')}}}\n\n    \"\"\"", "function_dependencies": ["pathlib.Path", "pathlib.Path.is_file", "pathlib.Path.glob", "yaml.load", "yaml.load.keys", "numpy.empty", "numpy.full", "numpy.datetime64", "dateutil.parser.parse"], "project_create_time": "2019-03-11T15:19:03+00:00", "project_update_time": "2024-03-24T14:29:31+00:00", "file_create_time": "2022-06-03T00:35:38Z", "file_update_time": "2024-02-01T01:02:56Z", "function_update_time": "2022-06-03T00:35:38Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["numpy.datetime64", "yaml.load"], "test_function": [{"file_path": "/ACT-v2.0.5/ACT-2.0.5/tests/qc/test_add_supplemental_qc.py", "class_name": null, "function_name": "test_read_yaml_supplemental_qc", "code": "\ndef test_read_yaml_supplemental_qc():\n    ds = read_arm_netcdf(\n        EXAMPLE_MET1, keep_variables=['temp_mean', 'qc_temp_mean'], cleanup_qc=True\n    )\n\n    result = read_yaml_supplemental_qc(ds, EXAMPLE_MET_YAML)\n    assert isinstance(result, dict)\n    assert len(result.keys()) == 3\n\n    result = read_yaml_supplemental_qc(\n        ds,\n        Path(EXAMPLE_MET_YAML).parent,\n        variables='temp_mean',\n        assessments=['Bad', 'Incorrect', 'Suspect'],\n    )\n    assert len(result.keys()) == 2\n    assert sorted(result['temp_mean'].keys()) == ['Bad', 'Suspect']\n\n    result = read_yaml_supplemental_qc(ds, 'sgpmetE13.b1.yaml', quiet=True)\n    assert result is None\n\n    apply_supplemental_qc(ds, EXAMPLE_MET_YAML)\n    assert ds['qc_temp_mean'].attrs['flag_masks'] == [1, 2, 4, 8, 16, 32, 64, 128, 256]\n    assert ds['qc_temp_mean'].attrs['flag_assessments'] == [\n        'Bad',\n        'Bad',\n        'Bad',\n        'Indeterminate',\n        'Bad',\n        'Bad',\n        'Suspect',\n        'Good',\n        'Bad',\n    ]\n    assert ds['qc_temp_mean'].attrs['flag_meanings'][0] == 'Value is equal to missing_value.'\n    assert ds['qc_temp_mean'].attrs['flag_meanings'][-1] == 'Values are bad for all'\n    assert ds['qc_temp_mean'].attrs['flag_meanings'][-2] == 'Values are good'\n    assert np.sum(ds['qc_temp_mean'].values) == 81344\n    assert np.count_nonzero(ds['qc_temp_mean'].values) == 1423\n\n    del ds\n\n    ds = read_arm_netcdf(\n        EXAMPLE_MET1, keep_variables=['temp_mean', 'qc_temp_mean'], cleanup_qc=True\n    )\n    apply_supplemental_qc(ds, Path(EXAMPLE_MET_YAML).parent, apply_all=False)\n    assert ds['qc_temp_mean'].attrs['flag_masks'] == [1, 2, 4, 8, 16, 32, 64, 128]\n\n    ds = read_arm_netcdf(EXAMPLE_MET1, cleanup_qc=True)\n    apply_supplemental_qc(ds, Path(EXAMPLE_MET_YAML).parent, exclude_all_variables='temp_mean')\n    assert ds['qc_rh_mean'].attrs['flag_masks'] == [1, 2, 4, 8, 16, 32, 64, 128]\n    assert 'Values are bad for all' in ds['qc_rh_mean'].attrs['flag_meanings']\n    assert 'Values are bad for all' not in ds['qc_temp_mean'].attrs['flag_meanings']\n\n    del ds\n\n    ds = read_arm_netcdf(EXAMPLE_MET1, keep_variables=['temp_mean', 'rh_mean'])\n    apply_supplemental_qc(\n        ds,\n        Path(EXAMPLE_MET_YAML).parent,\n        exclude_all_variables='temp_mean',\n        assessments='Bad',\n        quiet=True,\n    )\n    assert ds['qc_rh_mean'].attrs['flag_assessments'] == ['Bad']\n    assert ds['qc_temp_mean'].attrs['flag_assessments'] == ['Bad', 'Bad']\n    assert np.sum(ds['qc_rh_mean'].values) == 124\n    assert np.sum(ds['qc_temp_mean'].values) == 2840\n\n    del ds"}]}, {"git_group": "aimclub", "git_name": "FEDOT", "version": "v0.7.3.1", "language": "Python", "project_name": "FEDOT-v0.7.3.1.zip", "file_path": "/FEDOT-v0.7.3.1/FEDOT-0.7.3.1/fedot/preprocessing/structure.py", "file_name": "structure.py", "focal_class": "PipelineStructureExplorer", "focal_name": "check_structure_by_tag", "focal_parameter": [], "solution": "    def check_structure_by_tag(pipeline: 'Pipeline', tag_to_check: str, source_name: str = DEFAULT_SOURCE_NAME):\n        if len(pipeline.nodes) < 2:\n            # Preprocessing needed for single-node pipeline\n            return False\n\n        graph, node_labels = graph_structure_as_nx_graph(pipeline)\n\n        # Assign information for all nodes in the graph\n        graph, info_df = PipelineStructureExplorer._enrich_with_information(graph, node_labels)\n\n        primary_df = info_df[info_df['node_type'] == 'primary']\n        root_df = info_df[info_df['node_type'] == 'root']\n        root_id = root_df.iloc[0]['node_id']\n\n        paths = {}\n        path_id = 0\n        for i, node_info in primary_df.iterrows():\n            primary_id = node_info['node_id']\n            node_name = node_info['node_label'].operation.operation_type\n            if source_name in (node_name, DEFAULT_SOURCE_NAME):\n                for path in nx.all_simple_paths(graph, source=primary_id, target=root_id):\n                    # Check the path (branch) whether it has wanted operation in correct location or not\n                    path_info = PipelineStructureExplorer.check_path(graph, path, tag_to_check)\n                    paths[path_id] = path_info\n                    path_id += 1\n\n        correct_branches = (branch['correctness'] for branch in paths.values())\n        # 'False' means that least one branch in the graph cannot process desired type of data\n        return all(correct_branches)", "function_signature": "def check_structure_by_tag(pipeline: 'Pipeline', tag_to_check: str, source_name: str = DEFAULT_SOURCE_NAME) :", "left_context": "from typing import Tuple, List, Dict, Any\n\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nfrom golem.core.dag.convert import graph_structure_as_nx_graph\n\nDEFAULT_SOURCE_NAME = 'default'\n\n\nclass PipelineStructureExplorer:\n    \"\"\" Class for performing pipeline structure exploration.\n    The class allows you to convert pipelines into a networkx graph and considers\n    all possible paths from primary PipelineNode (or primary PipelineNodes) to root node. If at least\n    one of the paths contains an invalid sequence of operations, the search performed\n    by this class will detect it\n    \"\"\"\n\n    _invariant_tags = {'encoding': 'categorical-ignore',\n                       'imputation': 'nans-ignore'}\n\n    @staticmethod", "right_context": "\n    @staticmethod\n    def check_path(graph: nx.DiGraph, path: list, tag_to_check: str) -> Dict[str, Any]:\n        \"\"\"\n        Checking the path for operations take right places in the pipeline.\n\n        :param graph: graph for checking paths\n        :param path: path in the graph from primary PipelineNode to root\n        :param tag_to_check: find appropriate operation by desired tag\n        \"\"\"\n        operation_path, is_appropriate_operation, is_independent_operation = \\\n            PipelineStructureExplorer._calculate_binary_paths(graph, path, tag_to_check)\n\n        # Define is branch correct or not\n        is_branch_correct = PipelineStructureExplorer.is_current_branch_correct(is_appropriate_operation,\n                                                                                is_independent_operation,\n                                                                                len(path))\n\n        path_info = {'correctness': is_branch_correct,\n                     'path': operation_path}\n\n        return path_info\n\n    @staticmethod\n    def _enrich_with_information(graph: nx.DiGraph, node_labels: dict) -> Tuple[nx.DiGraph, pd.DataFrame]:\n        \"\"\"\n        Set additional information (operation name and node type) to nodes as attributes.\n        There is also preparing pandas DataFrame with such information\n\n        :param node_labels: dictionary with ids and operation names\n        \"\"\"\n        # Set names to nodes and additional info\n        number_of_out_edges = graph.degree._nodes\n        number_of_in_edges = graph.in_edges._adjdict\n\n        info_df = []\n        for node_id, node_label in node_labels.items():\n            parent_numbers = len(number_of_in_edges[node_id])\n            child_numbers = len(number_of_out_edges[node_id])\n\n            if parent_numbers == 0:\n                node_type = 'primary'\n            else:\n                if child_numbers == 0:\n                    # It is final node in the pipeline\n                    node_type = 'root'\n                else:\n                    node_type = 'secondary'\n\n            attrs = {node_id: {'operation': node_label, 'type': node_type}}\n            nx.set_node_attributes(graph, attrs)\n\n            info_df.append([node_id, node_type, node_label, parent_numbers, child_numbers])\n\n        info_df = pd.DataFrame(info_df, columns=['node_id', 'node_type', 'node_label',\n                                                 'parent_number', 'child_number'])\n        return graph, info_df\n\n    @staticmethod\n    def _calculate_binary_paths(graph: nx.Graph, path: list, tag_to_check: str) \\\n            -> Tuple[List[str], List[bool], List[bool]]:\n        \"\"\"\n        Calculate binary masks for considering path in the graph.\n        For example, branch\n        UID 584fef54 -> UID 568984edr45 -> UID 4566895ef13\n\n        Can be decomposed as follows (if tag_to_check is encoding):\n\n            * operations_path:\n            imputation -> encoding -> ridge\n            Listing the names of the operations in the nodes\n\n            * is_appropriate_operation:\n            False -> True -> False\n            Whether there is an operation with a matching tag in the node or not\n\n            * is_independent_operation\n            True -> False -> False\n            Are there any operations in the branch that can process data without\n            errors, even if they have not been previously processed by encoding,\n            e.g.\n        \"\"\"\n        ignore_tag = PipelineStructureExplorer._invariant_tags.get(tag_to_check)\n\n        # List with names of operations in the branch\n        operations_path = []\n        # Is the operation contain desired tag or not\n        is_appropriate_operation = []\n        # Can operation ignore errors when processing data without applying \"desired operation\"\n        # For example: \"desired operation\" is imputing. If considering operation can process\n        # data with nans in can_be_ignored list \"True\" will be added\n        is_independent_operation = []\n        for node_id in path:\n            current_node = graph.nodes.get(node_id)\n\n            node_tags = current_node['operation'].tags\n            if tag_to_check in node_tags:\n                is_appropriate_operation.append(True)\n            else:\n                # Operation in the node is not wanted one\n                is_appropriate_operation.append(False)\n\n                if ignore_tag is not None:\n                    if ignore_tag in node_tags:\n                        is_independent_operation.append(True)\n                    else:\n                        is_independent_operation.append(False)\n\n            operations_path.append(current_node['operation'])\n\n        return operations_path, is_appropriate_operation, is_independent_operation\n\n    @staticmethod\n    def is_current_branch_correct(is_appropriate_operation,\n                                  is_independent_operation,\n                                  path_len):\n        \"\"\"\n        Based on predefined rules, the branch is checked for correctness.\n        1) is_appropriate_operation\n        2) is_independent_operation\n        For example correct branch is:\n            1) False -> True -> False\n            2) True -> False -> False\n            Data can be processed without errors due to wanted operation applied\n            after independent operation\n            True -> True -> after operation output everything is ok\n\n        Incorrect branch is:\n            1) False -> False -> True\n            2) True -> False -> False\n            Second operation is not wanted one and at the same time cannot\n            process data without any transformation. So it will have an error\n            True -> X False X -> True\n\n        :param is_appropriate_operation: list with bool values is wanted operation placed in the node or not\n        :param is_independent_operation: list with bool values is independent operation placed in the node or not\n        :param path_len: length of list with graph nodes\n        \"\"\"\n\n        # Check if operations were in the path\n        is_appropriate_operation = np.array(is_appropriate_operation)\n        # Number of True in list\n        number_operations = is_appropriate_operation.sum()\n\n        # True > 0 - so find ids with appropriate operations\n        operation_ids = np.ravel(np.argwhere(is_appropriate_operation > 0))\n\n        # Find independent operations in the path\n        if len(is_independent_operation) > 0:\n            is_independent_operation = np.array(is_independent_operation)\n        else:\n            is_independent_operation = np.array([False] * path_len)\n\n        # Check independent operation presence in path\n        number_independent_operations = np.sum(is_independent_operation)\n\n        # By default it is incorrect\n        is_branch_correct = False\n        if number_operations == 0 and number_independent_operations == 0:\n            # Path contain non independent operations and have no good ones\n            is_branch_correct = False\n\n        elif number_operations > 0 and number_independent_operations == 0:\n            # Path contain good operations but have no independent\n            if operation_ids[0] == 0:\n                # Wanted operation in the first place\n                is_branch_correct = True\n            else:\n                is_branch_correct = False\n\n        elif number_operations == 0 and number_independent_operations > 0:\n            # Have no wanted operations but have independent operations\n            if number_independent_operations == path_len:\n                # All operations are independent\n                is_branch_correct = True\n            else:\n                is_branch_correct = False\n\n        elif number_operations > 0 and number_independent_operations > 0:\n            # Have both independent operation(s) and wanted operations\n            path_before_good_operation = is_independent_operation[:operation_ids[0]]\n            if path_before_good_operation.sum() == len(path_before_good_operation):\n                # All operations are independent before wanted operation applying\n                is_branch_correct = True\n            else:\n                is_branch_correct = False\n\n        return is_branch_correct\n", "import_text": ["typing.Tuple", "typing.List", "typing.Dict", "typing.Any", "networkx", "numpy", "pandas", "golem.core.dag.convert.graph_structure_as_nx_graph"], "prompt": "\"\"\"\nDescription: This function checks the structure of a pipeline by a given tag.\n\nArgs:\n    pipeline (Pipeline): The pipeline to be checked.\n    tag_to_check (str): The tag to check in the pipeline.\n    source_name (str): The name of the source. Defaults to DEFAULT_SOURCE_NAME.\n\nReturns:\n    bool: True if all branches in the pipeline can process the desired type of data, False otherwise.\n\"\"\"", "comment": "        \"\"\"\n        In the pipeline structure, a node with an operation with the appropriate tag is searched for.\n        In this case the operations must have priority in the pipeline - in the primary Node or not far from it.\n\n        Correct pipeline:\n        operation with tag -> linear\n        Incorrect pipeline:\n        linear -> operation with tag\n\n        :param pipeline: pipeline to check\n        :param tag_to_check: find appropriate operation by desired tag\n        (for example encoding or imputing)\n        :param source_name: label of primary node for current input data. Set as 'default' if\n        pipeline prepared for unimodal data\n        \"\"\"", "prompt_is_gen_from_api": true, "function_dependencies": ["golem.core.dag.convert.graph_structure_as_nx_graph", "networkx.all_simple_paths"], "project_create_time": "2020-01-13T12:48:37+00:00", "project_update_time": "2024-04-16T04:50:35+00:00", "file_create_time": "2021-12-01T20:05:18Z", "file_update_time": "2023-02-20T11:09:00Z", "function_update_time": "2021-12-01T20:05:18Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["networkx.all_simple_paths"], "test_function": [{"file_path": "/FEDOT-v0.7.3.1/FEDOT-0.7.3.1/test/unit/pipelines/test_pipeline_structure.py", "class_name": null, "function_name": "test_correct_pipeline_encoder_imputer_validation", "code": "def test_correct_pipeline_encoder_imputer_validation():\n    first_source = PipelineNode('data_source_table')\n    second_imputer = PipelineNode('simple_imputation', nodes_from=[first_source])\n    third_encoder = PipelineNode('one_hot_encoding', nodes_from=[second_imputer])\n    fourth_imputer = PipelineNode('simple_imputation', nodes_from=[third_encoder])\n    root = PipelineNode('linear', nodes_from=[fourth_imputer])\n    pipeline = Pipeline(root)\n\n    encoding_correct = PipelineStructureExplorer().check_structure_by_tag(pipeline, tag_to_check='encoding')\n    imputer_correct = PipelineStructureExplorer().check_structure_by_tag(pipeline, tag_to_check='imputation')\n\n    assert encoding_correct is True\n    assert imputer_correct is True"}, {"file_path": "/FEDOT-v0.7.3.1/FEDOT-0.7.3.1/test/unit/pipelines/test_pipeline_structure.py", "class_name": null, "function_name": "test_non_correct_pipeline_encoder_imputer_validation", "code": "def test_non_correct_pipeline_encoder_imputer_validation():\n    first_imputation = PipelineNode('simple_imputation')\n    first_encoder = PipelineNode('one_hot_encoding')\n\n    second_rfr = PipelineNode('rfr', nodes_from=[first_imputation])\n    second_ridge = PipelineNode('ridge', nodes_from=[first_encoder])\n    second_encoder = PipelineNode('one_hot_encoding', nodes_from=[first_imputation])\n\n    third_imputer = PipelineNode('simple_imputation', nodes_from=[second_ridge])\n    root = PipelineNode('linear', nodes_from=[second_rfr, third_imputer, second_encoder])\n    pipeline = Pipeline(root)\n\n    encoding_correct = PipelineStructureExplorer().check_structure_by_tag(pipeline, tag_to_check='encoding')\n    imputer_correct = PipelineStructureExplorer().check_structure_by_tag(pipeline, tag_to_check='imputation')\n\n    assert encoding_correct is False\n    assert imputer_correct is False"}]}, {"git_group": "ivannz", "git_name": "cplxmodule", "version": "v2022.06", "language": "Python", "project_name": "cplxmodule-v2022.06.zip", "file_path": "/cplxmodule-v2022.06/cplxmodule-2022.06/cplxmodule/cplx.py", "file_name": "cplx.py", "focal_class": null, "focal_name": "einsum", "focal_parameter": ["equation"], "solution": "def einsum(equation, *tensors):\n    if not tensors:\n        raise RuntimeError(\"`einsum()` requires at least one tensor.\")\n\n    cplx1, *tensors = tensors\n    if not tensors:\n        # no complex multiplication with only one tensor\n        return Cplx(\n            torch.einsum(equation, cplx1.real), torch.einsum(equation, cplx1.imag)\n        )\n\n    cplx2, *tensors = tensors\n    if not tensors:\n        # Einsum is complex bilinear -- the logic is same here.\n        re = torch.einsum(equation, cplx1.real, cplx2.real) - torch.einsum(\n            equation, cplx1.imag, cplx2.imag\n        )\n\n        im = torch.einsum(equation, cplx1.real, cplx2.imag) + torch.einsum(\n            equation, cplx1.imag, cplx2.real\n        )\n\n        return Cplx(re, im)\n\n    raise RuntimeError(\n        f\"`Cplx.einsum` does not support more than 2 tensors. Got {2 + len(tensors)}.\"\n    )", "function_signature": "def einsum(equation, *tensors) :", "left_context": "from copy import deepcopy\n\nimport torch\nimport torch.nn.functional as F\n\nfrom math import sqrt\nfrom .utils import complex_view, fix_dim\n\n\nclass Cplx(object):\n    r\"\"\"A type partially implementing complex valued tensors in torch.\n\n    Details\n    -------\n    Creates a complex tensor object from the real and imaginary torch tensors,\n    or pythonic floats and complex numbers. This is a container-wrapper which\n    does not copy the supplied torch tensors on creation.\n    \"\"\"\n    __slots__ = (\"__real\", \"__imag\")\n\n    def __new__(cls, real, imag=None):\n        if isinstance(real, cls):\n            return real\n\n        if isinstance(real, complex):\n            # Silently ignore imag if real is complex\n            real, imag = torch.tensor(real.real), torch.tensor(real.imag)\n\n        elif isinstance(real, float):\n            if imag is None:\n                imag = 0.0\n\n            elif not isinstance(imag, float):\n                raise TypeError(\"Imaginary part must be float.\")\n\n            real, imag = torch.tensor(real), torch.tensor(imag)\n\n        elif not isinstance(real, torch.Tensor):\n            raise TypeError(\"Real part must be torch.Tensor.\")\n\n        if imag is None:\n            imag = torch.zeros_like(real)\n\n        elif not isinstance(imag, torch.Tensor):\n            raise TypeError(\"Imaginary part must be torch.Tensor.\")\n\n        if real.shape != imag.shape:\n            raise ValueError(\"Real and imaginary parts have mistmatching shape.\")\n\n        self = super().__new__(cls)\n        self.__real, self.__imag = real, imag\n        return self\n\n    def __copy__(self):\n        r\"\"\"Shallow: a new instance with references to the real-imag data.\"\"\"\n        return type(self)(self.__real, self.__imag)\n\n    def __deepcopy__(self, memo):\n        r\"\"\"Deep: a new instance with copies of the real-imag data.\"\"\"\n        real = deepcopy(self.__real, memo)\n        imag = deepcopy(self.__imag, memo)\n        return type(self)(real, imag)\n\n    @property\n    def real(self):\n        r\"\"\"Real part of the complex tensor.\"\"\"\n        return self.__real\n\n    @property\n    def imag(self):\n        r\"\"\"Imaginary part of the complex tensor.\"\"\"\n        return self.__imag\n\n    def __getitem__(self, key):\n        r\"\"\"Index the complex tensor.\"\"\"\n        return type(self)(self.__real[key], self.__imag[key])\n\n    def __setitem__(self, key, value):\n        r\"\"\"Alter the complex tensor at index inplace.\"\"\"\n        if not isinstance(value, (Cplx, complex)):\n            self.__real[key], self.__imag[key] = value, value\n        else:\n            self.__real[key], self.__imag[key] = value.real, value.imag\n\n    def __iter__(self):\n        r\"\"\"Iterate over the zero-th dimension of the complex tensor.\"\"\"\n        return map(type(self), self.__real, self.__imag)\n\n    def __reversed__(self):\n        r\"\"\"Reverse the complex tensor along the zero-th dimension.\"\"\"\n        return type(self)(reversed(self.__real), reversed(self.__imag))\n\n    def clone(self):\n        r\"\"\"Clone a complex tensor.\"\"\"\n        return type(self)(self.__real.clone(), self.__imag.clone())\n\n    @property\n    def conj(self):\n        r\"\"\"The complex conjugate of the complex tensor.\"\"\"\n        return type(self)(self.__real, -self.__imag)\n\n    def conjugate(self):\n        r\"\"\"The complex conjugate of the complex tensor.\"\"\"\n        return self.conj\n\n    def __pos__(self):\n        r\"\"\"Return the complex tensor as is.\"\"\"\n        return self\n\n    def __neg__(self):\n        r\"\"\"Flip the sign of the complex tensor.\"\"\"\n        return type(self)(-self.__real, -self.__imag)\n\n    def __add__(u, v):\n        r\"\"\"Sum of complex tensors.\"\"\"\n        if not isinstance(v, (Cplx, complex)):\n            return type(u)(u.__real + v, u.__imag)\n        return type(u)(u.__real + v.real, u.__imag + v.imag)\n\n    __radd__ = __add__\n    __iadd__ = __add__\n\n    def __sub__(u, v):\n        r\"\"\"Difference of complex tensors.\"\"\"\n        if not isinstance(v, (Cplx, complex)):\n            return type(u)(u.__real - v, u.__imag)\n        return type(u)(u.__real - v.real, u.__imag - v.imag)\n\n    def __rsub__(u, v):\n        r\"\"\"Difference of complex tensors.\"\"\"\n        return -u + v\n\n    __isub__ = __sub__\n\n    def __mul__(u, v):\n        r\"\"\"Elementwise product of complex tensors.\"\"\"\n        if not isinstance(v, (Cplx, complex)):\n            return type(u)(u.__real * v, u.__imag * v)\n\n        # (a + ib) (u + iv) = au - bv + i(av + bu)\n        # (a+u)(b+v) = ab + uv + (av + ub)\n        # (a-v)(b+u) = ab - uv + (au - vb)\n        return type(u)(\n            u.__real * v.real - u.__imag * v.imag,\n            u.__imag * v.real + u.__real * v.imag,\n        )\n\n    __rmul__ = __mul__\n    __imul__ = __mul__\n\n    def __truediv__(u, v):\n        r\"\"\"Elementwise division of complex tensors.\"\"\"\n        if not isinstance(v, (Cplx, complex)):\n            return type(u)(u.__real / v, u.__imag / v)\n\n        denom = v.real * v.real + v.imag * v.imag\n        return u * (v.conjugate() / denom)\n\n    def __rtruediv__(u, v):\n        r\"\"\"Elementwise division of something by a complex tensor.\"\"\"\n        # v / u and v is not Cplx\n        denom = u.__real * u.__real + u.__imag * u.__imag\n        return (u.conjugate() / denom) * v\n\n    __itruediv__ = __truediv__\n\n    def __matmul__(u, v):\n        r\"\"\"Complex matrix-matrix product of complex tensors.\"\"\"\n        if not isinstance(v, Cplx):\n            return type(u)(torch.matmul(u.__real, v), torch.matmul(u.__imag, v))\n\n        re = torch.matmul(u.__real, v.__real) - torch.matmul(u.__imag, v.__imag)\n        im = torch.matmul(u.__imag, v.__real) + torch.matmul(u.__real, v.__imag)\n        return type(u)(re, im)\n\n    def __rmatmul__(u, v):\n        r\"\"\"Matrix multiplication by a complex tensor from the right.\"\"\"\n        # v @ u and v is not Cplx\n        return type(u)(torch.matmul(v, u.__real), torch.matmul(v, u.__imag))\n\n    __imatmul__ = __matmul__\n\n    def __abs__(self):\n        r\"\"\"Compute the complex modulus:\n        $$\n            \\mathbb{C}^{\\ldots \\times d}\n                \\to \\mathbb{R}_+^{\\ldots \\times d}\n            \\colon u + i v \\mapsto \\lvert u + i v \\rvert\n            \\,. $$\n        \"\"\"\n        input = torch.stack([self.__real, self.__imag], dim=0)\n        return torch.norm(input, p=2, dim=0, keepdim=False)\n\n    @property\n    def angle(self):\n        r\"\"\"Compute the complex argument:\n        $$\n            \\mathbb{C}^{\\ldots \\times d}\n                \\to \\mathbb{R}^{\\ldots \\times d}\n            \\colon \\underbrace{u + i v}_{r e^{i\\phi}} \\mapsto \\phi\n                    = \\arctan \\tfrac{v}{u}\n            \\,. $$\n        \"\"\"\n        return torch.atan2(self.__imag, self.__real)\n\n    def apply(self, f, *a, **k):\n        r\"\"\"Applies the function to real and imaginary parts.\"\"\"\n        return type(self)(f(self.__real, *a, **k), f(self.__imag, *a, **k))\n\n    @property\n    def shape(self):\n        r\"\"\"Returns the shape of the complex tensor.\"\"\"\n        return self.__real.shape\n\n    def __len__(self):\n        r\"\"\"The size of the zero-th dimension of the complex tensor.\"\"\"\n        return self.shape[0]\n\n    def t(self):\n        r\"\"\"The transpose of a 2d compelx tensor.\"\"\"\n        return type(self)(self.__real.t(), self.__imag.t())\n\n    def h(self):\n        r\"\"\"The Hermitian transpose of a 2d compelx tensor.\"\"\"\n        return self.conj.t()  # Cplx(self.__real.t(), -self.__imag.t())\n\n    def flatten(self, start_dim=0, end_dim=-1):\n        return type(self)(\n            self.__real.flatten(start_dim, end_dim),\n            self.__imag.flatten(start_dim, end_dim),\n        )\n\n    def view(self, *shape):\n        r\"\"\"Return a view of the complex tensor.\"\"\"\n        shape = shape[0] if shape and isinstance(shape[0], tuple) else shape\n        return type(self)(self.__real.view(*shape), self.__imag.view(*shape))\n\n    def view_as(self, other):\n        r\"\"\"Return a view of the complex tensor of shape other.\"\"\"\n        shape = other.shape\n        return self.view(*shape)\n\n    def reshape(self, *shape):\n        r\"\"\"Reshape the complex tensor.\"\"\"\n        shape = shape[0] if shape and isinstance(shape[0], tuple) else shape\n        return type(self)(self.__real.reshape(*shape), self.__imag.reshape(*shape))\n\n    def size(self, *dim):\n        r\"\"\"Returns the size of the complex tensor.\"\"\"\n        return self.__real.size(*dim)\n\n    def squeeze(self, dim=None):\n        r\"\"\"Returns the complex tensor with all the dimensions of input of\n        size one removed.\n        \"\"\"\n        if dim is None:\n            return type(self)(self.__real.squeeze(), self.__imag.squeeze())\n        else:\n            return type(self)(\n                self.__real.squeeze(dim=dim), self.__imag.squeeze(dim=dim)\n            )\n\n    def unsqueeze(self, dim=None):\n        r\"\"\"Returns a new complex tensor with a dimension of size one inserted\n        at the specified position.\n        \"\"\"\n        if dim is None:\n            return type(self)(self.__real.unsqueeze(), self.__imag.unsqueeze())\n        else:\n            return type(self)(\n                self.__real.unsqueeze(dim=dim), self.__imag.unsqueeze(dim=dim)\n            )\n\n    def item(self):\n        r\"\"\"The scalar value of zero-dim complex tensor.\"\"\"\n        return float(self.__real) + 1j * float(self.__imag)\n\n    @classmethod\n    def from_numpy(cls, numpy):\n        r\"\"\"Create a complex tensor from numpy array.\"\"\"\n        re = torch.from_numpy(numpy.real)\n        im = torch.from_numpy(numpy.imag)\n        return cls(re, im)\n\n    def numpy(self):\n        r\"\"\"Export a complex tensor as complex numpy array.\"\"\"\n        return self.__real.numpy() + 1j * self.__imag.numpy()\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(\\n\"\n            f\"  real={self.__real},\\n  imag={self.__imag}\\n)\"\n        )\n\n    def detach(self):\n        r\"\"\"Return a copy of the complex tensor detached from autograd graph.\"\"\"\n        return type(self)(self.__real.detach(), self.__imag.detach())\n\n    def requires_grad_(self, requires_grad=True):\n        r\"\"\"Toggle the gradient of real and imaginary parts.\"\"\"\n        return type(self)(\n            self.__real.requires_grad_(requires_grad),\n            self.__imag.requires_grad_(requires_grad),\n        )\n\n    @property\n    def grad(self):\n        r\"\"\"Collect the accumulated gradinet of the complex tensor.\"\"\"\n        re, im = self.__real.grad, self.__imag.grad\n        return None if re is None or im is None else type(self)(re, im)\n\n    def cuda(self, device=None, non_blocking=False):\n        r\"\"\"Move the complex tensor to a CUDA device.\"\"\"\n        re = self.__real.cuda(device=device, non_blocking=non_blocking)\n        im = self.__imag.cuda(device=device, non_blocking=non_blocking)\n        return type(self)(re, im)\n\n    def cpu(self):\n        r\"\"\"Move the complex tensor to CPU.\"\"\"\n        return type(self)(self.__real.cpu(), self.__imag.cpu())\n\n    def to(self, *args, **kwargs):\n        r\"\"\"Move / typecast the complex tensor.\"\"\"\n        return type(self)(\n            self.__real.to(*args, **kwargs), self.__imag.to(*args, **kwargs)\n        )\n\n    @property\n    def device(self):\n        r\"\"\"The hosting device of the complex tensor.\"\"\"\n        return self.__real.device\n\n    @property\n    def dtype(self):\n        r\"\"\"The base dtype of the complex tensor.\"\"\"\n        return self.__real.dtype\n\n    def dim(self):\n        r\"\"\"The number of dimensions in the complex tensor.\"\"\"\n        return len(self.shape)\n\n    def permute(self, *dims):\n        r\"\"\"Shuffle the dimensions of the complex tensor.\"\"\"\n        return type(self)(self.__real.permute(*dims), self.__imag.permute(*dims))\n\n    def transpose(self, dim0, dim1):\n        r\"\"\"Transpose the specified dimensions of the complex tensor.\"\"\"\n        return type(self)(\n            self.__real.transpose(dim0, dim1), self.__imag.transpose(dim0, dim1)\n        )\n\n    def is_complex(self):\n        r\"\"\"Test if the tensor indeed represents a complex number.\"\"\"\n        return True\n\n    @classmethod\n    def empty(cls, *sizes, dtype=None, device=None, requires_grad=False):\n        r\"\"\"Create an empty complex tensor.\"\"\"\n        re = torch.empty(\n            *sizes, dtype=dtype, device=device, requires_grad=requires_grad\n        )\n        return cls(re, torch.empty_like(re, requires_grad=requires_grad))\n\n    @classmethod\n    def zeros(cls, *sizes, dtype=None, device=None, requires_grad=False):\n        r\"\"\"Create an empty complex tensor.\"\"\"\n        re = torch.zeros(\n            *sizes, dtype=dtype, device=device, requires_grad=requires_grad\n        )\n        return cls(re, torch.zeros_like(re, requires_grad=requires_grad))\n\n    @classmethod\n    def ones(cls, *sizes, dtype=None, device=None, requires_grad=False):\n        r\"\"\"Create an empty complex tensor.\"\"\"\n        re = torch.ones(*sizes, dtype=dtype, device=device, requires_grad=requires_grad)\n        return cls(re, torch.zeros_like(re, requires_grad=requires_grad))\n\n\ndef cat(tensors, dim):\n    tensors = [*map(Cplx, tensors)]\n    return Cplx(\n        torch.cat([z.real for z in tensors], dim=dim),\n        torch.cat([z.imag for z in tensors], dim=dim),\n    )\n\n\ndef split(input, split_size_or_sections, dim=0):\n    \"\"\"see documentation for `torch.split`\"\"\"\n    return tuple(\n        Cplx(re, im)\n        for re, im in zip(\n            torch.split(input.real, split_size_or_sections, dim),\n            torch.split(input.imag, split_size_or_sections, dim),\n        )\n    )\n\n\ndef chunk(input, chunks, dim=0):\n    \"\"\"see documentation for `torch.chunk`\"\"\"\n    return tuple(\n        Cplx(re, im)\n        for re, im in zip(\n            torch.chunk(input.real, chunks, dim),\n            torch.chunk(input.imag, chunks, dim),\n        )\n    )\n\n\ndef stack(tensors, dim):\n    tensors = [*map(Cplx, tensors)]\n    return Cplx(\n        torch.stack([z.real for z in tensors], dim=dim),\n        torch.stack([z.imag for z in tensors], dim=dim),\n    )\n\n\ndef unbind(input, dim=0):\n    \"\"\"see documentation for `torch.unbind`\"\"\"\n    return tuple(\n        Cplx(re, im)\n        for re, im in zip(\n            torch.unbind(input.real, dim),\n            torch.unbind(input.imag, dim),\n        )\n    )\n\n\ndef take(input, index):\n    \"\"\"see documentation for `torch.take`\"\"\"\n    return Cplx(torch.take(input.real, index), torch.take(input.imag, index))\n\n\ndef narrow(input, dim, start, length):\n    \"\"\"see documentation for `torch.narrow`\"\"\"\n    return Cplx(\n        torch.narrow(input.real, dim, start, length),\n        torch.narrow(input.imag, dim, start, length),\n    )\n\n\ndef squeeze(input, dim=None):\n    \"\"\"see documentation for `torch.squeeze`\"\"\"\n    return Cplx(torch.squeeze(input.real, dim), torch.squeeze(input.imag, dim))\n\n\ndef unsqueeze(input, dim):\n    \"\"\"see documentation for `torch.unsqueeze`\"\"\"\n    return Cplx(torch.unsqueeze(input.real, dim), torch.unsqueeze(input.imag, dim))\n\n\ndef from_interleaved_real(input, copy=True, dim=-1):\n    \"\"\"Map real tensor input `... x [D * 2]` to a pair (re, im) with dim `... x D`.\"\"\"\n    output = Cplx(*complex_view(input, dim, squeeze=False))\n    return output.clone() if copy else output\n\n\nfrom_real = from_interleaved_real\n\n\ndef from_concatenated_real(input, copy=True, dim=-1):\n    \"\"\"Map real tensor input `... x [2 * D]` to a pair (re, im) with dim `... x D`.\"\"\"\n    output = Cplx(*torch.chunk(input, 2, dim=dim))\n    return output.clone() if copy else output\n\n\ndef to_interleaved_real(input, flatten=True, dim=-1):\n    \"\"\"Interleave the complex re-im pair into a real tensor.\"\"\"\n    dim = 1 + fix_dim(dim, input.dim())\n    input = torch.stack([input.real, input.imag], dim=dim)\n    return input.flatten(dim - 1, dim) if flatten else input\n\n\nto_real = to_interleaved_real\n\n\ndef to_concatenated_real(input, flatten=None, dim=-1):\n    \"\"\"Map real tensor input `... x [2 * D]` to a pair (re, im) with dim `... x D`.\"\"\"\n    assert flatten is None\n    return torch.cat([input.real, input.imag], dim=dim)\n\n\ndef exp(input):\n    r\"\"\"Compute the exponential of the complex tensor in re-im pair.\"\"\"\n    scale = torch.exp(input.real)\n    return Cplx(scale * torch.cos(input.imag), scale * torch.sin(input.imag))\n\n\ndef log(input):\n    r\"\"\"Compute the logarithm of the complex tensor in re-im pair.\"\"\"\n    return Cplx(torch.log(abs(input)), input.angle)\n\n\ndef sin(input):\n    r\"\"\"Compute the sine of the complex tensor in re-im pair.\"\"\"\n    return Cplx(\n        torch.sin(input.real) * torch.cosh(input.imag),\n        torch.cos(input.real) * torch.sinh(input.imag),\n    )\n\n\ndef cos(input):\n    r\"\"\"Compute the cosine of the complex tensor in re-im pair.\"\"\"\n    return Cplx(\n        torch.cos(input.real) * torch.cosh(input.imag),\n        -torch.sin(input.real) * torch.sinh(input.imag),\n    )\n\n\ndef tan(input):\n    r\"\"\"Compute the tangent of the complex tensor in re-im pair.\"\"\"\n    return sin(input) / cos(input)\n\n\ndef sinh(input):\n    r\"\"\"Compute the hyperbolic sine of the complex tensor in re-im pair.\n\n    sinh(z) = - j sin(j z)\n    \"\"\"\n    return Cplx(\n        torch.sinh(input.real) * torch.cos(input.imag),\n        torch.cosh(input.real) * torch.sin(input.imag),\n    )\n\n\ndef cosh(input):\n    r\"\"\"Compute the hyperbolic cosine of the complex tensor in re-im pair.\n\n    cosh(z) = cos(j z)\n    \"\"\"\n    return Cplx(\n        torch.cosh(input.real) * torch.cos(input.imag),\n        torch.sinh(input.real) * torch.sin(input.imag),\n    )\n\n\ndef tanh(input):\n    r\"\"\"Compute the hyperbolic tangent of the complex tensor in re-im pair.\n\n    tanh(z) = j tan(z)\n    \"\"\"\n    return sinh(input) / cosh(input)\n\n\ndef randn(*size, dtype=None, device=None, requires_grad=False):\n    \"\"\"Generate standard complex Gaussian noise.\"\"\"\n    normal = torch.randn(\n        2, *size, dtype=dtype, layout=torch.strided, device=device, requires_grad=False\n    ) / sqrt(2)\n    z = Cplx(normal[0], normal[1])\n    return z.requires_grad_(True) if requires_grad else z\n\n\ndef randn_like(input, dtype=None, device=None, requires_grad=False):\n    \"\"\"Returns a tensor with the same size as `input` that is filled with\n    standard comlpex Gaussian random numbers.\n    \"\"\"\n    return randn(\n        *input.size(),\n        dtype=input.dtype if dtype is None else dtype,\n        device=input.device if device is None else device,\n        requires_grad=requires_grad,\n    )\n\n\ndef modrelu(input, threshold=0.5):\n    r\"\"\"Soft-threshold the modulus of the complex tensor.\n\n    Parameters\n    ----------\n    input : Cplx tensor\n        The complex valued data to which modReLU is to be applied elementwise.\n\n    threshold : float, default=0.5\n        The clipping threshold of this version of modReLU. See details.\n\n    Returns\n    -------\n    output : Cplx tensor\n        The values which have their complex modulus soft-thresholded and their\n        complex phase retained.\n\n    Details\n    -------\n    This function actually implements a slightly reparameterized version\n    of modReLU (note the negative sign):\n\n    $$\n    \\operatorname{modReLU}\n        \\colon \\mathbb{C} \\times \\mathbb{R} \\to  \\mathbb{C}\n        \\colon\n            (z, \\tau)\n                \\mapsto z \\max \\biggl\\{\n                    0, 1 - \\frac{\\tau}{\\lvert z \\rvert}\n                \\biggr\\}\n                = \\max \\biggl\\{\n                    0, \\lvert z \\rvert - \\tau\n                \\biggr\\} e^{j \\phi}\n        \\,. $$\n\n    This parameterization deviates from the non-linearity, originally proposed\n    in\n\n        [Arjovsky et al. (2016)](http://proceedings.mlr.press/v48/arjovsky16.html)\n\n    by having the sign of the bias parameter $b$ in eq.~(8) FLIPPED.\n\n    The rationale behind this discrepancy in the implementation was that this\n    non-linearity resembles the soft-thresholding operator, with the parameter\n    playing the role of the zeroing threshold:\n\n        the higher the threshold the larger should the magnitude of the complex\n        number be in order to avoid being zeroed.\n    \"\"\"\n    # scale = (1 - \\trfac{b}{|z|})_+\n    modulus = torch.clamp(abs(input), min=1e-5)\n    return input * torch.relu(1.0 - threshold / modulus)\n\n\ndef phaseshift(input, phi=0.0):\n    r\"\"\"\n    Apply phase shift to the complex tensor in re-im pair.\n    $$\n        F\n        \\colon \\mathbb{C} \\to \\mathbb{C}\n        \\colon z \\mapsto z e^{i\\phi}\n                = u cos \\phi - v sin \\phi\n                    + i (u sin \\phi + v cos \\phi)\n        \\,, $$\n    with $\\phi$ in radians.\n    \"\"\"\n    return input * Cplx(torch.cos(phi), torch.sin(phi))\n\n\ndef linear_naive(input, weight, bias=None):\n    r\"\"\"Applies a complex linear transformation to the incoming complex\n    data: :math:`y = x A^T + b`.\n    \"\"\"\n    # W = U + i V,  z = u + i v, c = \\Re c + i \\Im c\n    #  W z + c = (U + i V) (u + i v) + \\Re c + i \\Im c\n    #          = (U u + \\Re c - V v) + i (V u + \\Im c + U v)\n    re = F.linear(input.real, weight.real) - F.linear(input.imag, weight.imag)\n    im = F.linear(input.real, weight.imag) + F.linear(input.imag, weight.real)\n\n    output = Cplx(re, im)\n    if bias is not None:\n        output += bias\n\n    return output\n\n\ndef linear_cat(input, weight, bias=None):\n    # [n_out, n_in] -> [2 * n_out, 2 * n_in] : [[U, V], [-V, U]]\n    ww = torch.cat(\n        [\n            torch.cat([weight.real, weight.imag], dim=0),\n            torch.cat([-weight.imag, weight.real], dim=0),\n        ],\n        dim=1,\n    )\n\n    xx = to_concatenated_real(input, dim=-1)  # [..., 2 * n_in]\n    output = from_concatenated_real(F.linear(xx, ww, None))\n    if bias is not None:\n        output += bias\n\n    return output\n\n\ndef linear_3m(input, weight, bias=None):\n    r\"\"\"Applies a complex linear transformation to the incoming complex\n    data: :math:`y = x A^T + b`.\n\n    Strassen's 3M\n    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.1356&rep=rep1&type=pdf\n\n    This method trades off one `multiplication` for three extra additions\n    https://cnx.org/contents/4kChocHM@6/Efficient-FFT-Algorithm-and-Programming-Tricks#p2\n    https://en.wikipedia.org/wiki/Multiplication_algorithm#Complex_multiplication_algorithm\n    \"\"\"\n    # W = U + j V,  z = u + j v, c = \\Re c + j \\Im c\n    #  W z + c = (U + j V) (u + j v) + \\Re c + j \\Im c\n    #          = (U u + \\Re c - V v) + j (V u + \\Im c + U v)\n    #          = [\u00b1 U (v + j u)]  # Gauss trick\n    #          =     (U (u + v) - (U + V) v + \\Re c)\n    #            + j (U (u + v) + (V - U) u + \\Im c)\n    K1 = F.linear(input.real + input.imag, weight.real)\n    K2 = F.linear(input.real, weight.imag - weight.real)\n    K3 = F.linear(input.imag, weight.real + weight.imag)\n\n    output = Cplx(K1 - K3, K1 + K2)\n    if bias is not None:\n        output += bias\n\n    return output\n\n\n# use naive multiplication by default\nlinear = linear_naive\n\n\ndef symmetric_circular_padding(input, padding):\n    # `F.pad` works only for 3d, 4d, and 5d inputs\n    assert input.dim() > 2\n    if isinstance(padding, int):\n        padding = (input.dim() - 2) * [padding]\n\n    assert isinstance(padding, (tuple, list))\n    assert len(padding) + 2 == input.dim()\n\n    expanded_padding = []\n    for pad in padding:\n        expanded_padding.extend(((pad + 1) // 2, pad // 2))\n\n    return input.apply(F.pad, tuple(expanded_padding), mode=\"circular\")\n\n\ndef convnd_naive(conv, input, weight, stride=1, padding=0, dilation=1, groups=1):\n\n    re = conv(input.real, weight.real, None, stride, padding, dilation, groups) - conv(\n        input.imag, weight.imag, None, stride, padding, dilation, groups\n    )\n    im = conv(input.real, weight.imag, None, stride, padding, dilation, groups) + conv(\n        input.imag, weight.real, None, stride, padding, dilation, groups\n    )\n\n    return Cplx(re, im)\n\n\ndef convnd_quick(conv, input, weight, stride=1, padding=0, dilation=1):\n    r\"\"\"Applies a complex convolution transformation to the complex data\n    :math:`y = x \\ast W + b` using two calls to `conv` at the cost of extra\n    concatenation and slicing.\n    \"\"\"\n    n_out = int(weight.shape[0])\n\n    ww = torch.cat([weight.real, weight.imag], dim=0)\n    wr = conv(input.real, ww, None, stride, padding, dilation, 1)\n    wi = conv(input.imag, ww, None, stride, padding, dilation, 1)\n\n    rwr, iwr = wr[:, :n_out], wr[:, n_out:]\n    rwi, iwi = wi[:, :n_out], wi[:, n_out:]\n    return Cplx(rwr - iwi, iwr + rwi)\n\n\ndef convnd_3m(conv, input, weight, stride=1, padding=0, dilation=1, groups=1):\n    r\"\"\"Applies a complex convolution transformation to the complex data\n    :math:`y = x \\ast W + b` using 3 calls to `conv`.\n    \"\"\"\n    # W = U + j V,  z = u + j v\n    #  W z = (U + j V) (u + j v)\n    #          = (U u - V v) + j (V u + U v)\n    #          = [\u00b1 U (v + j u)]  # Gauss trick\n    #          =     (U (u + v) - (U + V) v)\n    #            + j (U (u + v) + (V - U) u)\n    K1 = conv(\n        input.real + input.imag, weight.real, None, stride, padding, dilation, groups\n    )\n\n    K2 = conv(\n        input.real, weight.imag - weight.real, None, stride, padding, dilation, groups\n    )\n\n    K3 = conv(\n        input.imag, weight.real + weight.imag, None, stride, padding, dilation, groups\n    )\n\n    return Cplx(K1 - K3, K1 + K2)\n\n\ndef convnd(\n    conv,\n    input,\n    weight,\n    bias=None,\n    stride=1,\n    padding=0,\n    dilation=1,\n    groups=1,\n    padding_mode=\"zeros\",\n):\n    r\"\"\"Applies a complex n-d convolution to the incoming complex\n    tensor `B x c_in x L_1 x ... L_n`: :math:`y = x \\star W + b`.\n    \"\"\"\n    if padding_mode == \"circular\":\n        input = symmetric_circular_padding(input, padding)\n        padding = 0\n    elif padding_mode != \"zeros\":\n        raise ValueError(\"padding_mode must be 'zeros' or 'circular'.\")\n\n    if groups == 1:\n        # ungroupped convolution can be done a little bit faster\n        output = convnd_quick(conv, input, weight, stride, padding, dilation)\n    else:\n        output = convnd_naive(conv, input, weight, stride, padding, dilation, groups)\n\n    if bias is not None:\n        broadcast = (input.dim() - 2) * [1]\n        output += bias.reshape(-1, *broadcast)\n\n    return output\n\n\ndef conv1d(\n    input,\n    weight,\n    bias=None,\n    stride=1,\n    padding=0,\n    dilation=1,\n    groups=1,\n    padding_mode=\"zeros\",\n):\n    r\"\"\"Applies a complex 1d convolution to the incoming complex\n    tensor `B x c_in x L`: :math:`y = x \\star W + b`.\n    \"\"\"\n\n    return convnd(\n        F.conv1d, input, weight, bias, stride, padding, dilation, groups, padding_mode\n    )\n\n\ndef conv2d(\n    input,\n    weight,\n    bias=None,\n    stride=1,\n    padding=0,\n    dilation=1,\n    groups=1,\n    padding_mode=\"zeros\",\n):\n    r\"\"\"Applies a complex 2d convolution to the incoming complex\n    tensor `B x c_in x H x W`: :math:`y = x \\star W + b`.\n    \"\"\"\n\n    return convnd(\n        F.conv2d, input, weight, bias, stride, padding, dilation, groups, padding_mode\n    )\n\n\ndef conv3d(\n    input,\n    weight,\n    bias=None,\n    stride=1,\n    padding=0,\n    dilation=1,\n    groups=1,\n    padding_mode=\"zeros\",\n):\n    r\"\"\"Applies a complex 3d convolution to the incoming complex\n    tensor `B x c_in x H x W x D`: :math:`y = x \\star W + b`.\n    \"\"\"\n\n    return convnd(\n        F.conv3d, input, weight, bias, stride, padding, dilation, groups, padding_mode\n    )\n\n\ndef conv_transposend_naive(\n    conv_t, input, weight, stride=1, padding=0, output_padding=0, groups=1, dilation=1\n):\n    re = conv_t(\n        input.real, weight.real, None, stride, padding, output_padding, groups, dilation\n    ) - conv_t(\n        input.imag, weight.imag, None, stride, padding, output_padding, groups, dilation\n    )\n    im = conv_t(\n        input.real, weight.imag, None, stride, padding, output_padding, groups, dilation\n    ) + conv_t(\n        input.imag, weight.real, None, stride, padding, output_padding, groups, dilation\n    )\n    return Cplx(re, im)\n\n\ndef conv_transposend_3m(\n    conv_t, input, weight, stride=1, padding=0, output_padding=0, groups=1, dilation=1\n):\n    \"\"\"3m interface for transposed convolutions.\"\"\"\n    K1 = conv_t(\n        input.real + input.imag,\n        weight.real,\n        None,\n        stride,\n        padding,\n        output_padding,\n        groups,\n        dilation,\n    )\n\n    K2 = conv_t(\n        input.real,\n        weight.imag - weight.real,\n        None,\n        stride,\n        padding,\n        output_padding,\n        groups,\n        dilation,\n    )\n\n    K3 = conv_t(\n        input.imag,\n        weight.real + weight.imag,\n        None,\n        stride,\n        padding,\n        output_padding,\n        groups,\n        dilation,\n    )\n\n    return Cplx(K1 - K3, K1 + K2)\n\n\ndef conv_transposend(\n    conv,\n    input,\n    weight,\n    bias=None,\n    stride=1,\n    padding=0,\n    output_padding=1,\n    groups=1,\n    dilation=1,\n    padding_mode=\"zeros\",\n):\n    r\"\"\"Applies a complex n-d transposed convolution to the\n    incoming complex tensor. See torch.nn.ConvTranspose2d\n    for documentation.\"\"\"\n    if padding_mode == \"circular\":\n        input = symmetric_circular_padding(input, padding)\n        padding = 0\n    elif padding_mode != \"zeros\":\n        raise ValueError(\"padding_mode must be 'zeros' or 'circular'.\")\n\n    output = conv_transposend_naive(\n        conv, input, weight, stride, padding, output_padding, groups, dilation\n    )\n\n    if bias is not None:\n        broadcast = (input.dim() - 2) * [1]\n        output += bias.reshape(-1, *broadcast)\n\n    return output\n\n\ndef conv_transpose1d(\n    input,\n    weight,\n    bias=None,\n    stride=1,\n    padding=0,\n    output_padding=0,\n    groups=0,\n    dilation=1,\n    padding_mode=\"zeros\",\n):\n    r\"\"\"Applies a complex 1d transposed convolution to the\n    incoming complex tensor. See torch.nn.ConvTranspose1d\n    for documentation.\"\"\"\n    return conv_transposend(\n        F.conv_transpose1d,\n        input,\n        weight,\n        bias,\n        stride,\n        padding,\n        output_padding,\n        groups,\n        dilation,\n        padding_mode,\n    )\n\n\ndef conv_transpose2d(\n    input,\n    weight,\n    bias=None,\n    stride=1,\n    padding=0,\n    output_padding=0,\n    groups=0,\n    dilation=1,\n    padding_mode=\"zeros\",\n):\n    r\"\"\"Applies a complex 2d transposed convolution to the\n    incoming complex tensor. See torch.nn.ConvTranspose2d\n    for documentation.\"\"\"\n    return conv_transposend(\n        F.conv_transpose2d,\n        input,\n        weight,\n        bias,\n        stride,\n        padding,\n        output_padding,\n        groups,\n        dilation,\n        padding_mode,\n    )\n\n\ndef conv_transpose3d(\n    input,\n    weight,\n    bias=None,\n    stride=1,\n    padding=0,\n    output_padding=0,\n    groups=0,\n    dilation=1,\n    padding_mode=\"zeros\",\n):\n    r\"\"\"Applies a complex 3d transposed convolution to the\n    incoming complex tensor. See torch.nn.ConvTranspose3d\n    for documentation.\"\"\"\n    return conv_transposend(\n        F.conv_transpose3d,\n        input,\n        weight,\n        bias,\n        stride,\n        padding,\n        output_padding,\n        groups,\n        dilation,\n        padding_mode,\n    )\n\n", "right_context": "\n\ndef bilinear_naive(input1, input2, weight, bias=None, conjugate=True):\n    r\"\"\"Applies a complex bilinear transformation to the incoming complex\n    data: :math:`y = x^(T/H) W z + b`.\n    \"\"\"\n\n    n_out = int(weight.shape[0])\n\n    ww = torch.cat([weight.real, weight.imag], dim=0)\n    a, b = input1.real, input1.imag\n    u, v = input2.real, input2.imag\n    au, av = F.bilinear(a, u, ww, bias=None), F.bilinear(a, v, ww, bias=None)\n    bu, bv = F.bilinear(b, u, ww, bias=None), F.bilinear(b, v, ww, bias=None)\n\n    if conjugate:\n        pp, qq = au + bv, av - bu\n    else:\n        pp, qq = au - bv, av + bu\n\n    repp, impp = pp[..., :n_out], pp[..., n_out:]\n    reqq, imqq = qq[..., :n_out], qq[..., n_out:]\n\n    output = Cplx(repp - imqq, impp + reqq)\n    if bias is not None:\n        output += bias\n\n    return output\n\n\nbilinear = bilinear_naive\n\n\ndef bilinear_cat(input1, input2, weight, bias=None, conjugate=True):\n    # [n_out, n_in1, n_in2] -> [2 * n_out, 2 * n_in1, 2 * n_in2]\n    U, V = weight.real, weight.imag\n\n    UV = torch.cat([U, -V], dim=2)\n    VU = torch.cat([V, U], dim=2)\n    if conjugate:\n        ww = torch.cat([torch.cat([UV, VU], dim=1), torch.cat([VU, -UV], dim=1)], dim=0)\n    else:\n        ww = torch.cat([torch.cat([UV, -VU], dim=1), torch.cat([VU, UV], dim=1)], dim=0)\n\n    x1 = to_concatenated_real(input1, dim=-1)\n    x2 = to_concatenated_real(input2, dim=-1)\n\n    output = from_concatenated_real(F.bilinear(x1, x2, ww, None))\n    if bias is not None:\n        output += bias\n\n    return output\n\n\ndef max_poolnd(\n    pool, input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False\n):\n    r\"\"\"Applies complex version of n-d max-pooling to the incoming complex\n    tensor `B x c_in x L_1 x ... L_n`.\n\n    Details\n    -------\n    The variant is based on\n\n        [Zhang et al. (eq. (3))](https://ieeexplore.ieee.org/document/8039431)\n\n    wherein the indices of the complex values, which remain after pooling,\n    are determined by the maximum among the absolute values of the complex\n    numbers within the pooling window. For abstract indices the operation\n    has the following form\n\n    $$\n        y_\\alpha = x_{\\xi(\\alpha)}\n            \\,,\n            \\xi(\\alpha)\n                = \\arg \\max_{\\beta \\in P(\\alpha)}\n                    \\lvert x_{\\beta} \\rvert\n        \\,, $$\n\n    where $P(\\alpha)$ is the subset of indices within the input magnitudes,\n    that specify the pooling window associated with the index $\\alpha$ in\n    the output.\n\n    WARNING\n    -------\n    For real tensor embedded as complex tensors, i.e.\n\n         $x \\in \\mathbb{R}^{\\cdot} \\subset \\mathbb{C}^{\\cdot}$,\n\n    the real-valued max-pooling and this complex variant would yield different\n    results by design!\n    \"\"\"\n\n    # 1. get the abs max-pooling indices\n    #  XXX should we care about the extra sqrt in abs?\n    _, indices = pool(\n        abs(input),\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        ceil_mode,\n        return_indices=True,\n    )\n\n    # 2. the indices refer to the positions in the trailing flattened\n    #    magnitudes, so we flatten the trailing dims after the channels.\n    flat = input.flatten(2, -1)\n\n    # 3. gather the picked values from re-im parts\n    ix = indices.flatten(2, -1)\n    re = torch.gather(flat.real, -1, ix)\n    im = torch.gather(flat.imag, -1, ix)\n\n    # 4. reassemble\n    return Cplx(re, im).reshape(input.shape[:2] + indices.shape[2:])\n\n\ndef max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False):\n    \"\"\"Complex 1d max pooling on the complex tensor `B x c_in x L`.\"\"\"\n\n    return max_poolnd(\n        F.max_pool1d, input, kernel_size, stride, padding, dilation, ceil_mode\n    )\n\n\ndef max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False):\n    \"\"\"Complex 2d max pooling on the complex tensor `B x c_in x H x W`.\"\"\"\n\n    return max_poolnd(\n        F.max_pool2d, input, kernel_size, stride, padding, dilation, ceil_mode\n    )\n\n\ndef max_pool3d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False):\n    \"\"\"Complex 3d max pooling on the complex tensor `B x c_in x H x W x D`.\"\"\"\n\n    return max_poolnd(\n        F.max_pool3d, input, kernel_size, stride, padding, dilation, ceil_mode\n    )\n", "import_text": ["copy.deepcopy", "torch", "torch.nn.functional", "math.sqrt"], "prompt": "\"\"\"\nDescription: This function performs an einsum operation on complex tensors.\n\nArgs:\n    equation (str): The equation to be used for the einsum operation.\n    *tensors (Cplx): Variable length argument, representing complex tensors.\n\nReturns:\n    Cplx: The result of the einsum operation on the complex tensors.\n\nRaises:\n    RuntimeError: If no tensors are provided or if more than 2 tensors are provided.\n\nNotes:\n    The einsum operation is a powerful tool for performing operations on arrays.\n    This function specifically handles complex tensors, performing the einsum operation on the real and imaginary parts separately.\n    The result is a complex tensor.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"2-tensor einstein summation.\"\"\"", "function_dependencies": ["torch.einsum"], "project_create_time": "2019-04-25T05:40:47+00:00", "project_update_time": "2024-03-27T14:09:51+00:00", "file_create_time": "2019-04-25T10:38:22Z", "file_update_time": "2022-06-08T10:22:02Z", "function_update_time": "2019-12-25T14:08:05Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["torch.einsum"], "test_function": [{"file_path": "/cplxmodule-v2022.06/cplxmodule-2022.06/tests/test_cplx.py", "class_name": null, "function_name": "test_enisum", "code": "\ndef test_enisum(random_state):\n    a = random_state.randn(10, 32, 64) + 1j * random_state.randn(10, 32, 64)\n    b = random_state.randn(10, 64, 32) + 1j * random_state.randn(10, 64, 32)\n    c = random_state.randn(10, 10, 10) + 1j * random_state.randn(10, 10, 10)\n\n    p, q, r = map(cplx.Cplx.from_numpy, (a, b, c))\n\n    assert cplx_allclose_numpy(cplx.einsum(\"ijk\", r), np.einsum(\"ijk\", c))\n\n    equations = [\"iij\", \"iji\", \"jii\", \"iii\"]\n    for eq in equations:\n        assert cplx_allclose_numpy(cplx.einsum(eq, r), np.einsum(eq, c))\n        with pytest.raises(RuntimeError, match=\"but the sizes don't match\"):\n            cplx.einsum(eq, p)\n\n    equations = [\n        \"ijk, ikj\",\n        \"ijk, ikj -> ij\",\n        \"ijk, ikj -> k\",\n        \"ijk, lkj\",\n        \"ijk, lkj -> li\",\n        \"ijk, lkj -> lji\",\n        \"ijk, lkp\",\n    ]\n    for eq in equations:\n        assert cplx_allclose_numpy(cplx.einsum(eq, p, q), np.einsum(eq, a, b))\n\n    with pytest.raises(RuntimeError, match=\"does not support more\"):\n        cplx.einsum(\"...\", p, q, r)"}]}, {"git_group": "apache", "git_name": "tvm", "version": "v0.17.dev0", "language": "Python", "project_name": "tvm-v0.17.dev0.zip", "file_path": "/tvm-v0.17.dev0/tvm-0.17.dev0/python/tvm/autotvm/feature.py", "file_name": "feature.py", "focal_class": null, "focal_name": "flatten_itervar_feature", "focal_parameter": ["fea"], "solution": "def flatten_itervar_feature(fea):\n    flatten = []\n    for axis in fea:\n        for pair in axis[1:]:\n            flatten.append(pair[1:])\n    return np.concatenate(flatten)", "function_signature": "def flatten_itervar_feature(fea) :", "left_context": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n# pylint: disable=invalid-name,\n\"\"\"Extract feature of iter vars\n\nThere are two types of feature\n1) Itervar feature\n   This feature is extracted based on loop variables.\n   Different loop structures will result in different shapes of feature\n2) Curve sample feature (relation feature)\n   This feature is extracted by sampling relation curve.\n   This feature is invariant of loop structure.\n\"\"\"\n\nimport struct\nimport numpy as np\nimport tvm._ffi\n\nfrom tvm.target import Target\nfrom tvm.driver import build_module\n\n\ndef ana_lower(sch, args, binds=None, simple_mode=True):\n    \"\"\"Do lower while keeping all axes in IR\n    i.e. Do not eliminate loop with extent of 1, do not vectorize, unroll or inject virtual threads\n    \"\"\"\n    sch = sch.normalize()\n    # Phase 0\n    context = tvm.transform.PassContext(config={\"tir.debug_keep_trivial_loop\": True})\n    with context:\n        mod = build_module.schedule_to_module(sch, args, binds=binds)\n\n    mod = tvm.tir.transform.StorageFlatten(64)(mod._move())\n    mod = tvm.tir.transform.Simplify()(mod._move())\n    assert simple_mode\n    return mod[\"main\"].body\n\n\ntry:\n    _get_buffer_curve_sample_flatten = tvm._ffi.get_global_func(\n        \"autotvm.feature.GetCurveSampleFeatureFlatten\"\n    )\n    _get_itervar_feature = tvm._ffi.get_global_func(\"autotvm.feature.GetItervarFeature\")\n    _get_itervar_feature_flatten = tvm._ffi.get_global_func(\n        \"autotvm.feature.GetItervarFeatureFlatten\"\n    )\nexcept ValueError as e:\n\n    def raise_error(*args, **kwargs):  # pylint: disable=unused-argument\n        raise RuntimeError(\"Cannot load autotvm c++ API\")\n\n    _get_buffer_curve_sample_flatten = (\n        _get_itervar_feature\n    ) = _get_itervar_feature_flatten = raise_error\n\n\ndef get_itervar_feature(sch, args, take_log=False):\n    \"\"\"get features of iter vars\n\n    Parameters\n    ----------\n    sch: tvm.te.schedule.Schedule\n    args: Array of te.tensor.Tensor\n        the buffer args for lower\n    take_log: bool\n        whether take log of numerical statics\n\n    Returns\n    -------\n    features of every axis in the IR, see doc/features.md for detail\n    \"\"\"\n    stmt = ana_lower(sch, args, simple_mode=True)\n    feas = _get_itervar_feature(stmt, take_log)\n\n    # convert tvm node to python type\n    ret = []\n    for row in feas:\n        tmp = []\n        tmp.append([row[0][0].value, row[0][1]])\n        for item in row[1:]:\n            tmp.append([item[0].value] + [x.value for x in item[1:]])\n        ret.append(tmp)\n    return ret\n\n", "right_context": "\n\ndef get_itervar_feature_flatten(sch, args, take_log=True):\n    \"\"\"get flatten features of iter vars\n    this is equivalent to get_itervar_feature + flatten_itervar_feature, but much faster.\n\n    Parameters\n    ----------\n    sch: tvm.te.schedule.Schedule\n    args: Array of te.tensor.Tensor\n        the buffer args for lower\n    take_log: bool\n        whether take log of numerical statics\n\n    Returns\n    -------\n    flatten_feature: np.ndarray\n        one-dimensional vector\n    \"\"\"\n    stmt = ana_lower(sch, args, simple_mode=True)\n    feas = _get_itervar_feature_flatten(stmt, take_log)\n    feas = struct.unpack(f\"{len(feas) // 4}f\", feas)\n    return feas\n\n\ndef get_flatten_name(fea):\n    \"\"\"Get names of feature after flatten.\n\n    Parameters\n    ----------\n    fea: list or str\n        return value of get_itervar_feature or a line of logfile\n\n    Returns\n    -------\n    feature_names: Array of str\n    \"\"\"\n\n    feature_name = {\n        \"_attr_\": [\"length\", \"nest_level\", \"topdown\", \"bottomup\"] + [f\"ann_{i}\" for i in range(20)],\n        \"_arith_\": [\"add\", \"mul\", \"div\"],\n        \"buf_touch\": [\"stride\", \"mod\", \"count\", \"reuse\", \"T_count\", \"T_reuse\"],\n    }\n\n    if isinstance(fea, str):\n        # pylint: disable=import-outside-toplevel\n        from .record import decode\n\n        # flatten line to feature\n        line = fea\n        ret = decode(line)\n        if ret is None:\n            raise ValueError(\"Unsupported AutoTVM log format\")\n        inp, _ = ret\n        target = Target(inp.target)\n        with target:\n            s, args = inp.template.instantiate(inp.config)\n        fea = get_itervar_feature(s, args)\n\n    names = []\n    ct = 0\n    for row in fea:\n        var_name = str(row[0][1])\n        for pair in row[1:]:\n            key = pair[0]\n            if key in feature_name:\n                name_list = feature_name[key]\n            else:\n                name_list = feature_name[\"buf_touch\"]\n\n            for i in range(len((pair[1:]))):\n                names.append(\".\".join([f\"f{ct}\", var_name, key, name_list[i]]))\n                ct += 1\n    return names\n\n\ndef get_buffer_curve_sample_flatten(sch, args, sample_n=30):\n    \"\"\"\n    Get flatten curve sample feature (relation feature)\n\n    Parameters\n    ----------\n    sch: tvm.te.schedule.Schedule\n    args: Array of te.tensor.Tensor\n        the buffer args for lower\n    sample_n: int\n        number of sample points along one dimension\n\n    Returns\n    -------\n    flatten_feature: np.ndarray\n        one-dimensional vector\n    \"\"\"\n    stmt = ana_lower(sch, args, simple_mode=True)\n    feas = _get_buffer_curve_sample_flatten(stmt, sample_n, False)\n    feas = struct.unpack(f\"{len(feas) // 4}f\", feas)\n    return feas\n", "import_text": ["struct", "numpy", "tvm._ffi", "tvm.target.Target", "tvm.driver.build_module"], "prompt": "\"\"\"\nDescription: This function flattens a feature by iterating over each axis and pair in the feature, \n             appending the pair to a flattened list, and then concatenating the flattened list into a numpy array.\n\nArgs:\n    fea (list): A list of features to be flattened.\n\nReturns:\n    numpy.ndarray: A flattened numpy array of the features.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"flatten features into one-dimensional feature vectors\n\n    Parameters\n    ----------\n    fea: list\n        return value of get_itervar_feature\n\n    Returns\n    -------\n    flatten_feature: np.ndarray\n        one-dimensional vector\n    \"\"\"", "function_dependencies": ["numpy.concatenate"], "project_create_time": "2016-10-12T22:20:28+00:00", "project_update_time": "2024-04-18T02:21:36+00:00", "file_create_time": "2018-07-12T23:54:15Z", "file_update_time": "2023-05-19T09:49:28Z", "function_update_time": "2018-07-12T23:54:15Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["numpy.concatenate"], "test_function": [{"file_path": "/tvm-v0.17.dev0/tvm-0.17.dev0/tests/python/autotvm/test_autotvm_feature.py", "class_name": null, "function_name": "test_feature_shape", "code": "def test_feature_shape():\n\n    N = 1024\n    n_sample = 100\n\n    def get_gemm_feature(target):\n        k = te.reduce_axis((0, N), \"k\")\n        A = te.placeholder((N, N), name=\"A\")\n        B = te.placeholder((N, N), name=\"B\")\n        C = te.compute(A.shape, lambda y, x: te.sum(A[y, k] * B[k, x], axis=k), name=\"C\")\n\n        s = te.create_schedule(C.op)\n\n        y, x = s[C].op.axis\n        axes = list(s[C].tile(y, x, 8, 8)) + [k]\n        perm = np.random.permutation(5)\n        axes = [axes[x] for x in perm]\n        s[C].reorder(*axes)\n\n        if \"gpu\" in target.keys:\n            pick = []\n            # filter out reduction axis\n            for i in range(len(perm)):\n                if perm[i] != 4:\n                    pick.append(axes[i])\n            s[C].bind(pick[0], te.thread_axis(\"blockIdx.x\"))\n            s[C].bind(pick[1], te.thread_axis(\"vthread\"))\n            s[C].bind(pick[2], te.thread_axis(\"threadIdx.y\"))\n\n        with target:\n            feas = feature.get_itervar_feature(s, [A, B, C])\n            feas = feature.flatten_itervar_feature(feas)\n        return feas\n\n    targets = [\n        tvm.target.cuda(),\n        tvm.target.mali(),\n        tvm.target.arm_cpu(),\n    ]\n\n    for target in targets:\n        dim = len(get_gemm_feature(target))\n        for i in range(n_sample):\n            assert dim == len(get_gemm_feature(target)), (\n                \"dimensions of feature do not match\" \" for different configurations\"\n            )"}]}, {"git_group": "polyaxon", "git_name": "haupt", "version": "v2.1.8", "language": "Python", "project_name": "haupt-v2.1.8.zip", "file_path": "/haupt-v2.1.8/haupt-2.1.8/haupt/haupt/orchestration/crons/stats.py", "file_name": "stats.py", "focal_class": "CronsStatsManager", "focal_name": "stats_calculation_projects", "focal_parameter": [], "solution": "\n    def stats_calculation_projects():\n        projects = Models.Project.objects.filter(\n            Q(updated_at__gt=F(\"latest_stats__updated_at\"))\n            | Q(latest_stats__isnull=True)\n        )\n        project_ids = set(projects.values_list(\"id\", flat=True))\n\n        if not project_ids:\n            return\n\n        for project_id in project_ids:\n            workers.send(\n                SchedulerCeleryTasks.STATS_CALCULATION_PROJECT,\n                kwargs={\"project_id\": project_id},\n            )\n        return project_ids", "function_signature": "def stats_calculation_projects() :", "left_context": "from django.db.models import F, Q\n\nfrom haupt.background.celeryp.tasks import SchedulerCeleryTasks\nfrom haupt.common import workers\nfrom haupt.db.defs import Models\n\n\nclass CronsStatsManager:\n    @staticmethod", "right_context": "", "import_text": ["django.db.models.F", "django.db.models.Q", "haupt.background.celeryp.tasks.SchedulerCeleryTasks", "haupt.common.workers", "haupt.db.defs.Models"], "prompt": "\"\"\"\nDescription: This function calculates statistics for projects in a Django application.\n\nArgs:\n    None\n\nReturns:\n    set: A set of project IDs for which the statistics have been calculated.\n\nRaises:\n    None\n\nNotes:\n    This function uses the Django ORM to query the Project model. It filters projects based on the updated_at field of the Project model and the latest_stats field. If the latest_stats field is null or the updated_at field of the Project model is greater than the updated_at field of the latest_stats field, the project is considered for statistics calculation.\n\n    The function then sends a task to a Celery worker to calculate the statistics for each project. The task is identified by the SchedulerCeleryTasks.STATS_CALCULATION_PROJECT constant. The project_id is passed as a keyword argument to the task.\n\n    The function returns a set of project IDs for which the statistics have been calculated. If no project IDs are found, the function returns None.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["haupt.db.defs.Models.Project.objects.filter", "django.db.models.Q", "django.db.models.F", "haupt.db.defs.Models.Project.objects.filter.values_list", "haupt.common.workers.send"], "project_create_time": "2016-05-15T20:22:59+00:00", "project_update_time": "2024-04-13T11:07:21+00:00", "file_create_time": "2023-09-08T11:34:56Z", "file_update_time": "2023-09-10T10:40:45Z", "function_update_time": "2023-09-08T11:34:56Z", "license": {"key": "agpl-3.0", "name": "GNU Affero General Public License v3.0", "spdx_id": "AGPL-3.0", "url": "https://api.github.com/licenses/agpl-3.0", "node_id": "MDc6TGljZW5zZTE="}, "reference_api": ["django.db.models.F"], "test_function": [{"file_path": "/haupt-v2.1.8/haupt-2.1.8/haupt/tests/tests_orchestration/test_crons/test_stats_calculation.py", "class_name": "TestStatsCalculation", "function_name": "test_stats_calculation_projects", "code": "\n    def test_stats_calculation_projects(self):\n        assert self.project.latest_stats is None\n        with patch(\"haupt.common.workers.send\") as mock_send:\n            CronsStatsManager.stats_calculation_projects()\n\n        assert mock_send.call_count == 1\n\n        # Add a new project stats\n        self.project.latest_stats = ProjectStats(project=self.project)\n        self.project.latest_stats.save()\n        self.project.save(update_fields=[\"latest_stats\"])\n\n        assert self.project.updated_at < self.project.latest_stats.updated_at\n        with patch(\"haupt.common.workers.send\") as mock_send:\n            CronsStatsManager.stats_calculation_projects()\n\n        assert mock_send.call_count == 0\n\n        # Update project updated_at\n        self.project.name = \"new-name\"\n        self.project.save()\n\n        assert self.project.updated_at > self.project.latest_stats.updated_at\n\n        with patch(\"haupt.common.workers.send\") as mock_send:\n            CronsStatsManager.stats_calculation_projects()\n\n        assert mock_send.call_count == 1"}]}, {"git_group": "AstusRush", "git_name": "AMaDiA", "version": "master", "language": "Python", "project_name": "AMaDiA-master.zip", "file_path": "/AMaDiA-master/AMaDiA-master/External_Libraries/python_control_master/control/statesp.py", "file_name": "statesp.py", "focal_class": "StateSpace", "focal_name": "freqresp", "focal_parameter": ["omega"], "solution": "    def freqresp(self, omega):\n\n        # In case omega is passed in as a list, rather than a proper array.\n        omega = np.asarray(omega)\n\n        numFreqs = len(omega)\n        Gfrf = np.empty((self.outputs, self.inputs, numFreqs),\n                        dtype=np.complex128)\n\n        # Sort frequency and calculate complex frequencies on either imaginary\n        # axis (continuous time) or unit circle (discrete time).\n        omega.sort()\n        if isdtime(self, strict=True):\n            dt = timebase(self)\n            cmplx_freqs = exp(1.j * omega * dt)\n            if max(np.abs(omega)) * dt > math.pi:\n                warn(\"freqresp: frequency evaluation above Nyquist frequency\")\n        else:\n            cmplx_freqs = omega * 1.j\n\n        # Do the frequency response evaluation. Use TB05AD from Slycot\n        # if it's available, otherwise use the built-in horners function.\n        try:\n            from slycot import tb05ad\n\n            n = np.shape(self.A)[0]\n            m = self.inputs\n            p = self.outputs\n            # The first call both evaluates C(sI-A)^-1 B and also returns\n            # Hessenberg transformed matrices at, bt, ct.\n            result = tb05ad(n, m, p, cmplx_freqs[0], self.A,\n                            self.B, self.C, job='NG')\n            # When job='NG', result = (at, bt, ct, g_i, hinvb, info)\n            at = result[0]\n            bt = result[1]\n            ct = result[2]\n\n            # TB05AD frequency evaluation does not include direct feedthrough.\n            Gfrf[:, :, 0] = result[3] + self.D\n\n            # Now, iterate through the remaining frequencies using the\n            # transformed state matrices, at, bt, ct.\n\n            # Start at the second frequency, already have the first.\n            for kk, cmplx_freqs_kk in enumerate(cmplx_freqs[1:numFreqs]):\n                result = tb05ad(n, m, p, cmplx_freqs_kk, at,\n                                bt, ct, job='NH')\n                # When job='NH', result = (g_i, hinvb, info)\n\n                # kk+1 because enumerate starts at kk = 0.\n                # but zero-th spot is already filled.\n                Gfrf[:, :, kk+1] = result[0] + self.D\n\n        except ImportError:  # Slycot unavailable. Fall back to horner.\n            for kk, cmplx_freqs_kk in enumerate(cmplx_freqs):\n                Gfrf[:, :, kk] = self.horner(cmplx_freqs_kk)\n\n        #      mag           phase           omega\n        return np.abs(Gfrf), np.angle(Gfrf), omega", "function_signature": "def freqresp(self, omega) :", "left_context": "\"\"\"statesp.py\n\nState space representation and functions.\n\nThis file contains the StateSpace class, which is used to represent linear\nsystems in state space.  This is the primary representation for the\npython-control library.\n\n\"\"\"\n\n# Python 3 compatibility (needs to go here)\nfrom __future__ import print_function\nfrom __future__ import division # for _convertToStateSpace\n\n\"\"\"Copyright (c) 2010 by California Institute of Technology\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\n3. Neither the name of the California Institute of Technology nor\n   the names of its contributors may be used to endorse or promote\n   products derived from this software without specific prior\n   written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\nFOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL CALTECH\nOR THE CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF\nUSE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT\nOF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGE.\n\nAuthor: Richard M. Murray\nDate: 24 May 09\nRevised: Kevin K. Chen, Dec 10\n\n$Id$\n\"\"\"\n\nimport math\nimport numpy as np\nfrom numpy import any, array, asarray, concatenate, cos, delete, \\\n    dot, empty, exp, eye, isinf, ones, pad, sin, zeros, squeeze\nfrom numpy.random import rand, randn\nfrom numpy.linalg import solve, eigvals, matrix_rank\nfrom numpy.linalg.linalg import LinAlgError\nimport scipy as sp\nfrom scipy.signal import lti, cont2discrete\nfrom warnings import warn\nfrom .lti import LTI, timebase, timebaseEqual, isdtime\nfrom . import config\nfrom copy import deepcopy\n\n__all__ = ['StateSpace', 'ss', 'rss', 'drss', 'tf2ss', 'ssdata']\n\n\n# Define module default parameter values\n_statesp_defaults = {\n    'statesp.use_numpy_matrix':True,\n}\n\n\ndef _ssmatrix(data, axis=1):\n    \"\"\"Convert argument to a (possibly empty) state space matrix.\n\n    Parameters\n    ----------\n    data : array, list, or string\n        Input data defining the contents of the 2D array\n    axis : 0 or 1\n        If input data is 1D, which axis to use for return object.  The default\n        is 1, corresponding to a row matrix.\n\n    Returns\n    -------\n    arr : 2D array, with shape (0, 0) if a is empty\n\n    \"\"\"\n    # Convert the data into an array or matrix, as configured\n    # If data is passed as a string, use (deprecated?) matrix constructor\n    if config.defaults['statesp.use_numpy_matrix'] or isinstance(data, str):\n        arr = np.matrix(data, dtype=float)\n    else:\n        arr = np.array(data, dtype=float)\n    ndim = arr.ndim\n    shape = arr.shape\n\n    # Change the shape of the array into a 2D array\n    if (ndim > 2):\n        raise ValueError(\"state-space matrix must be 2-dimensional\")\n\n    elif (ndim == 2 and shape == (1, 0)) or \\\n         (ndim == 1 and shape == (0, )):\n        # Passed an empty matrix or empty vector; change shape to (0, 0)\n        shape = (0, 0)\n\n    elif ndim == 1:\n        # Passed a row or column vector\n        shape = (1, shape[0]) if axis == 1 else (shape[0], 1)\n\n    elif ndim == 0:\n        # Passed a constant; turn into a matrix\n        shape = (1, 1)\n\n    #  Create the actual object used to store the result\n    return arr.reshape(shape)\n\n\nclass StateSpace(LTI):\n    \"\"\"StateSpace(A, B, C, D[, dt])\n\n    A class for representing state-space models\n\n    The StateSpace class is used to represent state-space realizations of linear\n    time-invariant (LTI) systems:\n\n        dx/dt = A x + B u\n            y = C x + D u\n\n    where u is the input, y is the output, and x is the state.\n\n    The main data members are the A, B, C, and D matrices.  The class also\n    keeps track of the number of states (i.e., the size of A).  The data\n    format used to store state space matrices is set using the value of\n    `config.defaults['use_numpy_matrix']`.  If True (default), the state space\n    elements are stored as `numpy.matrix` objects; otherwise they are\n    `numpy.ndarray` objects.  The :func:`~control.use_numpy_matrix` function\n    can be used to set the storage type.\n\n    Discrete-time state space system are implemented by using the 'dt'\n    instance variable and setting it to the sampling period.  If 'dt' is not\n    None, then it must match whenever two state space systems are combined.\n    Setting dt = 0 specifies a continuous system, while leaving dt = None\n    means the system timebase is not specified.  If 'dt' is set to True, the\n    system will be treated as a discrete time system with unspecified sampling\n    time.\n\n    \"\"\"\n\n    # Allow ndarray * StateSpace to give StateSpace._rmul_() priority\n    __array_priority__ = 11     # override ndarray and matrix types\n\n\n    def __init__(self, *args, **kw):\n        \"\"\"\n        StateSpace(A, B, C, D[, dt])\n\n        Construct a state space object.\n\n        The default constructor is StateSpace(A, B, C, D), where A, B, C, D\n        are matrices or equivalent objects.  To create a discrete time system,\n        use StateSpace(A, B, C, D, dt) where 'dt' is the sampling time (or\n        True for unspecified sampling time).  To call the copy constructor,\n        call StateSpace(sys), where sys is a StateSpace object.\n\n        \"\"\"\n        if len(args) == 4:\n            # The user provided A, B, C, and D matrices.\n            (A, B, C, D) = args\n            dt = None\n        elif len(args) == 5:\n            # Discrete time system\n            (A, B, C, D, dt) = args\n        elif len(args) == 1:\n            # Use the copy constructor.\n            if not isinstance(args[0], StateSpace):\n                raise TypeError(\"The one-argument constructor can only take in a StateSpace \"\n                                \"object.  Received %s.\" % type(args[0]))\n            A = args[0].A\n            B = args[0].B\n            C = args[0].C\n            D = args[0].D\n            try:\n                dt = args[0].dt\n            except NameError:\n                dt = None\n        else:\n            raise ValueError(\"Needs 1 or 4 arguments; received %i.\" % len(args))\n\n        # Process keyword arguments\n        remove_useless = kw.get('remove_useless', True)\n\n        # Convert all matrices to standard form\n        A = _ssmatrix(A)\n        B = _ssmatrix(B, axis=0)\n        C = _ssmatrix(C, axis=1)\n        if np.isscalar(D) and D == 0 and B.shape[1] > 0 and C.shape[0] > 0:\n            # If D is a scalar zero, broadcast it to the proper size\n            D = np.zeros((C.shape[0], B.shape[1]))\n        D = _ssmatrix(D)\n\n        # TODO: use super here?\n        LTI.__init__(self, inputs=D.shape[1], outputs=D.shape[0], dt=dt)\n        self.A = A\n        self.B = B\n        self.C = C\n        self.D = D\n\n        self.states = A.shape[1]\n\n        if 0 == self.states:\n            # static gain\n            # matrix's default \"empty\" shape is 1x0\n            A.shape = (0,0)\n            B.shape = (0,self.inputs)\n            C.shape = (self.outputs,0)\n\n        # Check that the matrix sizes are consistent.\n        if self.states != A.shape[0]:\n            raise ValueError(\"A must be square.\")\n        if self.states != B.shape[0]:\n            raise ValueError(\"A and B must have the same number of rows.\")\n        if self.states != C.shape[1]:\n            raise ValueError(\"A and C must have the same number of columns.\")\n        if self.inputs != B.shape[1]:\n            raise ValueError(\"B and D must have the same number of columns.\")\n        if self.outputs != C.shape[0]:\n            raise ValueError(\"C and D must have the same number of rows.\")\n\n        # Check for states that don't do anything, and remove them.\n        if remove_useless: self._remove_useless_states()\n\n    def _remove_useless_states(self):\n        \"\"\"Check for states that don't do anything, and remove them.\n\n        Scan the A, B, and C matrices for rows or columns of zeros.  If the\n        zeros are such that a particular state has no effect on the input-output\n        dynamics, then remove that state from the A, B, and C matrices.\n\n        \"\"\"\n\n        # Search for useless states and get indices of these states.\n        #\n        # Note: shape from np.where depends on whether we are storing state\n        # space objects as np.matrix or np.array.  Code below will work\n        # correctly in either case.\n        ax1_A = np.where(~self.A.any(axis=1))[0]\n        ax1_B = np.where(~self.B.any(axis=1))[0]\n        ax0_A = np.where(~self.A.any(axis=0))[-1]\n        ax0_C = np.where(~self.C.any(axis=0))[-1]\n        useless_1 = np.intersect1d(ax1_A, ax1_B, assume_unique=True)\n        useless_2 = np.intersect1d(ax0_A, ax0_C, assume_unique=True)\n        useless = np.union1d(useless_1, useless_2)\n\n        # Remove the useless states.\n        self.A = delete(self.A, useless, 0)\n        self.A = delete(self.A, useless, 1)\n        self.B = delete(self.B, useless, 0)\n        self.C = delete(self.C, useless, 1)\n\n        self.states = self.A.shape[0]\n        self.inputs = self.B.shape[1]\n        self.outputs = self.C.shape[0]\n\n    def __str__(self):\n        \"\"\"String representation of the state space.\"\"\"\n\n        str = \"A = \" + self.A.__str__() + \"\\n\\n\"\n        str += \"B = \" + self.B.__str__() + \"\\n\\n\"\n        str += \"C = \" + self.C.__str__() + \"\\n\\n\"\n        str += \"D = \" + self.D.__str__() + \"\\n\"\n        # TODO: replace with standard calls to lti functions\n        if (type(self.dt) == bool and self.dt is True):\n            str += \"\\ndt unspecified\\n\"\n        elif (not (self.dt is None) and type(self.dt) != bool and self.dt > 0):\n            str += \"\\ndt = \" + self.dt.__str__() + \"\\n\"\n        return str\n\n    # represent as string, makes display work for IPython\n    __repr__ = __str__\n\n    # Negation of a system\n    def __neg__(self):\n        \"\"\"Negate a state space system.\"\"\"\n\n        return StateSpace(self.A, self.B, -self.C, -self.D, self.dt)\n\n    # Addition of two state space systems (parallel interconnection)\n    def __add__(self, other):\n        \"\"\"Add two LTI systems (parallel connection).\"\"\"\n\n        # Check for a couple of special cases\n        if isinstance(other, (int, float, complex, np.number)):\n            # Just adding a scalar; put it in the D matrix\n            A, B, C = self.A, self.B, self.C\n            D = self.D + other\n            dt = self.dt\n        else:\n            other = _convertToStateSpace(other)\n\n            # Check to make sure the dimensions are OK\n            if ((self.inputs != other.inputs) or\n                    (self.outputs != other.outputs)):\n                raise ValueError(\"Systems have different shapes.\")\n\n            # Figure out the sampling time to use\n            if self.dt is None and other.dt is not None:\n                dt = other.dt       # use dt from second argument\n            elif (other.dt is None and self.dt is not None) or \\\n                    (timebaseEqual(self, other)):\n                dt = self.dt        # use dt from first argument\n            else:\n                raise ValueError(\"Systems have different sampling times\")\n\n            # Concatenate the various arrays\n            A = concatenate((\n                concatenate((self.A, zeros((self.A.shape[0],\n                                           other.A.shape[-1]))),axis=1),\n                concatenate((zeros((other.A.shape[0], self.A.shape[-1])),\n                                other.A),axis=1)\n                            ),axis=0)\n            B = concatenate((self.B, other.B), axis=0)\n            C = concatenate((self.C, other.C), axis=1)\n            D = self.D + other.D\n\n        return StateSpace(A, B, C, D, dt)\n\n    # Right addition - just switch the arguments\n    def __radd__(self, other):\n        \"\"\"Right add two LTI systems (parallel connection).\"\"\"\n\n        return self + other\n\n    # Subtraction of two state space systems (parallel interconnection)\n    def __sub__(self, other):\n        \"\"\"Subtract two LTI systems.\"\"\"\n\n        return self + (-other)\n\n    def __rsub__(self, other):\n        \"\"\"Right subtract two LTI systems.\"\"\"\n\n        return other + (-self)\n\n    # Multiplication of two state space systems (series interconnection)\n    def __mul__(self, other):\n        \"\"\"Multiply two LTI objects (serial connection).\"\"\"\n\n        # Check for a couple of special cases\n        if isinstance(other, (int, float, complex, np.number)):\n            # Just multiplying by a scalar; change the output\n            A, B = self.A, self.B\n            C = self.C * other\n            D = self.D * other\n            dt = self.dt\n        else:\n            other = _convertToStateSpace(other)\n\n            # Check to make sure the dimensions are OK\n            if self.inputs != other.outputs:\n                raise ValueError(\"C = A * B: A has %i column(s) (input(s)), \\\nbut B has %i row(s)\\n(output(s)).\" % (self.inputs, other.outputs))\n\n            # Figure out the sampling time to use\n            if (self.dt == None and other.dt != None):\n                dt = other.dt       # use dt from second argument\n            elif (other.dt == None and self.dt != None) or \\\n                    (timebaseEqual(self, other)):\n                dt = self.dt        # use dt from first argument\n            else:\n                raise ValueError(\"Systems have different sampling times\")\n\n            # Concatenate the various arrays\n            A = concatenate(\n                (concatenate((other.A,\n                              zeros((other.A.shape[0], self.A.shape[1]))),\n                             axis=1),\n                 concatenate((np.dot(self.B, other.C), self.A), axis=1)),\n                axis=0)\n            B = concatenate((other.B, np.dot(self.B, other.D)), axis=0)\n            C = concatenate((np.dot(self.D, other.C), self.C),axis=1)\n            D = np.dot(self.D, other.D)\n\n        return StateSpace(A, B, C, D, dt)\n\n    # Right multiplication of two state space systems (series interconnection)\n    # Just need to convert LH argument to a state space object\n    # TODO: __rmul__ only works for special cases (??)\n    def __rmul__(self, other):\n        \"\"\"Right multiply two LTI objects (serial connection).\"\"\"\n\n        # Check for a couple of special cases\n        if isinstance(other, (int, float, complex, np.number)):\n            # Just multiplying by a scalar; change the input\n            A, C = self.A, self.C\n            B = self.B * other\n            D = self.D * other\n            return StateSpace(A, B, C, D, self.dt)\n\n        # is lti, and convertible?\n        if isinstance(other, LTI):\n            return _convertToStateSpace(other) * self\n\n        # try to treat this as a matrix\n        try:\n            X = _ssmatrix(other)\n            C = np.dot(X, self.C)\n            D = np.dot(X, self.D)\n            return StateSpace(self.A, self.B, C, D, self.dt)\n\n        except Exception as e:\n            print(e)\n            pass\n        raise TypeError(\"can't interconnect systems\")\n\n    # TODO: __div__ and __rdiv__ are not written yet.\n    def __div__(self, other):\n        \"\"\"Divide two LTI systems.\"\"\"\n\n        raise NotImplementedError(\"StateSpace.__div__ is not implemented yet.\")\n\n    def __rdiv__(self, other):\n        \"\"\"Right divide two LTI systems.\"\"\"\n\n        raise NotImplementedError(\"StateSpace.__rdiv__ is not implemented yet.\")\n\n    def evalfr(self, omega):\n        \"\"\"Evaluate a SS system's transfer function at a single frequency.\n\n        self._evalfr(omega) returns the value of the transfer function matrix\n        with input value s = i * omega.\n\n        \"\"\"\n        warn(\"StateSpace.evalfr(omega) will be deprecated in a future \"\n             \"release of python-control; use evalfr(sys, omega*1j) instead\",\n             PendingDeprecationWarning)\n        return self._evalfr(omega)\n\n    def _evalfr(self, omega):\n        \"\"\"Evaluate a SS system's transfer function at a single frequency\"\"\"\n        # Figure out the point to evaluate the transfer function\n        if isdtime(self, strict=True):\n            dt = timebase(self)\n            s = exp(1.j * omega * dt)\n            if omega * dt > math.pi:\n                warn(\"_evalfr: frequency evaluation above Nyquist frequency\")\n        else:\n            s = omega * 1.j\n\n        return self.horner(s)\n\n    def horner(self, s):\n        \"\"\"Evaluate the systems's transfer function for a complex variable\n\n        Returns a matrix of values evaluated at complex variable s.\n        \"\"\"\n        resp = np.dot(self.C, solve(s * eye(self.states) - self.A,\n                                    self.B)) + self.D\n        return array(resp)\n\n    # Method for generating the frequency response of the system", "right_context": "\n    # Compute poles and zeros\n    def pole(self):\n        \"\"\"Compute the poles of a state space system.\"\"\"\n\n        return eigvals(self.A) if self.states else np.array([])\n\n    def zero(self):\n        \"\"\"Compute the zeros of a state space system.\"\"\"\n\n        if not self.states:\n            return np.array([])\n\n        # Use AB08ND from Slycot if it's available, otherwise use\n        # scipy.lingalg.eigvals().\n        try:\n            from slycot import ab08nd\n\n            out = ab08nd(self.A.shape[0], self.B.shape[1], self.C.shape[0],\n                         self.A, self.B, self.C, self.D)\n            nu = out[0]\n            if nu == 0:\n                return np.array([])\n            else:\n                return sp.linalg.eigvals(out[8][0:nu, 0:nu], out[9][0:nu, 0:nu])\n\n        except ImportError:  # Slycot unavailable. Fall back to scipy.\n            if self.C.shape[0] != self.D.shape[1]:\n                raise NotImplementedError(\"StateSpace.zero only supports \"\n                                          \"systems with the same number of \"\n                                          \"inputs as outputs.\")\n\n            # This implements the QZ algorithm for finding transmission zeros\n            # from\n            # https://dspace.mit.edu/bitstream/handle/1721.1/841/P-0802-06587335.pdf.\n            # The QZ algorithm solves the generalized eigenvalue problem: given\n            # `L = [A, B; C, D]` and `M = [I_nxn 0]`, find all finite lambda\n            # for which there exist nontrivial solutions of the equation\n            # `Lz - lamba Mz`.\n            #\n            # The generalized eigenvalue problem is only solvable if its\n            # arguments are square matrices.\n            L = concatenate((concatenate((self.A, self.B), axis=1),\n                             concatenate((self.C, self.D), axis=1)), axis=0)\n            M = pad(eye(self.A.shape[0]), ((0, self.C.shape[0]),\n                                           (0, self.B.shape[1])), \"constant\")\n            return np.array([x for x in sp.linalg.eigvals(L, M, overwrite_a=True)\n                             if not isinf(x)])\n\n    # Feedback around a state space system\n    def feedback(self, other=1, sign=-1):\n        \"\"\"Feedback interconnection between two LTI systems.\"\"\"\n\n        other = _convertToStateSpace(other)\n\n        # Check to make sure the dimensions are OK\n        if (self.inputs != other.outputs) or (self.outputs != other.inputs):\n                raise ValueError(\"State space systems don't have compatible inputs/outputs for \"\n                                 \"feedback.\")\n\n        # Figure out the sampling time to use\n        if self.dt is None and other.dt is not None:\n            dt = other.dt       # use dt from second argument\n        elif other.dt is None and self.dt is not None or timebaseEqual(self, other):\n            dt = self.dt        # use dt from first argument\n        else:\n            raise ValueError(\"Systems have different sampling times\")\n\n        A1 = self.A\n        B1 = self.B\n        C1 = self.C\n        D1 = self.D\n        A2 = other.A\n        B2 = other.B\n        C2 = other.C\n        D2 = other.D\n\n        F = eye(self.inputs) - sign * np.dot(D2, D1)\n        if matrix_rank(F) != self.inputs:\n            raise ValueError(\"I - sign * D2 * D1 is singular to working precision.\")\n\n        # Precompute F\\D2 and F\\C2 (E = inv(F))\n        # We can solve two linear systems in one pass, since the\n        # coefficients matrix F is the same. Thus, we perform the LU\n        # decomposition (cubic runtime complexity) of F only once!\n        # The remaining back substitutions are only quadratic in runtime.\n        E_D2_C2 = solve(F, concatenate((D2, C2), axis=1))\n        E_D2 = E_D2_C2[:, :other.inputs]\n        E_C2 = E_D2_C2[:, other.inputs:]\n\n        T1 = eye(self.outputs) + sign * np.dot(D1, E_D2)\n        T2 = eye(self.inputs) + sign * np.dot(E_D2, D1)\n\n        A = concatenate(\n            (concatenate(\n                (A1 + sign * np.dot(np.dot(B1, E_D2), C1),\n                 sign * np.dot(B1, E_C2)), axis=1),\n             concatenate(\n                 (np.dot(B2, np.dot(T1, C1)),\n                  A2 + sign * np.dot(np.dot(B2, D1), E_C2)), axis=1)),\n            axis=0)\n        B = concatenate((np.dot(B1, T2), np.dot(np.dot(B2, D1), T2)), axis=0)\n        C = concatenate((np.dot(T1, C1), sign * np.dot(D1, E_C2)), axis=1)\n        D = np.dot(D1, T2)\n\n        return StateSpace(A, B, C, D, dt)\n\n    def lft(self, other, nu=-1, ny=-1):\n        \"\"\"Return the Linear Fractional Transformation.\n\n        A definition of the LFT operator can be found in Appendix A.7,\n        page 512 in the 2nd Edition, Multivariable Feedback Control by\n        Sigurd Skogestad.\n\n        An alternative definition can be found here:\n        https://www.mathworks.com/help/control/ref/lft.html\n\n        Parameters\n        ----------\n        other : LTI\n            The lower LTI system\n        ny : int, optional\n            Dimension of (plant) measurement output.\n        nu : int, optional\n            Dimension of (plant) control input.\n\n        \"\"\"\n        other = _convertToStateSpace(other)\n        # maximal values for nu, ny\n        if ny == -1:\n            ny = min(other.inputs, self.outputs)\n        if nu == -1:\n            nu = min(other.outputs, self.inputs)\n        # dimension check\n        # TODO\n\n        # Figure out the sampling time to use\n        if (self.dt == None and other.dt != None):\n            dt = other.dt       # use dt from second argument\n        elif (other.dt == None and self.dt != None) or \\\n                timebaseEqual(self, other):\n            dt = self.dt        # use dt from first argument\n        else:\n            raise ValueError(\"Systems have different time bases\")\n\n        # submatrices\n        A = self.A\n        B1 = self.B[:, :self.inputs - nu]\n        B2 = self.B[:, self.inputs - nu:]\n        C1 = self.C[:self.outputs - ny, :]\n        C2 = self.C[self.outputs - ny:, :]\n        D11 = self.D[:self.outputs - ny, :self.inputs - nu]\n        D12 = self.D[:self.outputs - ny, self.inputs - nu:]\n        D21 = self.D[self.outputs - ny:, :self.inputs - nu]\n        D22 = self.D[self.outputs - ny:, self.inputs - nu:]\n\n        # submatrices\n        Abar = other.A\n        Bbar1 = other.B[:, :ny]\n        Bbar2 = other.B[:, ny:]\n        Cbar1 = other.C[:nu, :]\n        Cbar2 = other.C[nu:, :]\n        Dbar11 = other.D[:nu, :ny]\n        Dbar12 = other.D[:nu, ny:]\n        Dbar21 = other.D[nu:, :ny]\n        Dbar22 = other.D[nu:, ny:]\n\n        # well-posed check\n        F = np.block([[np.eye(ny), -D22], [-Dbar11, np.eye(nu)]])\n        if matrix_rank(F) != ny + nu:\n            raise ValueError(\"lft not well-posed to working precision.\")\n\n        # solve for the resulting ss by solving for [y, u] using [x,\n        # xbar] and [w1, w2].\n        TH = np.linalg.solve(F, np.block(\n            [[C2, np.zeros((ny, other.states)), D21, np.zeros((ny, other.inputs - ny))],\n             [np.zeros((nu, self.states)), Cbar1, np.zeros((nu, self.inputs - nu)), Dbar12]]\n        ))\n        T11 = TH[:ny, :self.states]\n        T12 = TH[:ny, self.states: self.states + other.states]\n        T21 = TH[ny:, :self.states]\n        T22 = TH[ny:, self.states: self.states + other.states]\n        H11 = TH[:ny, self.states + other.states: self.states + other.states + self.inputs - nu]\n        H12 = TH[:ny, self.states + other.states + self.inputs - nu:]\n        H21 = TH[ny:, self.states + other.states: self.states + other.states + self.inputs - nu]\n        H22 = TH[ny:, self.states + other.states + self.inputs - nu:]\n\n        Ares = np.block([\n            [A + B2.dot(T21), B2.dot(T22)],\n            [Bbar1.dot(T11), Abar + Bbar1.dot(T12)]\n        ])\n\n        Bres = np.block([\n            [B1 + B2.dot(H21), B2.dot(H22)],\n            [Bbar1.dot(H11), Bbar2 + Bbar1.dot(H12)]\n        ])\n\n        Cres = np.block([\n            [C1 + D12.dot(T21), D12.dot(T22)],\n            [Dbar21.dot(T11), Cbar2 + Dbar21.dot(T12)]\n        ])\n\n        Dres = np.block([\n            [D11 + D12.dot(H21), D12.dot(H22)],\n            [Dbar21.dot(H11), Dbar22 + Dbar21.dot(H12)]\n        ])\n        return StateSpace(Ares, Bres, Cres, Dres, dt)\n\n    def minreal(self, tol=0.0):\n        \"\"\"Calculate a minimal realization, removes unobservable and\n        uncontrollable states\"\"\"\n        if self.states:\n            try:\n                from slycot import tb01pd\n                B = empty((self.states, max(self.inputs, self.outputs)))\n                B[:,:self.inputs] = self.B\n                C = empty((max(self.outputs, self.inputs), self.states))\n                C[:self.outputs,:] = self.C\n                A, B, C, nr = tb01pd(self.states, self.inputs, self.outputs,\n                                     self.A, B, C, tol=tol)\n                return StateSpace(A[:nr,:nr], B[:nr,:self.inputs],\n                                  C[:self.outputs,:nr], self.D)\n            except ImportError:\n                raise TypeError(\"minreal requires slycot tb01pd\")\n        else:\n            return StateSpace(self)\n\n\n    # TODO: add discrete time check\n    def returnScipySignalLTI(self):\n        \"\"\"Return a list of a list of scipy.signal.lti objects.\n\n        For instance,\n\n        >>> out = ssobject.returnScipySignalLTI()\n        >>> out[3][5]\n\n        is a signal.scipy.lti object corresponding to the transfer function from\n        the 6th input to the 4th output.\"\"\"\n\n        # Preallocate the output.\n        out = [[[] for _ in range(self.inputs)] for _ in range(self.outputs)]\n\n        for i in range(self.outputs):\n            for j in range(self.inputs):\n                out[i][j] = lti(asarray(self.A), asarray(self.B[:, j]),\n                                asarray(self.C[i, :]), self.D[i, j])\n\n        return out\n\n    def append(self, other):\n        \"\"\"Append a second model to the present model. The second\n        model is converted to state-space if necessary, inputs and\n        outputs are appended and their order is preserved\"\"\"\n        if not isinstance(other, StateSpace):\n            other = _convertToStateSpace(other)\n\n        if self.dt != other.dt:\n            raise ValueError(\"Systems must have the same time step\")\n\n        n = self.states + other.states\n        m = self.inputs + other.inputs\n        p = self.outputs + other.outputs\n        A = zeros((n, n))\n        B = zeros((n, m))\n        C = zeros((p, n))\n        D = zeros((p, m))\n        A[:self.states, :self.states] = self.A\n        A[self.states:, self.states:] = other.A\n        B[:self.states, :self.inputs] = self.B\n        B[self.states:, self.inputs:] = other.B\n        C[:self.outputs, :self.states] = self.C\n        C[self.outputs:, self.states:] = other.C\n        D[:self.outputs, :self.inputs] = self.D\n        D[self.outputs:, self.inputs:] = other.D\n        return StateSpace(A, B, C, D, self.dt)\n\n    def __getitem__(self, indices):\n        \"\"\"Array style access\"\"\"\n        if len(indices) != 2:\n            raise IOError('must provide indices of length 2 for state space')\n        i = indices[0]\n        j = indices[1]\n        return StateSpace(self.A, self.B[:, j], self.C[i, :], self.D[i, j], self.dt)\n\n    def sample(self, Ts, method='zoh', alpha=None):\n        \"\"\"Convert a continuous time system to discrete time\n\n        Creates a discrete-time system from a continuous-time system by\n        sampling.  Multiple methods of conversion are supported.\n\n        Parameters\n        ----------\n        Ts : float\n            Sampling period\n        method :  {\"gbt\", \"bilinear\", \"euler\", \"backward_diff\", \"zoh\"}\n            Which method to use:\n\n            * gbt: generalized bilinear transformation\n            * bilinear: Tustin's approximation (\"gbt\" with alpha=0.5)\n            * euler: Euler (or forward differencing) method (\"gbt\" with\n              alpha=0)\n            * backward_diff: Backwards differencing (\"gbt\" with alpha=1.0)\n            * zoh: zero-order hold (default)\n\n        alpha : float within [0, 1]\n            The generalized bilinear transformation weighting parameter, which\n            should only be specified with method=\"gbt\", and is ignored\n            otherwise\n\n        Returns\n        -------\n        sysd : StateSpace\n            Discrete time system, with sampling rate Ts\n\n        Notes\n        -----\n        Uses the command 'cont2discrete' from scipy.signal\n\n        Examples\n        --------\n        >>> sys = StateSpace(0, 1, 1, 0)\n        >>> sysd = sys.sample(0.5, method='bilinear')\n\n        \"\"\"\n        if not self.isctime():\n            raise ValueError(\"System must be continuous time system\")\n\n        sys = (self.A, self.B, self.C, self.D)\n        Ad, Bd, C, D, dt = cont2discrete(sys, Ts, method, alpha)\n        return StateSpace(Ad, Bd, C, D, dt)\n\n    def dcgain(self):\n        \"\"\"Return the zero-frequency gain\n\n        The zero-frequency gain of a continuous-time state-space\n        system is given by:\n\n        .. math: G(0) = - C A^{-1} B + D\n\n        and of a discrete-time state-space system by:\n\n        .. math: G(1) = C (I - A)^{-1} B + D\n\n        Returns\n        -------\n        gain : ndarray\n            An array of shape (outputs,inputs); the array will either\n            be the zero-frequency (or DC) gain, or, if the frequency\n            response is singular, the array will be filled with np.nan.\n        \"\"\"\n        try:\n            if self.isctime():\n                gain = np.asarray(self.D-self.C.dot(np.linalg.solve(self.A, self.B)))\n            else:\n                gain = self.horner(1)\n        except LinAlgError:\n            # eigenvalue at DC\n            gain = np.tile(np.nan, (self.outputs, self.inputs))\n        return np.squeeze(gain)\n\n\n# TODO: add discrete time check\ndef _convertToStateSpace(sys, **kw):\n    \"\"\"Convert a system to state space form (if needed).\n\n    If sys is already a state space, then it is returned.  If sys is a transfer\n    function object, then it is converted to a state space and returned.  If sys\n    is a scalar, then the number of inputs and outputs can be specified\n    manually, as in:\n\n    >>> sys = _convertToStateSpace(3.) # Assumes inputs = outputs = 1\n    >>> sys = _convertToStateSpace(1., inputs=3, outputs=2)\n\n    In the latter example, A = B = C = 0 and D = [[1., 1., 1.]\n                                                  [1., 1., 1.]].\n\n    \"\"\"\n    from .xferfcn import TransferFunction\n    import itertools\n    if isinstance(sys, StateSpace):\n        if len(kw):\n            raise TypeError(\"If sys is a StateSpace, _convertToStateSpace \\\ncannot take keywords.\")\n\n        # Already a state space system; just return it\n        return sys\n    elif isinstance(sys, TransferFunction):\n        try:\n            from slycot import td04ad\n            if len(kw):\n                raise TypeError(\"If sys is a TransferFunction, \"\n                                \"_convertToStateSpace cannot take keywords.\")\n\n            # Change the numerator and denominator arrays so that the transfer\n            # function matrix has a common denominator.\n            # matrices are also sized/padded to fit td04ad\n            num, den, denorder = sys.minreal()._common_den()\n\n            # transfer function to state space conversion now should work!\n            ssout = td04ad('C', sys.inputs, sys.outputs,\n                           denorder, den, num, tol=0)\n\n            states = ssout[0]\n            return StateSpace(ssout[1][:states, :states], ssout[2][:states, :sys.inputs],\n                              ssout[3][:sys.outputs, :states], ssout[4], sys.dt)\n        except ImportError:\n            # No Slycot.  Scipy tf->ss can't handle MIMO, but static\n            # MIMO is an easy special case we can check for here\n            maxn = max(max(len(n) for n in nrow)\n                       for nrow in sys.num)\n            maxd = max(max(len(d) for d in drow)\n                       for drow in sys.den)\n            if 1 == maxn and 1 == maxd:\n                D = empty((sys.outputs, sys.inputs), dtype=float)\n                for i, j in itertools.product(range(sys.outputs), range(sys.inputs)):\n                    D[i, j] = sys.num[i][j][0] / sys.den[i][j][0]\n                return StateSpace([], [], [], D, sys.dt)\n            else:\n                if sys.inputs != 1 or sys.outputs != 1:\n                    raise TypeError(\"No support for MIMO without slycot\")\n\n                # TODO: do we want to squeeze first and check dimenations?\n                # I think this will fail if num and den aren't 1-D after\n                # the squeeze\n                A, B, C, D = sp.signal.tf2ss(squeeze(sys.num), squeeze(sys.den))\n                return StateSpace(A, B, C, D, sys.dt)\n\n    elif isinstance(sys, (int, float, complex, np.number)):\n        if \"inputs\" in kw:\n            inputs = kw[\"inputs\"]\n        else:\n            inputs = 1\n        if \"outputs\" in kw:\n            outputs = kw[\"outputs\"]\n        else:\n            outputs = 1\n\n        # Generate a simple state space system of the desired dimension\n        # The following Doesn't work due to inconsistencies in ltisys:\n        #   return StateSpace([[]], [[]], [[]], eye(outputs, inputs))\n        return StateSpace(0., zeros((1, inputs)), zeros((outputs, 1)),\n            sys * ones((outputs, inputs)))\n\n    # If this is a matrix, try to create a constant feedthrough\n    try:\n        D = _ssmatrix(sys)\n        return StateSpace([], [], [], D)\n    except Exception as e:\n        print(\"Failure to assume argument is matrix-like in\" \\\n            \" _convertToStateSpace, result %s\" % e)\n\n    raise TypeError(\"Can't convert given type to StateSpace system.\")\n\n# TODO: add discrete time option\ndef _rss_generate(states, inputs, outputs, type):\n    \"\"\"Generate a random state space.\n\n    This does the actual random state space generation expected from rss and\n    drss.  type is 'c' for continuous systems and 'd' for discrete systems.\n\n    \"\"\"\n\n    # Probability of repeating a previous root.\n    pRepeat = 0.05\n    # Probability of choosing a real root.  Note that when choosing a complex\n    # root, the conjugate gets chosen as well.  So the expected proportion of\n    # real roots is pReal / (pReal + 2 * (1 - pReal)).\n    pReal = 0.6\n    # Probability that an element in B or C will not be masked out.\n    pBCmask = 0.8\n    # Probability that an element in D will not be masked out.\n    pDmask = 0.3\n    # Probability that D = 0.\n    pDzero = 0.5\n\n    # Check for valid input arguments.\n    if states < 1 or states % 1:\n        raise ValueError(\"states must be a positive integer.  states = %g.\" %\n            states)\n    if inputs < 1 or inputs % 1:\n        raise ValueError(\"inputs must be a positive integer.  inputs = %g.\" %\n            inputs)\n    if outputs < 1 or outputs % 1:\n        raise ValueError(\"outputs must be a positive integer.  outputs = %g.\" %\n            outputs)\n\n    # Make some poles for A.  Preallocate a complex array.\n    poles = zeros(states) + zeros(states) * 0.j\n    i = 0\n\n    while i < states:\n        if rand() < pRepeat and i != 0 and i != states - 1:\n            # Small chance of copying poles, if we're not at the first or last\n            # element.\n            if poles[i-1].imag == 0:\n                # Copy previous real pole.\n                poles[i] = poles[i-1]\n                i += 1\n            else:\n                # Copy previous complex conjugate pair of poles.\n                poles[i:i+2] = poles[i-2:i]\n                i += 2\n        elif rand() < pReal or i == states - 1:\n            # No-oscillation pole.\n            if type == 'c':\n                poles[i] = -exp(randn()) + 0.j\n            elif type == 'd':\n                poles[i] = 2. * rand() - 1.\n            i += 1\n        else:\n            # Complex conjugate pair of oscillating poles.\n            if type == 'c':\n                poles[i] = complex(-exp(randn()), 3. * exp(randn()))\n            elif type == 'd':\n                mag = rand()\n                phase = 2. * math.pi * rand()\n                poles[i] = complex(mag * cos(phase), mag * sin(phase))\n            poles[i+1] = complex(poles[i].real, -poles[i].imag)\n            i += 2\n\n    # Now put the poles in A as real blocks on the diagonal.\n    A = zeros((states, states))\n    i = 0\n    while i < states:\n        if poles[i].imag == 0:\n            A[i, i] = poles[i].real\n            i += 1\n        else:\n            A[i, i] = A[i+1, i+1] = poles[i].real\n            A[i, i+1] = poles[i].imag\n            A[i+1, i] = -poles[i].imag\n            i += 2\n    # Finally, apply a transformation so that A is not block-diagonal.\n    while True:\n        T = randn(states, states)\n        try:\n            A = dot(solve(T, A), T)  # A = T \\ A * T\n            break\n        except LinAlgError:\n            # In the unlikely event that T is rank-deficient, iterate again.\n            pass\n\n    # Make the remaining matrices.\n    B = randn(states, inputs)\n    C = randn(outputs, states)\n    D = randn(outputs, inputs)\n\n    # Make masks to zero out some of the elements.\n    while True:\n        Bmask = rand(states, inputs) < pBCmask\n        if any(Bmask):  # Retry if we get all zeros.\n            break\n    while True:\n        Cmask = rand(outputs, states) < pBCmask\n        if any(Cmask):  # Retry if we get all zeros.\n            break\n    if rand() < pDzero:\n        Dmask = zeros((outputs, inputs))\n    else:\n        Dmask = rand(outputs, inputs) < pDmask\n\n    # Apply masks.\n    B = B * Bmask\n    C = C * Cmask\n    D = D * Dmask\n\n    return StateSpace(A, B, C, D)\n\n\n# Convert a MIMO system to a SISO system\n# TODO: add discrete time check\ndef _mimo2siso(sys, input, output, warn_conversion=False):\n    #pylint: disable=W0622\n    \"\"\"\n    Convert a MIMO system to a SISO system. (Convert a system with multiple\n    inputs and/or outputs, to a system with a single input and output.)\n\n    The input and output that are used in the SISO system can be selected\n    with the parameters ``input`` and ``output``. All other inputs are set\n    to 0, all other outputs are ignored.\n\n    If ``sys`` is already a SISO system, it will be returned unaltered.\n\n    Parameters\n    ----------\n    sys : StateSpace\n        Linear (MIMO) system that should be converted.\n    input : int\n        Index of the input that will become the SISO system's only input.\n    output : int\n        Index of the output that will become the SISO system's only output.\n    warn_conversion : bool, optional\n        If `True`, print a message when sys is a MIMO system,\n        warning that a conversion will take place.  Default is False.\n\n    Returns\n    sys : StateSpace\n        The converted (SISO) system.\n    \"\"\"\n    if not (isinstance(input, int) and isinstance(output, int)):\n        raise TypeError(\"Parameters ``input`` and ``output`` must both \"\n                        \"be integer numbers.\")\n    if not (0 <= input < sys.inputs):\n        raise ValueError(\"Selected input does not exist. \"\n                         \"Selected input: {sel}, \"\n                         \"number of system inputs: {ext}.\"\n                         .format(sel=input, ext=sys.inputs))\n    if not (0 <= output < sys.outputs):\n        raise ValueError(\"Selected output does not exist. \"\n                         \"Selected output: {sel}, \"\n                         \"number of system outputs: {ext}.\"\n                         .format(sel=output, ext=sys.outputs))\n    #Convert sys to SISO if necessary\n    if sys.inputs > 1 or sys.outputs > 1:\n        if warn_conversion:\n            warn(\"Converting MIMO system to SISO system. \"\n                 \"Only input {i} and output {o} are used.\"\n                 .format(i=input, o=output))\n        # $X = A*X + B*U\n        #  Y = C*X + D*U\n        new_B = sys.B[:, input]\n        new_C = sys.C[output, :]\n        new_D = sys.D[output, input]\n        sys = StateSpace(sys.A, new_B, new_C, new_D, sys.dt)\n\n    return sys\n\n\ndef _mimo2simo(sys, input, warn_conversion=False):\n    # pylint: disable=W0622\n    \"\"\"\n    Convert a MIMO system to a SIMO system. (Convert a system with multiple\n    inputs and/or outputs, to a system with a single input but possibly\n    multiple outputs.)\n\n    The input that is used in the SIMO system can be selected with the\n    parameter ``input``. All other inputs are set to 0, all other\n    outputs are ignored.\n\n    If ``sys`` is already a SIMO system, it will be returned unaltered.\n\n    Parameters\n    ----------\n    sys: StateSpace\n        Linear (MIMO) system that should be converted.\n    input: int\n        Index of the input that will become the SIMO system's only input.\n    warn_conversion: bool\n        If True: print a warning message when sys is a MIMO system.\n        Warn that a conversion will take place.\n\n    Returns\n    -------\n    sys: StateSpace\n        The converted (SIMO) system.\n    \"\"\"\n    if not (isinstance(input, int)):\n        raise TypeError(\"Parameter ``input`` be an integer number.\")\n    if not (0 <= input < sys.inputs):\n        raise ValueError(\"Selected input does not exist. \"\n                         \"Selected input: {sel}, \"\n                         \"number of system inputs: {ext}.\"\n                         .format(sel=input, ext=sys.inputs))\n    # Convert sys to SISO if necessary\n    if sys.inputs > 1:\n        if warn_conversion:\n            warn(\"Converting MIMO system to SIMO system. \"\n                 \"Only input {i} is used.\" .format(i=input))\n        # $X = A*X + B*U\n        #  Y = C*X + D*U\n        new_B = sys.B[:, input]\n        new_D = sys.D[:, input]\n        sys = StateSpace(sys.A, new_B, sys.C, new_D, sys.dt)\n\n    return sys\n\n\ndef ss(*args):\n    \"\"\"ss(A, B, C, D[, dt])\n\n    Create a state space system.\n\n    The function accepts either 1, 4 or 5 parameters:\n\n    ``ss(sys)``\n        Convert a linear system into space system form. Always creates a\n        new system, even if sys is already a StateSpace object.\n\n    ``ss(A, B, C, D)``\n        Create a state space system from the matrices of its state and\n        output equations:\n\n        .. math::\n            \\\\dot x = A \\\\cdot x + B \\\\cdot u\n\n            y = C \\\\cdot x + D \\\\cdot u\n\n    ``ss(A, B, C, D, dt)``\n        Create a discrete-time state space system from the matrices of\n        its state and output equations:\n\n        .. math::\n            x[k+1] = A \\\\cdot x[k] + B \\\\cdot u[k]\n\n            y[k] = C \\\\cdot x[k] + D \\\\cdot u[ki]\n\n        The matrices can be given as *array like* data types or strings.\n        Everything that the constructor of :class:`numpy.matrix` accepts is\n        permissible here too.\n\n    Parameters\n    ----------\n    sys: StateSpace or TransferFunction\n        A linear system\n    A: array_like or string\n        System matrix\n    B: array_like or string\n        Control matrix\n    C: array_like or string\n        Output matrix\n    D: array_like or string\n        Feed forward matrix\n    dt: If present, specifies the sampling period and a discrete time\n        system is created\n\n    Returns\n    -------\n    out: :class:`StateSpace`\n        The new linear system\n\n    Raises\n    ------\n    ValueError\n        if matrix sizes are not self-consistent\n\n    See Also\n    --------\n    StateSpace\n    tf\n    ss2tf\n    tf2ss\n\n    Examples\n    --------\n    >>> # Create a StateSpace object from four \"matrices\".\n    >>> sys1 = ss(\"1. -2; 3. -4\", \"5.; 7\", \"6. 8\", \"9.\")\n\n    >>> # Convert a TransferFunction to a StateSpace object.\n    >>> sys_tf = tf([2.], [1., 3])\n    >>> sys2 = ss(sys_tf)\n\n    \"\"\"\n\n    if len(args) == 4 or len(args) == 5:\n        return StateSpace(*args)\n    elif len(args) == 1:\n        from .xferfcn import TransferFunction\n        sys = args[0]\n        if isinstance(sys, StateSpace):\n            return deepcopy(sys)\n        elif isinstance(sys, TransferFunction):\n            return tf2ss(sys)\n        else:\n            raise TypeError(\"ss(sys): sys must be a StateSpace or \\\nTransferFunction object.  It is %s.\" % type(sys))\n    else:\n        raise ValueError(\"Needs 1 or 4 arguments; received %i.\" % len(args))\n\n\ndef tf2ss(*args):\n    \"\"\"tf2ss(sys)\n\n    Transform a transfer function to a state space system.\n\n    The function accepts either 1 or 2 parameters:\n\n    ``tf2ss(sys)``\n        Convert a linear system into transfer function form. Always creates\n        a new system, even if sys is already a TransferFunction object.\n\n    ``tf2ss(num, den)``\n        Create a transfer function system from its numerator and denominator\n        polynomial coefficients.\n\n        For details see: :func:`tf`\n\n    Parameters\n    ----------\n    sys: LTI (StateSpace or TransferFunction)\n        A linear system\n    num: array_like, or list of list of array_like\n        Polynomial coefficients of the numerator\n    den: array_like, or list of list of array_like\n        Polynomial coefficients of the denominator\n\n    Returns\n    -------\n    out: StateSpace\n        New linear system in state space form\n\n    Raises\n    ------\n    ValueError\n        if `num` and `den` have invalid or unequal dimensions, or if an\n        invalid number of arguments is passed in\n    TypeError\n        if `num` or `den` are of incorrect type, or if sys is not a\n        TransferFunction object\n\n    See Also\n    --------\n    ss\n    tf\n    ss2tf\n\n    Examples\n    --------\n    >>> num = [[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]]\n    >>> den = [[[9., 8., 7.], [6., 5., 4.]], [[3., 2., 1.], [-1., -2., -3.]]]\n    >>> sys1 = tf2ss(num, den)\n\n    >>> sys_tf = tf(num, den)\n    >>> sys2 = tf2ss(sys_tf)\n\n    \"\"\"\n\n    from .xferfcn import TransferFunction\n    if len(args) == 2 or len(args) == 3:\n        # Assume we were given the num, den\n        return _convertToStateSpace(TransferFunction(*args))\n\n    elif len(args) == 1:\n        sys = args[0]\n        if not isinstance(sys, TransferFunction):\n            raise TypeError(\"tf2ss(sys): sys must be a TransferFunction \\\nobject.\")\n        return _convertToStateSpace(sys)\n    else:\n        raise ValueError(\"Needs 1 or 2 arguments; received %i.\" % len(args))\n\n\ndef rss(states=1, outputs=1, inputs=1):\n    \"\"\"\n    Create a stable *continuous* random state space object.\n\n    Parameters\n    ----------\n    states : integer\n        Number of state variables\n    inputs : integer\n        Number of system inputs\n    outputs : integer\n        Number of system outputs\n\n    Returns\n    -------\n    sys : StateSpace\n        The randomly created linear system\n\n    Raises\n    ------\n    ValueError\n        if any input is not a positive integer\n\n    See Also\n    --------\n    drss\n\n    Notes\n    -----\n    If the number of states, inputs, or outputs is not specified, then the\n    missing numbers are assumed to be 1.  The poles of the returned system\n    will always have a negative real part.\n\n    \"\"\"\n\n    return _rss_generate(states, inputs, outputs, 'c')\n\n\ndef drss(states=1, outputs=1, inputs=1):\n    \"\"\"\n    Create a stable *discrete* random state space object.\n\n    Parameters\n    ----------\n    states : integer\n        Number of state variables\n    inputs : integer\n        Number of system inputs\n    outputs : integer\n        Number of system outputs\n\n    Returns\n    -------\n    sys : StateSpace\n        The randomly created linear system\n\n    Raises\n    ------\n    ValueError\n        if any input is not a positive integer\n\n    See Also\n    --------\n    rss\n\n    Notes\n    -----\n    If the number of states, inputs, or outputs is not specified, then the\n    missing numbers are assumed to be 1.  The poles of the returned system\n    will always have a magnitude less than 1.\n\n    \"\"\"\n\n    return _rss_generate(states, inputs, outputs, 'd')\n\n\ndef ssdata(sys):\n    \"\"\"\n    Return state space data objects for a system\n\n    Parameters\n    ----------\n    sys : LTI (StateSpace, or TransferFunction)\n        LTI system whose data will be returned\n\n    Returns\n    -------\n    (A, B, C, D): list of matrices\n        State space data for the system\n    \"\"\"\n    ss = _convertToStateSpace(sys)\n    return ss.A, ss.B, ss.C, ss.D\n", "import_text": ["math", "numpy", "numpy.any", "numpy.array", "numpy.asarray", "numpy.concatenate", "numpy.cos", "numpy.delete", "numpy.dot", "numpy.empty", "numpy.exp", "numpy.eye", "numpy.isinf", "numpy.ones", "numpy.pad", "numpy.sin", "numpy.zeros", "numpy.squeeze", "numpy.random.rand", "numpy.random.randn", "numpy.linalg.solve", "numpy.linalg.eigvals", "numpy.linalg.matrix_rank", "numpy.linalg.linalg.LinAlgError", "scipy", "scipy.signal.lti", "scipy.signal.cont2discrete", "warnings.warn", "copy.deepcopy"], "prompt": "\"\"\"\nDescription: This function calculates the frequency response of a system.\n\nArgs:\n    self (object): The instance of the class that contains the system matrices (A, B, C, D).\n    omega (array-like): The frequencies at which the frequency response is to be evaluated.\n\nReturns:\n    tuple: A tuple containing the magnitude, phase, and frequencies of the frequency response.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"Evaluate the system's transfer func. at a list of freqs, omega.\n\n        mag, phase, omega = self.freqresp(omega)\n\n        Reports the frequency response of the system,\n\n             G(j*omega) = mag*exp(j*phase)\n\n        for continuous time. For discrete time systems, the response is\n        evaluated around the unit circle such that\n\n             G(exp(j*omega*dt)) = mag*exp(j*phase).\n\n        Parameters\n        ----------\n        omega : array\n            A list of frequencies in radians/sec at which the system should be\n            evaluated. The list can be either a python list or a numpy array\n            and will be sorted before evaluation.\n\n        Returns\n        -------\n        mag : float\n            The magnitude (absolute value, not dB or log10) of the system\n            frequency response.\n\n        phase : float\n            The wrapped phase in radians of the system frequency response.\n\n        omega : array\n            The list of sorted frequencies at which the response was\n            evaluated.\n\n        \"\"\"", "function_dependencies": ["numpy.asarray", "numpy.empty", "numpy.asarray.sort", "numpy.exp", "numpy.abs", "warnings.warn", "numpy.shape", "numpy.angle"], "project_create_time": "2019-09-10T16:09:42+00:00", "project_update_time": "2024-03-31T21:06:37+00:00", "file_create_time": "2019-11-07T16:54:26Z", "file_update_time": "2019-11-07T16:54:26Z", "function_update_time": "2019-11-07T16:54:26Z", "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "reference_api": ["numpy.shape"], "test_function": [{"file_path": "/AMaDiA-master/AMaDiA-master/External_Libraries/python_control_master/control/tests/statesp_test.py", "class_name": "TestStateSpace", "function_name": "test_freq_resp", "code": "    def test_freq_resp(self):\n\n        A = [[-2, 0.5], [0.5, -0.3]]\n        B = [[0.3, -1.3], [0.1, 0.]]\n        C = [[0., 0.1], [-0.3, -0.2]]\n        D = [[0., -0.8], [-0.3, 0.]]\n        sys = StateSpace(A, B, C, D)\n\n        true_mag = [[[0.0852992637230322, 0.00103596611395218],\n                    [0.935374692849736, 0.799380720864549]],\n                   [[0.55656854563842, 0.301542699860857],\n                    [0.609178071542849, 0.0382108097985257]]]\n        true_phase = [[[-0.566195599644593, -1.68063565332582],\n                      [3.0465958317514, 3.14141384339534]],\n                     [[2.90457947657161, 3.10601268291914],\n                      [-0.438157380501337, -1.40720969147217]]]\n        true_omega = [0.1, 10.]\n\n        mag, phase, omega = sys.freqresp(true_omega)\n\n        np.testing.assert_almost_equal(mag, true_mag)\n        np.testing.assert_almost_equal(phase, true_phase)\n        np.testing.assert_equal(omega, true_omega)"}, {"file_path": "/AMaDiA-master/AMaDiA-master/External_Libraries/python_control_master/control/tests/statesp_array_test.py", "class_name": "TestStateSpace", "function_name": "test_freq_resp", "code": "    def test_freq_resp(self):\n\n        A = [[-2, 0.5], [0.5, -0.3]]\n        B = [[0.3, -1.3], [0.1, 0.]]\n        C = [[0., 0.1], [-0.3, -0.2]]\n        D = [[0., -0.8], [-0.3, 0.]]\n        sys = StateSpace(A, B, C, D)\n\n        true_mag = [[[0.0852992637230322, 0.00103596611395218],\n                    [0.935374692849736, 0.799380720864549]],\n                   [[0.55656854563842, 0.301542699860857],\n                    [0.609178071542849, 0.0382108097985257]]]\n        true_phase = [[[-0.566195599644593, -1.68063565332582],\n                      [3.0465958317514, 3.14141384339534]],\n                     [[2.90457947657161, 3.10601268291914],\n                      [-0.438157380501337, -1.40720969147217]]]\n        true_omega = [0.1, 10.]\n\n        mag, phase, omega = sys.freqresp(true_omega)\n\n        np.testing.assert_almost_equal(mag, true_mag)\n        np.testing.assert_almost_equal(phase, true_phase)\n        np.testing.assert_equal(omega, true_omega)"}]}, {"git_group": "indico", "git_name": "indico", "version": "v3.3.2", "language": "Python", "project_name": "indico-v3.3.2.zip", "file_path": "/indico-v3.3.2/indico-3.3.2/indico/modules/rb/tasks.py", "file_name": "tasks.py", "focal_class": null, "focal_name": "roombooking_end_notifications", "focal_parameter": [], "solution": "\ndef roombooking_end_notifications():\n    if not config.ENABLE_ROOMBOOKING:\n        logger.info('Notifications not sent because room booking is disabled')\n        return\n    if not rb_settings.get('end_notifications_enabled'):\n        logger.info('Notifications not sent because they are globally disabled')\n        return\n\n    defaults = {\n        'default': rb_settings.get('end_notification_daily'),\n        'weekly': rb_settings.get('end_notification_weekly'),\n        'monthly': rb_settings.get('end_notification_monthly')\n    }\n\n    room_columns = {\n        'default': Room.end_notification_daily,\n        'weekly': Room.end_notification_weekly,\n        'monthly': Room.end_notification_monthly\n    }\n\n    cte = (db.session.query(Reservation.id, Reservation.repeat_frequency)\n           .add_columns(db.func.max(ReservationOccurrence.start_dt).label('last_valid_end_dt'))\n           .join(Reservation.room)\n           .join(Reservation.occurrences)\n           .filter(ReservationOccurrence.is_valid,\n                   ReservationOccurrence.end_dt >= datetime.now(),\n                   Reservation.is_accepted,\n                   Reservation.repeat_frequency != RepeatFrequency.NEVER,\n                   Room.end_notifications_enabled,\n                   ~Reservation.end_notification_sent,\n                   ~Room.is_deleted)\n           .group_by(Reservation.id)\n           .cte())\n\n    reservations = (Reservation.query\n                    .options(noload('created_by_user'))\n                    .join(cte, cte.c.id == Reservation.id)\n                    .join(Reservation.room)\n                    .filter(_make_occurrence_date_filter(cte.c.last_valid_end_dt, defaults, room_columns,\n                                                         cte.c.repeat_frequency))\n                    .order_by('booked_for_id', 'start_dt', 'room_id')\n                    .all())\n\n    for user, user_reservations in groupby(reservations, key=attrgetter('booked_for_user')):\n        user_reservations = list(user_reservations)\n        notify_about_finishing_bookings(user, list(user_reservations))\n        for user_reservation in user_reservations:\n            user_reservation.end_notification_sent = True\n\n    db.session.commit()", "function_signature": "def roombooking_end_notifications() :", "left_context": "# This file is part of Indico.\n# Copyright (C) 2002 - 2024 CERN\n#\n# Indico is free software; you can redistribute it and/or\n# modify it under the terms of the MIT License; see the\n# LICENSE file for more details.\n\nfrom datetime import date, datetime\nfrom itertools import groupby\nfrom operator import attrgetter\n\nfrom celery.schedules import crontab\nfrom sqlalchemy.orm import contains_eager, noload\n\nfrom indico.core.celery import celery\nfrom indico.core.config import config\nfrom indico.core.db import db\nfrom indico.modules.rb import logger, rb_settings\nfrom indico.modules.rb.models.reservation_occurrences import ReservationOccurrence\nfrom indico.modules.rb.models.reservations import RepeatFrequency, Reservation\nfrom indico.modules.rb.models.rooms import Room\nfrom indico.modules.rb.notifications.reservation_occurrences import notify_upcoming_occurrences\nfrom indico.modules.rb.notifications.reservations import notify_about_finishing_bookings\nfrom indico.util.console import cformat\n\n\ndef _make_occurrence_date_filter(date_column, default_values, room_columns, value_col=Reservation.repeat_frequency):\n    notification_before = db.case({RepeatFrequency.WEEK.value: room_columns['weekly'],\n                                   RepeatFrequency.MONTH.value: room_columns['monthly']},\n                                  else_=room_columns['default'], value=value_col)\n    notification_before_default = db.case({RepeatFrequency.WEEK.value: default_values['weekly'],\n                                           RepeatFrequency.MONTH.value: default_values['monthly']},\n                                          else_=default_values['default'], value=value_col)\n    notification_before_days = db.func.coalesce(notification_before, notification_before_default)\n    days_until = db.cast(date_column, db.Date) - date.today()\n    return days_until == notification_before_days\n\n\ndef _print_occurrences(user, occurrences, _defaults={}, _overrides={}):  # noqa: B006\n    if not _defaults or not _overrides:\n        _defaults.update({RepeatFrequency.WEEK: rb_settings.get('notification_before_days_weekly'),\n                          RepeatFrequency.MONTH: rb_settings.get('notification_before_days_monthly'),\n                          RepeatFrequency.NEVER: rb_settings.get('notification_before_days'),\n                          RepeatFrequency.DAY: rb_settings.get('notification_before_days')})\n        _overrides.update({RepeatFrequency.WEEK: lambda r: r.notification_before_days_weekly,\n                           RepeatFrequency.MONTH: lambda r: r.notification_before_days_monthly,\n                           RepeatFrequency.NEVER: lambda r: r.notification_before_days,\n                           RepeatFrequency.DAY: lambda r: r.notification_before_days})\n    print(cformat('%{grey!}*** {} ({}) ***').format(user.full_name, user.email))\n    for occ in occurrences:\n        default = _defaults[occ.reservation.repeat_frequency]\n        override = _overrides[occ.reservation.repeat_frequency](occ.reservation.room)\n        days = default if override is None else override\n        days_until = (occ.start_dt.date() - date.today()).days\n        print(cformat('  * %{yellow}{}%{reset} %{green}{:5}%{reset} {} {} {} \\t %{blue!}{}%{reset} {} ({})').format(\n            occ.start_dt.date(), occ.reservation.repeat_frequency.name,\n            days,\n            default if override is not None and override != default else ' ',\n            days_until,\n            occ.reservation.id,\n            occ.reservation.room.full_name,\n            occ.reservation.room.id\n        ))\n\n\ndef _notify_occurrences(user, occurrences):\n    notify_upcoming_occurrences(user, occurrences)\n    for occ in occurrences:\n        occ.notification_sent = True\n        if occ.reservation.repeat_frequency == RepeatFrequency.DAY:\n            future_occurrences_query = (occ.reservation.occurrences\n                                        .filter(ReservationOccurrence.start_dt >= datetime.now()))\n            future_occurrences_query.update({'notification_sent': True})\n\n\n@celery.periodic_task(name='roombooking_occurrences', run_every=crontab(minute='15', hour='8'))\ndef roombooking_occurrences(debug=False):\n    if not config.ENABLE_ROOMBOOKING:\n        logger.info('Notifications not sent because room booking is disabled')\n        return\n    if not rb_settings.get('notifications_enabled'):\n        logger.info('Notifications not sent because they are globally disabled')\n        return\n\n    defaults = {\n        'default': rb_settings.get('notification_before_days'),\n        'weekly': rb_settings.get('notification_before_days_weekly'),\n        'monthly': rb_settings.get('notification_before_days_monthly')\n    }\n\n    room_columns = {\n        'default': Room.notification_before_days,\n        'weekly': Room.notification_before_days_weekly,\n        'monthly': Room.notification_before_days_monthly\n    }\n\n    occurrences = (ReservationOccurrence.query\n                   .join(ReservationOccurrence.reservation)\n                   .join(Reservation.room)\n                   .filter(~Room.is_deleted,\n                           Room.notifications_enabled,\n                           Reservation.is_accepted,\n                           Reservation.booked_for_id.isnot(None),\n                           ReservationOccurrence.is_valid,\n                           ReservationOccurrence.start_dt >= datetime.now(),\n                           ~ReservationOccurrence.notification_sent,\n                           _make_occurrence_date_filter(ReservationOccurrence.start_dt, defaults, room_columns))\n                   .order_by(Reservation.booked_for_id, ReservationOccurrence.start_dt, Room.id)\n                   .options(contains_eager('reservation').contains_eager('room'))\n                   .all())\n\n    for user, user_occurrences in groupby(occurrences, key=attrgetter('reservation.booked_for_user')):\n        user_occurrences = list(user_occurrences)\n        if debug:\n            _print_occurrences(user, user_occurrences)\n        else:\n            _notify_occurrences(user, user_occurrences)\n    if not debug:\n        db.session.commit()\n\n\n@celery.periodic_task(name='roombooking_end_notifications', run_every=crontab(minute='0', hour='8'))", "right_context": "", "import_text": ["datetime.date", "datetime.datetime", "itertools.groupby", "operator.attrgetter", "celery.schedules.crontab", "sqlalchemy.orm.contains_eager", "sqlalchemy.orm.noload", "indico.core.celery.celery", "indico.core.config.config", "indico.core.db.db", "indico.modules.rb.logger", "indico.modules.rb.rb_settings", "indico.modules.rb.models.reservation_occurrences.ReservationOccurrence", "indico.modules.rb.models.reservations.RepeatFrequency", "indico.modules.rb.models.reservations.Reservation", "indico.modules.rb.models.rooms.Room", "indico.modules.rb.notifications.reservation_occurrences.notify_upcoming_occurrences", "indico.modules.rb.notifications.reservations.notify_about_finishing_bookings", "indico.util.console.cformat"], "prompt": "\"\"\"\nDescription: This function sends notifications about room bookings ending.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n    None\n\nNotes:\n    - This function uses the API of sqlalchemy.orm.noload to load related objects without them being loaded immediately.\n    - The function checks if room booking is enabled and if end notifications are globally enabled.\n    - It then queries the database for reservations that have not been sent notifications yet and have ended.\n    - It groups these reservations by the user who booked them.\n    - It then sends notifications to each user about their bookings ending.\n    - It marks each reservation as having sent the end notification.\n    - Finally, it commits the changes to the database.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["indico.modules.rb.logger.info", "indico.modules.rb.rb_settings.get", "indico.core.db.db.session.query", "indico.core.db.db.session.query.add_columns", "indico.core.db.db.session.query.add_columns.join", "indico.core.db.db.session.query.add_columns.join.join", "indico.core.db.db.session.query.add_columns.join.join.filter", "indico.core.db.db.session.query.add_columns.join.join.filter.group_by", "indico.core.db.db.session.query.add_columns.join.join.filter.group_by.cte", "indico.core.db.db.func.max", "indico.core.db.db.func.max.label", "datetime.datetime.now", "indico.modules.rb.models.reservations.Reservation.query\n                    .options", "indico.modules.rb.models.reservations.Reservation.query\n                    .options.join", "indico.modules.rb.models.reservations.Reservation.query\n                    .options.join.join", "indico.modules.rb.models.reservations.Reservation.query\n                    .options.join.join.filter", "indico.modules.rb.models.reservations.Reservation.query\n                    .options.join.join.filter.order_by", "indico.modules.rb.models.reservations.Reservation.query\n                    .options.join.join.filter.order_by.all", "sqlalchemy.orm.noload", "itertools.groupby", "operator.attrgetter", "indico.modules.rb.notifications.reservations.notify_about_finishing_bookings", "indico.core.db.db.session.commit"], "project_create_time": "2011-07-27T13:56:30+00:00", "project_update_time": "2024-04-17T05:53:16+00:00", "file_create_time": "2015-05-20T15:31:54Z", "file_update_time": "2024-01-02T19:38:40Z", "function_update_time": "2019-03-06T10:07:05Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["sqlalchemy.orm.noload"], "test_function": [{"file_path": "/indico-v3.3.2/indico-3.3.2/indico/modules/rb/tasks_test.py", "class_name": null, "function_name": "test_roombooking_notifications", "code": "\ndef test_roombooking_notifications(mocker, create_user, create_room, create_reservation, freeze_time):\n    rb_settings.set_multi(settings)\n    user_map = {key: create_user(id_, **data) for id_, (key, data) in enumerate(users.items(), 1)}\n    room_map = {key: create_room(**data) for key, data in rooms.items()}\n\n    notification_map = defaultdict(dict)\n    end_notification_map = defaultdict(dict)\n    for data in chain(reservations, finishing_reservations):\n        data['start_dt'] = dateutil.parser.parse(data['start_dt'])\n        data['end_dt'] = dateutil.parser.parse(data['end_dt'])\n        data['booked_for_user'] = user = user_map[data.pop('user')]\n        data['room'] = room_map[data['room']]\n        notification = data.pop('notification', None)\n        end_notification = data.pop('end_notification', None)\n        reservation = create_reservation(**data)\n        if notification:\n            notification_map[user][reservation] = dateutil.parser.parse(notification).date()\n        if end_notification is not None:\n            end_notification_map[user][reservation] = end_notification\n\n    notify_upcoming_occurrences = mocker.patch('indico.modules.rb.tasks.notify_upcoming_occurrences')\n    notify_about_finishing_bookings = mocker.patch('indico.modules.rb.tasks.notify_about_finishing_bookings')\n    freeze_time(datetime(2017, 4, 1, 8, 0, 0))\n    roombooking_occurrences()\n    for (user, occurrences), __ in notify_upcoming_occurrences.call_args_list:\n        notifications = notification_map.pop(user)\n        for occ in occurrences:\n            date = notifications.pop(occ.reservation)\n            assert occ.start_dt.date() == date\n            assert occ.notification_sent\n            past_occs = [x for x in occ.reservation.occurrences if x.start_dt.date() < date.today()]\n            future_occs = [x for x in occ.reservation.occurrences if x.start_dt.date() > date.today() and x != occ]\n            assert not any(x.notification_sent for x in past_occs)\n            if occ.reservation.repeat_frequency == RepeatFrequency.DAY:\n                assert all(x.notification_sent for x in future_occs)\n            else:\n                assert not any(x.notification_sent for x in future_occs)\n        assert not notifications  # no extra notifications\n    assert not notification_map  # no extra users\n\n    freeze_time(datetime(2019, 7, 8, 8, 0, 0))\n    roombooking_end_notifications()\n    for (user, user_finishing_reservations), __ in notify_about_finishing_bookings.call_args_list:\n        end_notifications = end_notification_map.pop(user)\n        for reservation in user_finishing_reservations:\n            should_be_sent = end_notifications.pop(reservation)\n            assert reservation.end_notification_sent == should_be_sent\n        assert all(not r.end_notification_sent for r in end_notifications)\n    assert not end_notification_map"}]}, {"git_group": "bayesiains", "git_name": "nflows", "version": "v0.14", "language": "Python", "project_name": "nflows-v0.14.zip", "file_path": "/nflows-v0.14/nflows-0.14/nflows/transforms/normalization.py", "file_name": "normalization.py", "focal_class": "BatchNorm", "focal_name": "inverse", "focal_parameter": ["inputs"], "solution": "\n    def inverse(self, inputs, context=None):\n        if self.training:\n            raise InverseNotAvailable(\n                \"Batch norm inverse is only available in eval mode, not in training mode.\"\n            )\n        if inputs.dim() != 2:\n            raise ValueError(\n                \"Expected 2-dim inputs, got inputs of shape: {}\".format(inputs.shape)\n            )\n\n        outputs = (\n            torch.sqrt(self.running_var + self.eps)\n            * ((inputs - self.bias) / self.weight)\n            + self.running_mean\n        )\n\n        logabsdet_ = -torch.log(self.weight) + 0.5 * torch.log(\n            self.running_var + self.eps\n        )\n        logabsdet = torch.sum(logabsdet_) * inputs.new_ones(inputs.shape[0])\n\n        return outputs, logabsdet", "function_signature": "def inverse(self, inputs, context=None) :", "left_context": "\"\"\"Implementation of normalization-based transforms.\"\"\"\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom nflows.transforms.base import InverseNotAvailable, Transform\nimport nflows.utils.typechecks as check\n\n# class BatchNorm(Transform):\n#     \"\"\"Transform that performs batch normalization.\n#\n#     Limitations:\n#         * It works only for 1-dim inputs.\n#         * Inverse is not available in training mode, only in eval mode.\n#     \"\"\"\n#\n#     def __init__(self, features, eps=1e-5, momentum=0.1, affine=True):\n#         if not check.is_positive_int(features):\n#             raise TypeError('Number of features must be a positive integer.')\n#         super().__init__()\n#\n#         self.batch_norm = nets.BatchNorm1d(\n#             num_features=features,\n#             eps=eps,\n#             momentum=momentum,\n#             affine=affine,\n#             track_running_stats=True,  # We mustn't use batch statistics in eval mode.\n#         )\n#\n#     def forward(self, inputs):\n#         if inputs.dim() != 2:\n#             raise ValueError('Expected 2-dim inputs, got inputs of shape: {}'.format(inputs.shape))\n#\n#         outputs = self.batch_norm(inputs)\n#\n#         if self.training:\n#             var = torch.var(inputs, dim=0, unbiased=False)\n#         else:\n#             var = self.batch_norm.running_var\n#         logabsdet = -0.5 * torch.log(var + self.batch_norm.eps)\n#         if self.batch_norm.affine:\n#             logabsdet += torch.log(self.batch_norm.weight)\n#         logabsdet = torch.sum(logabsdet)\n#         logabsdet = logabsdet * torch.ones(inputs.shape[0])\n#\n#         return outputs, logabsdet\n#\n#     def inverse(self, inputs):\n#         if self.training:\n#             raise InverseNotAvailable(\n#                 'Batch norm inverse is only available in eval mode, not in training mode.')\n#         if inputs.dim() != 2:\n#             raise ValueError('Expected 2-dim inputs, got inputs of shape: {}'.format(inputs.shape))\n#\n#         outputs = inputs.clone()\n#         if self.batch_norm.affine:\n#             outputs -= self.batch_norm.bias\n#             outputs /= self.batch_norm.weight\n#         outputs *= torch.sqrt(self.batch_norm.running_var + self.batch_norm.eps)\n#         outputs += self.batch_norm.running_mean\n#\n#         logabsdet = 0.5 * torch.log(self.batch_norm.running_var + self.batch_norm.eps)\n#         if self.batch_norm.affine:\n#             logabsdet -= torch.log(self.batch_norm.weight)\n#         logabsdet = torch.sum(logabsdet)\n#         logabsdet = logabsdet * torch.ones(inputs.shape[0])\n#\n#         return outputs, logabsdet\n\n\nclass BatchNorm(Transform):\n    \"\"\"Transform that performs batch normalization.\n\n    Limitations:\n        * It works only for 1-dim inputs.\n        * Inverse is not available in training mode, only in eval mode.\n    \"\"\"\n\n    def __init__(self, features, eps=1e-5, momentum=0.1, affine=True):\n        if not check.is_positive_int(features):\n            raise TypeError(\"Number of features must be a positive integer.\")\n        super().__init__()\n\n        self.momentum = momentum\n        self.eps = eps\n        constant = np.log(np.exp(1 - eps) - 1)\n        self.unconstrained_weight = nn.Parameter(constant * torch.ones(features))\n        self.bias = nn.Parameter(torch.zeros(features))\n\n        self.register_buffer(\"running_mean\", torch.zeros(features))\n        self.register_buffer(\"running_var\", torch.zeros(features))\n\n    @property\n    def weight(self):\n        return F.softplus(self.unconstrained_weight) + self.eps\n\n    def forward(self, inputs, context=None):\n        if inputs.dim() != 2:\n            raise ValueError(\n                \"Expected 2-dim inputs, got inputs of shape: {}\".format(inputs.shape)\n            )\n\n        if self.training:\n            mean, var = inputs.mean(0), inputs.var(0)\n            self.running_mean.mul_(1 - self.momentum).add_(mean.detach() * self.momentum)\n            self.running_var.mul_(1 - self.momentum).add_(var.detach() * self.momentum)\n        else:\n            mean, var = self.running_mean, self.running_var\n\n        outputs = (\n            self.weight * ((inputs - mean) / torch.sqrt((var + self.eps))) + self.bias\n        )\n\n        logabsdet_ = torch.log(self.weight) - 0.5 * torch.log(var + self.eps)\n        logabsdet = torch.sum(logabsdet_) * inputs.new_ones(inputs.shape[0])\n\n        return outputs, logabsdet\n", "right_context": "\n\nclass ActNorm(Transform):\n    def __init__(self, features):\n        \"\"\"\n        Transform that performs activation normalization. Works for 2D and 4D inputs. For 4D\n        inputs (images) normalization is performed per-channel, assuming BxCxHxW input shape.\n\n        Reference:\n        > D. Kingma et. al., Glow: Generative flow with invertible 1x1 convolutions, NeurIPS 2018.\n        \"\"\"\n        if not check.is_positive_int(features):\n            raise TypeError(\"Number of features must be a positive integer.\")\n        super().__init__()\n\n        self.register_buffer(\"initialized\", torch.tensor(False, dtype=torch.bool))\n        self.log_scale = nn.Parameter(torch.zeros(features))\n        self.shift = nn.Parameter(torch.zeros(features))\n\n    @property\n    def scale(self):\n        return torch.exp(self.log_scale)\n\n    def _broadcastable_scale_shift(self, inputs):\n        if inputs.dim() == 4:\n            return self.scale.view(1, -1, 1, 1), self.shift.view(1, -1, 1, 1)\n        else:\n            return self.scale.view(1, -1), self.shift.view(1, -1)\n\n    def forward(self, inputs, context=None):\n        if inputs.dim() not in [2, 4]:\n            raise ValueError(\"Expecting inputs to be a 2D or a 4D tensor.\")\n\n        if self.training and not self.initialized:\n            self._initialize(inputs)\n\n        scale, shift = self._broadcastable_scale_shift(inputs)\n        outputs = scale * inputs + shift\n\n        if inputs.dim() == 4:\n            batch_size, _, h, w = inputs.shape\n            logabsdet = h * w * torch.sum(self.log_scale) * outputs.new_ones(batch_size)\n        else:\n            batch_size, _ = inputs.shape\n            logabsdet = torch.sum(self.log_scale) * outputs.new_ones(batch_size)\n\n        return outputs, logabsdet\n\n    def inverse(self, inputs, context=None):\n        if inputs.dim() not in [2, 4]:\n            raise ValueError(\"Expecting inputs to be a 2D or a 4D tensor.\")\n\n        scale, shift = self._broadcastable_scale_shift(inputs)\n        outputs = (inputs - shift) / scale\n\n        if inputs.dim() == 4:\n            batch_size, _, h, w = inputs.shape\n            logabsdet = -h * w * torch.sum(self.log_scale) * outputs.new_ones(batch_size)\n        else:\n            batch_size, _ = inputs.shape\n            logabsdet = -torch.sum(self.log_scale) * outputs.new_ones(batch_size)\n\n        return outputs, logabsdet\n\n    def _initialize(self, inputs):\n        \"\"\"Data-dependent initialization, s.t. post-actnorm activations have zero mean and unit\n        variance. \"\"\"\n        if inputs.dim() == 4:\n            num_channels = inputs.shape[1]\n            inputs = inputs.permute(0, 2, 3, 1).reshape(-1, num_channels)\n\n        with torch.no_grad():\n            std = inputs.std(dim=0)\n            mu = (inputs / std).mean(dim=0)\n            self.log_scale.data = -torch.log(std)\n            self.shift.data = -mu\n            self.initialized.data = torch.tensor(True, dtype=torch.bool)\n", "import_text": ["numpy", "torch", "torch.nn", "torch.nn.functional", "nflows.transforms.base.InverseNotAvailable", "nflows.transforms.base.Transform", "nflows.utils.typechecks"], "prompt": "\"\"\"\nDescription: This function performs the inverse operation of a batch normalization layer.\n\nArgs:\n    inputs (torch.Tensor): The input tensor to be inverted.\n    context (optional): The context of the operation. Defaults to None.\n\nRaises:\n    InverseNotAvailable: If the function is called in training mode.\n    ValueError: If the input tensor is not 2-dimensional.\n\nReturns:\n    tuple: A tuple containing the inverted tensor and the log-absolute determinant of the Jacobian matrix.\n\nNotes:\n    This function is only available in evaluation mode, not in training mode.\n    The input tensor must be 2-dimensional.\n    The inverse operation is performed using the running mean and variance of the batch normalization layer.\n    The log-absolute determinant of the Jacobian matrix is computed using the weight and bias of the batch normalization layer.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["nflows.transforms.base.InverseNotAvailable", "torch.sqrt", "torch.log", "torch.sum"], "project_create_time": "2020-02-17T12:36:32+00:00", "project_update_time": "2024-04-17T13:19:11+00:00", "file_create_time": "2020-02-14T12:55:40Z", "file_update_time": "2020-09-25T13:45:18Z", "function_update_time": "2020-02-14T12:55:40Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["torch.sum", "torch.log", "torch.sqrt"], "test_function": [{"file_path": "/nflows-v0.14/nflows-0.14/tests/transforms/normalization_test.py", "class_name": "BatchNormTest", "function_name": "test_inverse", "code": "\n    def test_inverse(self):\n        features = 100\n        batch_size = 50\n        inputs = torch.randn(batch_size, features)\n\n        for affine in [True, False]:\n            with self.subTest(affine=affine):\n                transform = norm.BatchNorm(features=features, affine=affine)\n                with self.assertRaises(base.InverseNotAvailable):\n                    transform.inverse(inputs)\n                transform.eval()\n                outputs, logabsdet = transform.inverse(inputs)\n                self.assert_tensor_is_good(outputs, [batch_size, features])\n                self.assert_tensor_is_good(logabsdet, [batch_size])"}]}, {"git_group": "Flagsmith", "git_name": "flagsmith", "version": "v2.109.0", "language": "Python", "project_name": "flagsmith-v2.109.0.zip", "file_path": "/flagsmith-v2.109.0/flagsmith-2.109.0/api/core/redis_cluster.py", "file_name": "redis_cluster.py", "focal_class": "ClusterConnectionFactory", "focal_name": "get_connection", "focal_parameter": [], "solution": "    def get_connection(self, connection_params: dict) -> RedisCluster:\n        try:\n            client_cls_kwargs = deepcopy(self.redis_client_cls_kwargs)\n            # ... and smash 'em together (crashing if there's conflicts)...\n            for key, value in connection_params.items():\n                if key in client_cls_kwargs:\n                    raise ImproperlyConfigured(\n                        f\"Found '{key}' in both the connection and the client kwargs\"\n                    )\n                client_cls_kwargs[key] = value\n\n            # Add explicit socket timeout\n            client_cls_kwargs[\"socket_timeout\"] = SOCKET_TIMEOUT\n            client_cls_kwargs[\"socket_keepalive\"] = True\n            # ... and then build and return the client\n            return RedisCluster(**client_cls_kwargs)\n        except Exception as e:\n            # Let django redis handle the exception\n            raise ConnectionInterrupted(connection=None) from e", "function_signature": "def get_connection(self, connection_params: dict) -> RedisCluster :", "left_context": "\"\"\"\nTemporary module that adds support for Redis Cluster to django-redis by implementing\na connection factory class(`ClusterConnectionFactory`).\nThis module should be removed once [this](https://github.com/jazzband/django-redis/issues/606)\nis resolved.\n\nUsage:\n------\nInclude the following configuration in Django project's settings.py file:\n\n```python\n# settings.py\n\n\"cache_name: {\n        \"BACKEND\": ...,\n        \"LOCATION\": ...,\n        \"OPTIONS\": {\n            \"CLIENT_CLASS\": \"core.redis_cluster.SafeRedisClusterClient\",\n\n        },\n    },\n\"\"\"\n\nimport threading\nfrom copy import deepcopy\n\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django_redis.client.default import DefaultClient\nfrom django_redis.exceptions import ConnectionInterrupted\nfrom django_redis.pool import ConnectionFactory\nfrom redis.cluster import RedisCluster\nfrom redis.exceptions import RedisClusterException\n\nSOCKET_TIMEOUT = 0.2\n\n\nclass SafeRedisClusterClient(DefaultClient):\n    SAFE_METHODS = [\n        \"set\",\n        \"get\",\n        \"incr_version\",\n        \"delete\",\n        \"delete_pattern\",\n        \"delete_many\",\n        \"clear\",\n        \"get_many\",\n        \"set_many\",\n        \"incr\",\n        \"has_key\",\n        \"keys\",\n    ]\n\n    @staticmethod\n    def _safe_operation(func):\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except RedisClusterException as e:\n                raise ConnectionInterrupted(connection=None) from e\n\n        return wrapper\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # Dynamically generate safe versions of methods\n        for method_name in self.SAFE_METHODS:\n            setattr(\n                self, method_name, self._safe_operation(getattr(super(), method_name))\n            )\n\n        # Let's use our own connection factory here\n        self.connection_factory = ClusterConnectionFactory(options=self._options)\n\n\nclass ClusterConnectionFactory(ConnectionFactory):\n    \"\"\"A connection factory for redis.cluster.RedisCluster\n    The cluster client manages connection pools internally, so we don't want to\n    do it at this level like the base ConnectionFactory does.\n    \"\"\"\n\n    # A global cache of URL->client so that within a process, we will reuse a\n    # single client, and therefore a single set of connection pools.\n    _clients = {}\n    _clients_lock = threading.Lock()\n\n    def connect(self, url: str) -> RedisCluster:\n        \"\"\"Given a connection url, return a client instance.\n        Prefer to return from our cache but if we don't yet have one build it\n        to populate the cache.\n        \"\"\"\n        if url not in self._clients:\n            with self._clients_lock:\n                if url not in self._clients:\n                    params = self.make_connection_params(url)\n                    self._clients[url] = self.get_connection(params)\n\n        return self._clients[url]\n", "right_context": "\n    def disconnect(self, connection: RedisCluster):\n        connection.disconnect_connection_pools()\n", "import_text": ["threading", "copy.deepcopy", "django.core.exceptions.ImproperlyConfigured", "django_redis.client.default.DefaultClient", "django_redis.exceptions.ConnectionInterrupted", "django_redis.pool.ConnectionFactory", "redis.cluster.RedisCluster", "redis.exceptions.RedisClusterException"], "prompt": "\"\"\"\nDescription: This function is used to establish a connection to a Redis Cluster.\n\nArgs:\n    connection_params (dict): A dictionary containing the connection parameters.\n\nReturns:\n    RedisCluster: An instance of RedisCluster with the provided connection parameters.\n\nRaises:\n    ImproperlyConfigured: If a key from connection_params is found in the client_cls_kwargs.\n    ConnectionInterrupted: If any other exception occurs during the connection process.\n\"\"\"", "comment": "        \"\"\"\n        Given connection_params, return a new client instance.\n        Basic django-redis ConnectionFactory manages a cache of connection\n        pools and builds a fresh client each time. because the cluster client\n        manages its own connection pools, we will instead merge the\n        \"connection\" and \"client\" kwargs and throw them all at the client to\n        sort out.\n        If we find conflicting client and connection kwargs, we'll raise an\n        error.\n        \"\"\"", "prompt_is_gen_from_api": true, "function_dependencies": ["copy.deepcopy", "django.core.exceptions.ImproperlyConfigured", "redis.cluster.RedisCluster", "django_redis.exceptions.ConnectionInterrupted"], "project_create_time": "2018-06-05T10:49:57+00:00", "project_update_time": "2024-04-17T15:09:22+00:00", "file_create_time": "2024-01-31T03:22:57Z", "file_update_time": "2024-02-12T09:38:14Z", "function_update_time": "2024-01-31T03:22:57Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["redis.cluster.RedisCluster", "django.core.exceptions.ImproperlyConfigured"], "test_function": [{"file_path": "/flagsmith-v2.109.0/flagsmith-2.109.0/api/tests/unit/core/test_redis_cluster.py", "class_name": null, "function_name": "test_cluster_connection_factory__get_connection_with_non_conflicting_params", "code": "\ndef test_cluster_connection_factory__get_connection_with_non_conflicting_params(\n    mocker: MockerFixture,\n):\n    # Given\n    mockRedisCluster = mocker.patch(\"core.redis_cluster.RedisCluster\")\n    connection_factory = ClusterConnectionFactory(\n        options={\"REDIS_CLIENT_KWARGS\": {\"decode_responses\": False}}\n    )\n    connection_params = {\"host\": \"localhost\", \"port\": 6379}\n\n    # When\n    connection_factory.get_connection(connection_params)\n\n    # Then\n    mockRedisCluster.assert_called_once_with(\n        decode_responses=False,\n        host=\"localhost\",\n        port=6379,\n        socket_keepalive=True,\n        socket_timeout=0.2,\n    )"}, {"file_path": "/flagsmith-v2.109.0/flagsmith-2.109.0/api/tests/unit/core/test_redis_cluster.py", "class_name": null, "function_name": "test_cluster_connection_factory__get_connection_with_conflicting_params", "code": "\ndef test_cluster_connection_factory__get_connection_with_conflicting_params(\n    mocker: MockerFixture,\n):\n    # Given\n    mockRedisCluster = mocker.patch(\"core.redis_cluster.RedisCluster\")\n    connection_factory = ClusterConnectionFactory(\n        options={\"REDIS_CLIENT_KWARGS\": {\"decode_responses\": False}}\n    )\n    connection_params = {\"decode_responses\": True}\n\n    # When\n    with pytest.raises(ConnectionInterrupted):\n        connection_factory.get_connection(connection_params)\n\n    # Then - ImproperlyConfigured exception is raised\n    mockRedisCluster.assert_not_called()"}]}, {"git_group": "catalyst-cooperative", "git_name": "pudl", "version": "v2024.2.6", "language": "Python", "project_name": "pudl-v2024.2.6.zip", "file_path": "/pudl-v2024.2.6/pudl-2024.2.6/src/pudl/transform/ferc1.py", "file_name": "ferc1.py", "focal_class": null, "focal_name": "fill_dbf_to_xbrl_map", "focal_parameter": [], "solution": "def fill_dbf_to_xbrl_map(\n    df: pd.DataFrame, dbf_years: list[int] | None = None\n) -> pd.DataFrame:\n    if not dbf_years:\n        dbf_years = Ferc1Settings().dbf_years\n    # If the first year that we're trying to produce isn't mapped, we won't be able to\n    # forward fill.\n    if min(dbf_years) not in df.report_year.unique():\n        raise ValueError(\n            \"Invalid combination of years and DBF-XBRL mapping. The first year cannot\\n\"\n            \"be filled and **must** be mapped.\\n\"\n            f\"First year: {min(dbf_years)}, \"\n            f\"Mapped years: {sorted(df.report_year.unique())}\\n\"\n            f\"{df}\"\n        )\n\n    if df.loc[(df.row_type == \"header\"), \"xbrl_factoid\"].notna().any():\n        raise ValueError(\"Found non-null XBRL column value mapped to a DBF header row.\")\n    df.loc[df.row_type == \"header\", \"xbrl_factoid\"] = \"HEADER_ROW\"\n\n    if df[\"xbrl_factoid\"].isna().any():\n        raise ValueError(\n            \"Found NA XBRL values in the DBF-XBRL mapping, which shouldn't happen. \\n\"\n            f\"{df[df['xbrl_factoid'].isna()]}\"\n        )\n    df = df.drop([\"row_type\"], axis=\"columns\")\n\n    # Create an index containing all combinations of report_year and row_number\n    idx_cols = [\"report_year\", \"row_number\", \"sched_table_name\"]\n    idx = pd.MultiIndex.from_product(\n        [dbf_years, df.row_number.unique(), df.sched_table_name.unique()],\n        names=idx_cols,\n    )\n\n    # Concatenate the row map with the empty index, so we have blank spaces to fill:\n    df = pd.concat(\n        [\n            pd.DataFrame(index=idx),\n            df.set_index(idx_cols),\n        ],\n        axis=\"columns\",\n    ).reset_index()\n\n    # Forward fill missing XBRL column names, until a new definition for the row\n    # number is encountered:\n    df.loc[:, [\"xbrl_factoid\", \"row_literal\"]] = df.groupby(\n        [\"row_number\", \"sched_table_name\"]\n    )[[\"xbrl_factoid\", \"row_literal\"]].transform(\"ffill\")\n    # Drop NA values produced in the broadcasting merge onto the exhaustive index.\n    df = df.dropna(subset=\"xbrl_factoid\")\n    # There should be no NA values left at this point:\n    if df.isnull().any(axis=None):\n        raise ValueError(\n            \"Filled DBF-XBRL map contains NA values, which should never happen:\"\n            f\"\\n{df[df.isnull().any(axis='columns')]}\"\n        )\n    return df", "function_signature": "def fill_dbf_to_xbrl_map(\n    df: pd.DataFrame, dbf_years: list[int] | None = None\n) -> pd.DataFrame :", "left_context": "\"\"\"Classes & functions to process FERC Form 1 data before loading into the PUDL DB.\n\nNote that many of the classes/objects here inherit from/are instances of classes defined\nin :mod:`pudl.transform.classes`. Their design and relationships to each other are\ndocumented in that module.\n\nSee :mod:`pudl.transform.params.ferc1` for the values that parameterize many of these\ntransformations.\n\"\"\"\nimport enum\nimport importlib.resources\nimport itertools\nimport json\nimport re\nfrom abc import abstractmethod\nfrom collections import namedtuple\nfrom collections.abc import Mapping\nfrom typing import Annotated, Any, Literal, Self\n\nimport numpy as np\nimport pandas as pd\nimport sqlalchemy as sa\nfrom dagster import AssetIn, AssetsDefinition, asset\nfrom pandas.core.groupby import DataFrameGroupBy\nfrom pydantic import BaseModel, Field, field_validator\n\nimport pudl\nfrom pudl.extract.ferc1 import TABLE_NAME_MAP_FERC1\nfrom pudl.helpers import assert_cols_areclose, convert_cols_dtypes\nfrom pudl.metadata.fields import apply_pudl_dtypes\nfrom pudl.settings import Ferc1Settings\nfrom pudl.transform.classes import (\n    AbstractTableTransformer,\n    InvalidRows,\n    RenameColumns,\n    TableTransformParams,\n    TransformParams,\n    cache_df,\n    enforce_snake_case,\n)\n\nlogger = pudl.logging_helpers.get_logger(__name__)\n\n\n@asset\ndef _core_ferc1_xbrl__metadata_json(\n    raw_ferc1_xbrl__metadata_json: dict[str, dict[str, list[dict[str, Any]]]],\n) -> dict[str, dict[str, list[dict[str, Any]]]]:\n    \"\"\"Generate cleaned json xbrl metadata.\n\n    For now, this only runs :func:`add_source_tables_to_xbrl_metadata`.\n    \"\"\"\n    return add_source_tables_to_xbrl_metadata(raw_ferc1_xbrl__metadata_json)\n\n\ndef add_source_tables_to_xbrl_metadata(\n    raw_ferc1_xbrl__metadata_json: dict[str, dict[str, list[dict[str, Any]]]],\n) -> dict[str, dict[str, list[dict[str, Any]]]]:\n    \"\"\"Add a ``source_tables`` field into metadata calculation components.\n\n    When a particular component of a calculation does not originate from the table in\n    which the calculated field is being reported, label the source table.\n    \"\"\"\n\n    def all_fields_in_table(table_meta) -> list:\n        \"\"\"Compile a list of all of the fields reported in a table.\"\"\"\n        return [\n            field[\"name\"] for meta_list in table_meta.values() for field in meta_list\n        ]\n\n    def extract_tables_to_fields(xbrl_meta: dict) -> dict[str : list[str]]:\n        \"\"\"Compile a dictionary of table names (keys) to list of fields.\"\"\"\n        return {\n            table_name: all_fields_in_table(table_meta)\n            for table_name, table_meta in xbrl_meta.items()\n        }\n\n    def label_source_tables(calc_component: dict, tables_to_fields: str) -> dict:\n        \"\"\"Add a ``source_tables`` element to the calculation component.\"\"\"\n        calc_component[\"source_tables\"] = [\n            other_table_name\n            for other_table_name, fields in tables_to_fields.items()\n            if calc_component[\"name\"] in fields\n        ]\n        # weirdly there are a number of nuclear xbrl_factoid calc components that seem\n        # to have no source_tables.\n        if not calc_component[\"source_tables\"]:\n            logger.debug(f\"Found no source table for {calc_component['name']}.\")\n        return calc_component\n\n    tables_to_fields = extract_tables_to_fields(raw_ferc1_xbrl__metadata_json)\n    # for each table loop through all of the calculations within each field\n    for table_name, table_meta in raw_ferc1_xbrl__metadata_json.items():\n        for list_of_facts in table_meta.values():\n            for xbrl_fact in list_of_facts:\n                # all facts have ``calculations``, but they are empty lists when null\n                for calc_component in xbrl_fact[\"calculations\"]:\n                    # does the calc component show up in the table? if not, add a label\n                    if calc_component[\"name\"] not in tables_to_fields[table_name]:\n                        calc_component = label_source_tables(\n                            calc_component, tables_to_fields\n                        )\n                    else:\n                        calc_component[\"source_tables\"] = [table_name]\n    return raw_ferc1_xbrl__metadata_json\n\n\n################################################################################\n# FERC 1 Transform Parameter Models\n################################################################################\n@enum.unique\nclass SourceFerc1(enum.Enum):\n    \"\"\"Enumeration of allowed FERC 1 raw data sources.\"\"\"\n\n    XBRL = \"xbrl\"\n    DBF = \"dbf\"\n\n\n@enum.unique\nclass TableIdFerc1(enum.Enum):\n    \"\"\"Enumeration of the allowed FERC 1 table IDs.\n\n    Hard coding this doesn't seem ideal. Somehow it should be either defined in the\n    context of the Package, the Ferc1Settings, an etl_group, or DataSource. All of the\n    table transformers associated with a given data source should have a table_id that's\n    from that data source's subset of the database. Where should this really happen?\n    Alternatively, the allowable values could be derived *from* the structure of the\n    Package. But this works for now.\n    \"\"\"\n\n    STEAM_PLANTS_FUEL = \"core_ferc1__yearly_steam_plants_fuel_sched402\"\n    STEAM_PLANTS = \"core_ferc1__yearly_steam_plants_sched402\"\n    HYDROELECTRIC_PLANTS = \"core_ferc1__yearly_hydroelectric_plants_sched406\"\n    SMALL_PLANTS = \"core_ferc1__yearly_small_plants_sched410\"\n    PUMPED_STORAGE_PLANTS = \"core_ferc1__yearly_pumped_storage_plants_sched408\"\n    PLANT_IN_SERVICE = \"core_ferc1__yearly_plant_in_service_sched204\"\n    PURCHASED_POWER_AND_EXCHANGES = (\n        \"core_ferc1__yearly_purchased_power_and_exchanges_sched326\"\n    )\n    TRANSMISSION_LINES = \"core_ferc1__yearly_transmission_lines_sched422\"\n    ENERGY_SOURCES = \"core_ferc1__yearly_energy_sources_sched401\"\n    ENERGY_DISPOSITIONS = \"core_ferc1__yearly_energy_dispositions_sched401\"\n    UTILITY_PLANT_SUMMARY = \"core_ferc1__yearly_utility_plant_summary_sched200\"\n    OPERATING_EXPENSES = \"core_ferc1__yearly_operating_expenses_sched320\"\n    BALANCE_SHEET_LIABILITIES = \"core_ferc1__yearly_balance_sheet_liabilities_sched110\"\n    DEPRECIATION_SUMMARY = \"core_ferc1__yearly_depreciation_summary_sched336\"\n    BALANCE_SHEET_ASSETS = \"core_ferc1__yearly_balance_sheet_assets_sched110\"\n    RETAINED_EARNINGS = \"core_ferc1__yearly_retained_earnings_sched118\"\n    INCOME_STATEMENTS = \"core_ferc1__yearly_income_statements_sched114\"\n    DEPRECIATION_CHANGES = \"core_ferc1__yearly_depreciation_changes_sched219\"\n    OPERATING_REVENUES = \"core_ferc1__yearly_operating_revenues_sched300\"\n    DEPRECIATION_BY_FUNCTION = \"core_ferc1__yearly_depreciation_by_function_sched219\"\n    CASH_FLOWS = \"core_ferc1__yearly_cash_flows_sched120\"\n    SALES_BY_RATE_SCHEDULES = \"core_ferc1__yearly_sales_by_rate_schedules_sched304\"\n    OTHER_REGULATORY_LIABILITIES = (\n        \"core_ferc1__yearly_other_regulatory_liabilities_sched278\"\n    )\n\n\n################################################################################\n# FERC 1 specific Column, MultiColumn, and Table Transform Functions\n################################################################################\n\n\nclass RenameColumnsFerc1(TransformParams):\n    \"\"\"Dictionaries for renaming either XBRL or DBF derived FERC 1 columns.\n\n    This is FERC 1 specific, because we need to store both DBF and XBRL rename\n    dictionaires separately. Note that this parameter model does not have its own unique\n    transform function. Like the generic :class:`pudl.transform.classes.RenameColumns`\n    it depends on the build in :meth:`pd.rename` method, which is called with the values\n    DBF or XBRL parameters depending on the context.\n\n    Potential parameters validations that could be implemented\n\n    * Validate that all keys appear in the original dbf/xbrl sources.\n      This has to be true, but right now we don't have stored metadata enumerating all\n      of the columns that exist in the raw data, so we don't have anything to check\n      against. Implement once when we have schemas defined for after the extract step.\n\n    * Validate all values appear in PUDL tables, and all expected PUDL names are mapped.\n      Actually we can't require that the rename values appear in the PUDL tables,\n      because there will be cases in which the original column gets dropped or modified,\n      e.g. in the case of unit conversions with a column rename.\n    \"\"\"\n\n    dbf: RenameColumns = RenameColumns()\n    xbrl: RenameColumns = RenameColumns()\n    duration_xbrl: RenameColumns = RenameColumns()\n    instant_xbrl: RenameColumns = RenameColumns()\n\n    @property\n    def rename_dicts_xbrl(self):\n        \"\"\"Compile all of the XBRL rename dictionaries into an ordered list.\"\"\"\n        # add the xbrl last bc it happens after the inst/dur renames\n        return [self.duration_xbrl, self.instant_xbrl, self.xbrl]\n\n\nclass WideToTidy(TransformParams):\n    \"\"\"Parameters for converting a wide table to a tidy table with value types.\"\"\"\n\n    idx_cols: list[str] | None = None\n    \"\"\"List of column names to treat as the table index.\"\"\"\n\n    stacked_column_name: str | None = None\n    \"\"\"Name of column that will contain the stacked categories.\"\"\"\n\n    value_types: list[str] | None = None\n    \"\"\"List of names of value types that will end up being the column names.\n\n    Some of the FERC tables have multiple data types spread across many different\n    categories.  In the input dataframe given to :func:`wide_to_tidy`, the value types\n    must be the suffixes of the column names. If the table does not natively have the\n    pattern of \"{to-be stacked category}_{value_type}\", rename the columns using a\n    ``rename_columns.duration_xbrl``, ``rename_columns.instant_xbrl`` or\n    ``rename_columns.dbf`` parameter which will be employed in\n    :meth:`process_duration_xbrl`, :meth:`process_instant_xbrl` or :meth:`process_dbf`.\n    \"\"\"\n\n    expected_drop_cols: int = 0\n    \"\"\"The number of columns that are expected to be dropped.\n\n    :func:`wide_to_tidy_xbrl` will generate a regex pattern assuming the ``value_types``\n    are the column name's suffixes. If a column does not conform to that pattern, it\n    will be filtered out. This is helpful for us to not include a bunch of columns from\n    the input dataframe incorrectly included in the stacking process. We could enumerate\n    every column that we want to drop, but this could be tedious and potentially error\n    prone. But this does mean that if a column is incorrectly named - or barely missing\n    the pattern, it will be dropped. This parameter enables us to lock the number of\n    expected columns. If the dropped columns are a different number, an error will be\n    raised.\n    \"\"\"\n\n\nclass WideToTidySourceFerc1(TransformParams):\n    \"\"\"Parameters for converting either or both XBRL and DBF table from wide to tidy.\"\"\"\n\n    xbrl: WideToTidy | list[WideToTidy] = WideToTidy()\n    dbf: WideToTidy | list[WideToTidy] = WideToTidy()\n\n    @property\n    def value_types(self) -> list[str]:\n        \"\"\"Compile a list of all of the ``value_types`` from ``wide_to_tidy``.\"\"\"\n        # each wtt source could be either a list of WideToTidy or a WideToTidy.\n        # so we need to drill in to either grab the value_types\n        value_types = []\n        for wide_to_tidy in [self.xbrl, self.dbf]:\n            if isinstance(wide_to_tidy, WideToTidy):\n                value_types.append(wide_to_tidy.value_types)\n            elif isinstance(wide_to_tidy, list):\n                for rly_wide_to_tidy in wide_to_tidy:\n                    value_types.append(rly_wide_to_tidy.value_types)\n        # remove None's & flatten/dedupe\n        value_types = [v for v in value_types if v is not None]\n        flattened_values = []\n        for item in value_types:\n            if isinstance(item, list):\n                flattened_values += list(item)\n            elif isinstance(item, str):\n                flattened_values.append(item)\n        return flattened_values\n\n\ndef wide_to_tidy(df: pd.DataFrame, params: WideToTidy) -> pd.DataFrame:\n    \"\"\"Reshape wide tables with FERC account columns to tidy format.\n\n    The XBRL table coming into this method could contain all the data from both the\n    instant and duration tables in a wide format -- with one column for every\n    combination of value type (e.g. additions, ending_balance) and value category, which\n    means ~500 columns for some tables.\n\n    We tidy this into a long table with one column for each of the value types in\n    ``params.value_types`` and a new column named ``xbrl_factoid`` that contains\n    categories that were previously the XBRL column name stems.\n\n    This allows aggregations of multiple ``xbrl_factoid`` categories in a columnar\n    fashion such as aggregation across groups of rows to total up various hierarchical\n    accounting categories (hydraulic turbines -> hydraulic production plant -> all\n    production plant -> all electric utility plant) though the categorical columns\n    required for that aggregation are added later.\n\n    For table that have a internal relationship between the values in the\n    ``params.value_types``, such as the :ref:`core_ferc1__yearly_plant_in_service_sched204` table, this also\n    enables aggregation across columns to calculate the ending balance based on the\n    starting balance and all of the reported changes.\n    \"\"\"\n    suffixes = \"|\".join(params.value_types)\n    pat = r\"(^.*)_(\" + suffixes + r\"$)\"\n    # filter out any columns that don't match the pattern\n    df_out = df.set_index(params.idx_cols).filter(regex=pat)\n    # check if there are unexpected columns being dropped\n    dropped_cols = [col for col in df if col not in df_out.reset_index().columns]\n    logger.debug(f\"dropping: {dropped_cols}\")\n    if params.expected_drop_cols != len(dropped_cols):\n        raise AssertionError(\n            f\"Unexpected number of columns dropped: ({len(dropped_cols)}) instead of \"\n            f\"({params.expected_drop_cols}). Columns dropped: {dropped_cols}\"\n        )\n\n    new_cols = pd.MultiIndex.from_tuples(\n        [(re.sub(pat, r\"\\1\", col), re.sub(pat, r\"\\2\", col)) for col in df_out.columns],\n        names=[params.stacked_column_name, \"value_type\"],\n    )\n    df_out.columns = new_cols\n    df_out = (\n        df_out.stack(params.stacked_column_name, dropna=False)\n        .loc[:, params.value_types]\n        .reset_index()\n    )\n    # remove the name of the columns which was the name of the renaming layer of the\n    # multi-index\n    df_out.columns.name = None\n    return df_out\n\n\nclass MergeXbrlMetadata(TransformParams):\n    \"\"\"Parameters for merging in XBRL metadata.\"\"\"\n\n    rename_columns: dict[str, str] = {}\n    \"\"\"Dictionary to rename columns in the normalized metadata before merging.\n\n    This dictionary will be passed as :func:`pd.DataFrame.rename` ``columns`` parameter.\n    \"\"\"\n\n    on: str | None = None\n    \"\"\"Column name to merge on in :func:`merge_xbrl_metadata`.\"\"\"\n\n\ndef merge_xbrl_metadata(\n    df: pd.DataFrame, xbrl_metadata: pd.DataFrame, params: MergeXbrlMetadata\n) -> pd.DataFrame:\n    \"\"\"Merge metadata based on params.\"\"\"\n    return pd.merge(\n        df,\n        xbrl_metadata.rename(columns=params.rename_columns),\n        on=params.on,\n        how=\"left\",\n        validate=\"many_to_one\",\n    )\n\n\nclass DropDuplicateRowsDbf(TransformParams):\n    \"\"\"Parameter for dropping duplicate DBF rows.\"\"\"\n\n    table_name: TableIdFerc1 | None = None\n    \"\"\"Name of table used to grab primary keys of PUDL table to check for duplicates.\"\"\"\n\n    data_columns: list = []\n    \"\"\"List of data column names to ensure primary key duplicates have the same data.\"\"\"\n\n\ndef drop_duplicate_rows_dbf(\n    df: pd.DataFrame,\n    params: DropDuplicateRowsDbf,\n    return_dupes_w_unique_data: bool = False,\n) -> pd.DataFrame:\n    \"\"\"Drop duplicate DBF rows if duplicates have indentical data or one row has nulls.\n\n    There are several instances of the DBF data reporting the same value on multiple\n    rows. This function checks to see if all of the duplicate values that have the same\n    primary keys have reported the same data or have records with null data in any of\n    the data columns while the other record has complete data. If the duplicates have no\n    unique data, the duplicates are dropped with ``keep=\"first\"``. If any duplicates do\n    not contain the same data or half null data, an assertion will be raised.\n\n    Args:\n        df: DBF table containing PUDL primary key columns\n        params: an instance of :class:`DropDuplicateRowsDbf`\n        return_dupes_w_unique_data: Boolean flag used for debuging only which returns\n            the duplicates which contain actually unique data instead of raising\n            assertion. Default is False.\n    \"\"\"\n    pks = pudl.metadata.classes.Resource.from_id(\n        params.table_name.value\n    ).schema.primary_key\n    # add a column that indicates whether or not any of the data columns contain null data\n    df.loc[:, \"null_data\"] = df[params.data_columns].isnull().any(axis=\"columns\")\n\n    # checks to make sure the drop is targeted as expected\n    # of the PK dupes, drop all instances when the data *is also the same*\n    dupes_w_possible_unique_data = df.drop_duplicates(\n        pks + params.data_columns, keep=False\n    )\n    dupes_w_possible_unique_data = dupes_w_possible_unique_data[\n        dupes_w_possible_unique_data.duplicated(pks, keep=False)\n    ]\n    # if there are pk+data dupes, is there one record with some null data\n    # an other with completely non-null data??\n    # OR are there any records that have some null data and some actually unique\n    # data\n    nunique_data_columns = [f\"{col}_nunique\" for col in params.data_columns]\n    dupes_w_possible_unique_data.loc[\n        :, nunique_data_columns + [\"null_data_nunique\"]\n    ] = (\n        dupes_w_possible_unique_data.groupby(pks)[params.data_columns + [\"null_data\"]]\n        .transform(\"nunique\")\n        .add_suffix(\"_nunique\")\n    )\n    dupes_w_unique_data = dupes_w_possible_unique_data[\n        (dupes_w_possible_unique_data.null_data_nunique != 2)\n        | (\n            dupes_w_possible_unique_data[\n                dupes_w_possible_unique_data[nunique_data_columns] != 1\n            ].any(axis=\"columns\")\n        )\n    ].sort_values(by=pks)\n    if not dupes_w_unique_data.empty:\n        if return_dupes_w_unique_data:\n            logger.warning(\"Returning duplicate records for debugging.\")\n            return dupes_w_unique_data\n        raise AssertionError(\n            \"Duplicates have unique data and should not be dropped. Unique data: \"\n            f\"{len(dupes_w_unique_data)}: \\n{dupes_w_unique_data.sort_values(by=pks)}\"\n        )\n    len_og = len(df)\n    df = (\n        df.sort_values(by=[\"null_data\"], ascending=True)\n        .drop_duplicates(pks, keep=\"first\")\n        .drop(columns=[\"null_data\"])\n    )\n    logger.info(\n        f\"Dropped {len_og - len(df)} duplicate records: {(len_og - len(df))/len_og:.1%}\"\n        \" of total rows.\"\n    )\n    return df\n\n\nclass AlignRowNumbersDbf(TransformParams):\n    \"\"\"Parameters for aligning DBF row numbers with metadata from mannual maps.\"\"\"\n\n    dbf_table_names: list[str] | None = None\n    \"\"\"DBF table to use to grab the row map in :func:`align_row_numbers_dbf`.\n\n    Default is ``None``.\n    \"\"\"\n\n\ndef align_row_numbers_dbf(df: pd.DataFrame, params: AlignRowNumbersDbf) -> pd.DataFrame:\n    \"\"\"Rename the xbrl_factoid column after :meth:`align_row_numbers_dbf`.\"\"\"\n    if params.dbf_table_names:\n        logger.info(\n            f\"Aligning row numbers from DBF row to XBRL map for {params.dbf_table_names}\"\n        )\n        row_map = (\n            read_dbf_to_xbrl_map(dbf_table_names=params.dbf_table_names)\n            .pipe(fill_dbf_to_xbrl_map)\n            .drop(columns=[\"sched_table_name\", \"row_literal\"])\n        )\n        if row_map.isnull().any(axis=None):\n            raise ValueError(\n                \"Filled DBF-XBRL map contains NA values, which should never happen:\"\n                f\"{row_map}\"\n            )\n\n        df = pd.merge(df, row_map, on=[\"report_year\", \"row_number\"], how=\"left\")\n        if df.xbrl_factoid.isna().any():\n            raise ValueError(\n                \"Found null row labels after aligning DBF/XBRL rows.\\n\"\n                f\"{df[df.xbrl_factoid.isna()]}\"\n            )\n        # eliminate the header rows since they (should!) contain no data in either the\n        # DBF or XBRL records:\n        df = df[df.xbrl_factoid != \"HEADER_ROW\"]\n    return df\n\n\nclass SelectDbfRowsByCategory(TransformParams):\n    \"\"\"Parameters for :func:`select_dbf_rows_by_category`.\"\"\"\n\n    column_name: str | None = None\n    \"\"\"The column name containing categories to select by.\"\"\"\n    select_by_xbrl_categories: bool = False\n    \"\"\"Boolean flag to indicate whether or not to use the categories in the XBRL table.\n\n    If True, :func:`select_dbf_rows_by_category` will find the list of categories that\n    exist in the passed in ``processed_xbrl`` to select by.\n    \"\"\"\n    additional_categories: list[str] = []\n    \"\"\"List of additional categories to select by.\n\n    If ``select_by_xbrl_categories`` is ``True``, these categories will be added to the\n    XBRL categories and both will be used to select rows from the DBF data. If\n    ``select_by_xbrl_categories`` is ``False``, only the \"additional\" categories will be\n    the used to select rows from the DBF data.\n    \"\"\"\n    len_expected_categories_to_drop: int = 0\n    \"\"\"Number of categories that are expected to be dropped from the DBF data.\n\n    This is here to ensure no unexpected manipulations to the categories have occured. A\n    warning will be flagged if this number is different than the number of categories\n    that are being dropped.\n    \"\"\"\n\n\ndef select_dbf_rows_by_category(\n    processed_dbf: pd.DataFrame,\n    processed_xbrl: pd.DataFrame,\n    params: SelectDbfRowsByCategory,\n) -> pd.DataFrame:\n    \"\"\"Select DBF rows with values listed or found in XBRL in a categorical-like column.\n\n    The XBRL data often breaks out sub-sections of DBF tables into their own table.\n    These breakout tables are often messy, unstructured portions of a particular\n    schedule or page on the FERC1 PDF. We often want to preserve some of the ways the\n    XBRL data is segmented so we need to be able to select only portions of the DBF\n    table to be concatenated with the XBRL data.\n\n    In mapping DBF data to XBRL data for the tables that rely on their ``row_number``\n    we map each row to its corresponding ``xbrl_factoid``. The standard use of this\n    transformer is to use the ``column_name`` that corresponds to the ``xbrl_factoid``\n    that was merged into the DBF data via :func:`align_row_numbers_dbf` and was\n    converted into a column in the XBRL data via :func:`wide_to_tidy`.\n\n    Note: Often, the unstructured portion of the DBF table that (possibly) sums up into\n    a single value in structured data has the same ``xbrl_factoid`` name in the XBRL\n    tables. By convention, we are employing a pattern in the ``dbf_to_xbrl.csv`` map\n    that involves adding an ``_unstructed`` suffix to the rows that correspond to the\n    unstructured portion of the table. This enables a simple selection of the structured\n    part of the table. When processing the unstructured table, you can either rename the\n    XBRL data's factoid name to include an ``_unstructed`` suffix or you can specify\n    the categories with ``_unstructed`` suffixes using the ``additional_categories``\n    parameter.\n    \"\"\"\n    # compile the list of categories from the possible options.\n    categories_to_select = []\n    if params.select_by_xbrl_categories:\n        categories_to_select = categories_to_select + list(\n            processed_xbrl[params.column_name].unique()\n        )\n    if params.additional_categories:\n        categories_to_select = categories_to_select + params.additional_categories\n\n    # check if we are getting the same number of expected categories to drop\n    categories_to_drop = [\n        cat\n        for cat in processed_dbf[params.column_name].unique()\n        if cat not in categories_to_select\n    ]\n    if len(categories_to_drop) != params.len_expected_categories_to_drop:\n        logger.warning(\n            f\"Dropping {len(categories_to_drop)} DBF categories that contain the \"\n            f\"following values in {params.column_name} but expected \"\n            f\"{params.len_expected_categories_to_drop}:\"\n            f\"{categories_to_drop}\"\n        )\n    # now select only the rows which contain the categories we want to include in the\n    # column that we care about. Copy bc this is a slice of the og dataframe.\n    category_mask = processed_dbf[params.column_name].isin(categories_to_select)\n    return processed_dbf.loc[category_mask].copy()\n\n\nclass UnstackBalancesToReportYearInstantXbrl(TransformParams):\n    \"\"\"Parameters for :func:`unstack_balances_to_report_year_instant_xbrl`.\"\"\"\n\n    unstack_balances_to_report_year: bool = False\n    \"\"\"If True unstack balances to a single year (the report year).\"\"\"\n\n\ndef unstack_balances_to_report_year_instant_xbrl(\n    df: pd.DataFrame,\n    params: UnstackBalancesToReportYearInstantXbrl,\n    primary_key_cols: list[str],\n) -> pd.DataFrame:\n    \"\"\"Turn start year end year rows into columns for each value type.\n\n    Called in :meth:`Ferc1AbstractTableTransformer.process_instant_xbrl`.\n\n    Some instant tables report year-end data, with their datestamps in different years,\n    but we want year-start and year-end data within a single report_year (which is\n    equivalent) stored in two separate columns for compatibility with the DBF data.\n\n    This function unstacks that table and adds the suffixes ``_starting_balance`` and\n    ``_ending_balance`` to each of the columns. These can then be used as\n    ``value_types`` in :func:`wide_to_tidy` to normalize the table.\n\n    There are two checks in place:\n\n    First, it will make sure that there are not duplicate entries for a single year +\n    other primary key fields. Ex: a row for 2020-12-31 and 2020-06-30 for entitiy_id X\n    means that the data isn't annually unique. We could just drop these mid-year\n    values, but we might want to keep them or at least check that there is no funny\n    business with the data.\n\n    We also check that there are no mid-year dates at all. If an entity reports a value\n    from the middle of the year, we can't identify it as a start/end of year value.\n\n    Params:\n        primary_key_cols: The columns that should be used to check for duplicated data,\n            and also for unstacking the balance -- these are set to be the index before\n            unstack is called. These are typically set by the wrapping method and\n            generated automatically based on other class transformation parameters via\n            :meth:`Ferc1AbstractTableTransformer.source_table_primary_key`.\n    \"\"\"\n    if not params.unstack_balances_to_report_year:\n        return df\n\n    # report year always corresponds to the year of \"date\"\n    unique_cols = set(primary_key_cols).union({\"report_year\"})\n    if df.duplicated(unique_cols).any():\n        raise AssertionError(\n            \"Looks like there are multiple entries per year--not sure which to use \"\n            f\"for the start/end balance. {params=} {primary_key_cols=}\"\n        )\n    if not pd.to_datetime(df[\"date\"]).dt.is_year_end.all():\n        raise AssertionError(\n            \"Looks like there are some values in here that aren't from the end of \"\n            \"the year. We can't use those to calculate start and end balances.\"\n        )\n\n    ending_balances = df.assign(balance_type=\"ending_balance\")\n    starting_balances = df.assign(\n        report_year=df.report_year + 1, balance_type=\"starting_balance\"\n    )\n    all_balances = pd.concat([starting_balances, ending_balances])\n    # for the first year, we expect no starting balances; for the last year, we expect no ending balances.\n    first_last_year_stripped = all_balances.loc[\n        lambda df: ~df.report_year.isin({df.report_year.min(), df.report_year.max()})\n    ]\n    unstacked_by_year = (\n        first_last_year_stripped.drop(columns=[\"date\"])\n        .set_index(primary_key_cols + [\"balance_type\", \"sched_table_name\"])\n        .unstack(\"balance_type\")\n    )\n    # munge multi-index into flat index, separated by _\n    unstacked_by_year.columns = [\n        \"_\".join(items) for items in unstacked_by_year.columns.to_flat_index()\n    ]\n    return unstacked_by_year.reset_index()\n\n\nclass CombineAxisColumnsXbrl(TransformParams):\n    \"\"\"Parameters for :func:`combine_axis_columns_xbrl`.\"\"\"\n\n    axis_columns_to_combine: list | None = None\n    \"\"\"List of axis columns to combine.\"\"\"\n\n    new_axis_column_name: str | None = None\n    \"\"\"The name of the combined axis column -- must end with the suffix ``_axis``!.\"\"\"\n\n    @field_validator(\"new_axis_column_name\")\n    @classmethod\n    def doesnt_end_with_axis(cls, v):\n        \"\"\"Ensure that new axis column ends in _axis.\"\"\"\n        if v is not None and not v.endswith(\"_axis\"):\n            raise ValueError(\n                \"The new axis column name must end with the suffix '_axis'!\"\n            )\n        return v\n\n\ndef combine_axis_columns_xbrl(\n    df: pd.DataFrame, params: CombineAxisColumnsXbrl\n) -> pd.DataFrame:\n    \"\"\"Combine axis columns from squished XBRL tables into one column with no NAs.\n\n    Called in :meth:`Ferc1AbstractTableTransformer.process_xbrl`.\n\n    There are instances (ex: sales_by_rate_schedule_ferc1) where the DBF table is equal\n    to several concatenated XBRL tables. These XBRL tables are extracted together with\n    the function :func:`extract_xbrl_concat`. Once combined, we need to deal with their\n    axis columns.\n\n    We use the axis columns (the primary key for the raw XBRL tables) in the creation\n    of ``record_id``s for each of the rows. If each of the concatinated XBRL tables has\n    the same axis column name then there's no need to fret. However, if the columns have\n    slightly different names (ex: ``residential_sales_axis`` vs.\n    ``industrial_sales_axis``), we'll need to combine them. We combine them to get rid\n    of NA values which aren't allowed in primary keys. Otherwise it would look like\n    this:\n\n        +-------------------------+-------------------------+\n        | residential_sales_axis  | industrial_sales_axis   |\n        +=========================+=========================+\n        | value1                  | NA                      |\n        +-------------------------+-------------------------+\n        | value2                  | NA                      |\n        +-------------------------+-------------------------+\n        | NA                      | valueA                  |\n        +-------------------------+-------------------------+\n        | NA                      | valueB                  |\n        +-------------------------+-------------------------+\n\n    vs. this:\n\n        +-------------------------+\n        | sales_axis              |\n        +=========================+\n        | value1                  |\n        +-------------------------+\n        | value2                  |\n        +-------------------------+\n        | valueA                  |\n        +-------------------------+\n        | valueB                  |\n        +-------------------------+\n    \"\"\"\n    # First, make sure that the new_axis_column_name param as the word axis in it\n    if not params.new_axis_column_name.endswith(\"_axis\"):\n        raise ValueError(\n            \"Your new_axis_column_name must end with the suffix _axis so that it gets \"\n            \"included in the record_id.\"\n        )\n    # Now, make sure there are no overlapping axis columns\n    if (df[params.axis_columns_to_combine].count(axis=1) > 1).any():\n        raise AssertionError(\n            \"You're trying to combine axis columns, but there's more than one axis \"\n            \"column value per row.\"\n        )\n    # Now combine all of the columns into one and remove the old axis columns\n    df[params.new_axis_column_name] = pd.NA\n    for col in params.axis_columns_to_combine:\n        df[params.new_axis_column_name] = df[params.new_axis_column_name].fillna(\n            df[col]\n        )\n    df = df.drop(columns=params.axis_columns_to_combine)\n    return df\n\n\nclass AssignQuarterlyDataToYearlyDbf(TransformParams):\n    \"\"\"Parameters for transfering quarterly reported data to annual columns.\"\"\"\n\n    quarterly_to_yearly_column_map: dict[str, str] = {}\n    quarterly_filed_years: list[int] = []\n\n\ndef assign_quarterly_data_to_yearly_dbf(\n    df: pd.DataFrame, params: AssignQuarterlyDataToYearlyDbf\n) -> pd.DataFrame:\n    \"\"\"Transfer 4th quarter reported data to the annual columns.\n\n    For some reason in the dbf data for this table reported all of the\n    balance data as quarterly data between specific years. We already choose\n    the end of the year in :meth:`select_annual_rows_dbf`. This ensures that\n    by this point, any quarterly data remaining in the input dataframe pertains\n    to the 4th quarter.\n    \"\"\"\n    bad_years_mask = df.report_year.isin(params.quarterly_filed_years)\n    # ensure this filling in treatment is necessary!\n    if (\n        not df.loc[bad_years_mask, list(params.quarterly_to_yearly_column_map.values())]\n        .isnull()\n        .all(axis=None)\n    ):\n        raise AssertionError(\n            f\"We expected that all balance data in years {params.quarterly_filed_years} \"\n            \"to be all null. Found non-null records, so the annual columns may no \"\n            \"longer need to be filled in with quarterly data.\"\n        )\n    for quarterly_col, yearly_col in params.quarterly_to_yearly_column_map.items():\n        df.loc[bad_years_mask, yearly_col] = df.loc[bad_years_mask, quarterly_col]\n    return df\n\n\nclass AddColumnWithUniformValue(TransformParams):\n    \"\"\"Parameters for adding a column to a table with a single value.\"\"\"\n\n    column_value: Any = None\n    is_dimension: bool = False\n\n\nclass AddColumnsWithUniformValues(TransformParams):\n    \"\"\"Parameters for adding columns to a table with a single value.\"\"\"\n\n    columns_to_add: dict[str, AddColumnWithUniformValue] = {}\n    \"Dictionary of column names (keys) with :class:`AddColumnWithUniformValue` (values)\"\n\n    @property\n    def assign_cols(self) -> dict[str, str]:\n        \"\"\"Dictionary of column_name (key) to uniform value (value) to use with pd.assign.\"\"\"\n        return {\n            col: col_info.column_value\n            for (col, col_info) in self.columns_to_add.items()\n        }\n\n\ndef add_columns_with_uniform_values(\n    df: pd.DataFrame, params: AddColumnsWithUniformValues\n) -> pd.DataFrame:\n    \"\"\"Add a column to a table with a single value.\"\"\"\n    return df.assign(**params.assign_cols)\n\n\nclass IsCloseTolerance(TransformParams):\n    \"\"\"Info for testing a particular check.\"\"\"\n\n    isclose_rtol: Annotated[float, Field(ge=0.0)] = 1e-5\n    \"\"\"Relative tolerance to use in :func:`np.isclose` for determining equality.\"\"\"\n\n    isclose_atol: Annotated[float, Field(ge=0.0, le=0.01)] = 1e-8\n    \"\"\"Absolute tolerance to use in :func:`np.isclose` for determining equality.\"\"\"\n\n\nclass CalculationIsCloseTolerance(TransformParams):\n    \"\"\"Calc params organized by check type.\"\"\"\n\n    error_frequency: IsCloseTolerance = IsCloseTolerance()\n    relative_error_magnitude: IsCloseTolerance = IsCloseTolerance(isclose_atol=1e-3)\n    null_calculated_value_frequency: IsCloseTolerance = IsCloseTolerance()\n    absolute_error_magnitude: IsCloseTolerance = IsCloseTolerance()\n    null_reported_value_frequency: IsCloseTolerance = IsCloseTolerance()\n\n\nclass MetricTolerances(TransformParams):\n    \"\"\"Tolerances for all data checks to be preformed within a grouped df.\"\"\"\n\n    error_frequency: Annotated[float, Field(ge=0.0, le=1.0)] = 0.01\n    relative_error_magnitude: Annotated[float, Field(ge=0.0)] = 0.02\n    null_calculated_value_frequency: Annotated[float, Field(ge=0.0, le=1.0)] = 0.7\n    \"\"\"Fraction of records with non-null reported values and null calculated values.\"\"\"\n    absolute_error_magnitude: Annotated[float, Field(ge=0.0)] = np.inf\n    null_reported_value_frequency: Annotated[float, Field(ge=0.0, le=1.0)] = 1.0\n    # ooof this one is just bad\n\n\nclass GroupMetricTolerances(TransformParams):\n    \"\"\"Data quality expectations related to FERC 1 calculations.\n\n    We are doing a lot of comparisons between calculated and reported values to identify\n    reporting errors in the data, errors in FERC's metadata, and bugs in our own code.\n    This class provides a structure for encoding our expectations about the level of\n    acceptable (or at least expected) errors, and allows us to pass them around.\n\n    In the future we might also want to specify much more granular expectations,\n    pertaining to individual tables, years, utilities, or facts to ensure that we don't\n    have low overall error rates, but a problem with the way the data or metadata is\n    reported in a particular year.  We could also define per-filing and per-table error\n    tolerances to help us identify individual utilities that have e.g. used an outdated\n    version of Form 1 when filing.\n    \"\"\"\n\n    ungrouped: MetricTolerances = MetricTolerances(\n        error_frequency=0.0005,\n        relative_error_magnitude=0.0086,\n        null_calculated_value_frequency=0.50,\n        null_reported_value_frequency=0.68,\n    )\n    xbrl_factoid: MetricTolerances = MetricTolerances(\n        error_frequency=0.018,\n        relative_error_magnitude=0.0086,\n        null_calculated_value_frequency=1.0,\n    )\n    utility_id_ferc1: MetricTolerances = MetricTolerances(\n        error_frequency=0.038,\n        relative_error_magnitude=0.04,\n        null_calculated_value_frequency=1.0,\n    )\n    report_year: MetricTolerances = MetricTolerances(\n        error_frequency=0.006,\n        relative_error_magnitude=0.04,\n        null_calculated_value_frequency=0.7,\n    )\n    table_name: MetricTolerances = MetricTolerances(\n        error_frequency=0.0005,\n        relative_error_magnitude=0.001,\n        null_calculated_value_frequency=0.50,\n        null_reported_value_frequency=0.68,\n    )\n\n\nclass GroupMetricChecks(TransformParams):\n    \"\"\"Input for checking calculations organized by group and test.\"\"\"\n\n    groups_to_check: list[\n        Literal[\n            \"ungrouped\", \"table_name\", \"xbrl_factoid\", \"utility_id_ferc1\", \"report_year\"\n        ]\n    ] = [\n        \"ungrouped\",\n        \"report_year\",\n        \"xbrl_factoid\",\n        \"utility_id_ferc1\",\n    ]\n    metrics_to_check: list[str] = [\n        \"error_frequency\",\n        \"relative_error_magnitude\",\n        \"null_calculated_value_frequency\",\n        \"null_reported_value_frequency\",\n    ]\n    group_metric_tolerances: GroupMetricTolerances = GroupMetricTolerances()\n    is_close_tolerance: CalculationIsCloseTolerance = CalculationIsCloseTolerance()\n\n    # TODO: The mechanics of this validation are a pain, given the bajillion combos\n    # of tolerances we have in the matrix of checks. It works, but actually specifying\n    # all of the relative values is not currently ergonomic, so it is disabled for the\n    # moment.\n    # @model_validator(mode=\"after\")\n    def grouped_tol_ge_ungrouped_tol(self: Self):\n        \"\"\"Grouped tolerance should always be greater than or equal to ungrouped.\"\"\"\n        for group in self.groups_to_check:\n            metric_tolerances = self.group_metric_tolerances.model_dump().get(group)\n            for metric_name, tolerance in metric_tolerances.items():\n                ungrouped_tolerance = self.group_metric_tolerances.model_dump()[\n                    \"ungrouped\"\n                ].get(metric_name)\n                if tolerance < ungrouped_tolerance:\n                    raise AssertionError(\n                        f\"In {group=}, {tolerance=} for {metric_name} should be \"\n                        f\"greater than {ungrouped_tolerance=}.\"\n                    )\n        return self\n\n\nclass ReconcileTableCalculations(TransformParams):\n    \"\"\"Parameters for reconciling xbrl-metadata based calculations within a table.\"\"\"\n\n    column_to_check: str | None = None\n    \"\"\"Name of data column to check.\n\n    This will typically be ``dollar_value`` or ``ending_balance`` column for the income\n    statement and the balance sheet tables.\n    \"\"\"\n    group_metric_checks: GroupMetricChecks = GroupMetricChecks()\n    \"\"\"Fraction of calculated values which we allow not to match reported values.\"\"\"\n\n    subtotal_column: str | None = None\n    \"\"\"Sub-total column name (e.g. utility type) to compare calculations against in\n    :func:`reconcile_table_calculations`.\"\"\"\n\n    subtotal_calculation_tolerance: float = 0.05\n    \"\"\"Fraction of calculated sub-totals allowed not to match reported values.\"\"\"\n\n\ndef reconcile_table_calculations(\n    df: pd.DataFrame,\n    calculation_components: pd.DataFrame,\n    xbrl_metadata: pd.DataFrame,\n    xbrl_factoid_name: str,\n    table_name: str,\n    params: ReconcileTableCalculations,\n) -> pd.DataFrame:\n    \"\"\"Ensure intra-table calculated values match reported values within a tolerance.\n\n    In addition to checking whether all reported \"calculated\" values match the output\n    of our repaired calculations, this function adds a correction record to the\n    dataframe that is included in the calculations so that after the fact the\n    calculations match exactly. This is only done when the fraction of records that\n    don't match within the tolerances of :func:`numpy.isclose` is below a set\n    threshold.\n\n    Note that only calculations which are off by a significant amount result in the\n    creation of a correction record. Many calculations are off from the reported values\n    by exaclty one dollar, presumably due to rounding errrors. These records typically\n    do not fail the :func:`numpy.isclose()` test and so are not corrected.\n\n    Args:\n        df: processed table containing data values to check.\n        calculation_components: processed calculation component metadata.\n        xbrl_metadata: A dataframe of fact-level metadata, required for inferring the\n            sub-dimension total calculations.\n        xbrl_factoid_name: The name of the column which contains XBRL factoid values in\n            the processed table.\n        table_name: name of the PUDL table whose data and metadata is being processed.\n            This is necessary so we can ensure the metadata has the same structure as\n            the calculation components, which at a minimum need both ``table_name`` and\n            ``xbrl_factoid`` to identify them.\n        params: :class:`ReconcileTableCalculations` parameters.\n\n    Returns:\n        A dataframe that includes new ``*_correction`` records with values that ensure\n        the calculations all match to within the required tolerance. It will also\n        contain columns created by the calculation checking process like ``abs_diff``\n        and ``rel_diff``.\n    \"\"\"\n    # If we don't have this value, we aren't doing any calculation checking:\n    if params.column_to_check is None or calculation_components.empty:\n        return df\n\n    # Use the calculation components which reference ONLY values within the table\n    intra_table_calcs = calculation_components[\n        calculation_components.is_within_table_calc\n    ]\n    # To interact with the calculation components, we need uniformly named columns\n    # for xbrl_factoid, and table_name\n    df = df.rename(columns={xbrl_factoid_name: \"xbrl_factoid\"}).assign(\n        table_name=table_name\n    )\n    dim_cols = [\n        dim\n        for dim in other_dimensions(table_names=list(FERC1_TFR_CLASSES))\n        if dim in df.columns\n    ]\n    calc_idx = [\"xbrl_factoid\", \"table_name\"] + dim_cols\n\n    if dim_cols:\n        table_dims = df[calc_idx].drop_duplicates(keep=\"first\")\n        intra_table_calcs = _add_intra_table_calculation_dimensions(\n            intra_table_calcs=intra_table_calcs,\n            table_dims=table_dims,\n            dim_cols=dim_cols,\n        )\n        # Check the subdimension totals, but don't add correction records for these\n        # intra-fact calculations:\n        if params.subtotal_column:\n            calc_comps_w_totals = _calculation_components_subtotal_calculations(\n                intra_table_calcs=intra_table_calcs,\n                table_dims=table_dims,\n                xbrl_metadata=xbrl_metadata,\n                dim_cols=dim_cols,\n                table_name=table_name,\n            )\n            _check_subtotal_calculations(\n                df=df,\n                params=params,\n                calc_comps_w_totals=calc_comps_w_totals,\n                calc_idx=calc_idx,\n            )\n\n    calculated_df = (\n        calculate_values_from_components(\n            data=df,\n            calculation_components=intra_table_calcs,\n            calc_idx=calc_idx,\n            value_col=params.column_to_check,\n        )\n        .pipe(\n            check_calculation_metrics,\n            group_metric_checks=params.group_metric_checks,\n        )\n        .pipe(\n            add_corrections,\n            value_col=params.column_to_check,\n            is_close_tolerance=IsCloseTolerance(),\n            table_name=table_name,\n        )\n        # Rename back to the original xbrl_factoid column name before returning:\n        .rename(columns={\"xbrl_factoid\": xbrl_factoid_name})\n    )\n\n    return calculated_df\n\n\ndef _calculation_components_subtotal_calculations(\n    intra_table_calcs: pd.DataFrame,\n    table_dims: pd.DataFrame,\n    xbrl_metadata: pd.DataFrame,\n    dim_cols: list[str],\n    table_name: str,\n) -> pd.DataFrame:\n    \"\"\"Add total to subtotal calculations into calculation components.\"\"\"\n    meta_w_dims = xbrl_metadata.assign(\n        **{dim: pd.NA for dim in dim_cols} | {\"table_name\": table_name}\n    ).pipe(\n        make_xbrl_factoid_dimensions_explicit,\n        table_dimensions_ferc1=table_dims,\n        dimensions=dim_cols,\n    )\n    calc_comps_w_totals = infer_intra_factoid_totals(\n        intra_table_calcs,\n        meta_w_dims=meta_w_dims,\n        table_dimensions=table_dims,\n        dimensions=dim_cols,\n    )\n    return calc_comps_w_totals\n\n\ndef _check_subtotal_calculations(\n    df: pd.DataFrame,\n    params: \"Ferc1TableTransformParams\",\n    calc_comps_w_totals: pd.DataFrame,\n    calc_idx: list[str],\n) -> None:\n    \"\"\"Check that sub-dimension calculations sum to the reported totals.\n\n    No correction records are added to the sub-dimensions calculations. This is only an\n    error check, and returns nothing.\n    \"\"\"\n    logger.info(f\"Checking total-to-subtotal calculations in {params.subtotal_column}\")\n    subtotal_calcs = calculate_values_from_components(\n        data=df,\n        calculation_components=calc_comps_w_totals[\n            calc_comps_w_totals.is_total_to_subdimensions_calc\n        ],\n        calc_idx=calc_idx,\n        value_col=params.column_to_check,\n    )\n    subtotal_calcs = check_calculation_metrics(\n        calculated_df=subtotal_calcs,\n        group_metric_checks=params.group_metric_checks,\n    )\n\n\ndef _add_intra_table_calculation_dimensions(\n    intra_table_calcs: pd.DataFrame,\n    table_dims: pd.DataFrame,\n    dim_cols: list[str],\n) -> pd.DataFrame:\n    \"\"\"Add all observed subdimensions into the calculation components.\"\"\"\n    ######## Add all observed subdimensions into the calculation components!!!\n    # First determine what dimensions matter in this table:\n    # - usually params.subtotal_column has THE ONE dimension in the table...\n    # - BUT some tables have more than one dimension so we grab from all of the\n    #   the dims in the transformers.\n\n    # need to add in the correction dimensions. they don't show up in the data at\n    # this point so we don't have the dimensions yet. NOTE: this could have been\n    # done by adding the dims into table_dims..... maybe would have been more\n    # straightforward\n    correction_mask = intra_table_calcs.xbrl_factoid.str.endswith(\"_correction\")\n    intra_table_calcs = pd.concat(\n        [\n            intra_table_calcs[~correction_mask],\n            pd.merge(\n                intra_table_calcs[correction_mask].drop(columns=dim_cols),\n                table_dims[[\"table_name\"] + dim_cols].drop_duplicates(),\n                on=[\"table_name\"],\n            ),\n        ]\n    )\n    intra_table_calcs = make_xbrl_factoid_dimensions_explicit(\n        intra_table_calcs,\n        table_dimensions_ferc1=table_dims,\n        dimensions=dim_cols,\n    ).pipe(\n        assign_parent_dimensions,\n        table_dimensions=table_dims,\n        dimensions=dim_cols,\n    )\n    # this is for the income statement table specifically, but is general:\n    # remove all the bits where we have a child dim but not a parent dim\n    # sometimes there are child dimensions that have utility_type == \"other2\" etc\n    # where the parent dimension has nothing\n    for dim in dim_cols:\n        intra_table_calcs = intra_table_calcs[\n            ~(\n                intra_table_calcs[dim].notnull()\n                & intra_table_calcs[f\"{dim}_parent\"].isnull()\n            )\n        ]\n    return intra_table_calcs\n\n\ndef calculate_values_from_components(\n    calculation_components: pd.DataFrame,\n    data: pd.DataFrame,\n    calc_idx: list[str],\n    value_col: str,\n) -> pd.DataFrame:\n    \"\"\"Apply calculations derived from XBRL metadata to reported XBRL data.\n\n    Args:\n        calculation_components: Table defining the calculations, with each row defining\n            a single component, including its weight. Groups of rows identified by\n            ``table_name_parent`` and ``xbrl_factoid_parent`` indicate the values being\n            calculated.\n        data: exploded FERC data to apply the calculations to. Primary key should be\n            ``report_year``, ``utility_id_ferc1``, ``table_name``, ``xbrl_factoid``, and\n            whatever additional dimensions are relevant to the data.\n        calc_idx: primary key columns that uniquely identify a calculation component\n            (not including the ``_parent`` columns).\n        value_col: label of the column in ``data`` that contains the values to apply the\n            calculations to (typically ``dollar_value`` or ``ending_balance``).\n    \"\"\"\n    # Merge the reported data and the calculation component metadata to enable\n    # validation of calculated values. Here the data table exploded is supplying the\n    # values associated with individual calculation components, and the table_name\n    # and xbrl_factoid to which we aggregate are coming from the calculation\n    # components table. After merging we use the weights to adjust the reported\n    # values so they can be summed directly. This gives us aggregated calculated\n    # values that can later be compared to the higher level reported values.\n\n    # infer the pks of the data by adding in the util/year\n    data_idx = calc_idx + [\"utility_id_ferc1\", \"report_year\"]\n    # we are going to merge the data onto the calc components with the _parent\n    # column names, so the groupby after the merge needs a set of by cols with the\n    # _parent suffix\n    gby_parent = [f\"{col}_parent\" for col in calc_idx] + [\n        \"utility_id_ferc1\",\n        \"report_year\",\n    ]\n    try:\n        calc_df = (\n            pd.merge(\n                calculation_components,\n                data,\n                validate=\"one_to_many\",\n                on=calc_idx,\n            )\n            # apply the weight from the calc to convey the sign before summing.\n            .assign(calculated_value=lambda x: x[value_col] * x.weight)\n            .groupby(gby_parent, as_index=False, dropna=False)[[\"calculated_value\"]]\n            .sum(min_count=1)\n        )\n    except pd.errors.MergeError as err:  # Make debugging easier.\n        raise pd.errors.MergeError(\n            \"Merge failed, duplicated merge keys in left dataset:\\n\"\n            f\"{calculation_components[calculation_components.duplicated(calc_idx)]}\"\n        ) from err\n    # remove the _parent suffix so we can merge these calculated values back onto\n    # the data using the original pks\n    calc_df.columns = calc_df.columns.str.removesuffix(\"_parent\")\n    calculated_df = pd.merge(\n        data,\n        calc_df,\n        on=data_idx,\n        how=\"outer\",\n        validate=\"1:1\",\n        indicator=True,\n    )\n\n    assert calculated_df[\n        (calculated_df._merge == \"right_only\") & (calculated_df[value_col].notnull())\n    ].empty\n\n    calculated_df = calculated_df.drop(columns=[\"_merge\"])\n    # Force value_col to be a float to prevent any hijinks with calculating differences.\n    # Data types were very messy here, including pandas Float64 for the\n    # calculated_value columns which did not work with the np.isclose(). Not sure\n    # why these are cropping up.\n    calculated_df = calculated_df.convert_dtypes(convert_floating=False).astype(\n        {value_col: \"float64\", \"calculated_value\": \"float64\"}\n    )\n    calculated_df = calculated_df.assign(\n        diff=lambda x: x[value_col] - x.calculated_value,\n        abs_diff=lambda x: abs(x[\"diff\"]),\n        rel_diff=lambda x: np.where(\n            (x[value_col] != 0.0),\n            abs(x.abs_diff / x[value_col]),\n            np.nan,\n        ),\n    )\n    # Uniformity here helps keep the error checking functions simpler:\n    calculated_df[\"reported_value\"] = calculated_df[value_col]\n    return calculated_df\n\n\ndef check_calculation_metrics_by_group(\n    calculated_df: pd.DataFrame,\n    group_metric_checks: GroupMetricChecks,\n) -> pd.DataFrame:\n    \"\"\"Tabulate the results of the calculation checks by group.\n\n    Convert all of the groups' checks into a big df. This will have two indexes: first\n    for the group name (group) and one for the groups values. the columns will include\n    three for each test: the test mertic that is the same name as the test (ex:\n    error_frequency), the tolerance for that group/test and a boolean indicating\n    whether or not that metric failed to meet the tolerance.\n    \"\"\"\n    results_dfs = {}\n    # for each groupby grouping: calculate metrics for each test\n    # then check if each test is within acceptable tolerance levels\n    for group_name in group_metric_checks.groups_to_check:\n        group_metrics = {}\n        for (\n            metric_name,\n            metric_tolerance,\n        ) in group_metric_checks.group_metric_tolerances.model_dump()[\n            group_name\n        ].items():\n            if metric_name in group_metric_checks.metrics_to_check:\n                # this feels icky. the param name for the metrics are all snake_case while\n                # the metric classes are all TitleCase. So we convert to TitleCase\n                title_case_test = metric_name.title().replace(\"_\", \"\")\n                group_metric_checker = globals()[title_case_test](\n                    by=group_name,\n                    is_close_tolerance=group_metric_checks.is_close_tolerance.model_dump()[\n                        metric_name\n                    ],\n                    metric_tolerance=metric_tolerance,\n                )\n                group_metric = group_metric_checker.check(\n                    calculated_df=calculated_df\n                ).rename(columns={group_name: \"group_value\"})\n                # we want to set the index values as the same for all groups, but both the\n                # ungrouped and table_name group require a special exception because we need\n                # to add table_name into the columns\n                if group_name == \"ungrouped\":\n                    group_metric = group_metric.assign(table_name=\"ungrouped\")\n                if group_name == \"table_name\":\n                    # we end up having two columns w/ table_name values in order to keep all the\n                    # outputs having the same indexes.\n                    group_metric = group_metric.assign(\n                        table_name=lambda x: x[\"group_value\"]\n                    )\n                # make a uniform multi-index w/ group name and group values\n                group_metrics[metric_name] = group_metric.set_index(\n                    [\"group\", \"table_name\", \"group_value\"]\n                )\n        results_dfs[group_name] = pd.concat(group_metrics.values(), axis=\"columns\")\n    results = pd.concat(results_dfs.values(), axis=\"index\")\n    return results\n\n\ndef check_calculation_metrics(\n    calculated_df: pd.DataFrame,\n    group_metric_checks: GroupMetricChecks,\n) -> pd.DataFrame:\n    \"\"\"Run the calculation metrics and determine if calculations are within tolerance.\"\"\"\n    # DO ERROR CHECKS\n    results = check_calculation_metrics_by_group(calculated_df, group_metric_checks)\n    # get the records w/ errors in any! of their checks\n    errors = results[results.filter(like=\"is_error\").any(axis=1)]\n    if not errors.empty:\n        # it miiight be good to isolate just the error columns..\n        raise AssertionError(\n            f\"Found errors while running tests on the calculations:\\n{errors}\"\n        )\n    return calculated_df\n\n\n########################################################################################\n# Calculation Error Checking Functions\n# - These functions all take a dataframe and return a float.\n# - They are intended to be used in GroupBy.apply() (or on a whole dataframe).\n# - They require a uniform `reported_value` column so that they can all have the same\n#   call signature, which allows us to iterate over all of them in a matrix.\n########################################################################################\n\n\nclass ErrorMetric(BaseModel):\n    \"\"\"Base class for checking a particular metric within a group.\"\"\"\n\n    by: Literal[\n        \"ungrouped\", \"table_name\", \"xbrl_factoid\", \"utility_id_ferc1\", \"report_year\"\n    ]\n    \"\"\"Name of group to check the metric based on.\n\n    With the exception of the ungrouped case, all groups depend on table_name as well as\n    the other column specified via by.\n\n    If by==\"table_name\" then that is the only column used in the groupby().\n\n    If by==\"ungrouped\" then all records are included in the \"group\" (via a dummy column\n    named ungrouped that contains only the value ungrouped). This allows us to use the\n    same infrastructure for applying the metrics to grouped and ungrouped data.\n    \"\"\"\n\n    is_close_tolerance: IsCloseTolerance\n    \"\"\"Inputs for the metric to determine :meth:`is_not_close`. Instance of :class:`IsCloseTolerance`.\"\"\"\n\n    metric_tolerance: float\n    \"\"\"Tolerance for checking the metric within the ``by`` group.\"\"\"\n\n    required_cols: list[str] = [\n        \"table_name\",\n        \"xbrl_factoid\",\n        \"report_year\",\n        \"utility_id_ferc1\",\n        \"reported_value\",\n        \"calculated_value\",\n        \"abs_diff\",\n        \"rel_diff\",\n    ]\n\n    def has_required_cols(self: Self, df: pd.DataFrame):\n        \"\"\"Check that the input dataframe has all required columns.\"\"\"\n        missing_required_cols = [\n            col for col in self.required_cols if col not in df.columns\n        ]\n        if missing_required_cols:\n            raise AssertionError(\n                f\"The table is missing the following required columns: {missing_required_cols}\"\n            )\n        return True\n\n    @abstractmethod\n    def metric(self: Self, gb: DataFrameGroupBy) -> pd.Series:\n        \"\"\"Metric function that will be applied to each group of values being checked.\"\"\"\n        ...\n\n    def is_not_close(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"Flag records where reported and calculated values differ significantly.\n\n        We only want to check this metric when there is a non-null ``abs_diff`` because\n        we want to avoid the instances in which there are either null reported or\n        calculated values.\n        \"\"\"\n        return pd.Series(\n            ~np.isclose(\n                df[\"calculated_value\"],\n                df[\"reported_value\"],\n                rtol=self.is_close_tolerance.isclose_rtol,\n                atol=self.is_close_tolerance.isclose_atol,\n            )\n            & df[\"abs_diff\"].notnull()\n        )\n\n    def groupby_cols(self: Self) -> list[str]:\n        \"\"\"The list of columns to group by.\n\n        We want to default to adding the table_name into all groupby's, but two of our\n        ``by`` options need special treatment.\n        \"\"\"\n        gb_by = [\"table_name\", self.by]\n        if self.by in [\"ungrouped\", \"table_name\"]:\n            gb_by = [self.by]\n        return gb_by\n\n    def apply_metric(self: Self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"Generate the metric values within each group through an apply method.\n\n        This method adds a column ``is_not_close`` into the df before the groupby\n        because that column is used in many of the :meth:`metric`.\n        \"\"\"\n        # return a df instead of a series\n        df[\"is_not_close\"] = self.is_not_close(df)\n        return df.groupby(by=self.groupby_cols()).apply(self.metric)\n\n    def _snake_case_metric_name(self: Self) -> str:\n        \"\"\"Convert the TitleCase class name to a snake_case string.\"\"\"\n        class_name = self.__class__.__name__\n        return re.sub(\"(?!^)([A-Z]+)\", r\"_\\1\", class_name).lower()\n\n    def check(self: Self, calculated_df) -> pd.DataFrame:\n        \"\"\"Make a df w/ the metric, tolerance and is_error columns.\"\"\"\n        self.has_required_cols(calculated_df)\n        # ungrouped is special because the rest of the stock group names are column\n        # names.\n        if self.by == \"ungrouped\":\n            calculated_df = calculated_df.assign(\n                ungrouped=\"ungrouped\",\n            )\n\n        metric_name = self._snake_case_metric_name()\n        df = (\n            pd.DataFrame(self.apply_metric(calculated_df), columns=[metric_name])\n            .assign(\n                **{  # totolerance_ is just for reporting so you can know of off you are\n                    f\"tolerance_{metric_name}\": self.metric_tolerance,\n                    f\"is_error_{metric_name}\": lambda x: x[metric_name]\n                    > self.metric_tolerance,\n                }\n            )\n            .assign(group=self.by)\n            .reset_index()\n        )\n        return df\n\n\nclass ErrorFrequency(ErrorMetric):\n    \"\"\"Check error frequency in XBRL calculations.\"\"\"\n\n    def metric(self: Self, gb: DataFrameGroupBy) -> pd.Series:\n        \"\"\"Calculate the frequency with which records are tagged as errors.\"\"\"\n        try:\n            out = gb[gb.is_not_close].shape[0] / gb.shape[0]\n        except ZeroDivisionError:\n            # Will only occur if all reported values are NaN when calculated values\n            # exist, or vice versa.\n            logger.warning(\n                \"Calculated values have no corresponding reported values in this table.\"\n            )\n            out = np.nan\n        return out\n\n\nclass RelativeErrorMagnitude(ErrorMetric):\n    \"\"\"Check relative magnitude of errors in XBRL calculations.\"\"\"\n\n    def metric(self: Self, gb: DataFrameGroupBy) -> pd.Series:\n        \"\"\"Calculate the mangnitude of the errors relative to total reported value.\"\"\"\n        try:\n            return gb.abs_diff.abs().sum() / gb[\"reported_value\"].abs().sum()\n        except ZeroDivisionError:\n            return np.nan\n\n\nclass AbsoluteErrorMagnitude(ErrorMetric):\n    \"\"\"Check absolute magnitude of errors in XBRL calculations.\n\n    These numbers may vary wildly from table to table so no default values for the\n    expected errors are provided here...\n    \"\"\"\n\n    def metric(self: Self, gb: DataFrameGroupBy) -> pd.Series:\n        \"\"\"Calculate the absolute mangnitude of XBRL calculation errors.\"\"\"\n        return gb.abs_diff.abs().sum()\n\n\nclass NullCalculatedValueFrequency(ErrorMetric):\n    \"\"\"Check the frequency of null calculated values.\"\"\"\n\n    def apply_metric(self: Self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"Only apply metric to rows that contain calculated values.\"\"\"\n        return (\n            df[df.row_type_xbrl == \"calculated_value\"]\n            .groupby(self.groupby_cols())\n            .apply(self.metric)\n        )\n\n    def metric(self: Self, gb: DataFrameGroupBy) -> pd.Series:\n        \"\"\"Fraction of non-null reported values that have null corresponding calculated values.\"\"\"\n        non_null_reported = gb[\"reported_value\"].notnull()\n        null_calculated = gb[\"calculated_value\"].isnull()\n        try:\n            return (non_null_reported & null_calculated).sum() / non_null_reported.sum()\n        except ZeroDivisionError:\n            return np.nan\n\n\nclass NullReportedValueFrequency(ErrorMetric):\n    \"\"\"Check the frequency of null reported values.\"\"\"\n\n    def metric(self: Self, gb: DataFrameGroupBy) -> pd.Series:\n        \"\"\"Frequency with which the reported values are Null.\"\"\"\n        return gb[\"reported_value\"].isnull().sum() / gb.shape[0]\n\n\ndef add_corrections(\n    calculated_df: pd.DataFrame,\n    value_col: str,\n    is_close_tolerance: IsCloseTolerance,\n    table_name: str,\n) -> pd.DataFrame:\n    \"\"\"Add corrections to discrepancies between reported & calculated values.\n\n    To isolate the sources of error, and ensure that all totals add up as expected in\n    later phases of the transformation, we add correction records to the dataframe\n    which compensate for any difference between the calculated and reported values. The\n    ``_correction`` factoids that are added here have already been added to the\n    calculation components during the metadata processing.\n\n    Args:\n        calculated_df: DataFrame containing the data to correct. Must already have\n            ``abs_diff`` column that was added by :func:`check_calculation_metrics`\n        value_col: Label of the column whose values are being calculated.\n        calculation_tolerance: Data structure containing various calculation tolerances.\n        table_name: Name of the table whose data we are working with. For logging.\n    \"\"\"\n    corrections = calculated_df[\n        ~np.isclose(\n            calculated_df[\"calculated_value\"],\n            calculated_df[value_col],\n            rtol=is_close_tolerance.isclose_rtol,\n            atol=is_close_tolerance.isclose_atol,\n        )\n        & (calculated_df[\"abs_diff\"].notnull())\n    ].copy()\n\n    corrections[value_col] = (\n        corrections[value_col].fillna(0.0) - corrections[\"calculated_value\"]\n    )\n    corrections = corrections.assign(\n        xbrl_factoid_corrected=lambda x: x[\"xbrl_factoid\"],\n        xbrl_factoid=lambda x: x[\"xbrl_factoid\"] + \"_correction\",\n        row_type_xbrl=\"correction\",\n        is_within_table_calc=False,\n        record_id=pd.NA,\n    )\n    num_notnull_calcs = sum(calculated_df[\"abs_diff\"].notnull())\n    num_corrections = corrections.shape[0]\n    num_records = calculated_df.shape[0]\n    try:\n        corrected_fraction = num_corrections / num_notnull_calcs\n    except ZeroDivisionError:\n        corrected_fraction = np.nan\n    logger.info(\n        f\"{table_name}: Correcting {corrected_fraction:.2%} of all non-null reported \"\n        f\"values ({num_corrections}/{num_notnull_calcs}) out of a total of \"\n        f\"{num_records} original records.\"\n    )\n\n    return pd.concat(\n        [calculated_df, corrections.astype(calculated_df.dtypes, errors=\"ignore\")],\n        axis=\"index\",\n    )\n\n\nclass Ferc1TableTransformParams(TableTransformParams):\n    \"\"\"A model defining what TransformParams are allowed for FERC Form 1.\n\n    This adds additional parameter models beyond the ones inherited from the\n    :class:`pudl.transform.classes.AbstractTableTransformer` class.\n    \"\"\"\n\n    rename_columns_ferc1: RenameColumnsFerc1 = RenameColumnsFerc1(\n        dbf=RenameColumns(),\n        xbrl=RenameColumns(),\n        instant_xbrl=RenameColumns(),\n        duration_xbrl=RenameColumns(),\n    )\n    wide_to_tidy: WideToTidySourceFerc1 = WideToTidySourceFerc1(\n        dbf=WideToTidy(), xbrl=WideToTidy()\n    )\n    merge_xbrl_metadata: MergeXbrlMetadata = MergeXbrlMetadata()\n    align_row_numbers_dbf: AlignRowNumbersDbf = AlignRowNumbersDbf()\n    drop_duplicate_rows_dbf: DropDuplicateRowsDbf = DropDuplicateRowsDbf()\n    assign_quarterly_data_to_yearly_dbf: AssignQuarterlyDataToYearlyDbf = (\n        AssignQuarterlyDataToYearlyDbf()\n    )\n    select_dbf_rows_by_category: SelectDbfRowsByCategory = SelectDbfRowsByCategory()\n    unstack_balances_to_report_year_instant_xbrl: UnstackBalancesToReportYearInstantXbrl = UnstackBalancesToReportYearInstantXbrl()\n    combine_axis_columns_xbrl: CombineAxisColumnsXbrl = CombineAxisColumnsXbrl()\n    reconcile_table_calculations: ReconcileTableCalculations = (\n        ReconcileTableCalculations()\n    )\n    add_columns_with_uniform_values: AddColumnsWithUniformValues = (\n        AddColumnsWithUniformValues()\n    )\n\n    @property\n    def xbrl_factoid_name(self) -> str:\n        \"\"\"Access the column name of the ``xbrl_factoid``.\"\"\"\n        return self.merge_xbrl_metadata.on\n\n    @property\n    def rename_dicts_xbrl(self):\n        \"\"\"Compile all of the XBRL rename dictionaries into an ordered list.\"\"\"\n        return self.rename_columns_ferc1.rename_dicts_xbrl\n\n    @property\n    def wide_to_tidy_value_types(self) -> list[str]:\n        \"\"\"Compile a list of all of the ``value_types`` from ``wide_to_tidy``.\"\"\"\n        return self.wide_to_tidy.value_types\n\n    @property\n    def aligned_dbf_table_names(self) -> list[str]:\n        \"\"\"The list of DBF tables aligned by row number in this transform.\"\"\"\n        return self.align_row_numbers_dbf.dbf_table_names\n\n    @property\n    def dimension_columns(self) -> list[str]:\n        \"\"\"List of column names of dimensions.\"\"\"\n        dims = {\n            dim\n            for (\n                dim,\n                col_info,\n            ) in self.add_columns_with_uniform_values.columns_to_add.items()\n            if col_info.is_dimension\n        }\n        if self.reconcile_table_calculations.subtotal_column:\n            dims.update({self.reconcile_table_calculations.subtotal_column})\n        return list(dims)\n\n\n################################################################################\n# FERC 1 transform helper functions. Probably to be integrated into a class\n# below as methods or moved to a different module once it's clear where they belong.\n################################################################################\ndef get_ferc1_dbf_rows_to_map(ferc1_engine: sa.Engine) -> pd.DataFrame:\n    \"\"\"Identify DBF rows that need to be mapped to XBRL columns.\n\n    Select all records in the ``f1_row_lit_tbl`` where the row literal associated with a\n    given combination of table and row number is different from the preceeding year.\n    This is the smallest set of records which we can use to reproduce the whole table by\n    expanding the time series to include all years, and forward filling the row\n    literals.\n    \"\"\"\n    idx_cols = [\"sched_table_name\", \"row_number\", \"report_year\"]\n    data_cols = [\"row_literal\"]\n    row_lit = pd.read_sql(\n        \"f1_row_lit_tbl\", con=ferc1_engine, columns=idx_cols + data_cols\n    ).sort_values(idx_cols)\n    row_lit[\"shifted\"] = row_lit.groupby(\n        [\"sched_table_name\", \"row_number\"]\n    ).row_literal.shift()\n    row_lit[\"changed\"] = row_lit.row_literal != row_lit.shifted\n    return row_lit.loc[row_lit.changed, idx_cols + data_cols]\n\n\ndef update_dbf_to_xbrl_map(ferc1_engine: sa.Engine) -> pd.DataFrame:\n    \"\"\"Regenerate the FERC 1 DBF+XBRL glue while retaining existing mappings.\n\n    Reads all rows that need to be mapped out of the ``f1_row_lit_tbl`` and appends\n    columns containing any previously mapped values, returning the resulting dataframe.\n    \"\"\"\n    idx_cols = [\"sched_table_name\", \"row_number\", \"report_year\"]\n    all_rows = get_ferc1_dbf_rows_to_map(ferc1_engine).set_index(idx_cols)\n    mapped_rows = (\n        pd.read_csv(\n            importlib.resources.files(\"pudl.package_data.ferc1\") / \"dbf_to_xbrl.csv\"\n        )\n        .set_index(idx_cols)\n        .drop([\"row_literal\"], axis=\"columns\")\n    )\n    return (\n        pd.concat([all_rows, mapped_rows], axis=\"columns\")\n        .reset_index()\n        .sort_values([\"sched_table_name\", \"report_year\", \"row_number\"])\n    )\n\n\ndef read_dbf_to_xbrl_map(dbf_table_names: list[str]) -> pd.DataFrame:\n    \"\"\"Read the manually compiled DBF row to XBRL column mapping for a given table.\n\n    Args:\n        dbf_table_name: The original name of the table in the FERC Form 1 DBF database\n            whose mapping to the XBRL data you want to extract. for example\n            ``f1_plant_in_srvce``.\n\n    Returns:\n        DataFrame with columns ``[sched_table_name, report_year, row_number, row_type, xbrl_factoid]``\n    \"\"\"\n    row_map = pd.read_csv(\n        importlib.resources.files(\"pudl.package_data.ferc1\") / \"dbf_to_xbrl.csv\",\n        usecols=[\n            \"sched_table_name\",\n            \"report_year\",\n            \"row_number\",\n            \"row_type\",\n            \"row_literal\",\n            \"xbrl_factoid\",\n        ],\n    )\n    # Select only the rows that pertain to dbf_table_name\n    row_map = row_map.loc[row_map.sched_table_name.isin(dbf_table_names)]\n    return row_map\n\n", "right_context": "\n\ndef get_data_cols_raw_xbrl(\n    raw_xbrl_instant: pd.DataFrame,\n    raw_xbrl_duration: pd.DataFrame,\n) -> list[str]:\n    \"\"\"Get a list of all XBRL data columns appearing in a given XBRL table.\n\n    Returns:\n        A list of all the data columns found in the original XBRL DB that correspond to\n        the given PUDL table. Includes columns from both the instant and duration tables\n        but excludes structural columns that appear in all XBRL tables.\n    \"\"\"\n    excluded_cols = [\n        \"date\",\n        \"end_date\",\n        \"entity_id\",\n        \"filing_name\",\n        \"index\",\n        \"report_year\",\n        \"start_date\",\n    ]\n    return sorted(\n        set(raw_xbrl_instant.columns)\n        .union(raw_xbrl_duration.columns)\n        .difference(excluded_cols)\n    )\n\n\ndef read_xbrl_calculation_fixes() -> pd.DataFrame:\n    \"\"\"Read in the table of calculation fixes.\"\"\"\n    source = importlib.resources.files(\"pudl.package_data.ferc1\").joinpath(\n        \"xbrl_calculation_component_fixes.csv\"\n    )\n    with importlib.resources.as_file(source) as file:\n        calc_fixes = pd.read_csv(file)\n    return calc_fixes\n\n\n################################################################################\n# FERC 1 specific TableTransformer classes\n################################################################################\nclass Ferc1AbstractTableTransformer(AbstractTableTransformer):\n    \"\"\"An abstract class defining methods common to many FERC Form 1 tables.\n\n    This subclass remains abstract because it does not define transform_main(), which\n    is always going to be table-specific.\n\n    * Methods that only apply to XBRL data should end with _xbrl\n    * Methods that only apply to DBF data should end with _dbf\n    \"\"\"\n\n    table_id: TableIdFerc1\n    parameter_model = Ferc1TableTransformParams\n    params: parameter_model\n\n    has_unique_record_ids: bool = True\n    \"\"\"True if each record in the transformed table corresponds to one input record.\n\n    For tables that have been transformed from wide-to-tidy format, or undergone other\n    kinds of reshaping, there is not a simple one-to-one relationship between input and\n    output records, and so we should not expect record IDs to be unique. In those cases\n    they serve only a forensic purpose, telling us where to find the original source of\n    the transformed data.\n    \"\"\"\n    xbrl_metadata: pd.DataFrame = pd.DataFrame()\n    \"\"\"Dataframe combining XBRL metadata for both instant and duration table columns.\"\"\"\n    xbrl_calculations: pd.DataFrame | None = None\n    \"\"\"Dataframe of calculation components.\n\n    If ``None``, the calculations have not been instantiated. If the table has been\n    instantiated but is an empty table, then there are no calculations for that table.\n    \"\"\"\n\n    def __init__(\n        self,\n        xbrl_metadata_json: dict[Literal[\"instant\", \"duration\"], list[dict[str, Any]]]\n        | None = None,\n        params: TableTransformParams | None = None,\n        cache_dfs: bool = False,\n        clear_cached_dfs: bool = True,\n    ) -> None:\n        \"\"\"Augment inherited initializer to store XBRL metadata in the class.\"\"\"\n        super().__init__(\n            params=params,\n            cache_dfs=cache_dfs,\n            clear_cached_dfs=clear_cached_dfs,\n        )\n        if xbrl_metadata_json:\n            xbrl_metadata_converted = self.convert_xbrl_metadata_json_to_df(\n                xbrl_metadata_json\n            )\n            self.xbrl_calculations = self.process_xbrl_metadata_calculations(\n                xbrl_metadata_converted\n            )\n            self.xbrl_metadata = self.process_xbrl_metadata(\n                xbrl_metadata_converted, self.xbrl_calculations\n            )\n\n    @cache_df(key=\"start\")\n    def transform_start(\n        self,\n        raw_dbf: pd.DataFrame,\n        raw_xbrl_instant: pd.DataFrame,\n        raw_xbrl_duration: pd.DataFrame,\n    ) -> pd.DataFrame:\n        \"\"\"Process the raw data until the XBRL and DBF inputs have been unified.\"\"\"\n        processed_dbf = self.process_dbf(raw_dbf)\n        processed_xbrl = self.process_xbrl(raw_xbrl_instant, raw_xbrl_duration)\n        processed_dbf = self.select_dbf_rows_by_category(processed_dbf, processed_xbrl)\n        logger.info(f\"{self.table_id.value}: Concatenating DBF + XBRL dataframes.\")\n        return pd.concat([processed_dbf, processed_xbrl]).reset_index(drop=True)\n\n    @cache_df(key=\"main\")\n    def transform_main(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generic FERC1 main table transformer.\n\n        Params:\n            df: Pre-processed, concatenated XBRL and DBF data.\n\n        Returns:\n            A single transformed table concatenating multiple years of cleaned data\n            derived from the raw DBF and/or XBRL inputs.\n        \"\"\"\n        df = (\n            self.spot_fix_values(df)\n            .pipe(self.normalize_strings)\n            .pipe(self.categorize_strings)\n            .pipe(self.convert_units)\n            .pipe(self.strip_non_numeric_values)\n            .pipe(self.nullify_outliers)\n            .pipe(self.replace_with_na)\n            .pipe(self.drop_invalid_rows)\n            .pipe(\n                pudl.metadata.classes.Package.from_resource_ids()\n                .get_resource(self.table_id.value)\n                .encode\n            )\n            .pipe(self.merge_xbrl_metadata)\n            .pipe(self.add_columns_with_uniform_values)\n        )\n        return df\n\n    @cache_df(key=\"end\")\n    def transform_end(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Standardized final cleanup after the transformations are done.\n\n        Checks calculations. Enforces dataframe schema. Checks for empty dataframes and\n        null columns.\n        \"\"\"\n        df = self.reconcile_table_calculations(df).pipe(self.enforce_schema)\n        if df.empty:\n            raise ValueError(f\"{self.table_id.value}: Final dataframe is empty!!!\")\n        for col in df:\n            if df[col].isna().all():\n                raise ValueError(\n                    f\"{self.table_id.value}: Column {col} is entirely NULL!\"\n                )\n        return df\n\n    def select_dbf_rows_by_category(\n        self,\n        processed_dbf: pd.DataFrame,\n        processed_xbrl: pd.DataFrame,\n        params: SelectDbfRowsByCategory | None = None,\n    ) -> pd.DataFrame:\n        \"\"\"Wrapper method for :func:`select_dbf_rows_by_category`.\"\"\"\n        if not params:\n            params = self.params.select_dbf_rows_by_category\n        if params.column_name:\n            logger.info(\n                f\"{self.table_id.value}: Selecting DBF rows with desired values in {params.column_name}.\"\n            )\n            processed_dbf = select_dbf_rows_by_category(\n                processed_dbf=processed_dbf,\n                processed_xbrl=processed_xbrl,\n                params=params,\n            )\n        return processed_dbf\n\n    def convert_xbrl_metadata_json_to_df(\n        self: Self,\n        xbrl_metadata_json: dict[Literal[\"instant\", \"duration\"], list[dict[str, Any]]],\n    ) -> pd.DataFrame:\n        \"\"\"Normalize the XBRL JSON metadata, turning it into a dataframe.\n\n        This process concatenates and deduplicates the metadata which is associated with\n        the instant and duration tables, since the metadata is only combined with the\n        data after the instant and duration (and DBF) tables have been merged. This\n        happens in :meth:`Ferc1AbstractTableTransformer.merge_xbrl_metadata`.\n        \"\"\"\n        logger.info(f\"{self.table_id.value}: Processing XBRL metadata.\")\n        tbl_meta = (\n            pd.concat(\n                [\n                    pd.json_normalize(xbrl_metadata_json[\"instant\"]),\n                    pd.json_normalize(xbrl_metadata_json[\"duration\"]),\n                ]\n            )\n            .drop(\"references.form_location\", axis=\"columns\")\n            .drop_duplicates(subset=\"name\")\n            .rename(\n                columns={\n                    \"name\": \"xbrl_factoid\",\n                    \"references.account\": \"ferc_account\",\n                }\n            )\n            .assign(\n                calculations=lambda x: x.calculations.apply(json.dumps),\n            )\n            .astype(\n                {\n                    \"xbrl_factoid\": pd.StringDtype(),\n                    \"balance\": pd.StringDtype(),\n                    \"ferc_account\": pd.StringDtype(),\n                    \"calculations\": pd.StringDtype(),\n                }\n            )\n            .assign(\n                xbrl_factoid_original=lambda x: x.xbrl_factoid,\n                xbrl_factoid=lambda x: self.rename_xbrl_factoid(x.xbrl_factoid),\n            )\n            .pipe(self.deduplicate_xbrl_factoid_xbrl_metadata)\n        )\n        return tbl_meta\n\n    @cache_df(key=\"process_xbrl_metadata\")\n    def process_xbrl_metadata(\n        self: Self,\n        xbrl_metadata_converted: pd.DataFrame,\n        xbrl_calculations: pd.DataFrame,\n    ) -> pd.DataFrame:\n        \"\"\"Process XBRL metadata after the calculations have been cleaned.\n\n        Add ``row_type_xbrl`` and ``is_within_table_calc`` columns and create\n        ``xbrl_factoid`` records for the calculation corrections.\n\n        Args:\n            xbrl_metadata_converted: Dataframe of relatively unprocessed metadata.\n                Result of :meth:`convert_xbrl_metadata_json_to_df`.\n            xbrl_calculations: Dataframe of calculation components. Result of\n                :meth:`process_xbrl_metadata_calculations`.\n        \"\"\"\n        # drop the calcs bc we never want to use them again. the xbrl_calculations are\n        # now the main source of truth for the calcs. set index so we can easily\n        # graph some calc info onto the metadata using an index of xbrl_factoid_parent.\n        tbl_meta = xbrl_metadata_converted.drop(columns=[\"calculations\"]).set_index(\n            [\"xbrl_factoid\"]\n        )\n        # Flag metadata record types\n        tbl_meta.loc[:, \"row_type_xbrl\"] = (\n            (  # if there is nothing in the calc cols for a parent fact - its reported\n                xbrl_calculations.groupby([\"xbrl_factoid_parent\"])[\n                    [\"table_name\", \"xbrl_factoid\"]\n                ].count()\n                == 0\n            )\n            .all(axis=\"columns\")\n            .replace({True: \"reported_value\", False: \"calculated_value\"})\n            .astype(pd.StringDtype())\n        )\n        tbl_meta.loc[:, \"row_type_xbrl\"] = tbl_meta.loc[:, \"row_type_xbrl\"].fillna(\n            \"reported_value\"\n        )\n        # this bool column is created and used within the calculations. but its a\n        # helpful thing in the metadata table as well.\n        tbl_meta.loc[:, \"is_within_table_calc\"] = (\n            xbrl_calculations.groupby([\"xbrl_factoid_parent\"])[\"is_within_table_calc\"]\n            .all()\n            .astype(pd.BooleanDtype())\n        )\n        if self.params.add_columns_with_uniform_values.columns_to_add:\n            tbl_meta = tbl_meta.assign(\n                **self.params.add_columns_with_uniform_values.assign_cols\n            )\n        tbl_meta = tbl_meta.reset_index().pipe(self.add_metadata_corrections)\n        return tbl_meta\n\n    def deduplicate_xbrl_factoid_xbrl_metadata(\n        self, tbl_meta: pd.DataFrame\n    ) -> pd.DataFrame:\n        \"\"\"De-duplicate the xbrl_metadata based on ``xbrl_factoid``.\n\n        Default is to do nothing besides check for duplicate values because almost all\n        tables have no deduping. Deduplication needs to be applied before the\n        :meth:`apply_xbrl_calculation_fixes` inside of :meth:`process_xbrl_metadata`.\n        \"\"\"\n        if tbl_meta.duplicated(subset=[\"xbrl_factoid\"]).any() & (\n            self.params.merge_xbrl_metadata.on is not None\n        ):\n            raise AssertionError(\n                f\"Metadata for {self.table_id.value} has duplicative xbrl_factoid records.\"\n            )\n        return tbl_meta\n\n    def raw_xbrl_factoid_to_pudl_name(\n        self,\n        col_name_xbrl: str,\n    ) -> str:\n        \"\"\"Rename a column name from original XBRL name to the transformed PUDL name.\n\n        There are several transform params that either explicitly or implicity rename\n        columns:\n        * :class:`RenameColumnsFerc1`\n        * :class:`WideToTidySourceFerc1`\n        * :class:`UnstackBalancesToReportYearInstantXbrl`\n        * :class:`ConvertUnits`\n\n        This method attempts to use the table params to translate a column name.\n\n        Note: Instead of doing this for each individual column name, we could compile a\n        rename dict for the whole table with a similar processand then apply it for each\n        group of columns instead of running through this full process every time. If\n        this took longer than...  ~5 ms on a single table w/ lots of calcs this would\n        probably be worth it for simplicity.\n        \"\"\"\n        col_name_new = col_name_xbrl\n        for rename_stage in self.params.rename_dicts_xbrl:\n            col_name_new = str(rename_stage.columns.get(col_name_new, col_name_new))\n\n        for value_type in self.params.wide_to_tidy_value_types:\n            if col_name_new.endswith(f\"_{value_type}\"):\n                col_name_new = re.sub(f\"_{value_type}$\", \"\", col_name_new)\n\n        if self.params.unstack_balances_to_report_year_instant_xbrl:  # noqa: SIM102\n            # TODO: do something...? add starting_balance & ending_balance suffixes?\n            if self.params.merge_xbrl_metadata.on:\n                NotImplementedError(\n                    \"We haven't implemented a xbrl_factoid rename for the parameter \"\n                    \"unstack_balances_to_report_year_instant_xbrl. Since you are trying\"\n                    \"to merge the metadata on this table that has this treatment, a \"\n                    \"xbrl_factoid rename will be required.\"\n                )\n        if self.params.convert_units:  # noqa: SIM102\n            # TODO: use from_unit -> to_unit map. but none of the $$ tables have this rn.\n            if self.params.merge_xbrl_metadata.on:\n                NotImplementedError(\n                    \"We haven't implemented a xbrl_factoid rename for the parameter \"\n                    \"convert_units. Since you are trying to merge the metadata on this \"\n                    \"table that has this treatment, a xbrl_factoid rename will be \"\n                    \"required.\"\n                )\n        return col_name_new\n\n    def rename_xbrl_factoid(self, col: pd.Series) -> pd.Series:\n        \"\"\"Rename a series of raw to PUDL factoid names via :meth:`raw_xbrl_factoid_to_pudl_name`.\"\"\"\n        xbrl_factoid_name_map = {\n            xbrl_factoid_name_og: self.raw_xbrl_factoid_to_pudl_name(\n                xbrl_factoid_name_og\n            )\n            for xbrl_factoid_name_og in col\n        }\n        return col.map(xbrl_factoid_name_map)\n\n    def rename_xbrl_factoid_other_tables(self, calc_comps):\n        \"\"\"Rename the factoids from calculation components from other tables.\n\n        Note: It is probably possible to build an apply style function that takes a\n        series of factoid names and a series of table names and returns a table-specific\n        rename_xbrl_factoid.\n        \"\"\"\n        calc_tables = calc_comps.table_name.dropna().unique()\n        os_tables = [\n            tbl\n            for tbl in calc_tables\n            if (tbl != self.table_id.value) & (tbl in FERC1_TFR_CLASSES)\n        ]\n        for tbl in os_tables:\n            trns = FERC1_TFR_CLASSES[tbl]()\n            calc_comps = calc_comps.assign(\n                xbrl_factoid=lambda x, tbl=tbl, trns=trns: np.where(\n                    x.table_name == tbl,\n                    trns.rename_xbrl_factoid(x.xbrl_factoid),\n                    x.xbrl_factoid,\n                ),\n            )\n        return calc_comps\n\n    @staticmethod\n    def add_metadata_corrections(tbl_meta: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Create metadata records for the calculation correction factoids.\n\n        Args:\n            tbl_meta: processed metadata table which contains columns ``row_type_xbrl``.\n        \"\"\"\n        correction_meta = tbl_meta[tbl_meta.row_type_xbrl == \"calculated_value\"].assign(\n            is_within_table_calc=True,\n            row_type_xbrl=\"correction\",\n            xbrl_factoid=lambda x: x.xbrl_factoid + \"_correction\",\n        )\n        tbl_meta = (\n            pd.concat(\n                [tbl_meta, correction_meta.astype(tbl_meta.dtypes, errors=\"ignore\")]\n            )\n            .reset_index(drop=True)\n            .convert_dtypes()\n        )\n        return tbl_meta\n\n    def add_calculation_corrections(\n        self: Self, calc_components: pd.DataFrame\n    ) -> pd.DataFrame:\n        \"\"\"Add correction components and parent-only factoids to calculation metadata.\n\n        Args:\n            tbl_meta: Partially transformed table metadata in dataframe form.\n\n        Returns:\n            An updated version of the table metadata containing calculation definitions\n            that include a correction component.\n        \"\"\"\n        # If we haven't provided calculation check parameters, then we can't identify\n        # a appropriate correction factor.\n        if self.params.reconcile_table_calculations.column_to_check is None:\n            return calc_components\n\n        # split the calcs from non-calcs/make corrections/append\n        calcs = calc_components[calc_components.xbrl_factoid.notnull()]\n        correction_components = (\n            calcs[[\"table_name_parent\", \"xbrl_factoid_parent\"]]\n            .drop_duplicates()\n            .assign(\n                table_name=lambda t: t.table_name_parent,\n                xbrl_factoid=lambda x: x.xbrl_factoid_parent + \"_correction\",\n                weight=1,\n            )\n        )\n        return pd.concat([calc_components, correction_components])\n\n    def get_xbrl_calculation_fixes(self: Self) -> pd.DataFrame:\n        \"\"\"Grab the XBRL calculation file.\"\"\"\n        calc_fixes = read_xbrl_calculation_fixes()\n        # grab the fixes from this table only!\n        calc_fixes = calc_fixes[calc_fixes.table_name_parent == self.table_id.value]\n        return calc_fixes\n\n    def apply_xbrl_calculation_fixes(\n        self: Self, calc_components: pd.DataFrame, calc_fixes: pd.DataFrame\n    ) -> pd.DataFrame:\n        \"\"\"Use the fixes we've compiled to update calculations in the XBRL metadata.\n\n        Note: Temp fix. These updates should probably be moved into the table params\n        and integrated into the calculations via TableCalcs.\n        \"\"\"\n        calc_comp_idx = [\n            \"table_name_parent\",\n            \"xbrl_factoid_parent\",\n            \"table_name\",\n            \"xbrl_factoid\",\n        ]\n        if not (\n            dupes := calc_fixes[calc_fixes.duplicated(subset=calc_comp_idx, keep=False)]\n        ).empty:\n            raise AssertionError(\n                \"Duplicates found in the calculation fixes where none were expected.\"\n                f\"{dupes}\"\n            )\n\n        calc_fixes = calc_fixes.set_index(calc_comp_idx).sort_index()\n        calc_components = calc_components.set_index(calc_comp_idx).sort_index()\n        # find the fixes that need to be replaced. We id them\n        # by finding the fixes that share indexes with the calc components\n        # Note: we can't just dropna after adding the replacements instead\n        # of while finding the replacements because we have included all\n        # factoids in the calculation component table as parent factoids\n        # even if there are no/null calculation components.\n        replace_me = calc_fixes.loc[\n            calc_fixes.index.intersection(calc_components.index)\n        ].dropna(how=\"all\")\n        calc_components.loc[replace_me.index, list(replace_me.columns)] = replace_me\n\n        # find the lines that only show up in the fixes that need to be added\n        add_me = calc_fixes.loc[calc_fixes.index.difference(calc_components.index)]\n        calc_components = pd.concat([calc_components, add_me])\n        # sometimes we add fresh calculations to parent facts that originally didn't\n        # have any calculation components. So if we are adding those parent facts\n        # with child facts/calc components, we need to remove the non-calc records\n        # so make fake little parent facts with null childern from all the add_mes\n        null_calc_versions_of_add_mes = (\n            add_me.reset_index()[[\"table_name_parent\", \"xbrl_factoid_parent\"]]\n            .assign(table_name=pd.NA, xbrl_factoid=pd.NA)\n            .set_index(calc_comp_idx)\n        )\n        remove_the_non_cals_from_add_mes = calc_components.index.difference(\n            null_calc_versions_of_add_mes.index.drop_duplicates()\n        )\n        calc_components = calc_components.loc[remove_the_non_cals_from_add_mes]\n\n        # find the \"null\" fixes which correspond to records which need to be deleted.\n        delete_me = calc_fixes[calc_fixes.isnull().all(axis=1)]\n        calc_components = calc_components.loc[\n            calc_components.index.difference(delete_me.index)\n        ]\n        len_fixes_applied = len(replace_me) + len(add_me) + len(delete_me)\n        logger.debug(\n            f\"We've applied {len_fixes_applied} calculation fixes including \"\n            f\"{len(replace_me)} replacements, {len(add_me)} additions and \"\n            f\"{len(delete_me)} deletions.\"\n        )\n        if len(calc_fixes) != len_fixes_applied:\n            raise AssertionError(\n                f\"We've applied {len_fixes_applied} calculation fixes while we started \"\n                f\"with {len(calc_fixes)}. Length of applied and original fixes should \"\n                f\"be the same.\\n{replace_me=}\\n{add_me=}\\n{delete_me=}\"\n            )\n        return calc_components.reset_index()\n\n    def process_xbrl_metadata_calculations(\n        self, xbrl_metadata_converted: pd.DataFrame\n    ) -> pd.DataFrame:\n        \"\"\"Convert xbrl metadata calculations into a table of calculation components.\n\n        This method extracts the calculations from the ``xbrl_metadata_converted``\n        that are stored as json embedded within the ``calculations``column and convert\n        those into calculation component records. The resulting table includes columns\n        pertaining to both the calculation components and the parent factoid that the\n        components pertain to. The parental columns had suffixes of ``_parent``.\n\n        This method also adds fixes to the calculations via\n        :meth:`apply_xbrl_calculation_fixes`, adds corrections records via\n        :meth:`add_calculation_corrections` and adds the column\n        ``is_within_table_calc``.\n\n        Args:\n            xbrl_metadata_converted: Dataframe of relatively unprocessed metadata.\n                Result of :meth:`convert_xbrl_metadata_json_to_df`.\n        \"\"\"\n        metadata = xbrl_metadata_converted.copy()\n        metadata.calculations = metadata.calculations.apply(json.loads)\n        # reset the index post calc explosion so we can merge on index later\n        metadata = metadata.explode(\"calculations\").reset_index(drop=True)\n        if all(metadata.calculations.isnull()):\n            calc_comps = pd.DataFrame(columns=[\"name\", \"source_tables\"])\n        else:\n            calc_comps = pd.json_normalize(metadata.calculations)\n\n        calc_comps = (\n            calc_comps.explode(\"source_tables\")\n            .rename(\n                columns={\n                    \"name\": \"xbrl_factoid\",\n                    \"source_tables\": \"table_name\",\n                }\n            )\n            .merge(\n                metadata.drop(columns=[\"calculations\"]).rename(\n                    columns={\n                        \"xbrl_factoid\": \"xbrl_factoid_parent\",\n                    }\n                ),\n                left_index=True,\n                right_index=True,\n                how=\"left\",\n            )\n            .dropna(subset=[\"xbrl_factoid\"])\n            .reset_index(drop=True)\n            .assign(\n                table_name_parent=self.table_id.value,\n                xbrl_factoid=lambda x: np.where(\n                    x.table_name == self.table_id.value,\n                    self.rename_xbrl_factoid(x.xbrl_factoid),\n                    x.xbrl_factoid,\n                ),\n            )\n            .pipe(self.rename_xbrl_factoid_other_tables)\n            .pipe(\n                self.apply_xbrl_calculation_fixes,\n                calc_fixes=self.get_xbrl_calculation_fixes(),\n            )\n            .drop_duplicates(keep=\"first\")\n            .pipe(self.add_calculation_corrections)\n        )\n\n        # this is really a xbrl_factoid-level flag, but we need it while using this\n        # calc components.\n        calc_comps[\"is_within_table_calc\"] = (\n            # make a temp bool col to check if all the componets are intra table\n            # should the non-calc guys get a null or a true here? rn its true bc fillna\n            calc_comps.assign(\n                intra_table_calc_comp_flag=lambda x: (\n                    self.table_id.value == x.table_name.fillna(self.table_id.value)\n                )\n            )\n            .groupby([\"table_name_parent\", \"xbrl_factoid_parent\"])[\n                \"intra_table_calc_comp_flag\"\n            ]\n            .transform(\"all\")\n            .astype(pd.BooleanDtype())\n        )\n        # check for uniqueness only when we are reconciling the calculations\n        # bc that implies we have cleaned the calcs and are intending to use them.\n        if self.params.reconcile_table_calculations.column_to_check:\n            calc_comp_idx = [\n                \"table_name_parent\",\n                \"xbrl_factoid_parent\",\n                \"table_name\",\n                \"xbrl_factoid\",\n            ]\n            if not (\n                dupes := calc_comps[calc_comps.duplicated(subset=calc_comp_idx)]\n            ).empty:\n                raise AssertionError(\n                    \"Duplicates found in the calculation components where none were .\"\n                    f\"expected {dupes}\"\n                )\n        return calc_comps.convert_dtypes()\n\n    def add_columns_with_uniform_values(\n        self, df: pd.DataFrame, params: AddColumnsWithUniformValues | None = None\n    ) -> pd.DataFrame:\n        \"\"\"Add a column with a uniform value.\"\"\"\n        if not params:\n            params = self.params.add_columns_with_uniform_values\n        if params.columns_to_add:\n            logger.info(\n                f\"{self.table_id.value}: Adding uniform value columns. {params.assign_cols}\"\n            )\n            df = add_columns_with_uniform_values(df, params)\n        return df\n\n    @cache_df(key=\"merge_xbrl_metadata\")\n    def merge_xbrl_metadata(\n        self, df: pd.DataFrame, params: MergeXbrlMetadata | None = None\n    ) -> pd.DataFrame:\n        \"\"\"Combine XBRL-derived metadata with the data it pertains to.\n\n        While the metadata we're using to annotate the data comes from the more recent\n        XBRL data, it applies generally to all the historical DBF data as well! This\n        method reads the normalized metadata out of an attribute.\n        \"\"\"\n        if not params:\n            params = self.params.merge_xbrl_metadata\n        if params.on:\n            if self.xbrl_metadata.empty:\n                raise AssertionError(\n                    \"Metadata has not yet been generated. Must run process_xbrl_metadata\"\n                    \"and assign xbrl_metadata before merging metadata.\"\n                )\n            logger.info(f\"{self.table_id.value}: Merging metadata\")\n            df = merge_xbrl_metadata(df, self.xbrl_metadata, params)\n        return df\n\n    @cache_df(key=\"dbf\")\n    def align_row_numbers_dbf(\n        self, df: pd.DataFrame, params: AlignRowNumbersDbf | None = None\n    ) -> pd.DataFrame:\n        \"\"\"Align historical FERC1 DBF row numbers with XBRL account IDs.\n\n        Additional Parameterization TBD with additional experience. See:\n        https://github.com/catalyst-cooperative/pudl/issues/2012\n        \"\"\"\n        if params is None:\n            params = self.params.align_row_numbers_dbf\n        if params.dbf_table_names:\n            df = align_row_numbers_dbf(df, params=params)\n        return df\n\n    @cache_df(key=\"dbf\")\n    def drop_duplicate_rows_dbf(\n        self, df: pd.DataFrame, params: DropDuplicateRowsDbf | None = None\n    ) -> pd.DataFrame:\n        \"\"\"Drop the DBF rows where the PKs and data columns are duplicated.\n\n        Wrapper function for :func:`drop_duplicate_rows_dbf`.\n        \"\"\"\n        if params is None:\n            params = self.params.drop_duplicate_rows_dbf\n        if params.table_name:\n            logger.info(\n                f\"{self.table_id.value}: Dropping rows where primary key and data \"\n                \"columns are duplicated.\"\n            )\n            df = drop_duplicate_rows_dbf(df, params=params)\n        return df\n\n    @cache_df(key=\"dbf\")\n    def process_dbf(self, raw_dbf: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"DBF-specific transformations that take place before concatenation.\"\"\"\n        logger.info(f\"{self.table_id.value}: Processing DBF data pre-concatenation.\")\n        return (\n            raw_dbf.drop_duplicates()\n            .pipe(self.select_annual_rows_dbf)\n            .pipe(self.drop_footnote_columns_dbf)\n            .pipe(self.align_row_numbers_dbf)\n            .pipe(self.rename_columns, rename_stage=\"dbf\")\n            .pipe(self.assign_record_id, source_ferc1=SourceFerc1.DBF)\n            .pipe(self.drop_unused_original_columns_dbf)\n            .pipe(self.assign_utility_id_ferc1, source_ferc1=SourceFerc1.DBF)\n            .pipe(self.wide_to_tidy, source_ferc1=SourceFerc1.DBF)\n            .pipe(self.drop_duplicate_rows_dbf)\n            .pipe(self.assign_quarterly_data_to_yearly_dbf)\n        )\n\n    @cache_df(key=\"xbrl\")\n    def process_xbrl(\n        self,\n        raw_xbrl_instant: pd.DataFrame,\n        raw_xbrl_duration: pd.DataFrame,\n    ) -> pd.DataFrame:\n        \"\"\"XBRL-specific transformations that take place before concatenation.\"\"\"\n        logger.info(f\"{self.table_id.value}: Processing XBRL data pre-concatenation.\")\n        return (\n            self.merge_instant_and_duration_tables_xbrl(\n                raw_xbrl_instant, raw_xbrl_duration\n            )\n            .pipe(self.wide_to_tidy, source_ferc1=SourceFerc1.XBRL)\n            .pipe(self.rename_columns, rename_stage=\"xbrl\")\n            .pipe(self.combine_axis_columns_xbrl)\n            .pipe(self.assign_record_id, source_ferc1=SourceFerc1.XBRL)\n            .pipe(self.assign_utility_id_ferc1, source_ferc1=SourceFerc1.XBRL)\n        )\n\n    def rename_columns(\n        self,\n        df: pd.DataFrame,\n        rename_stage: Literal[\"dbf\", \"xbrl\", \"xbrl_instant\", \"xbrl_duration\"]\n        | None = None,\n        params: RenameColumns | None = None,\n    ):\n        \"\"\"Grab the params based on the rename stage and run default rename_columns.\n\n        Args:\n            df: Table to be renamed.\n            rename_stage: Name of stage in the transform process. Used to get specific\n                stage's parameters if None have been passed.\n            params: Rename column parameters.\n        \"\"\"\n        if not params:\n            params = self.params.rename_columns_ferc1.__getattribute__(rename_stage)\n        df = super().rename_columns(df, params=params)\n        return df\n\n    @cache_df(key=\"dbf\")\n    def select_annual_rows_dbf(self, df):\n        \"\"\"Select only annually reported DBF Rows.\n\n        There are some DBF tables that include a mix of reporting frequencies. For now,\n        the default for PUDL tables is to have only the annual records.\n        \"\"\"\n        if \"report_prd\" in df and list(df.report_prd.unique()) != [12]:\n            len_og = len(df)\n            df = df[df.report_prd == 12].copy()\n            logger.info(\n                f\"{self.table_id.value}: After selection of only annual records,\"\n                f\" we have {len(df)/len_og:.1%} of the original table.\"\n            )\n        return df\n\n    @cache_df(key=\"dbf\")\n    def assign_quarterly_data_to_yearly_dbf(\n        self, df, params: AssignQuarterlyDataToYearlyDbf | None = None\n    ):\n        \"\"\"Transfer quarterly filed data to annual columns.\"\"\"\n        if params is None:\n            params = self.params.assign_quarterly_data_to_yearly_dbf\n        if params.quarterly_to_yearly_column_map:\n            logger.info(\n                f\"{self.table_id.value}: Converting quarterly filed data to annual.\"\n            )\n            df = assign_quarterly_data_to_yearly_dbf(df, params=params)\n        return df\n\n    def unstack_balances_to_report_year_instant_xbrl(\n        self,\n        df: pd.DataFrame,\n        params: UnstackBalancesToReportYearInstantXbrl | None = None,\n    ) -> pd.DataFrame:\n        \"\"\"Turn start year end year rows into columns for each value type.\"\"\"\n        logger.info(f\"{self.table_id.value}: Unstacking balances to the report years.\")\n        if params is None:\n            params = self.params.unstack_balances_to_report_year_instant_xbrl\n        if params.unstack_balances_to_report_year:\n            df = unstack_balances_to_report_year_instant_xbrl(\n                df,\n                params=params,\n                primary_key_cols=self.source_table_primary_key(\n                    source_ferc1=SourceFerc1.XBRL\n                ),\n            )\n        return df\n\n    def wide_to_tidy(\n        self,\n        df: pd.DataFrame,\n        source_ferc1: SourceFerc1,\n        params: WideToTidy | None = None,\n    ) -> pd.DataFrame:\n        \"\"\"Reshape wide tables with FERC account columns to tidy format.\n\n        The XBRL table coming into this method contains all the data from both the\n        instant and duration tables in a wide format -- with one column for every\n        combination of value type (e.g. additions, ending_balance) and accounting\n        category, which means ~500 columns.\n\n        We tidy this into a long table with one column for each of the value types (6 in\n        all), and a new column that contains the accounting categories. This allows\n        aggregation across columns to calculate the ending balance based on the starting\n        balance and all of the reported changes, and aggregation across groups of rows\n        to total up various hierarchical accounting categories (hydraulic turbines ->\n        hydraulic production plant -> all  production plant -> all electric utility\n        plant) though the categorical columns required for that aggregation are added\n        later.\n        \"\"\"\n        if not params:\n            params = self.params.wide_to_tidy.__getattribute__(source_ferc1.value)\n\n        multiple_params = [params] if isinstance(params, WideToTidy) else params\n        for single_params in multiple_params:\n            if single_params.idx_cols or single_params.value_types:\n                logger.info(\n                    f\"{self.table_id.value}: applying wide_to_tidy for {source_ferc1.value}\"\n                )\n                df = wide_to_tidy(df, single_params)\n        return df\n\n    def combine_axis_columns_xbrl(\n        self,\n        df: pd.DataFrame,\n        params: CombineAxisColumnsXbrl | None = None,\n    ) -> pd.DataFrame:\n        \"\"\"Combine axis columns from squished XBRL tables into one column with no NA.\"\"\"\n        if params is None:\n            params = self.params.combine_axis_columns_xbrl\n        if params.axis_columns_to_combine:\n            logger.info(\n                f\"{self.table_id.value}: Combining axis columns: \"\n                f\"{params.axis_columns_to_combine} into {params.new_axis_column_name}\"\n            )\n            df = combine_axis_columns_xbrl(df, params=params)\n        return df\n\n    @cache_df(key=\"xbrl\")\n    def merge_instant_and_duration_tables_xbrl(\n        self,\n        raw_xbrl_instant: pd.DataFrame,\n        raw_xbrl_duration: pd.DataFrame,\n    ) -> pd.DataFrame:\n        \"\"\"Merge XBRL instant and duration tables, reshaping instant as needed.\n\n        FERC1 XBRL instant period signifies that it is true as of the reported date,\n        while a duration fact pertains to the specified time period. The ``date`` column\n        for an instant fact corresponds to the ``end_date`` column of a duration fact.\n\n        When merging the instant and duration tables, we need to preserve row order.\n        For the small generators table, row order is how we label and extract\n        information from header and note rows. Outer merging messes up the order, so we\n        need to use a one-sided merge. So far, it seems like the duration df contains\n        all the index values in the instant df. To be sure, there's a check that makes\n        sure there are no unique intant df index values. If that passes, we merge the\n        instant table into the duration table, and the row order is preserved.\n\n        Note: This should always be applied before :meth:``rename_columns``\n\n        Args:\n            raw_xbrl_instant: table representing XBRL instant facts.\n            raw_xbrl_duration: table representing XBRL duration facts.\n\n        Returns:\n            A unified table combining the XBRL duration and instant facts, if both types\n            of facts were present. If either input dataframe is empty, the other\n            dataframe is returned unchanged, except that several unused columns are\n            dropped. If both input dataframes are empty, an empty dataframe is returned.\n        \"\"\"\n        drop_cols = [\"filing_name\", \"index\"]\n        # Ignore errors in case not all drop_cols are present.\n        instant = raw_xbrl_instant.drop(columns=drop_cols, errors=\"ignore\")\n        duration = raw_xbrl_duration.drop(columns=drop_cols, errors=\"ignore\")\n\n        instant_axes = [\n            col for col in raw_xbrl_instant.columns if col.endswith(\"_axis\")\n        ]\n        duration_axes = [\n            col for col in raw_xbrl_duration.columns if col.endswith(\"_axis\")\n        ]\n        if (\n            bool(instant_axes)\n            & bool(duration_axes)\n            & (set(instant_axes) != set(duration_axes))\n        ):\n            raise ValueError(\n                f\"{self.table_id.value}: Instant and Duration XBRL Axes do not match.\\n\"\n                f\"    instant: {instant_axes}\\n\"\n                f\"    duration: {duration_axes}\"\n            )\n\n        # Do any table-specific preprocessing of the instant and duration tables\n        instant = self.process_instant_xbrl(instant)\n        duration = self.process_duration_xbrl(duration)\n\n        if instant.empty:\n            logger.info(f\"{self.table_id.value}: No XBRL instant table found.\")\n            out_df = duration\n        elif duration.empty:\n            logger.info(f\"{self.table_id.value}: No XBRL duration table found.\")\n            out_df = instant\n        else:\n            logger.info(\n                f\"{self.table_id.value}: Both XBRL instant & duration tables found.\"\n            )\n            instant_merge_keys = [\n                \"entity_id\",\n                \"report_year\",\n                \"sched_table_name\",\n            ] + instant_axes\n            duration_merge_keys = [\n                \"entity_id\",\n                \"report_year\",\n                \"sched_table_name\",\n            ] + duration_axes\n            # See if there are any values in the instant table that don't show up in the\n            # duration table.\n            unique_instant_rows = instant.set_index(\n                instant_merge_keys\n            ).index.difference(duration.set_index(duration_merge_keys).index)\n            if unique_instant_rows.empty:\n                logger.info(\n                    f\"{self.table_id.value}: Combining XBRL instant & duration tables \"\n                    \"using RIGHT-MERGE.\"\n                )\n                # Merge instant into duration.\n                out_df = pd.merge(\n                    instant,\n                    duration,\n                    how=\"right\",\n                    left_on=instant_merge_keys,\n                    right_on=duration_merge_keys,\n                    validate=\"1:1\",\n                )\n            else:\n                # TODO: Check whether our assumptions about these tables hold before\n                # concatenating them. May need to be table specific. E.g.\n                # * What fraction of their index values overlap? (it should be high!)\n                # * Do the instant/duration columns conform to expected naming conventions?\n                logger.info(\n                    f\"{self.table_id.value}: Combining XBRL instant & duration tables \"\n                    \"using CONCATENATION.\"\n                )\n                out_df = pd.concat(\n                    [\n                        instant.set_index(instant_merge_keys),\n                        duration.set_index(duration_merge_keys),\n                    ],\n                    axis=\"columns\",\n                ).reset_index()\n\n        return out_df.loc[out_df.report_year.isin(Ferc1Settings().xbrl_years)]\n\n    @cache_df(\"process_instant_xbrl\")\n    def process_instant_xbrl(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Pre-processing required to make instant and duration tables compatible.\n\n        Column renaming is sometimes required because a few columns in the instant and\n        duration tables do not have corresponding names that follow the naming\n        conventions of ~95% of all the columns, which we rely on programmatically when\n        reshaping and concatenating these tables together.\n        \"\"\"\n        df = self.rename_columns(df, rename_stage=\"instant_xbrl\").pipe(\n            self.unstack_balances_to_report_year_instant_xbrl\n        )\n        return df\n\n    @cache_df(\"process_duration_xbrl\")\n    def process_duration_xbrl(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Pre-processing required to make instant and duration tables compatible.\n\n        Column renaming is sometimes required because a few columns in the instant and\n        duration tables do not have corresponding names that follow the naming\n        conventions of ~95% of all the columns, which we rely on programmatically when\n        reshaping and concatenating these tables together.\n        \"\"\"\n        if not df.empty:\n            df = self.rename_columns(df, rename_stage=\"duration_xbrl\").pipe(\n                self.select_current_year_annual_records_duration_xbrl\n            )\n        return df\n\n    def select_current_year_annual_records_duration_xbrl(self, df):\n        \"\"\"Select for annual records within their report_year.\n\n        Select only records that have a start_date at begining of the report_year and\n        have an end_date at the end of the report_year.\n        \"\"\"\n        len_og = len(df)\n        df = df.astype({\"start_date\": \"datetime64[s]\", \"end_date\": \"datetime64[s]\"})\n        df = df[\n            (df.start_date.dt.year == df.report_year)\n            & (df.start_date.dt.month == 1)\n            & (df.start_date.dt.day == 1)\n            & (df.end_date.dt.year == df.report_year)\n            & (df.end_date.dt.month == 12)\n            & (df.end_date.dt.day == 31)\n        ]\n        len_out = len(df)\n        logger.info(\n            f\"{self.table_id.value}: After selection of dates based on the report year,\"\n            f\" we have {len_out/len_og:.1%} of the original table.\"\n        )\n        return df\n\n    @cache_df(key=\"dbf\")\n    def drop_footnote_columns_dbf(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Drop DBF footnote reference columns, which all end with _f.\"\"\"\n        logger.debug(f\"{self.table_id.value}: Dropping DBF footnote columns.\")\n        return df.drop(columns=df.filter(regex=r\".*_f$\").columns)\n\n    def source_table_primary_key(self, source_ferc1: SourceFerc1) -> list[str]:\n        \"\"\"Look up the pre-renaming source table primary key columns.\"\"\"\n        if source_ferc1 == SourceFerc1.DBF:\n            pk_cols = [\n                \"report_year\",\n                \"report_prd\",\n                \"respondent_id\",\n                \"spplmnt_num\",\n                \"row_number\",\n            ]\n        else:\n            assert source_ferc1 == SourceFerc1.XBRL  # nosec: B101\n            cols = self.params.rename_columns_ferc1.xbrl.columns\n            pk_cols = [\"report_year\", \"entity_id\"]\n            # Sort to avoid dependence on the ordering of rename_columns.\n            # Doing the sorting here because we have a particular ordering\n            # hard coded for the DBF primary keys.\n            pk_cols += sorted(col for col in cols if col.endswith(\"_axis\"))\n        return pk_cols\n\n    def renamed_table_primary_key(self, source_ferc1: SourceFerc1) -> list[str]:\n        \"\"\"Look up the post-renaming primary key columns.\"\"\"\n        if source_ferc1 == SourceFerc1.DBF:\n            cols = self.params.rename_columns_ferc1.dbf.columns\n        else:\n            assert source_ferc1 == SourceFerc1.XBRL  # nosec: B101\n            cols = self.params.rename_columns_ferc1.xbrl.columns\n        pk_cols = self.source_table_primary_key(source_ferc1=source_ferc1)\n        # Translate to the renamed columns\n        return [cols[col] for col in pk_cols]\n\n    @cache_df(key=\"dbf\")\n    def drop_unused_original_columns_dbf(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Remove residual DBF specific columns.\"\"\"\n        unused_cols = [\n            \"report_prd\",\n            \"spplmnt_num\",\n            \"row_number\",\n            \"row_seq\",\n            \"row_prvlg\",\n        ]\n        logger.debug(\n            f\"{self.table_id.value}: Dropping unused DBF structural columns: \"\n            f\"{unused_cols}\"\n        )\n        missing_cols = set(unused_cols).difference(df.columns)\n        if missing_cols:\n            raise ValueError(\n                f\"{self.table_id.value}: Trying to drop missing original DBF columns:\"\n                f\"{missing_cols}\"\n            )\n        return df.drop(columns=unused_cols)\n\n    def assign_record_id(\n        self, df: pd.DataFrame, source_ferc1: SourceFerc1\n    ) -> pd.DataFrame:\n        \"\"\"Add a column identifying the original source record for each row.\n\n        It is often useful to be able to tell exactly which record in the FERC Form 1\n        database a given record within the PUDL database came from.\n\n        Within each FERC Form 1 DBF table, each record is supposed to be uniquely\n        identified by the combination of: report_year, report_prd, utility_id_ferc1_dbf,\n        spplmnt_num, row_number.\n\n        The FERC Form 1 XBRL tables do not have these supplement and row number\n        columns, so we construct an id based on:\n        report_year, utility_id_ferc1_xbrl, and the primary key columns of the XBRL table\n\n        Args:\n            df: table to assign `record_id` to\n            source_ferc1: data source of raw ferc1 database.\n\n        Raises:\n            ValueError: If any of the primary key columns are missing from the DataFrame\n                being processed.\n            ValueError: If there are any null values in the primary key columns.\n            ValueError: If the resulting `record_id` column is non-unique.\n        \"\"\"\n        logger.debug(\n            f\"{self.table_id.value}: Assigning {source_ferc1.value} source record IDs.\"\n        )\n        pk_cols = self.renamed_table_primary_key(source_ferc1)\n        missing_pk_cols = set(pk_cols).difference(df.columns)\n        if missing_pk_cols:\n            raise ValueError(\n                f\"{self.table_id.value} ({source_ferc1.value}): Missing primary key \"\n                \"columns in dataframe while assigning source record_id: \"\n                f\"{missing_pk_cols}\"\n            )\n        if df[pk_cols].isnull().any(axis=None):\n            raise ValueError(\n                f\"{self.table_id.value} ({source_ferc1.value}): Found null primary key \"\n                \"values.\\n\"\n                f\"{df[pk_cols].isnull().any()}\"\n            )\n        df = df.assign(\n            record_id=lambda x: x.sched_table_name.str.cat(\n                x[pk_cols].astype(str), sep=\"_\"\n            ),\n        )\n        if df.sched_table_name.isnull().any():\n            raise ValueError(\n                f\"{self.table_id.value}: Null sched_table_name's were found where none \"\n                \"were expected.\"\n            )\n        df.record_id = enforce_snake_case(df.record_id)\n\n        dupe_ids = df.record_id[df.record_id.duplicated()].to_numpy()\n        if dupe_ids.any() and self.has_unique_record_ids:\n            logger.warning(\n                f\"{self.table_id.value}: Found {len(dupe_ids)} duplicate record_ids: \\n\"\n                f\"{dupe_ids}.\"\n            )\n        df = df.drop(columns=\"sched_table_name\")\n        return df\n\n    def assign_utility_id_ferc1(\n        self, df: pd.DataFrame, source_ferc1: SourceFerc1\n    ) -> pd.DataFrame:\n        \"\"\"Assign the PUDL-assigned utility_id_ferc1 based on the native utility ID.\n\n        We need to replace the natively reported utility ID from each of the two FERC1\n        sources with a PUDL-assigned utilty. The mapping between the native ID's and\n        these PUDL-assigned ID's can be accessed in the database tables\n        ``utilities_dbf_ferc1`` and ``utilities_xbrl_ferc1``.\n\n        Args:\n            df: the input table with the native utilty ID column.\n            source_ferc1: the\n\n        Returns:\n            an augemented version of the input ``df`` with a new column that replaces\n            the natively reported utility ID with the PUDL-assigned utility ID.\n        \"\"\"\n        logger.debug(\n            f\"{self.table_id.value}: Assigning {source_ferc1.value} source utility IDs.\"\n        )\n        utility_map_ferc1 = pudl.glue.ferc1_eia.get_utility_map_ferc1()\n        # use the source utility ID column to get a unique map and for merging\n        util_id_col = f\"utility_id_ferc1_{source_ferc1.value}\"\n        util_map_series = (\n            utility_map_ferc1.dropna(subset=[util_id_col])\n            .set_index(util_id_col)\n            .utility_id_ferc1\n        )\n\n        df[\"utility_id_ferc1\"] = df[util_id_col].map(util_map_series)\n        return df\n\n    def reconcile_table_calculations(\n        self: Self,\n        df: pd.DataFrame,\n        params: ReconcileTableCalculations | None = None,\n    ):\n        \"\"\"Check how well a table's calculated values match reported values.\"\"\"\n        if params is None:\n            params = self.params.reconcile_table_calculations\n        if params.column_to_check:\n            if self.xbrl_calculations is None:\n                raise AssertionError(\n                    \"No calculations table has been built. Must run process_xbrl_metadata_calculations\"\n                )\n            logger.info(\n                f\"{self.table_id.value}: Checking the XBRL metadata-based calculations.\"\n            )\n            df = reconcile_table_calculations(\n                df=df,\n                calculation_components=self.xbrl_calculations,\n                xbrl_factoid_name=self.params.xbrl_factoid_name,\n                xbrl_metadata=self.xbrl_metadata,\n                table_name=self.table_id.value,\n                params=params,\n            )\n        return df\n\n\nclass SteamPlantsFuelTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"A table transformer specific to the :ref:`core_ferc1__yearly_steam_plants_fuel_sched402` table.\n\n    The :ref:`core_ferc1__yearly_steam_plants_fuel_sched402` table reports data about fuel consumed by large thermal power\n    plants in the :ref:`core_ferc1__yearly_steam_plants_sched402` table. Each record in the steam table is\n    typically associated with several records in the fuel table, with each fuel record\n    reporting data for a particular type of fuel consumed by that plant over the course\n    of a year. The fuel table presents several challenges.\n\n    The type of fuel, which is part of the primary key for the table, is a freeform\n    string with hundreds of different nonstandard values. These strings are categorized\n    manually and converted to ``fuel_type_code_pudl``. Some values cannot be categorized\n    and are set to ``other``. In other string categorizations we set the unidentifiable\n    values to NA, but in this table the fuel type is part of the primary key and primary\n    keys cannot contain NA values.\n\n    This simplified categorization occasionally results in records with duplicate\n    primary keys. In those cases the records are aggregated into a single record if they\n    have the same apparent physical units. If the fuel units are different, only the\n    first record is retained.\n\n    Several columns have unspecified, inconsistent, fuel-type specific units of measure\n    associated with them. In order for records to be comparable and aggregatable, we\n    have to infer and standardize these units.\n\n    In the raw FERC Form 1 data there is a ``fuel_units`` column which describes the\n    units of fuel delivered or consumed. Most commonly this is short tons for solid\n    fuels (coal), thousands of cubic feet (Mcf) for gaseous fuels, and barrels (bbl) for\n    liquid fuels.  However, the ``fuel_units`` column is also a freeform string with\n    hundreds of nonstandard values which we have to manually categorize, and many of the\n    values do not map directly to the most commonly used units for fuel quantities. E.g.\n    some solid fuel quantities are reported in pounds, or thousands of pounds, not tons;\n    some liquid fuels are reported in gallons or thousands of gallons, not barrels; and\n    some gaseous fuels are reported in cubic feet not thousands of cubic feet.\n\n    Two additional columns report fuel price per unit of heat content and fuel heat\n    content per physical unit of fuel. The units of those columns are not explicitly\n    reported, vary by fuel, and are inconsistent within individual fuel types.\n\n    We adopt standardized units and attempt to convert all reported values in the fuel\n    table into those units. For physical fuel units we adopt those that are used by the\n    EIA: short tons (tons) for solid fuels, barrels (bbl) for liquid fuels, and\n    thousands of cubic feet (mcf) for gaseous fuels. For heat content per (physical)\n    unit of fuel, we use millions of British thermal units (mmbtu). All fuel prices are\n    converted to US dollars, while many are reported in cents.\n\n    Because the reported fuel price and heat content units are implicit, we have to\n    infer them based on observed values. This is only possible because these quantities\n    are ratios with well defined ranges of valid values. The common units that we\n    observe and attempt to standardize include:\n\n    * coal: primarily BTU/pound, but also MMBTU/ton and MMBTU/pound.\n    * oil: primarily BTU/gallon.\n    * gas: reported in a mix of MMBTU/cubic foot, and MMBTU/thousand cubic feet.\n    \"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.STEAM_PLANTS_FUEL\n\n    @cache_df(key=\"main\")\n    def transform_main(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Table specific transforms for core_ferc1__yearly_steam_plants_fuel_sched402.\n\n        Args:\n            df: Pre-processed, concatenated XBRL and DBF data.\n\n        Returns:\n            A single transformed table concatenating multiple years of cleaned data\n            derived from the raw DBF and/or XBRL inputs.\n        \"\"\"\n        return (\n            self.spot_fix_values(df)\n            .pipe(self.drop_invalid_rows)\n            .pipe(self.correct_units)\n        )\n\n    @cache_df(key=\"dbf\")\n    def process_dbf(self, raw_dbf: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Start with inherited method and do some fuel-specific processing.\n\n        We have to do most of the transformation before the DBF and XBRL data have been\n        concatenated because the fuel type column is part of the primary key and it is\n        extensively modified in the cleaning process.\n        \"\"\"\n        df = (\n            super()\n            .process_dbf(raw_dbf)\n            .pipe(self.to_numeric)\n            .pipe(self.convert_units)\n            .pipe(self.normalize_strings)\n            .pipe(self.categorize_strings)\n            .pipe(self.standardize_physical_fuel_units)\n        )\n        return df\n\n    @cache_df(key=\"xbrl\")\n    def process_xbrl(\n        self, raw_xbrl_instant: pd.DataFrame, raw_xbrl_duration: pd.DataFrame\n    ) -> pd.DataFrame:\n        \"\"\"Special pre-concat treatment of the :ref:`core_ferc1__yearly_steam_plants_fuel_sched402` table.\n\n        We have to do most of the transformation before the DBF and XBRL data have been\n        concatenated because the fuel type column is part of the primary key and it is\n        extensively modified in the cleaning process. For the XBRL data, this means we\n        can't create a record ID until that fuel type value is clean. In addition, the\n        categorization of fuel types results in a number of duplicate fuel records which\n        need to be aggregated.\n\n        Args:\n            raw_xbrl_instant: Freshly extracted XBRL instant fact table.\n            raw_xbrl_duration: Freshly extracted XBRL duration fact table.\n\n        Returns:\n            Almost fully transformed XBRL data table, with instant and duration facts\n            merged together.\n        \"\"\"\n        return (\n            self.merge_instant_and_duration_tables_xbrl(\n                raw_xbrl_instant, raw_xbrl_duration\n            )\n            .pipe(self.rename_columns, rename_stage=\"xbrl\")\n            .pipe(self.to_numeric)\n            .pipe(self.convert_units)\n            .pipe(self.normalize_strings)\n            .pipe(self.categorize_strings)\n            .pipe(self.standardize_physical_fuel_units)\n            .pipe(self.aggregate_duplicate_fuel_types_xbrl)\n            .pipe(self.assign_record_id, source_ferc1=SourceFerc1.XBRL)\n            .pipe(\n                self.assign_utility_id_ferc1,\n                source_ferc1=SourceFerc1.XBRL,\n            )\n        )\n\n    def to_numeric(self: Self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Convert columns containing numeric strings to numeric types.\"\"\"\n        numeric_cols = [\n            \"fuel_consumed_units\",\n            \"fuel_cost_per_unit_burned\",\n            \"fuel_cost_per_unit_delivered\",\n            \"fuel_cost_per_mmbtu\",\n        ]\n        for col in numeric_cols:\n            df[col] = pd.to_numeric(df[col])\n\n        return df\n\n    def standardize_physical_fuel_units(self: Self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Convert reported fuel quantities to standard units depending on fuel type.\n\n        Use the categorized fuel type and reported fuel units to convert all fuel\n        quantities to the following standard units, depending on whether the fuel is a\n        solid, liquid, or gas. When a single fuel reports its quantity in fundamentally\n        different units, convert based on typical values. E.g. 19.85 MMBTU per ton of\n        coal, 1.037 Mcf per MMBTU of natural gas, 7.46 barrels per ton of oil.\n\n          * solid fuels (coal and waste): short tons [ton]\n          * liquid fuels (oil): barrels [bbl]\n          * gaseous fuels (gas): thousands of cubic feet [mcf]\n\n        Columns to which these physical units apply:\n\n          * fuel_consumed_units (tons, bbl, mcf)\n          * fuel_cost_per_unit_burned (usd/ton, usd/bbl, usd/mcf)\n          * fuel_cost_per_unit_delivered (usd/ton, usd/bbl, usd/mcf)\n\n        One remaining challenge in this standardization is that nuclear fuel is reported\n        in both mass of Uranium and fuel heat content, and it's unclear if there's any\n        reasonable typical conversion between these units, since available heat content\n        depends on the degree of U235 enrichement, the type of reactor, and whether the\n        fuel is just Uranium, or a mix of Uranium and Plutonium from decommissioned\n        nuclear weapons. See:\n\n        https://world-nuclear.org/information-library/facts-and-figures/heat-values-of-various-fuels.aspx\n        \"\"\"\n        df = df.copy()\n\n        FuelFix = namedtuple(\"FuelFix\", [\"fuel\", \"from_unit\", \"to_unit\", \"mult\"])\n        fuel_fixes = [\n            # US average coal heat content is 19.85 mmbtu/short ton\n            FuelFix(\"coal\", \"mmbtu\", \"ton\", (1.0 / 19.85)),\n            FuelFix(\"coal\", \"btu\", \"ton\", (1.0 / 19.85e6)),\n            # 2000 lbs per short ton\n            FuelFix(\"coal\", \"lbs\", \"ton\", (1.0 / 2000.0)),\n            FuelFix(\"coal\", \"klbs\", \"ton\", (1.0 / 2.0)),\n            # 42 gallons per barrel. Seriously, who makes up these units?\n            FuelFix(\"oil\", \"gal\", \"bbl\", (1.0 / 42.0)),\n            FuelFix(\"oil\", \"kgal\", \"bbl\", (1000.0 / 42.0)),\n            # On average a \"ton of oil equivalent\" is 7.46 barrels\n            FuelFix(\"oil\", \"ton\", \"bbl\", 7.46),\n            FuelFix(\"gas\", \"mmbtu\", \"mcf\", (1.0 / 1.037)),\n            # Nuclear plants report either heat content or mass of heavy metal\n            # MW*days thermal to MWh thermal\n            FuelFix(\"nuclear\", \"mwdth\", \"mwhth\", 24.0),\n            # Straight energy equivalence between BTU and MWh here:\n            FuelFix(\"nuclear\", \"mmbtu\", \"mwhth\", (1.0 / 3.412142)),\n            FuelFix(\"nuclear\", \"btu\", \"mwhth\", (1.0 / 3412142)),\n            # Unclear if it's possible to convert heavy metal to heat reliably\n            FuelFix(\"nuclear\", \"grams\", \"kg\", (1.0 / 1000)),\n        ]\n        for fix in fuel_fixes:\n            fuel_mask = df.fuel_type_code_pudl == fix.fuel\n            unit_mask = df.fuel_units == fix.from_unit\n            df.loc[(fuel_mask & unit_mask), \"fuel_consumed_units\"] *= fix.mult\n            # Note: The 2 corrections below DIVIDE by the multiplier because the units\n            # are in the denominator (\"per_unit\") rather than the numerator.\n            df.loc[(fuel_mask & unit_mask), \"fuel_cost_per_unit_burned\"] /= fix.mult\n            df.loc[(fuel_mask & unit_mask), \"fuel_cost_per_unit_delivered\"] /= fix.mult\n            df.loc[(fuel_mask & unit_mask), \"fuel_units\"] = fix.to_unit\n\n        # Set all remaining non-standard units and affected columns to NA.\n        FuelAllowedUnits = namedtuple(\"FuelAllowedUnits\", [\"fuel\", \"allowed_units\"])\n        fuel_allowed_units = [\n            FuelAllowedUnits(\"coal\", (\"ton\",)),\n            FuelAllowedUnits(\"oil\", (\"bbl\",)),\n            FuelAllowedUnits(\"gas\", (\"mcf\",)),\n            FuelAllowedUnits(\"nuclear\", (\"kg\", \"mwhth\")),\n            FuelAllowedUnits(\"waste\", (\"ton\",)),\n            # All unidentified fuel types (\"other\") get units set to NA\n            FuelAllowedUnits(\"other\", ()),\n        ]\n        physical_units_cols = [\n            \"fuel_consumed_units\",\n            \"fuel_cost_per_unit_burned\",\n            \"fuel_cost_per_unit_delivered\",\n        ]\n        for fau in fuel_allowed_units:\n            fuel_mask = df.fuel_type_code_pudl == fau.fuel\n            invalid_unit_mask = ~df.fuel_units.isin(fau.allowed_units)\n            df.loc[(fuel_mask & invalid_unit_mask), physical_units_cols] = np.nan\n            df.loc[(fuel_mask & invalid_unit_mask), \"fuel_units\"] = pd.NA\n\n        return df\n\n    @cache_df(key=\"xbrl\")\n    def aggregate_duplicate_fuel_types_xbrl(\n        self, fuel_xbrl: pd.DataFrame\n    ) -> pd.DataFrame:\n        \"\"\"Aggregate the fuel records having duplicate primary keys.\"\"\"\n        pk_cols = self.renamed_table_primary_key(source_ferc1=SourceFerc1.XBRL) + [\n            \"sched_table_name\"\n        ]\n        fuel_xbrl.loc[:, \"fuel_units_count\"] = fuel_xbrl.groupby(pk_cols, dropna=False)[\n            \"fuel_units\"\n        ].transform(\"nunique\")\n\n        # split\n        dupe_mask = fuel_xbrl.duplicated(subset=pk_cols, keep=False)\n        multi_unit_mask = fuel_xbrl.fuel_units_count != 1\n\n        fuel_pk_dupes = fuel_xbrl[dupe_mask & ~multi_unit_mask].copy()\n        fuel_multi_unit = fuel_xbrl[dupe_mask & multi_unit_mask].copy()\n        fuel_non_dupes = fuel_xbrl[~dupe_mask & ~multi_unit_mask]\n\n        logger.info(\n            f\"{self.table_id.value}: Aggregating {len(fuel_pk_dupes)} rows with \"\n            f\"duplicate primary keys out of {len(fuel_xbrl)} total rows.\"\n        )\n        logger.info(\n            f\"{self.table_id.value}: Dropping {len(fuel_multi_unit)} records with \"\n            \"inconsistent fuel units preventing aggregation \"\n            f\"out of {len(fuel_xbrl)} total rows.\"\n        )\n        agg_row_fraction = (len(fuel_pk_dupes) + len(fuel_multi_unit)) / len(fuel_xbrl)\n        if agg_row_fraction > 0.15:\n            logger.error(\n                f\"{self.table_id.value}: {agg_row_fraction:.0%} of all rows are being \"\n                \"aggregated. Higher than the allowed value of 15%!\"\n            )\n        data_cols = [\n            \"fuel_consumed_units\",\n            \"fuel_mmbtu_per_unit\",\n            \"fuel_cost_per_unit_delivered\",\n            \"fuel_cost_per_unit_burned\",\n            \"fuel_cost_per_mmbtu\",\n            \"fuel_cost_per_mwh\",\n            \"fuel_mmbtu_per_mwh\",\n        ]\n        # apply\n        fuel_pk_dupes = pudl.helpers.sum_and_weighted_average_agg(\n            df_in=fuel_pk_dupes,\n            by=pk_cols + [\"start_date\", \"end_date\", \"fuel_units\"],\n            sum_cols=[\"fuel_consumed_units\"],\n            wtavg_dict={\n                k: \"fuel_consumed_units\"\n                for k in data_cols\n                if k != \"fuel_consumed_units\"\n            },\n        )\n        # We can't aggregate data when fuel units are inconsistent, but we don't want\n        # to lose the records entirely, so we'll keep the first one.\n        fuel_multi_unit.loc[:, data_cols] = np.nan\n        fuel_multi_unit = fuel_multi_unit.drop_duplicates(subset=pk_cols, keep=\"first\")\n        # combine\n        return pd.concat([fuel_non_dupes, fuel_pk_dupes, fuel_multi_unit]).drop(\n            columns=[\"fuel_units_count\"]\n        )\n\n    def drop_total_rows(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Drop rows that represent plant totals rather than individual fuels.\n\n        This is an imperfect, heuristic process. The rows we identify as probably\n        representing totals rather than individual fuels:\n\n        * have zero or null values in all of their numerical data columns\n        * have no identifiable fuel type\n        * have no identifiable fuel units\n        * DO report a value for MMBTU / MWh (heat rate)\n\n        In the case of the core_ferc1__yearly_steam_plants_fuel_sched402 table, we drop any row where all the data columns\n        are null AND there's a non-null value in the ``fuel_mmbtu_per_mwh`` column, as\n        it typically indicates a \"total\" row for a plant. We also require a null value\n        for the fuel_units and an \"other\" value for the fuel type.\n        \"\"\"\n        data_cols = [\n            \"fuel_consumed_units\",\n            \"fuel_mmbtu_per_unit\",\n            \"fuel_cost_per_unit_delivered\",\n            \"fuel_cost_per_unit_burned\",\n            \"fuel_cost_per_mmbtu\",\n            \"fuel_cost_per_mwh\",\n        ]\n        total_rows_idx = df[\n            df[data_cols].isna().all(axis=\"columns\")  # No normal numerical data\n            & df.fuel_units.isna()  # no recognizable fuel units\n            & (df.fuel_type_code_pudl == \"other\")  # No recognizable fuel type\n            & df.fuel_mmbtu_per_mwh.notna()  # But it DOES report heat rate!\n        ].index\n        logger.info(\n            f\"{self.table_id.value}: Dropping \"\n            f\"{len(total_rows_idx)}/{len(df)}\"\n            \"rows representing plant-level all-fuel totals.\"\n        )\n        return df.drop(index=total_rows_idx)\n\n    def drop_invalid_rows(\n        self, df: pd.DataFrame, params: InvalidRows | None = None\n    ) -> pd.DataFrame:\n        \"\"\"Drop invalid rows from the fuel table.\n\n        This method both drops rows in which all required data columns are null (using\n        the inherited parameterized method) and then also drops those rows we believe\n        represent plant totals. See :meth:`SteamPlantsFuelTableTransformer.drop_total_rows`.\n        \"\"\"\n        return super().drop_invalid_rows(df, params).pipe(self.drop_total_rows)\n\n\nclass SteamPlantsTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for the :ref:`core_ferc1__yearly_steam_plants_sched402` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.STEAM_PLANTS\n\n\nclass HydroelectricPlantsTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"A table transformer specific to the :ref:`core_ferc1__yearly_hydroelectric_plants_sched406` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.HYDROELECTRIC_PLANTS\n\n    def transform_main(self, df):\n        \"\"\"Add bespoke removal of duplicate record after standard transform_main.\"\"\"\n        return super().transform_main(df).pipe(self.targeted_drop_duplicates)\n\n    def targeted_drop_duplicates(self, df):\n        \"\"\"Targeted removal of known duplicate record.\n\n        There are two records in 2019 with a ``utility_id_ferc1`` of 200 and a\n        ``plant_name_ferc1`` of \"marmet\". The records are nearly duplicates of\n        eachother, except one have nulls in the capex columns. Surgically remove the\n        record with the nulls.\n        \"\"\"\n        null_columns = [\n            \"capex_land\",\n            \"capex_structures\",\n            \"capex_facilities\",\n            \"capex_equipment\",\n            \"capex_roads\",\n            \"asset_retirement_cost\",\n            \"capex_total\",\n            \"capex_per_mw\",\n        ]\n        dupe_mask = (\n            (df.report_year == 2019)\n            & (df.utility_id_ferc1 == 200)\n            & (df.plant_name_ferc1 == \"marmet\")\n        )\n        null_maks = df[null_columns].isnull().all(axis=\"columns\")\n\n        possible_dupes = df.loc[dupe_mask]\n        if (len(possible_dupes) != 2) & (2019 in df.report_year.unique()):\n            raise AssertionError(\n                f\"{self.table_id}: Expected 2 records for found: {possible_dupes}\"\n            )\n        dropping = df.loc[(dupe_mask & null_maks)]\n        logger.debug(\n            f\"Dropping {len(dropping)} duplicate record with null data in {null_columns}\"\n        )\n        df = df.loc[~(dupe_mask & null_maks)].copy()\n        return df\n\n\nclass PumpedStoragePlantsTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for :ref:`core_ferc1__yearly_pumped_storage_plants_sched408` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.PUMPED_STORAGE_PLANTS\n\n\nclass PurchasedPowerAndExchangesTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for :ref:`core_ferc1__yearly_purchased_power_and_exchanges_sched326`.\n\n    This table has data about inter-utility power purchases into the PUDL DB. This\n    includes how much electricty was purchased, how much it cost, and who it was\n    purchased from. Unfortunately the field describing which other utility the power was\n    being bought from is poorly standardized, making it difficult to correlate with\n    other data. It will need to be categorized by hand or with some fuzzy matching\n    eventually.\n    \"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.PURCHASED_POWER_AND_EXCHANGES\n\n\nclass PlantInServiceTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"A transformer for the :ref:`core_ferc1__yearly_plant_in_service_sched204` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.PLANT_IN_SERVICE\n    has_unique_record_ids: bool = False\n\n    @cache_df(\"process_xbrl_metadata\")\n    def process_xbrl_metadata(\n        self: Self,\n        xbrl_metadata_converted: pd.DataFrame,\n        xbrl_calculations: pd.DataFrame,\n    ) -> pd.DataFrame:\n        \"\"\"Transform the metadata to reflect the transformed data.\n\n        We fill in some gaps in the metadata, e.g. for FERC accounts that have been\n        split across multiple rows, or combined without being calculated. We also need\n        to rename the XBRL metadata categories to conform to the same naming convention\n        that we are using in the data itself (since FERC doesn't quite follow their own\n        naming conventions...). We use the same rename dictionary, but as an argument to\n        :meth:`pd.Series.replace` instead of :meth:`pd.DataFrame.rename`.\n        \"\"\"\n        tbl_meta = super().process_xbrl_metadata(\n            xbrl_metadata_converted, xbrl_calculations\n        )\n\n        # Set pseudo-account numbers for rows that split or combine FERC accounts, but\n        # which are not calculated values.\n        tbl_meta.loc[\n            tbl_meta.xbrl_factoid == \"electric_plant_purchased\",\n            [\"ferc_account\", \"plant_status\"],\n        ] = [\"102_purchased\", pd.NA]\n        tbl_meta.loc[\n            tbl_meta.xbrl_factoid == \"electric_plant_sold\",\n            [\"ferc_account\", \"plant_status\"],\n        ] = [\"102_sold\", pd.NA]\n        tbl_meta.loc[\n            tbl_meta.xbrl_factoid\n            == \"electric_plant_in_service_and_completed_construction_not_classified_electric\",\n            \"ferc_account\",\n        ] = \"101_and_106\"\n        return tbl_meta\n\n    def deduplicate_xbrl_factoid_xbrl_metadata(\n        self, tbl_meta: pd.DataFrame\n    ) -> pd.DataFrame:\n        \"\"\"De-duplicate the XBLR metadata.\n\n        We deduplicate the metadata on the basis of the ``xbrl_factoid`` name.\n        This table in particular has multiple ``wide_to_tidy`` ``value_types`` because\n        there are multiple dollar columns embedded (it has both the standard start/end\n        balances as well as modifcations like transfers/retirements). In the XBRL\n        metadata, each xbrl_fact has its own set of metadata and possibly its own set of\n        calculations. Which means that one ``xbrl_factoid`` for this table natively\n        could have multiple calculations or other metadata.\n\n        For merging, we need the metadata to have one field per ``xbrl_factoid``.\n        Because we normally only use the start/end balance in calculations, when there\n        are duplicate renamed ``xbrl_factoid`` s in our processed metadata, we are going\n        to prefer the one that refers to the start/end balances. In an ideal world, we\n        would be able to access this metadata based on both the ``xbrl_factoid`` and\n        any column from ``value_types`` but that would require a larger change in\n        architecture.\n        \"\"\"\n        # remove duplication of xbrl_factoid\n        same_calcs_mask = tbl_meta.duplicated(\n            subset=[\"xbrl_factoid\", \"calculations\"], keep=False\n        )\n        # if they key values are the same, select the records with values in ferc_account\n        same_calcs_deduped = (\n            tbl_meta[same_calcs_mask]\n            .sort_values([\"ferc_account\"])  # sort brings the nulls to the bottom\n            .drop_duplicates(subset=[\"xbrl_factoid\"], keep=\"first\")\n        )\n        # when the calcs are different, they are referring to the non-adjustments\n        suffixes = (\"_additions\", \"_retirements\", \"_adjustments\", \"_transfers\")\n        unique_calcs_deduped = tbl_meta[\n            ~same_calcs_mask & (~tbl_meta.xbrl_factoid_original.str.endswith(suffixes))\n        ]\n        tbl_meta_cleaned = pd.concat([same_calcs_deduped, unique_calcs_deduped])\n        assert set(tbl_meta_cleaned.xbrl_factoid.unique()) == set(\n            tbl_meta.xbrl_factoid.unique()\n        )\n        assert ~tbl_meta_cleaned.duplicated([\"xbrl_factoid\"]).all()\n        return tbl_meta_cleaned\n\n    def apply_sign_conventions(self, df) -> pd.DataFrame:\n        \"\"\"Adjust rows and column sign conventsion to enable aggregation by summing.\n\n        Columns have uniform sign conventions, which we have manually inferred from the\n        original metadata. This can and probably should be done programmatically in the\n        future. If not, we'll probably want to store the column_weights as a parameter\n        rather than hard-coding it in here.\n        \"\"\"\n        column_weights = {\n            \"starting_balance\": 1.0,\n            \"additions\": 1.0,\n            \"retirements\": -1.0,\n            \"transfers\": 1.0,\n            \"adjustments\": 1.0,\n            \"ending_balance\": 1.0,\n        }\n\n        # Set row weights based on the value of the \"balance\" field\n        df.loc[df.balance == \"debit\", \"row_weight\"] = 1.0\n        df.loc[df.balance == \"credit\", \"row_weight\"] = -1.0\n\n        # Apply column weightings. Can this be done all at once in a vectorized way?\n        for col in column_weights:\n            df.loc[:, col] *= column_weights[col]\n            df.loc[:, col] *= df[\"row_weight\"]\n\n        return df\n\n    def targeted_drop_duplicates_dbf(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Drop bad duplicate records from a specific utility in 2018.\n\n        This is a very specific fix, meant to get rid of a particular observed set of\n        duplicate records: FERC Respondent ID 187 in 2018 has two sets of plant in\n        service records, one of which contains a bunch of null data.\n\n        This method is part of the DBF processing because we want to be able to\n        hard-code a specific value of ``utility_id_ferc1_dbf`` and those IDs are no\n        longer available later in the process. I think.\n        \"\"\"\n        # A single utility has double reported data in 2018.\n        pk = [\"report_year\", \"utility_id_ferc1\", \"ferc_account_label\"]\n        dupe_mask = (\n            df.duplicated(subset=pk, keep=False)\n            & (df.report_year == 2018)\n            & (df.utility_id_ferc1_dbf == 187)\n        )\n        all_dupes = df[dupe_mask]\n        # The observed pairs of duplicate records have NA values in all of the\n        # additions, retirements, adjustments, and transfers columns. This selects\n        # only those duplicates that have *any* non-null value in those rows.\n        good_dupes = all_dupes[\n            all_dupes[[\"additions\", \"retirements\", \"adjustments\", \"transfers\"]]\n            .notnull()\n            .any(axis=\"columns\")\n        ]\n        # Make sure that the good and bad dupes have exactly the same indices:\n        pd.testing.assert_index_equal(\n            good_dupes.set_index(pk).index,\n            all_dupes.set_index(pk).index.drop_duplicates(),\n        )\n        deduped = pd.concat([df[~dupe_mask], good_dupes], axis=\"index\")\n        remaining_dupes = deduped[deduped.duplicated(subset=pk)]\n        logger.info(\n            f\"{self.table_id.value}: {len(remaining_dupes)} dupes remaining after \"\n            \"targeted deduplication.\"\n        )\n        return deduped\n\n    @cache_df(key=\"dbf\")\n    def process_dbf(self, raw_dbf: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Drop targeted duplicates in the DBF data so we can use FERC respondent ID.\"\"\"\n        return super().process_dbf(raw_dbf).pipe(self.targeted_drop_duplicates_dbf)\n\n    @cache_df(key=\"main\")\n    def transform_main(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"The main table-specific transformations, affecting contents not structure.\n\n        Annotates and alters data based on information from the XBRL taxonomy metadata.\n        Also assigns utility type, plant status & function for use in table explosions.\n        Make all electric_plant_sold balances positive.\n        \"\"\"\n        df = super().transform_main(df).pipe(self.apply_sign_conventions)\n        # Make all electric_plant_sold values positive\n        # This could probably be a FERC transformer class function or in the\n        # apply_sign_conventions function, but it doesn't seem like the best fit for\n        # now.\n        neg_values = (df[\"ferc_account_label\"] == \"electric_plant_sold\") & (\n            df[\"ending_balance\"] < 0\n        )\n        df.loc[neg_values, \"ending_balance\"] = abs(df[\"ending_balance\"])\n        logger.info(\n            f\"{self.table_id.value}: Converted {len(df[neg_values])} negative values to positive.\"\n        )\n        # Add two exceptions for plant status\n        df.loc[\n            df.ferc_account_label.isin(\n                [\"electric_plant_sold\", \"electric_plant_purchased\"]\n            )\n        ].plant_status = pd.NA\n        return df\n\n\nclass SmallPlantsTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"A table transformer specific to the :ref:`core_ferc1__yearly_small_plants_sched410` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.SMALL_PLANTS\n\n    @cache_df(key=\"main\")\n    def transform_main(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Table specific transforms for core_ferc1__yearly_small_plants_sched410.\n\n        Params:\n            df: Pre-processed, concatenated XBRL and DBF data.\n\n        Returns:\n            A single transformed table concatenating multiple years of cleaned data\n            derived from the raw DBF and/or XBRL inputs.\n        \"\"\"\n        df = (\n            self.spot_fix_values(df)\n            .pipe(self.normalize_strings)\n            .pipe(self.nullify_outliers)\n            .pipe(self.convert_units)\n            .pipe(self.extract_ferc1_license)\n            .pipe(self.label_row_types)\n            .pipe(self.prep_header_fuel_and_plant_types)\n            .pipe(self.map_plant_name_fuel_types)\n            .pipe(self.categorize_strings)\n            .pipe(self.map_header_fuel_and_plant_types)\n            .pipe(self.associate_notes_with_values)\n            .pipe(self.spot_fix_rows)\n            .pipe(self.drop_invalid_rows)\n            # Now remove the row_type columns because we've already moved totals to a\n            # different column\n            .drop(columns=[\"row_type\"])\n        )\n\n        return df\n\n    def extract_ferc1_license(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Extract FERC license number from ``plant_name_ferc1``.\n\n        Many FERC license numbers are embedded in the ``plant_name_ferc1`` column, but\n        not all numbers in the ``plant_name_ferc1`` column are FERC licenses. Some are\n        dates, dollar amounts, page numbers, or numbers of wind turbines. This function\n        extracts valid FERC license numbers and puts them in a new column called\n        ``license_id_ferc1``.\n\n        Potential FERC license numbers are valid when:\n\n        - Two or more integers were found.\n        - The found integers were accompanied by key phrases such as:\n          ``[\"license\", \"no.\", \"ferc\", \"project\"]``.\n        - The accompanying name does not contain phrases such as:\n          ``[\"page\", \"pg\", \"$\",  \"wind\", \"units\"]``.\n        - The found integers don't fall don't fall within the range of a valid year,\n          defined as: 1900-2050.\n        - The plant record is categorized as ``hydro`` or not categorized via the\n          ``plant_type`` and ``fuel_type`` columns.\n\n        This function also fills ``other`` fuel types with ``hydro`` for all plants with\n        valid FERC licenses because only hydro plants have FERC licenses.\n\n        Params:\n            df: Pre-processed, concatenated XBRL and DBF data.\n\n        Returns:\n            The same input DataFrame but with a new column called ``license_id_ferc1``\n            that contains FERC 1 license infromation extracted from\n            ``plant_name_ferc1``.\n        \"\"\"\n        logger.info(f\"{self.table_id.value}: Extracting FERC license from plant name\")\n        # Extract all numbers greater than 2 digits from plant_name_ferc1 and put them\n        # in a new column as integers.\n        out_df = df.assign(\n            license_id_ferc1=lambda x: (\n                x.plant_name_ferc1.str.extract(r\"(\\d{3,})\")\n                .astype(\"float\")\n                .astype(\"Int64\")\n            ),\n        )\n        # Define what makes a good license\n        obvious_license = out_df.plant_name_ferc1.str.contains(\n            r\"no\\.|license|ferc|project\", regex=True\n        )\n        not_license = out_df.plant_name_ferc1.str.contains(\n            r\"page|pg|\\$|wind|solar|nuclear|nonutility|units|surrendered\", regex=True\n        )\n        exceptions_to_is_year = out_df.plant_name_ferc1.str.contains(\n            r\"tomahawk|otter rapids|wausau|alexander|hooksett|north umpqua\", regex=True\n        )\n        is_year = out_df[\"license_id_ferc1\"].between(1900, 2050)\n        not_hydro = ~out_df[\"plant_type\"].isin([\"hydro\", np.nan, None]) | ~out_df[\n            \"fuel_type\"\n        ].isin([\"hydro\", \"other\"])\n        # Replace all the non-license numbers with NA\n        out_df.loc[\n            (not_hydro & ~obvious_license)\n            | not_license\n            | (is_year & ~obvious_license & ~exceptions_to_is_year),\n            \"license_id_ferc1\",\n        ] = np.nan\n        # Fill fuel type with hydro\n        out_df.loc[\n            out_df[\"license_id_ferc1\"].notna() & (out_df[\"fuel_type\"] == \"other\"),\n            \"fuel_type\",\n        ] = \"hydro\"\n\n        return out_df\n\n    def _find_possible_header_or_note_rows(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Find and label rows that might be headers or notes.\n\n        Called by the coordinating function :func:`label_row_types`.\n\n        This function creates a column called ``possible_header_or_note`` that is either\n        True or False depending on whether a group of columns are all NA. Rows labeled\n        as True will be further scrutinized in the :func:`_label_header_rows` and\n        :func:`_label_note_rows` functions to determine whether they are actually\n        headers or notes.\n\n        Params:\n            df: Pre-processed, concatenated XBRL and DBF data.\n\n        Returns:\n            The same input DataFrame but with a new column called\n            ``possible_header_or_note`` that flags rows that might contain useful header\n            or note information.\n        \"\"\"\n        # Define header qualifications\n        possible_header_or_note_if_cols_na = [\n            \"construction_year\",\n            \"net_generation_mwh\",\n            \"total_cost_of_plant\",\n            \"capex_total\",\n            \"capex_per_mw\",\n            \"opex_total\",\n            \"opex_fuel\",\n            \"opex_maintenance\",\n            \"fuel_cost_per_mmbtu\",\n            # \"peak_demand_mw\",\n            # \"opex_operations\"\n        ]\n        # Label possible header or note rows\n        df[\"possible_header_or_note\"] = (\n            df.filter(possible_header_or_note_if_cols_na).isna().all(1)\n        )\n        return df\n\n    def _find_note_clumps(\n        self, group: DataFrameGroupBy\n    ) -> tuple[DataFrameGroupBy, pd.DataFrame]:\n        \"\"\"Find groups of rows likely to be notes.\n\n        Once the :func:`_find_possible_header_or_note_rows` function identifies rows\n        that are either headers or notes, we must deterine which one they are. As\n        described in the :func:`_label_note_rows` function, notes rows are usually\n        adjecent rows with no content.\n\n        This function itentifies instances of two or more adjecent rows where\n        ``possible_header_or_note`` = True. It takes individual utility-year groups as a\n        parameter as opposed to the entire dataset because adjecent rows are only\n        meaningful if they are from the same reporting entity in the same year. If we\n        were to run this on the whole dataframe, we would see \"note clumps\" that are\n        actually notes from the end of one utility's report and headers from the\n        beginning of another. For this reason, we run this function from within the\n        :func:`_label_note_rows_group` function.\n\n        The output of this function is not a modified version of the original\n        utility-year group, rather, it is a DataFrame containing information about the\n        nature of the ``possible_header_or_note`` = True rows that is used to determine\n        if that row is a note or not. It also returns the original utility-year-group as\n        groupby objects seperated by each time ``possible_header_or_note`` changes from\n        True to False or vice versa.\n\n        If you pass in the following df:\n\n        +-------------------+-------------------------+\n        | plant_name_ferc1  | possible_header_or_note |\n        +===================+=========================+\n        | HYDRO:            | True                    |\n        +-------------------+-------------------------+\n        | rainbow falls (b) | False                   |\n        +-------------------+-------------------------+\n        | cadyville (a)     | False                   |\n        +-------------------+-------------------------+\n        | keuka (c)         | False                   |\n        +-------------------+-------------------------+\n        | (a) project #2738 | True                    |\n        +-------------------+-------------------------+\n        | (b) project #2835 | True                    |\n        +-------------------+-------------------------+\n        | (c) project #2852 | True                    |\n        +-------------------+-------------------------+\n\n        You will get the following output (in addition to the groupby objects for each\n        clump):\n\n        +----------------+----------------+\n        | header_or_note | rows_per_clump |\n        +================+================+\n        | True           | 1              |\n        +----------------+----------------+\n        | False          | 3              |\n        +----------------+----------------+\n        | True           | 3              |\n        +----------------+----------------+\n\n        This shows each clump of adjecent records where ``possible_header_or_note`` is\n        True or False and how many records are in each clump.\n\n        Params:\n            group: A utility-year grouping of the concatenated FERC XBRL and DBF tables.\n                This table must have been run through the\n                :func:`_find_possible_header_or_note_rows` function and contain the\n                column ``possible_header_or_note``.\n\n        Returns:\n            A tuple containing groupby objects for each of the note and non-note clumps\n            and a DataFrame indicating the number of rows in each note or non-note\n            clump.\n        \"\"\"\n        # Make groups based on consecutive sections where the group_col is alike.\n        clump_groups = group.groupby(\n            (\n                group[\"possible_header_or_note\"].shift()\n                != group[\"possible_header_or_note\"]\n            ).cumsum(),\n            as_index=False,\n        )\n\n        # Identify the first (and only) group_col value for each group and count\n        # how many rows are in each group.\n        clump_groups_df = clump_groups.agg(\n            header_or_note=(\"possible_header_or_note\", \"first\"),\n            rows_per_clump=(\"possible_header_or_note\", \"count\"),\n        )\n\n        return clump_groups, clump_groups_df\n\n    def _label_header_rows(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Label header rows by adding ``header`` to ``row_type`` column.\n\n        Called by the coordinating function :func:`label_row_types`.\n\n        Once possible header or notes rows have been identified via the\n        :func:`_find_possible_header_or_note_rows` function, this function sorts out\n        which ones are headers. It does this by identifying a list of strings that, when\n        found in the ``plant_name_ferc1`` column, indicate that the row is or is not a\n        header.\n\n        Sometimes this function identifies a header that is acutally a note. For this\n        reason, it's important that the function be called before\n        :func:`_label_note_rows` so that the bad header values get overridden by the\n        ``note`` designation.\n\n        Params:\n            df: Pre-processed, concatenated XBRL and DBF data that has been run through\n            the :func:`_find_possible_header_or_note_rows` function and contains the\n            column ``possible_header_or_note``.\n\n        Returns:\n            The same input DataFrame but with likely headers rows containing the string\n            ``header`` in the ``row_type`` column.\n        \"\"\"\n        # Possible headers/note rows that contains these strings are headers\n        header_strings = [\n            \"hydro\",\n            \"hyrdo\",\n            \"internal\",\n            \"wind\",\n            \"solar\",\n            \"gas\",\n            \"diesel\",\n            \"diesal\",\n            \"steam\",\n            \"other\",\n            \"combustion\",\n            \"combustine\",\n            \"fuel cell\",\n            \"hydraulic\",\n            \"waste\",\n            \"landfill\",\n            \"photovoltaic\",\n            \"nuclear\",\n            \"oil\",\n            \"renewable\",\n            \"facilities\",\n            \"combined cycle\",\n        ]\n        # Possible headers/note rows that contains these strings are not headers\n        nonheader_strings = [\n            \"#\",\n            r\"\\*\",\n            \"pg\",\n            \"solargenix\",\n            \"solargennix\",\n            r\"\\@\",\n            \"rockton\",\n            \"albany steam\",\n            \"other general ops. supervision & engineering\",\n        ]\n        # Any rows that contains these strings are headers\n        header_exceptions = [\n            \"hydro plants: licensed proj. no.\",\n            \"hydro license no.\",\n            \"hydro: license no.\",\n            \"hydro plants: licensed proj no.\",\n            \"photo voltaic generating plants:\",\n        ]\n\n        logger.info(f\"{self.table_id.value}: Labeling header rows\")\n\n        # Label good header rows (based on whether they contain key strings)\n        possible_header = df[\"possible_header_or_note\"]\n        good_header = df[\"plant_name_ferc1\"].str.contains(\"|\".join(header_strings))\n        bad_header = df[\"plant_name_ferc1\"].str.contains(\"|\".join(nonheader_strings))\n        df.loc[possible_header & good_header & ~bad_header, \"row_type\"] = \"header\"\n        # There are some headers that don't pass the possible_header test but are\n        # still definitely headers.\n        df.loc[df[\"plant_name_ferc1\"].isin(header_exceptions), \"row_type\"] = \"header\"\n\n        return df\n\n    def _label_note_rows_group(\n        self, util_year_group: DataFrameGroupBy\n    ) -> DataFrameGroupBy:\n        \"\"\"Label note rows by adding ``note`` to ``row_type`` column.\n\n        Called within the wraper function :func:`_label_note_rows`\n\n        This function breaks the data down by reporting unit (utility and year) and\n        determines whether a ``possible_header_note`` = True row is a note based on two\n        criteria:\n\n        - Clumps of 2 or more adjecent rows where ``possible_header_or_note`` is True.\n        - Instances where the last row in a utility-year group has\n          ``possible_header_or_note`` as True.\n\n        There are a couple of important exceptions that this function also\n        addresses. Utilities often have multiple headers in a single utility-year\n        grouping. You might see something like: ``pd.Series([header, plant1, plant2,\n        note, header, plant3, plant4])``. In this case, a note clump is actually\n        comprised of a note followed by a header. This function will not override the\n        header as a note. Unfortunately, there is always the possability that a header\n        row is followed by a plant that had no values reported. This would look like,\n        and therefore be categorized as a note clump. I haven't built a work around, but\n        hopefully there aren't very many of these.\n\n        Params:\n            util_year_group: A groupby object that contains a single year and utility.\n\n        Returns:\n            The same input but with likely note rows containing the string ``note`` in\n            the ``row_type`` column.\n        \"\"\"\n        # Create mini groups that count pockets of true and false for each\n        # utility and year. See _find_note_clumps docstring.\n        clump_group, clump_count = self._find_note_clumps(util_year_group)\n\n        # Used later to enable exceptions\n        max_df_val = util_year_group.index.max()\n\n        # Create a list of the index values where there is a note clump! This also\n        # includes instances where the last row in a group is a note.\n        note_clump_idx_list = list(\n            clump_count[\n                (clump_count[\"header_or_note\"])\n                & (\n                    (clump_count[\"rows_per_clump\"] > 1)\n                    | (clump_count.tail(1)[\"rows_per_clump\"] == 1)\n                )\n            ].index\n        )\n        # If there are any clumped/end headers:\n        if note_clump_idx_list:\n            for idx in note_clump_idx_list:\n                # If the last row in a clump looks like a header, and the clump is\n                # not the last clump in the utility_year group, then drop the last\n                # row from the note clump index range because it's a header!\n                note_clump_idx_range = clump_group.groups[idx + 1]\n                not_last_clump = clump_group.groups[idx + 1].max() < max_df_val\n                is_good_header = (\n                    util_year_group.loc[\n                        util_year_group.index.isin(clump_group.groups[idx + 1])\n                    ]\n                    .tail(1)[\"row_type\"]\n                    .str.contains(\"header\")\n                    .all()\n                )\n                if not_last_clump & is_good_header:\n                    note_clump_idx_range = [\n                        x\n                        for x in note_clump_idx_range\n                        if x != note_clump_idx_range.max()\n                    ]\n                # Label the note clump as a note\n                util_year_group.loc[\n                    util_year_group.index.isin(note_clump_idx_range), \"row_type\"\n                ] = \"note\"\n\n        return util_year_group\n\n    def _label_note_rows(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Wrapper for :func:`_label_note_rows_group`.\n\n        The small plants table has lots of note rows that contain useful information.\n        Unfortunately, the notes are in their own row rather than their own column! This\n        means that useful information pertaining to plant rows is floating around as a\n        junk row with no other information except the note in the ``plant_name_ferc1``\n        field. Luckily, the data are reported just like they would be on paper. I.e.,\n        The headers are at the top, and the notes are at the bottom. See the table in\n        :func:`label_row_types` for more detail. This function labels note rows.\n\n        Note rows are determined by row location within a given report, so we must break\n        the data into reporting units (utility and year) and then apply note-finding\n        methodology defined in :func:`_label_note_rows_group` to each group.\n\n        Params:\n            df: Pre-processed, concatenated XBRL and DBF data that has been run through\n            the :func:`_find_possible_header_or_note_rows` function and contains the\n            column ``possible_header_or_note``.\n\n        Returns:\n            The same input DataFrame but with likely note rows containing the string\n            ``note`` in the ``row_type`` column.\n        \"\"\"\n        logger.info(f\"{self.table_id.value}: Labeling notes rows\")\n\n        util_groups = df.groupby([\"utility_id_ferc1\", \"report_year\"])\n\n        return util_groups.apply(lambda x: self._label_note_rows_group(x))\n\n    def _label_total_rows(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Label total rows by adding ``total`` to ``row_type`` column.\n\n        Called within the wraper function :func:`_label_note_rows`\n\n        For the most part, when ``plant_name_ferc1`` contains the string ``total``, the\n        values therein are duplicates of what is already reported, i.e.: a total value.\n        However, there are some cases where that's not true. For example, the phrase\n        ``amounts are for the total`` appears when chunks of plants (usually but not\n        always wind) are reported together. It's a total, but it's not double counting\n        which is the reason for the ``total`` flag.\n\n        Similar to :func:`_label_header_rows`, it's important that this be called before\n        :func:`_label_note_rows` in :func:`label_row_types` so that not clumps can\n        override certain non-totals that are mistakenly labeled as such.\n\n        Params:\n            df: Pre-processed, concatenated XBRL and DBF data.\n\n        Returns:\n            The same input DataFrame but with likely total rows containing the string\n            ``total`` in the ``row_type`` column.\n        \"\"\"\n        # Label totals in row_type in case it overwrites any headers\n        logger.info(f\"{self.table_id.value}: Labeling total rows\")\n        df.loc[\n            df[\"plant_name_ferc1\"].str.contains(\"total\")\n            & ~df[\"plant_name_ferc1\"].str.contains(\"amounts are for the total\"),\n            \"row_type\",\n        ] = \"total\"\n\n        # This one gets overridden by notes: total solar operation/maintenance\n\n        return df\n\n    def label_row_types(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Coordinate labeling of ``row_types`` as headers, notes, or totals.\n\n        The small plants table is more like a digitized PDF than an actual data table.\n        The rows contain all sorts of information in addition to what the columns might\n        suggest. For instance, there are header rows, note rows, and total rows that\n        contain useful information, but cause confusion in their current state, mixed in\n        with the rest of the data.\n\n        Here's an example of what you might find in the small plants table:\n\n        +-------------------+------------+-----------------+\n        | plant_name_ferc1  | plant_type | capacity_mw     |\n        +===================+============+=================+\n        | HYDRO:            | NA         | NA              |\n        +-------------------+------------+-----------------+\n        | rainbow falls (b) | NA         | 30              |\n        +-------------------+------------+-----------------+\n        | cadyville (a)     | NA         | 100             |\n        +-------------------+------------+-----------------+\n        | keuka (c)         | NA         | 80              |\n        +-------------------+------------+-----------------+\n        | total plants      | NA         | 310             |\n        +-------------------+------------+-----------------+\n        | (a) project #2738 | NA         | NA              |\n        +-------------------+------------+-----------------+\n        | (b) project #2835 | NA         | NA              |\n        +-------------------+------------+-----------------+\n        | (c) project #2852 | NA         | NA              |\n        +-------------------+------------+-----------------+\n\n        Notice how misleading it is to have all this infomration in one column. The\n        goal of this function is to coordinate labeling functions so that we can\n        identify which rows contain specific plant information and which rows are\n        headers, notes, or totals.\n\n        Once labeled, other functions can either remove rows that might cause double\n        counting, extract useful plant or fuel type information from headers, and\n        extract useful context or license id information from notes.\n\n        Coordinates :func:`_label_header_rows`, :func:`_label_total_rows`,\n        :func:`_label_note_rows`.\n\n        Params:\n            df: Pre-processed, concatenated XBRL and DBF data that has been run through\n            the :func:`_find_possible_header_or_note_rows` function and contains the\n            column ``possible_header_or_note``.\n\n        Returns:\n            The same input DataFrame but with a column called ``row_type`` containg the\n            strings ``header``, ``note``, ``total``, or NA to indicate what type of row\n            it is.\n        \"\"\"\n        # Add a column to show final row type\n        df.insert(3, \"row_type\", np.nan)\n\n        # Label the row types\n        df_labeled = (\n            df.pipe(self._find_possible_header_or_note_rows)\n            .pipe(self._label_header_rows)\n            .pipe(self._label_total_rows)\n            .pipe(self._label_note_rows)\n            .drop(columns=[\"possible_header_or_note\"])\n        )\n\n        # Move total lables to a different column\n        df_labeled.loc[df_labeled[\"row_type\"] == \"total\", \"is_total\"] = True\n        df_labeled[\"is_total\"] = df_labeled.filter([\"row_type\"]).isin([\"total\"]).all(1)\n\n        return df_labeled\n\n    def prep_header_fuel_and_plant_types(\n        self, df: pd.DataFrame, show_unmapped_headers=False\n    ) -> pd.DataFrame:\n        \"\"\"Forward fill header rows to prep for fuel and plant type extraction.\n\n        The headers we've identified in :func:`_label_header_rows` can be used to\n        supplement the values in the ``plant_type`` and ``fuel_type`` columns.\n\n        This function groups the data by utility, year, and header; extracts the header\n        into a new column; and forward fills the headers so that each record in the\n        header group is associated with that header. Because the headers map to\n        different fuel types and plant types (ex: ``solar pv`` maps to fuel type\n        ``solar`` and plant type ``photovoltaic``), the new forward-filled header column\n        is duplicated and called ``fuel_type_from_header`` and\n        ``plant_type_from_header``. In :func:`map_header_fuel_and_plant_types`, these\n        columns will be mapped to their respective fuel and plant types, used\n        to fill in blank values in the ``plant_type`` and ``fuel_type``, and then\n        eventually removed.\n\n        Why separate the prep step from the map step?\n\n        We trust the values originally reported in the ``fuel_type`` and ``plant_type``\n        columns more than the extracted and forward filled header values, so we only\n        want to replace ``fuel_type`` and ``plant_type`` values that are labeled as\n        ``pd.NA`` or ``other``. The values reported to those columns are extremely messy\n        and must be cleaned via :func:`pudl.transform.classes.categorize_strings` in\n        order for us to know which are truely ``pd.NA`` or ``other``. Because we also\n        use :func:`pudl.transform.classes.categorize_strings` to map the headers to fuel\n        and plant types, it makes sense to clean all four columns at once and then\n        combine them.\n\n        Here's a look at what this function does. It starts with the following table:\n\n        +-------------------+------------+------------+----------+\n        | plant_name_ferc1  | plant_type | fuel_type  | row_type |\n        +===================+============+============+==========+\n        | HYDRO:            | NA         | NA         | header   |\n        +-------------------+------------+------------+----------+\n        | rainbow falls (b) | NA         | NA         | NA       |\n        +-------------------+------------+------------+----------+\n        | cadyville (a)     | NA         | NA         | NA       |\n        +-------------------+------------+------------+----------+\n        | keuka (c)         | NA         | NA         | NA       |\n        +-------------------+------------+------------+----------+\n        | Wind Turbines:    | NA         | NA         | header   |\n        +-------------------+------------+------------+----------+\n        | sunny grove       | NA         | NA         | NA       |\n        +-------------------+------------+------------+----------+\n        | green park wind   | NA         | wind       | NA       |\n        +-------------------+------------+------------+----------+\n\n        And ends with this:\n\n        +-------------------+---------+---------+----------------+--------------------+\n        | plant_name_ferc1  | plant   | fuel    | plant_type     | fuel_type          |\n        |                   | _type   | _type   | _from_header   | _from_header       |\n        +===================+=========+=========+================+====================+\n        | HYDRO:            | NA      | NA      | HYDRO:         | HYDRO:             |\n        +-------------------+---------+---------+----------------+--------------------+\n        | rainbow falls (b) | NA      | NA      | HYDRO:         | HYDRO:             |\n        +-------------------+---------+---------+----------------+--------------------+\n        | cadyville (a)     | NA      | NA      | HYDRO:         | HYDRO:             |\n        +-------------------+---------+---------+----------------+--------------------+\n        | keuka (c)         | NA      | NA      | HYDRO:         | HYDRO:             |\n        +-------------------+---------+---------+----------------+--------------------+\n        | Wind Turbines:    | NA      | NA      | Wind Turbines: | Wind Turbines:     |\n        +-------------------+---------+---------+----------------+--------------------+\n        | sunny grove       | NA      | NA      | Wind Turbines: | Wind Turbines:     |\n        +-------------------+---------+---------+----------------+--------------------+\n        | green park wind   | NA      | wind    | Wind Turbines: | Wind Turbines:     |\n        +-------------------+---------+---------+----------------+--------------------+\n\n        NOTE: If a utility's ``plant_name_ferc1`` values look like this: ``[\"STEAM\",\n        \"coal_plant1\", \"coal_plant2\", \"wind_turbine1\"]``, then this algorythem will\n        think that last wind turbine is a steam plant. Luckily, when a utility embeds\n        headers in the data it usually includes them for all plant types: ``[\"STEAM\",\n        \"coal_plant1\", \"coal_plant2\", \"WIND\", \"wind_turbine\"]``.\n\n        Params:\n            df: Pre-processed, concatenated XBRL and DBF data that has been run through\n            :func:`_label_row_type` and contains the columns ``row_type``.\n\n        Returns:\n            The same input DataFrame but with new columns ``plant_type_from_header``\n            and ``fuel_type_from_header`` that forward fill the values in the header\n            rows by utility, year, and header group.\n        \"\"\"\n        logger.info(\n            f\"{self.table_id.value}: Forward filling header fuel and plant types\"\n        )\n\n        # Create a column of just headers\n        df.loc[df[\"row_type\"] == \"header\", \"header\"] = df[\"plant_name_ferc1\"]\n\n        # Make groups based on utility, year, and header.\n        # The .cumsum() creates a new series with values that go up from 1 whenever\n        # there is a new header. So imagine row_type[\"header\", NA, NA, \"header\", NA].\n        # this creates a series of [1,1,1,2,2] so that the data can be grouped by\n        # header.\n        df = df.reset_index(drop=True)\n        df[\"header_group\"] = (df[\"row_type\"] == \"header\").cumsum()\n        df.loc[df[\"row_type\"] != \"note\", \"header\"] = df.groupby(\n            [\"utility_id_ferc1\", \"report_year\", \"header_group\"]\n        ).header.ffill()\n\n        # Create temporary columns for plant type and fuel type\n        df[\"plant_type_from_header\"] = df[\"header\"]\n        df[\"fuel_type_from_header\"] = df[\"header\"]\n        df = df.drop(columns=[\"header\", \"header_group\"])\n\n        return df\n\n    def map_header_fuel_and_plant_types(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fill ``pd.NA`` and ``other`` plant and fuel types with cleaned headers.\n\n        :func:`prep_header_fuel_and_plant_types` extracted and forward filled the header\n        values; :func:`pudl.transform.params.categorize_strings` cleaned them according\n        to both the fuel and plant type parameters. This function combines the\n        ``fuel_type_from_header`` with ``fuel_type`` and ``plant_type_from_header`` with\n        ``plant_type`` when the reported, cleaned values are ``pd.NA`` or ``other``.\n\n        To understand more about why these steps are necessary read the docstrings for\n        :func:`prep_header_fuel_and_plant_types`.\n\n        Params:\n            df: Pre-processed, concatenated XBRL and DBF data that has been run through\n            :func:`prep_header_fuel_and_plant_types` and contains the columns\n            ``fuel_type_from_header`` and ``plant_type_from_header``.\n\n        Returns:\n            The same input DataFrame but with rows with ``pd.NA`` or ``other`` in the\n            ``fuel_type`` and ``plant_type`` columns filled in with the respective\n            values from ``fuel_type_from_header`` and ``plant_type_from_header`` when\n            available. ``fuel_type_from_header`` and ``plant_type_from_header`` columns\n            removed.\n        \"\"\"\n        logger.info(\n            f\"{self.table_id.value}: Filling NA and 'other' fuel and plant types with\"\n            \" header info\"\n        )\n\n        # Stash the amount of NA values to check that the filling worked.\n        old_fuel_type_count = len(\n            df[~df[\"fuel_type\"].isin([pd.NA, \"other\"]) & df[\"row_type\"].isna()]\n        )\n        old_plant_type_count = len(\n            df[~df[\"plant_type\"].isin([pd.NA, \"other\"]) & df[\"row_type\"].isna()]\n        )\n\n        # Fill NA and \"other\" fields\n        df.loc[\n            df[\"plant_type\"].isin([pd.NA, \"other\"]), \"plant_type\"\n        ] = df.plant_type_from_header\n        df.loc[\n            df[\"fuel_type\"].isin([pd.NA, \"other\"]), \"fuel_type\"\n        ] = df.fuel_type_from_header\n\n        # Remove _from_header fields\n        df = df.drop(columns=[\"plant_type_from_header\", \"fuel_type_from_header\"])\n\n        # Check that this worked!\n        new_fuel_type_count = len(\n            df[~df[\"fuel_type\"].isin([pd.NA, \"other\"]) & df[\"row_type\"].isna()]\n        )\n        new_plant_type_count = len(\n            df[~df[\"plant_type\"].isin([pd.NA, \"other\"]) & df[\"row_type\"].isna()]\n        )\n\n        if not old_fuel_type_count < new_fuel_type_count:\n            raise AssertionError(\"No header fuel types added when there should be\")\n        if not old_plant_type_count < new_plant_type_count:\n            raise AssertionError(\"No header plant types added when there should be\")\n\n        useful_rows_len = len(df[df[\"row_type\"].isna()])\n\n        logger.info(\n            f\"Added fuel types to {new_fuel_type_count-old_fuel_type_count} plant rows \"\n            f\"({round((new_fuel_type_count-old_fuel_type_count)/useful_rows_len*100)}%). \"\n            f\"Added plant types to {new_plant_type_count-old_plant_type_count} plant \"\n            f\"rows ({round((new_plant_type_count-old_plant_type_count)/useful_rows_len*100)}%).\"\n        )\n\n        return df\n\n    def map_plant_name_fuel_types(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Suppliment ``fuel_type`` with information in ``plant_name_ferc1``.\n\n        Sometimes fuel type is embedded in a plant name (not just headers). In this case\n        we can identify that what that fuel is from the name and fill in empty\n        ``fuel_type`` values. Right now, this only works for hydro plants because the\n        rest are complicated and have a slew of exceptions. This could probably be\n        applied to the ``plant_type`` column in the future too.\n\n        Params:\n            df: Pre-processed, concatenated XBRL and DBF data.\n\n        Returns:\n            The same input DataFrame but with rows with ``other`` in the\n            ``fuel_type`` column filled in notable fuel types extracted from the the\n            ``plant_name_ferc1`` column.\n        \"\"\"\n        logger.info(f\"{self.table_id.value}: Getting fuel type (hydro) from plant name\")\n        df.loc[\n            (\n                df[\"plant_name_ferc1\"].str.contains(\"hydro\")\n                & (df[\"fuel_type\"] == \"other\")\n            ),\n            \"fuel_type\",\n        ] = \"hydro\"\n\n        return df\n\n    def associate_notes_with_values(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Use footnote indicators to map notes and FERC licenses to plant rows.\n\n        There are many utilities that report a bunch of mostly empty note rows at the\n        bottom of their yearly entry. These notes often pertain to specific plant rows\n        above. Sometimes the notes and their respective plant rows are linked by a\n        common footnote indicator such as (a) or (1) etc.\n\n        This function takes this:\n\n        +-------------------+------------+------------------+\n        | plant_name_ferc1  | row_type   | license_id_ferc1 |\n        +===================+============+==================+\n        | HYDRO:            | header     | NA               |\n        +-------------------+------------+------------------+\n        | rainbow falls (b) | NA         | NA               |\n        +-------------------+------------+------------------+\n        | cadyville (a)     | NA         | NA               |\n        +-------------------+------------+------------------+\n        | keuka (c)         | NA         | NA               |\n        +-------------------+------------+------------------+\n        | total plants      | total      | NA               |\n        +-------------------+------------+------------------+\n        | (a) project #2738 | note       | 2738             |\n        +-------------------+------------+------------------+\n        | (b) project #2835 | note       | 2738             |\n        +-------------------+------------+------------------+\n        | (c) project #2852 | note       | 2738             |\n        +-------------------+------------+------------------+\n\n        Finds the note rows with footnote indicators, maps the content from the note row\n        into a new note column that's associated with the value row, and maps any FERC\n        license extracted from this note column to the ``license_id_ferc1`` column in\n        the value row.\n\n        +-------------------+------------+-------------------+------------------+\n        | plant_name_ferc1  | row_type   | notes             | license_id_ferc1 |\n        +===================+============+===================+==================+\n        | HYDRO:            | header     | NA                | NA               |\n        +-------------------+------------+-------------------+------------------+\n        | rainbow falls (b) | NA         | (b) project #2835 | 2835             |\n        +-------------------+------------+-------------------+------------------+\n        | cadyville (a)     | NA         | (a) project #2738 | 2738             |\n        +-------------------+------------+-------------------+------------------+\n        | keuka (c)         | NA         | (c) project #2852 | 2752             |\n        +-------------------+------------+-------------------+------------------+\n        | total plants      | total      | NA                | NA               |\n        +-------------------+------------+-------------------+------------------+\n        | (a) project #2738 | note       | NA                | 2738             |\n        +-------------------+------------+-------------------+------------------+\n        | (b) project #2835 | note       | NA                | 2835             |\n        +-------------------+------------+-------------------+------------------+\n        | (c) project #2852 | note       | NA                | 2752             |\n        +-------------------+------------+-------------------+------------------+\n\n        (Header and note rows are removed later).\n\n        NOTE: Note rows that don't have a footnote indicator or note rows with a\n        footnote indicator that don't have a cooresponding plant row with the same\n        indicator are not captured. They will ultimately get removed and their content\n        will not be preserved.\n\n        Params:\n            df: Pre-processed, concatenated XBRL and DBF data that has been run through\n            :func:`label_row_types` and contains the column ``row_type``.\n\n        Returns:\n            The same input DataFrame but with a column called ``notes`` that contains\n            notes, reported below, in the same row as the plant values they pertain to.\n            Also, any further additions to the ``license_id_ferc1`` field as extracted\n            from these newly associated notes.\n        \"\"\"\n        logger.info(\n            f\"{self.table_id.value}: Mapping notes and ferc license from notes rows\"\n        )\n\n        def associate_notes_with_values_group(group):\n            \"\"\"Map footnotes within a given utility year group.\n\n            Because different utilities may use the same footnotes or the same utility\n            could reuse footnotes each year, we must do the footnote association within\n            utility-year groups.\n            \"\"\"\n            regular_row = group[\"row_type\"].isna()\n            has_note = group[\"row_type\"] == \"note\"\n\n            # Shorten execution time by only looking at groups with discernable\n            # footnotes\n            if group.footnote.any():\n                # Make a df that combines notes and ferc license with the same footnote\n                footnote_df = (\n                    group[has_note]\n                    .groupby(\"footnote\")\n                    .agg({\"plant_name_ferc1\": \", \".join, \"license_id_ferc1\": \"first\"})\n                    .rename(columns={\"plant_name_ferc1\": \"notes\"})\n                )\n\n                # Map these new license and note values onto the original df\n                updated_ferc_license_col = group.footnote.map(\n                    footnote_df[\"license_id_ferc1\"]\n                )\n                notes_col = group.footnote.map(footnote_df[\"notes\"])\n                # We update the ferc lic col because some were already there from the\n                # plant name extraction. However, we want to override with the notes\n                # ferc licenses because they are more likely to be accurate.\n                group.license_id_ferc1.update(updated_ferc_license_col)\n                group.loc[regular_row, \"notes\"] = notes_col\n\n            return group\n\n        footnote_pattern = r\"(\\(\\d?[a-z]?[A-Z]?\\))\"\n        df[\"notes\"] = pd.NA\n        # Create new footnote column\n        df.loc[:, \"footnote\"] = df.plant_name_ferc1.str.extract(\n            footnote_pattern, expand=False\n        )\n        # Group by year and utility and run footnote association\n        groups = df.groupby([\"report_year\", \"utility_id_ferc1\"])\n        sg_notes = groups.apply(lambda x: associate_notes_with_values_group(x))\n        # Remove footnote column now that rows are associated\n        sg_notes = sg_notes.drop(columns=[\"footnote\"])\n\n        notes_added = len(\n            sg_notes[sg_notes[\"notes\"].notna() & sg_notes[\"row_type\"].isna()]\n        )\n        logger.info(f\"Mapped {notes_added} notes to plant rows.\")\n\n        return sg_notes\n\n    def spot_fix_rows(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fix one-off row errors.\n\n        In 2004, utility_id_ferc1 251 reports clumps of units together. Each unit clump\n        looks something like this: ``intrepid wind farm (107 units @ 1.5 mw each)`` and\n        is followed by a row that looks like this: ``(amounts are for the total of all\n        107 units)``. For the most part, these rows are useless note rows. However,\n        there is one instance where important values are reported in this note row\n        rather than in the actual plant row above.\n\n        There are probably plenty of other spot fixes one could add here.\n\n        Params:\n            df: Pre-processed, concatenated XBRL and DBF data.\n\n        Returns:\n            The same input DataFrame but with some spot fixes corrected.\n        \"\"\"\n        logger.info(f\"{self.table_id.value}: Spot fixing some rows\")\n        # Define rows and columns to change\n        cols_to_change = df.select_dtypes(include=np.number).columns.tolist() + [\n            \"row_type\"\n        ]\n        row_with_info = (df[\"report_year\"] == 2004) & (\n            df[\"plant_name_ferc1\"] == \"(amounts are for the total of all 107 units)\"\n        )\n        row_missing_info = (df[\"report_year\"] == 2004) & (\n            df[\"plant_name_ferc1\"] == \"intrepid wind farm (107 units @ 1.5 mw each)\"\n        )\n\n        # Replace row missing information with data from row containing information\n        df.loc[row_missing_info, cols_to_change] = df[row_with_info][\n            cols_to_change\n        ].to_numpy()\n\n        # Remove row_with_info so there is no duplicate information\n        df = df[~row_with_info]\n\n        return df\n\n\nclass TransmissionLinesTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"A table transformer for the :ref:`core_ferc1__yearly_transmission_lines_sched422` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.TRANSMISSION_LINES\n    has_unique_record_ids: bool = False\n\n    def transform_main(self: Self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Do some string-to-numeric ninja moves.\"\"\"\n        df[\"num_transmission_circuits\"] = pd.to_numeric(df[\"num_transmission_circuits\"])\n        return super().transform_main(df)\n\n\nclass EnergySourcesTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for :ref:`core_ferc1__yearly_energy_sources_sched401` table.\n\n    The raw DBF and XBRL table will be split up into two tables. This transformer\n    generates the sources of electricity for utilities, dropping the information about\n    dispositions. For XBRL, this is a duration-only table. Right now we are merging in\n    the metadata but not actually keeping anything from it. We are also not yet doing\n    anything with the sign.\n    \"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.ENERGY_SOURCES\n    has_unique_record_ids: bool = False\n\n    def convert_xbrl_metadata_json_to_df(\n        self: Self,\n        xbrl_metadata_json: dict[Literal[\"instant\", \"duration\"], list[dict[str, Any]]],\n    ) -> pd.DataFrame:\n        \"\"\"Perform default xbrl metadata processing plus adding 1 new xbrl_factoid.\n\n        Note: we should probably parameterize this and add it into the standard\n        :meth:`process_xbrl_metadata`.\n        \"\"\"\n        tbl_meta = super().convert_xbrl_metadata_json_to_df(xbrl_metadata_json)\n        facts_to_add = [\n            {\n                \"xbrl_factoid\": new_fact,\n                \"calculations\": \"[]\",\n                \"balance\": \"credit\",\n                \"ferc_account\": pd.NA,\n                \"xbrl_factoid_original\": new_fact,\n                \"is_within_table_calc\": True,\n                \"row_type_xbrl\": \"reported_value\",\n            }\n            for new_fact in [\"megawatt_hours_purchased\", \"purchased_mwh\"]\n        ]\n        new_facts = pd.DataFrame(facts_to_add).convert_dtypes()\n        return pd.concat([tbl_meta, new_facts])\n\n\nclass EnergyDispositionsTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for :ref:`core_ferc1__yearly_energy_dispositions_sched401` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.ENERGY_DISPOSITIONS\n    has_unique_record_ids: bool = False\n\n\nclass UtilityPlantSummaryTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for :ref:`core_ferc1__yearly_utility_plant_summary_sched200` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.UTILITY_PLANT_SUMMARY\n    has_unique_record_ids: bool = False\n\n    def process_xbrl(\n        self: Self, raw_xbrl_instant: pd.DataFrame, raw_xbrl_duration: pd.DataFrame\n    ) -> pd.DataFrame:\n        \"\"\"Remove the end-of-previous-year instant data.\"\"\"\n        all_current_year = raw_xbrl_instant[\n            raw_xbrl_instant[\"date\"].astype(\"datetime64[ns]\").dt.year\n            == raw_xbrl_instant[\"report_year\"].astype(\"int64\")\n        ]\n        return super().process_xbrl(all_current_year, raw_xbrl_duration)\n\n    def convert_xbrl_metadata_json_to_df(\n        self: Self,\n        xbrl_metadata_json: dict[Literal[\"instant\", \"duration\"], list[dict[str, Any]]],\n    ) -> pd.DataFrame:\n        \"\"\"Do the default metadata processing plus add a new factoid.\n\n        The new factoid cooresponds to the aggregated factoid in\n        :meth:`aggregated_xbrl_factoids`.\n        \"\"\"\n        tbl_meta = super().convert_xbrl_metadata_json_to_df(xbrl_metadata_json)\n        # things that could be grabbed from a aggregated_xbrl_factoids param\n        new_factoid_name = (\n            \"utility_plant_in_service_classified_and_property_under_capital_leases\"\n        )\n        # point this new aggregated factiod to the PIS table's equivilant when the\n        # subdimensions line up\n        calc = [\n            {\n                \"name\": \"electric_plant_in_service_and_completed_construction_not_classified_electric\",\n                \"weight\": 1.0,\n                \"source_tables\": [\"plant_in_service_ferc1\"],\n                \"utility_type\": \"electric\",\n            }\n        ]\n        new_fact = pd.DataFrame(\n            {\n                \"xbrl_factoid\": [new_factoid_name],\n                \"calculations\": [json.dumps(calc)],\n                \"balance\": [\"debit\"],\n                \"ferc_account\": [pd.NA],\n                \"xbrl_factoid_original\": [new_factoid_name],\n                \"is_within_table_calc\": [False],\n                \"row_type_xbrl\": [\"calculated_value\"],\n            }\n        ).convert_dtypes()\n\n        tbl_meta = pd.concat(\n            [tbl_meta, new_fact.astype(tbl_meta.dtypes, errors=\"ignore\")]\n        ).reset_index(drop=True)\n        return tbl_meta\n\n    def transform_main(self: Self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Default transforming, plus spot fixing and building aggregate xbrl_factoid.\"\"\"\n        # we want to aggregate the factoids first here bc merge_xbrl_metadata is done\n        # at the end of super().transform_main\n        df = (\n            self.aggregated_xbrl_factoids(df)\n            .pipe(super().transform_main)\n            .pipe(self.spot_fix_bad_signs)\n        )\n        return df\n\n    def aggregated_xbrl_factoids(self: Self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Aggregate xbrl_factoids records for linking to :ref:`core_ferc1__yearly_plant_in_service_sched204`.\n\n        This table has two ``xbrl_factoid`` which can be linked via calcuations to one\n        ``xbrl_factoid`` in the :ref:`core_ferc1__yearly_plant_in_service_sched204`.\n        Doing this 2:1 linkage would be fine in theory. But the\n        :ref:`core_ferc1__yearly_plant_in_service_sched204` is in most senses\n        the table with the more details and of our desire to build tree-link\n        relationships between factoids, we need to build a new factoid to link in a 1:1\n        manner between this table and the :ref:`core_ferc1__yearly_plant_in_service_sched204`.\n\n        We'll also add this factoid into the metadata via :meth:`process_xbrl_metadata`\n        and add the linking calculation via :meth:`apply_xbrl_calculation_fixes`.\n        \"\"\"\n        # these guys could be params\n        factoids_to_agg = [\n            \"utility_plant_in_service_classified\",\n            \"utility_plant_in_service_property_under_capital_leases\",\n        ]\n        new_factoid_name = (\n            \"utility_plant_in_service_classified_and_property_under_capital_leases\"\n        )\n        cols_to_agg = [\"ending_balance\"]\n        # grab some key infor for the actual aggregation\n        xbrl_factoid_name = self.params.xbrl_factoid_name\n        pks = pudl.metadata.classes.Resource.from_id(\n            self.table_id.value\n        ).schema.primary_key\n        pks_wo_factoid = [col for col in pks if col != xbrl_factoid_name]\n\n        agg_mask = df[xbrl_factoid_name].isin(factoids_to_agg)\n        agg_df = (\n            df[agg_mask]\n            .groupby(pks_wo_factoid, as_index=False, dropna=False)[cols_to_agg]\n            .sum(min_count=1)\n            .assign(**{xbrl_factoid_name: new_factoid_name})\n        )\n        # note: this results in the \"loss\" of non-pk columns like record_id - which\n        # seems appropriate imo. still flag a warning\n        missing_cols = [\n            col for col in df.columns if col not in list(agg_df.columns) + [\"record_id\"]\n        ]\n        logger.warning(\n            f\"Post-aggregating a new xbrl_factoid, we are missing the following columns: {missing_cols}\"\n        )\n        # squish em back together\n        df = pd.concat([df, agg_df]).reset_index(drop=True)\n        return df\n\n    def spot_fix_bad_signs(self: Self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Spot fix depreciation_utility_plant_in_service records with bad signs.\"\"\"\n        primary_keys = [\n            \"report_year\",\n            \"utility_id_ferc1\",\n            \"utility_type\",\n            \"utility_plant_asset_type\",\n        ]\n\n        spot_fix_pks = [\n            (\n                2012,\n                156,\n                \"total\",\n                \"accumulated_provision_for_depreciation_amortization_and_depletion_of_plant_utility\",\n            ),\n            (\n                2012,\n                156,\n                \"total\",\n                \"depreciation_amortization_and_depletion_utility_plant_in_service\",\n            ),\n            (2012, 156, \"total\", \"depreciation_utility_plant_in_service\"),\n            (\n                2012,\n                156,\n                \"electric\",\n                \"accumulated_provision_for_depreciation_amortization_and_depletion_of_plant_utility\",\n            ),\n            (\n                2012,\n                156,\n                \"electric\",\n                \"depreciation_amortization_and_depletion_utility_plant_in_service\",\n            ),\n            (2012, 156, \"electric\", \"depreciation_utility_plant_in_service\"),\n            (\n                2013,\n                170,\n                \"total\",\n                \"accumulated_provision_for_depreciation_amortization_and_depletion_of_plant_utility\",\n            ),\n            (\n                2013,\n                170,\n                \"total\",\n                \"amortization_of_other_utility_plant_utility_plant_in_service\",\n            ),\n            (2013, 170, \"total\", \"amortization_of_plant_acquisition_adjustment\"),\n            (\n                2013,\n                170,\n                \"total\",\n                \"depreciation_amortization_and_depletion_utility_plant_in_service\",\n            ),\n            (2013, 170, \"total\", \"depreciation_utility_plant_in_service\"),\n            (\n                2013,\n                170,\n                \"electric\",\n                \"accumulated_provision_for_depreciation_amortization_and_depletion_of_plant_utility\",\n            ),\n            (\n                2013,\n                170,\n                \"electric\",\n                \"amortization_of_other_utility_plant_utility_plant_in_service\",\n            ),\n            (2013, 170, \"electric\", \"amortization_of_plant_acquisition_adjustment\"),\n            (\n                2013,\n                170,\n                \"electric\",\n                \"depreciation_amortization_and_depletion_utility_plant_in_service\",\n            ),\n            (2013, 170, \"electric\", \"depreciation_utility_plant_in_service\"),\n            (\n                2007,\n                393,\n                \"electric\",\n                \"accumulated_provision_for_depreciation_amortization_and_depletion_of_plant_utility\",\n            ),\n            (\n                2007,\n                393,\n                \"electric\",\n                \"depreciation_amortization_and_depletion_utility_plant_in_service\",\n            ),\n            (2007, 393, \"electric\", \"depreciation_utility_plant_in_service\"),\n            (\n                2007,\n                393,\n                \"total\",\n                \"accumulated_provision_for_depreciation_amortization_and_depletion_of_plant_utility\",\n            ),\n            (\n                2007,\n                393,\n                \"total\",\n                \"depreciation_amortization_and_depletion_utility_plant_in_service\",\n            ),\n            (2007, 393, \"total\", \"depreciation_utility_plant_in_service\"),\n        ]\n\n        spot_fix_pks += [\n            (year, 211, utility_type, column_name)\n            for year in [2006] + list(range(2009, 2021))\n            for utility_type in [\"electric\", \"total\"]\n            for column_name in [\n                \"accumulated_provision_for_depreciation_amortization_and_depletion_of_plant_utility\",\n                \"amortization_of_other_utility_plant_utility_plant_in_service\",\n                \"depreciation_amortization_and_depletion_utility_plant_in_service\",\n                \"depreciation_utility_plant_in_service\",\n            ]\n        ]\n\n        # Par down spot fixes to account for fast tests where not all years are used\n        df_years = df.report_year.unique().tolist()\n        spot_fix_pks = [x for x in spot_fix_pks if x[0] in df_years]\n        logger.info(f\"{self.table_id.value}: Spotfixing {len(spot_fix_pks)} records.\")\n\n        if spot_fix_pks:\n            # Create a df of the primary key of the records you want to fix\n            df_keys = pd.DataFrame(spot_fix_pks, columns=primary_keys).set_index(\n                primary_keys\n            )\n            df = df.set_index(primary_keys)\n            # Flip the signs for the values in \"ending balance\" all records in the original\n            # df that appear in the primary key df\n            df.loc[df_keys.index, \"ending_balance\"] *= -1\n            # All of these are flipping negative values to positive values,\n            # so let's make sure that's what happens\n            flipped_values = df.loc[df_keys.index]\n            if (flipped_values[\"ending_balance\"] < 0).any():\n                raise AssertionError(\"None of these spot fixes should be negative\")\n            df = df.reset_index()\n\n        return apply_pudl_dtypes(df, group=\"ferc1\")\n\n\nclass BalanceSheetLiabilitiesTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for :ref:`core_ferc1__yearly_balance_sheet_liabilities_sched110` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.BALANCE_SHEET_LIABILITIES\n    has_unique_record_ids: bool = False\n\n    @cache_df(key=\"main\")\n    def transform_main(self: Self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Duplicate data that appears in multiple distinct calculations.\n\n        There is a one case in which exactly the same data values are referenced in\n        multiple calculations which can't be resolved by choosing one of the\n        referenced values as the canonical location for that data. In order to preserve\n        all of the calculation structure, we need to duplicate those records in the\n        data, the metadata, and the calculation specifications.  Here we duplicate the\n        data and associated it with newly defined facts, which we will also add to\n        the metadata and calculations.\n        \"\"\"\n        df = super().transform_main(df)\n        facts_to_duplicate = [\n            \"long_term_portion_of_derivative_instrument_liabilities\",\n            \"long_term_portion_of_derivative_instrument_liabilities_hedges\",\n        ]\n        new_data = (\n            df[df.liability_type.isin(facts_to_duplicate)]\n            .copy()\n            .assign(liability_type=lambda x: \"less_\" + x.liability_type)\n        )\n\n        return pd.concat([df, new_data])\n\n    def convert_xbrl_metadata_json_to_df(\n        self: Self,\n        xbrl_metadata_json: dict[Literal[\"instant\", \"duration\"], list[dict[str, Any]]],\n    ) -> pd.DataFrame:\n        \"\"\"Perform default xbrl metadata processing plus adding 2 new xbrl_factoids.\n\n        We add two new factoids which are defined (by PUDL) only for the DBF data, and\n        also duplicate and redefine several factoids which are referenced in multiple\n        calculations and need to be distinguishable from each other.\n\n        Note: we should probably parameterize this and add it into the standard\n        :meth:`process_xbrl_metadata`.\n        \"\"\"\n        tbl_meta = super().convert_xbrl_metadata_json_to_df(xbrl_metadata_json)\n        facts_to_duplicate = [\n            \"long_term_portion_of_derivative_instrument_liabilities\",\n            \"long_term_portion_of_derivative_instrument_liabilities_hedges\",\n        ]\n        duplicated_facts = (\n            tbl_meta[tbl_meta.xbrl_factoid.isin(facts_to_duplicate)]\n            .copy()\n            .assign(\n                xbrl_factoid=lambda x: \"less_\" + x.xbrl_factoid,\n                xbrl_factoid_original=lambda x: \"less_\" + x.xbrl_factoid_original,\n                balance=\"credit\",\n            )\n        )\n        facts_to_add = [\n            {\n                \"xbrl_factoid\": new_fact,\n                \"calculations\": \"[]\",\n                \"balance\": \"credit\",\n                \"ferc_account\": pd.NA,\n                \"xbrl_factoid_original\": new_fact,\n                \"is_within_table_calc\": True,\n                \"row_type_xbrl\": \"reported_value\",\n            }\n            for new_fact in [\n                \"accumulated_deferred_income_taxes\",\n            ]\n        ]\n\n        new_facts = pd.DataFrame(facts_to_add).convert_dtypes()\n        return pd.concat([tbl_meta, new_facts, duplicated_facts]).reset_index(drop=True)\n\n\nclass BalanceSheetAssetsTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for :ref:`core_ferc1__yearly_balance_sheet_assets_sched110` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.BALANCE_SHEET_ASSETS\n    has_unique_record_ids: bool = False\n\n    @cache_df(key=\"main\")\n    def transform_main(self: Self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Duplicate data that appears in multiple distinct calculations.\n\n        There is a one case in which exactly the same data values are referenced in\n        multiple calculations which can't be resolved by choosing one of the\n        referenced values as the canonical location for that data. In order to preserve\n        all of the calculation structure, we need to duplicate those records in the\n        data, the metadata, and the calculation specifications.  Here we duplicate the\n        data and associated it with newly defined facts, which we will also add to\n        the metadata and calculations.\n        \"\"\"\n        df = super().transform_main(df)\n        facts_to_duplicate = [\n            \"noncurrent_portion_of_allowances\",\n            \"derivative_instrument_assets_long_term\",\n            \"derivative_instrument_assets_hedges_long_term\",\n        ]\n        new_data = (\n            df[df.asset_type.isin(facts_to_duplicate)]\n            .copy()\n            .assign(asset_type=lambda x: \"less_\" + x.asset_type)\n        )\n        return pd.concat([df, new_data])\n\n    def convert_xbrl_metadata_json_to_df(\n        self: Self,\n        xbrl_metadata_json: dict[Literal[\"instant\", \"duration\"], list[dict[str, Any]]],\n    ) -> pd.DataFrame:\n        \"\"\"Default xbrl metadata processing plus some error correction.\n\n        We add two new factoids which are defined (by PUDL) only for the DBF data, and\n        also duplicate and redefine several factoids which are referenced in multiple\n        calculations and need to be distinguishable from each other.\n\n        Note: we should probably parameterize this and add it into the standard\n        :meth:`process_xbrl_metadata`.\n        \"\"\"\n        tbl_meta = super().convert_xbrl_metadata_json_to_df(xbrl_metadata_json)\n\n        facts_to_duplicate = [\n            \"noncurrent_portion_of_allowances\",\n            \"derivative_instrument_assets_long_term\",\n            \"derivative_instrument_assets_hedges_long_term\",\n        ]\n        duplicated_facts = (\n            tbl_meta[tbl_meta.xbrl_factoid.isin(facts_to_duplicate)]\n            .copy()\n            .assign(\n                xbrl_factoid=lambda x: \"less_\" + x.xbrl_factoid,\n                xbrl_factoid_original=lambda x: \"less_\" + x.xbrl_factoid_original,\n                balance=\"credit\",\n            )\n        )\n        facts_to_add = [\n            {\n                \"xbrl_factoid\": new_fact,\n                \"calculations\": \"[]\",\n                \"balance\": \"credit\",\n                \"ferc_account\": pd.NA,\n                \"xbrl_factoid_original\": new_fact,\n                \"is_within_table_calc\": True,\n                \"row_type_xbrl\": \"reported_value\",\n            }\n            for new_fact in [\n                \"special_funds_all\",\n                \"nuclear_fuel\",\n                \"preliminary_natural_gas_and_other_survey_and_investigation_charges\",\n            ]\n        ]\n        new_facts = pd.DataFrame(facts_to_add).convert_dtypes()\n        return pd.concat([tbl_meta, new_facts, duplicated_facts])\n\n\nclass IncomeStatementsTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for the :ref:`core_ferc1__yearly_income_statements_sched114` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.INCOME_STATEMENTS\n    has_unique_record_ids: bool = False\n\n    def convert_xbrl_metadata_json_to_df(\n        self: Self,\n        xbrl_metadata_json: dict[Literal[\"instant\", \"duration\"], list[dict[str, Any]]],\n    ) -> pd.DataFrame:\n        \"\"\"Perform default xbrl metadata processing plus adding a new xbrl_factoid.\n\n        Note: we should probably parameterize this and add it into the standard\n        :meth:`process_xbrl_metadata`.\n        \"\"\"\n        tbl_meta = super().convert_xbrl_metadata_json_to_df(xbrl_metadata_json)\n        facts_to_add = {\n            \"xbrl_factoid\": [\"miscellaneous_deductions\"],\n            \"calculations\": [\"[]\"],\n            \"balance\": [\"debit\"],\n            \"ferc_account\": [pd.NA],\n            \"xbrl_factoid_original\": [\"miscellaneous_deductions\"],\n            \"is_within_table_calc\": [True],\n            \"row_type_xbrl\": [\"reported_value\"],\n        }\n        new_facts = pd.DataFrame(facts_to_add).convert_dtypes()\n        return pd.concat([tbl_meta, new_facts])\n\n    def process_dbf(self: Self, raw_dbf: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Drop incorrect row numbers from f1_incm_stmnt_2 before standard processing.\n\n        In 2003, two rows were added to the ``f1_income_stmnt`` dbf table, which bumped\n        the starting ``row_number`` of ``f1_incm_stmnt_2`` from 25 to 27. A small\n        handfull of respondents seem to have not gotten the memo about this this in\n        2003 and have information on these row numbers that shouldn't exist at all for\n        this table.\n\n        This step necessitates the ability to know which source table each record\n        actually comes from, which required adding a column (``sched_table_name``) in\n        the extract step before these two dbf input tables were concatenated.\n\n        Right now we are just dropping these bad row numbers. Should we actually be\n        bumping the whole respondent's row numbers - assuming they reported incorrectly\n        for the whole table? See: https://github.com/catalyst-cooperative/pudl/issues/471\n        \"\"\"\n        len_og = len(raw_dbf)\n        known_bad_income2_rows = [25, 26]\n        raw_dbf = raw_dbf[\n            ~(\n                (raw_dbf.sched_table_name == \"f1_incm_stmnt_2\")\n                & (raw_dbf.report_year == 2003)\n                & (raw_dbf.row_number.isin(known_bad_income2_rows))\n            )\n        ].copy()\n        logger.info(\n            f\"Dropped {len_og - len(raw_dbf)} records ({(len_og - len(raw_dbf))/len_og:.1%} of\"\n            \"total) records from 2003 from the f1_incm_stmnt_2 DBF table that have \"\n            \"known incorrect row numbers.\"\n        )\n        raw_dbf = super().process_dbf(raw_dbf)\n        return raw_dbf\n\n    def transform_main(self: Self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Drop duplicate records from f1_income_stmnt.\n\n        Because net_utility_operating_income is reported on both page 1 and 2 of the\n        form, it ends up introducing a bunch of duplicated records, so we need to drop\n        one of them. Since the value is used in the calculations that are part of the\n        second page, we'll drop it from the first page.\n        \"\"\"\n        df = super().transform_main(df)\n        df = df[\n            ~(\n                (df.record_id.str.startswith(\"f1_income_stmnt_\"))\n                & (df.income_type == \"net_utility_operating_income\")\n            )\n        ]\n        return apply_pudl_dtypes(df, group=\"ferc1\")\n\n\nclass RetainedEarningsTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for :ref:`core_ferc1__yearly_retained_earnings_sched118` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.RETAINED_EARNINGS\n    has_unique_record_ids: bool = False\n\n    current_year_types: set[str] = {\n        \"unappropriated_undistributed_subsidiary_earnings\",\n        \"unappropriated_retained_earnings\",\n    }\n    previous_year_types: set[str] = {\n        \"unappropriated_undistributed_subsidiary_earnings_previous_year\",\n        \"unappropriated_retained_earnings_previous_year\",\n    }\n\n    def convert_xbrl_metadata_json_to_df(\n        self: Self,\n        xbrl_metadata_json: dict[Literal[\"instant\", \"duration\"], list[dict[str, Any]]],\n    ) -> pd.DataFrame:\n        \"\"\"Transform the metadata to reflect the transformed data.\n\n        Beyond the standard :meth:`Ferc1AbstractTableTransformer.process_xbrl_metadata`\n        processing, add FERC account values for a few known values.\n        \"\"\"\n        meta = super().convert_xbrl_metadata_json_to_df(xbrl_metadata_json)\n        meta.loc[\n            meta.xbrl_factoid\n            == \"transfers_from_unappropriated_undistributed_subsidiary_earnings\",\n            \"ferc_account\",\n        ] = \"216.1\"\n        meta.loc[\n            meta.xbrl_factoid\n            == \"appropriated_retained_earnings_including_reserve_amortization\",\n            \"ferc_account\",\n        ] = \"215_and_215.1\"\n        meta.loc[\n            meta.xbrl_factoid == \"retained_earnings\",\n            \"ferc_account\",\n        ] = \"215_and_215.1_and_216\"\n        meta.loc[\n            meta.xbrl_factoid == \"unappropriated_retained_earnings\",\n            \"ferc_account\",\n        ] = \"216\"\n        meta.loc[\n            meta.xbrl_factoid == \"equity_in_earnings_of_subsidiary_companies\",\n            \"ferc_account\",\n        ] = \"418.1\"\n\n        # NOTE: Needs to happen before `process_xbrl_metadata_calculations`\n        facts_to_add = [\n            {\n                \"xbrl_factoid\": new_fact,\n                \"calculations\": \"[]\",\n                \"balance\": \"credit\",\n                \"ferc_account\": pd.NA,\n                \"xbrl_factoid_original\": new_fact,\n                \"is_within_table_calc\": True,\n                \"row_type_xbrl\": \"reported_value\",\n            }\n            for new_fact in [\n                \"unappropriated_retained_earnings_previous_year\",\n                \"unappropriated_undistributed_subsidiary_earnings_previous_year\",\n            ]\n        ]\n\n        new_facts = pd.DataFrame(facts_to_add).convert_dtypes()\n        return pd.concat([meta, new_facts])\n\n    def process_dbf(self, raw_dbf: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Preform generic :meth:`process_dbf`, plus deal with duplicates.\n\n        Along with the standard processing in\n        :meth:`Ferc1AbstractTableTransformer.process_dbf`, this method runs:\n        * :meth:`targeted_drop_duplicates_dbf`\n        * :meth:`reconcile_double_year_earnings_types_dbf`\n        \"\"\"\n        processed_dbf = (\n            super()\n            .process_dbf(raw_dbf)\n            .pipe(self.targeted_drop_duplicates_dbf)\n            .pipe(self.reconcile_double_year_earnings_types_dbf)\n        )\n        return processed_dbf\n\n    @cache_df(\"main\")\n    def transform_main(self, df):\n        \"\"\"Add `_previous_year` factoids after standard transform_main.\n\n        Add `_previous_year` factoids for `unappropriated_retained_earnings` and\n        `unappropriated_undistributed_subsidiary_earnings` after standard\n        transform_main. This should only affect XBRL data, but we do it after merging to\n        enable access to DBF data to fill this in as well.\n        \"\"\"\n        df = super().transform_main(df).pipe(self.add_previous_year_factoid)\n        return df\n\n    def transform_end(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Check ``_previous_year`` factoids for consistency after the transformation is done.\"\"\"\n        return super().transform_end(df).pipe(self.check_double_year_earnings_types)\n\n    def check_double_year_earnings_types(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Check previous year/current year factoids for consistency.\n\n        The terminology can be very confusing - here are the expectations:\n\n        1. \"inter year consistency\": earlier year's \"current starting/end\n           balance\" == later year's \"previous starting/end balance\"\n        2. \"intra year consistency\": each year's \"previous ending balance\" ==\n           \"current starting balance\"\n        \"\"\"\n        current_year_facts = df.loc[df.earnings_type.isin(self.current_year_types)]\n        previous_year_facts = df.loc[\n            df.earnings_type.isin(self.previous_year_types)\n        ].pipe(\n            lambda df: df.assign(\n                earnings_type=df.earnings_type.str.removesuffix(\"_previous_year\")\n            )\n        )\n\n        # inter year comparison requires us to match the earlier year's current facts\n        # to the later year's previous facts, so we add 1 to the report year & merge.\n        earlier_years = current_year_facts.assign(\n            report_year=current_year_facts.report_year + 1\n        )\n        later_years = previous_year_facts\n        idx = [\"utility_id_ferc1\", \"report_year\", \"earnings_type\"]\n        inter_year_facts = earlier_years.merge(\n            later_years,\n            on=idx,\n            suffixes=[\"_earlier\", \"_later\"],\n        ).dropna(\n            subset=[\n                \"starting_balance_earlier\",\n                \"starting_balance_later\",\n                \"ending_balance_earlier\",\n                \"ending_balance_later\",\n            ]\n        )\n\n        intra_year_facts = previous_year_facts.merge(\n            current_year_facts, on=idx, suffixes=[\"_previous\", \"_current\"]\n        )\n\n        assert_cols_areclose(\n            df=inter_year_facts,\n            a_cols=[\"starting_balance_earlier\"],\n            b_cols=[\"starting_balance_later\"],\n            mismatch_threshold=0.05,\n            message=\"'Current starting balance' for year X-1 doesn't match \"\n            \"'previous starting balance' for year X.\",\n        )\n\n        assert_cols_areclose(\n            df=inter_year_facts,\n            a_cols=[\"ending_balance_earlier\"],\n            b_cols=[\"ending_balance_later\"],\n            mismatch_threshold=0.05,\n            message=\"'Current ending balance' for year X-1 doesn't match \"\n            \"'previous ending balance' for year X.\",\n        )\n\n        assert_cols_areclose(\n            df=intra_year_facts,\n            a_cols=[\"ending_balance_previous\"],\n            b_cols=[\"starting_balance_current\"],\n            mismatch_threshold=0.05,\n            message=\"'Previous year ending balance' should be the same as \"\n            \"'current year starting balance' for all years!\",\n        )\n\n        return df\n\n    def targeted_drop_duplicates_dbf(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Drop duplicates with truly duplicate data.\n\n        There are instances of utilities that reported multiple values for several\n        earnings types for a specific year (utility_id_ferc1 68 in 1998 &\n        utility_id_ferc1 296 in 2015). We are taking the largest value reported and\n        dropping the rest. There very well could be a better strategey here, but there\n        are only 25 records that have this problem, so we've going with this.\n        \"\"\"\n        pks = (\n            pudl.metadata.classes.Package.from_resource_ids()\n            .get_resource(self.table_id.value)\n            .schema.primary_key\n        )\n        # we are not going to check all of the unstructed earnings types for dupes bc\n        # we will drop these later\n        dupe_mask = ~df.earnings_type.str.endswith(\"_unstructured\") & df.duplicated(\n            subset=pks, keep=False\n        )\n        dupes = df[dupe_mask]\n        if len(dupes) > 25:\n            raise AssertionError(\n                f\"{self.table_id.value}: Too many duplicates found ({len(dupes)}). \"\n                \"Expected 25 or less.\"\n            )\n        # we are simply sorting to get the biggest value and dropping the rest.\n        dupes = dupes.sort_values(\n            [\"starting_balance\", \"amount\"], ascending=False\n        ).drop_duplicates(subset=pks)\n        df = pd.concat([df[~dupe_mask], dupes])\n        return df\n\n    def reconcile_double_year_earnings_types_dbf(\n        self, df: pd.DataFrame\n    ) -> pd.DataFrame:\n        \"\"\"Reconcile current and past year data reported in 1 report_year.\n\n        The DBF table includes two different earnings types that have: \"Begining of\n        Period\" and \"End of Period\" rows. But the table has both an amount column that\n        corresponds to a balance and a starting balance column. For these two earnings\n        types, this means that there is in effect two years of data in this table for\n        each report year: a starting and ending balance for the pervious year and a\n        starting and ending balance for the current year. The ending balance for the\n        previous year should be the same as the starting balance for the current year.\n\n        We need to keep both pieces of data in order to calculate `ending_balances`,\n        so we want to check these assumptions, extract as much information from these\n        two years of data, and keep both records for each of these two earnings\n        types for each utility.\n\n        Raises:\n            AssertionError: There are a very small number of instances in which the\n                ending balance from the previous year does not match the starting\n                balance from the current year. The % of these non-matching instances\n                should be less than 2% of the records with these date duplicative\n                earnings types.\n        \"\"\"\n        logger.info(f\"{self.table_id.value}: Reconciling previous year's data.\")\n        # DBF has _current_year suffix while PUDL core version does not\n        current_year_types = [\n            \"unappropriated_undistributed_subsidiary_earnings_current_year\",\n            \"unappropriated_retained_earnings_current_year\",\n        ]\n        previous_year_types = [\n            \"unappropriated_undistributed_subsidiary_earnings_previous_year\",\n            \"unappropriated_retained_earnings_previous_year\",\n        ]\n        # assign() copies, so no need to double copy when extracting this slice\n        current_year = df[df.earnings_type.isin(current_year_types)].assign(\n            earnings_type=lambda x: x.earnings_type.str.removesuffix(\"_current_year\")\n        )\n        previous_year = df[df.earnings_type.isin(previous_year_types)].assign(\n            earnings_type=lambda x: x.earnings_type.str.removesuffix(\"_previous_year\")\n        )\n        idx = [\n            \"utility_id_ferc1_dbf\",\n            \"report_year\",\n            \"utility_id_ferc1\",\n            \"earnings_type\",\n        ]\n        data_columns = [\"amount\", \"starting_balance\"]\n        date_dupe_types = pd.merge(\n            current_year,\n            previous_year[idx + data_columns],\n            on=idx,\n            how=\"outer\",\n            suffixes=(\"\", \"_previous_year\"),\n        )\n\n        date_dupe_types.loc[:, \"ending_balance\"] = pd.NA\n        # check if the starting balance from the current year is actually\n        # the amount from the previous year\n        date_mismatch = date_dupe_types[\n            ~np.isclose(\n                date_dupe_types.starting_balance,\n                date_dupe_types.amount_previous_year,\n                equal_nan=True,\n            )\n            & (date_dupe_types.starting_balance.notnull())\n            & (date_dupe_types.amount_previous_year.notnull())\n        ]\n        data_mismatch_ratio = len(date_mismatch) / len(date_dupe_types)\n        if data_mismatch_ratio > 0.02:\n            raise AssertionError(\n                \"More records than expected have data that is not the same in \"\n                \"the starting_balance vs the amount column for the earnings_type \"\n                \"that reports both current and previous year. % of mismatch records: \"\n                f\"{data_mismatch_ratio:.01%} (expected less than 1%)\"\n            )\n\n        # the amount from the current year values should be the ending balance.\n        # the amount from the previous year should fill in the starting balance\n        # then drop all of the _previous_year columns\n        date_dupe_types = date_dupe_types.assign(\n            ending_balance=lambda x: x.amount,\n            amount=pd.NA,\n            starting_balance=lambda x: x.starting_balance.fillna(\n                x.amount_previous_year\n            ),\n        ).drop(columns=[\"amount_previous_year\", \"starting_balance_previous_year\"])\n\n        df = pd.concat(\n            [df[~df.earnings_type.isin(current_year_types)], date_dupe_types]\n        )\n\n        # Since we've created an ending balance column, we should use the 'amount'\n        # value to fill it across the table and drop the amount column.\n        df.ending_balance = df.ending_balance.fillna(df.amount)\n        df = df.drop(\"amount\", axis=1)\n\n        return df\n\n    def add_previous_year_factoid(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Create ``*_previous_year`` factoids for XBRL data.\n\n        XBRL doesn't include the previous year's data, but DBF does - so we try to\n        check that year X's ``*_current_year`` factoid has the same value as year X+1's\n        ``*_previous_year`` factoid.\n\n        To do this, we need to add some ``*_previous_year`` factoids to the XBRL data.\n        \"\"\"\n        current_year_facts = df[df.earnings_type.isin(self.current_year_types)]\n        previous_year_facts = df[df.earnings_type.isin(self.previous_year_types)]\n\n        missing_years = set(current_year_facts.report_year.unique()) - set(\n            previous_year_facts.report_year.unique()\n        )\n\n        to_copy_forward = current_year_facts[\n            (current_year_facts.report_year + 1).isin(missing_years)\n        ]\n\n        idx = [\n            \"utility_id_ferc1\",\n            \"earnings_type\",\n            \"report_year\",\n        ]\n        inferred_previous_year_facts = (\n            to_copy_forward.assign(\n                report_year=to_copy_forward.report_year + 1,\n                new_earnings_type=to_copy_forward.earnings_type + \"_previous_year\",\n            )\n            .merge(current_year_facts[idx])\n            .drop(columns=[\"earnings_type\"])\n            .rename(columns={\"new_earnings_type\": \"earnings_type\"})\n            .assign(row_type_xbrl=\"reported_value\")\n        )\n        return pd.concat([df, inferred_previous_year_facts])\n\n    def deduplicate_xbrl_factoid_xbrl_metadata(self, tbl_meta) -> pd.DataFrame:\n        \"\"\"Deduplicate the xbrl_metadata based on the ``xbrl_factoid``.\n\n        The metadata relating to dollar_value column *generally* had the same name as\n        the renamed xbrl_factoid. we'll double check that we a) didn't remove too many\n        factoid's by doing this AND that we have a fully deduped output below. In an\n        ideal world, we would have multiple pieces of metadata information (like\n        calucations and ferc account #'s), for every single :meth:`wide_to_tidy` value\n        column.\n\n        Note: This is **almost** the same as the method for\n        :ref:`core_ferc1__yearly_operating_revenues_sched300`. If we wanted to lean into this\n        version of deduplication more generally this might be a fine way start to an\n        abstraction, but ideally we wouldn't need to dedupe this at all and instead\n        enable metadata for every value column from :meth:`wide_to_tidy`.\n        \"\"\"\n        dupes_masks = tbl_meta.duplicated(subset=[\"xbrl_factoid\"], keep=False)\n        non_dupes = tbl_meta[~dupes_masks]\n        dupes = tbl_meta[dupes_masks]\n\n        deduped = dupes[dupes.xbrl_factoid == dupes.xbrl_factoid_original]\n        tbl_meta_cleaned = pd.concat([non_dupes, deduped])\n        assert ~tbl_meta_cleaned.duplicated(subset=[\"xbrl_factoid\"]).all()\n\n        missing = {\n            factoid\n            for factoid in tbl_meta.xbrl_factoid.unique()\n            if factoid not in tbl_meta_cleaned.xbrl_factoid.unique()\n        }\n        if missing:\n            raise AssertionError(\n                \"We expected to find no missing xbrl_factoid's after deduplication \"\n                f\"but found {missing}\"\n            )\n        return tbl_meta_cleaned\n\n\nclass DepreciationSummaryTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for :ref:`core_ferc1__yearly_depreciation_summary_sched336` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.DEPRECIATION_SUMMARY\n    has_unique_record_ids: bool = False\n\n    @cache_df(\"process_xbrl_metadata\")\n    def process_xbrl_metadata(\n        self: Self,\n        xbrl_metadata_converted: pd.DataFrame,\n        xbrl_calculations: pd.DataFrame,\n    ) -> pd.DataFrame:\n        \"\"\"Transform the metadata to reflect the transformed data.\n\n        Beyond the standard :meth:`Ferc1AbstractTableTransformer.process_xbrl_metadata`\n        processing, add FERC account values for a few known values.\n        \"\"\"\n        meta = super().process_xbrl_metadata(xbrl_metadata_converted, xbrl_calculations)\n        # logger.info(meta)\n        meta.loc[\n            meta.xbrl_factoid == \"depreciation_expense\",\n            \"ferc_account\",\n        ] = \"403\"\n        meta.loc[\n            meta.xbrl_factoid == \"depreciation_expense_asset_retirement\",\n            \"ferc_account\",\n        ] = \"403.1\"\n        meta.loc[\n            meta.xbrl_factoid == \"amortization_limited_term_electric_plant\",\n            \"ferc_account\",\n        ] = \"404\"\n        meta.loc[\n            meta.xbrl_factoid == \"amortization_other_electric_plant\",\n            \"ferc_account\",\n        ] = \"405\"\n        return meta\n\n\nclass DepreciationChangesTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for :ref:`core_ferc1__yearly_depreciation_changes_sched219` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.DEPRECIATION_CHANGES\n    has_unique_record_ids: bool = False\n\n    def convert_xbrl_metadata_json_to_df(\n        self: Self, xbrl_metadata_json\n    ) -> pd.DataFrame:\n        \"\"\"Transform the metadata to reflect the transformed data.\n\n        Warning: The calculations in this table are currently being corrected using\n        reconcile_table_calculations(), but they still contain high rates of error.\n        This function replaces the name of the single balance column reported in the\n        XBRL Instant table with starting_balance / ending_balance. We pull those two\n        values into their own separate labeled rows, each of which should get the\n        metadata from the original column. We do this pre-processing before we\n        call the main function in order for the calculation fixes and renaming to work\n        as expected.\n        \"\"\"\n        new_xbrl_metadata_json = xbrl_metadata_json\n        # Get instant metadata\n        instant = pd.json_normalize(new_xbrl_metadata_json[\"instant\"])\n        # Duplicate instant metadata, and add starting/ending suffix\n        # should just be balance begining of year\n        instant = pd.concat([instant] * 2).reset_index(drop=True)\n        instant[\"name\"] = instant[\"name\"] + [\"_starting_balance\", \"_ending_balance\"]\n        # Return to JSON format in order to continue processing\n        new_xbrl_metadata_json[\"instant\"] = json.loads(\n            instant.to_json(orient=\"records\")\n        )\n        self.xbrl_metadata_json = new_xbrl_metadata_json\n        tbl_meta = super().convert_xbrl_metadata_json_to_df(new_xbrl_metadata_json)\n        return tbl_meta\n\n    @cache_df(\"dbf\")\n    def process_dbf(self, raw_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Accumulated Depreciation table specific DBF cleaning operations.\n\n        The XBRL reports a utility_type which is always electric in this table, but\n        which may be necessary for differentiating between different values when this\n        data is combined with other tables. The DBF data doesn't report this value so\n        we are adding it here for consistency across the two data sources.\n\n        Also rename the ``ending_balance_accounts`` to ``ending_balance``\n        \"\"\"\n        df = super().process_dbf(raw_df).assign(utility_type=\"electric\")\n        df.loc[\n            df[\"depreciation_type\"] == \"ending_balance_accounts\", \"depreciation_type\"\n        ] = \"ending_balance\"\n        return df\n\n    @cache_df(\"process_instant_xbrl\")\n    def process_instant_xbrl(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Pre-processing required to make the instant and duration tables compatible.\n\n        This table has a rename that needs to take place in an unusual spot -- after the\n        starting / ending balances have been usntacked, but before the instant &\n        duration tables are merged. This method just reversed the order in which these\n        operations happen, comapared to the inherited method.\n        \"\"\"\n        df = self.unstack_balances_to_report_year_instant_xbrl(df).pipe(\n            self.rename_columns, rename_stage=\"instant_xbrl\"\n        )\n        return df\n\n\nclass DepreciationByFunctionTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer for :ref:`core_ferc1__yearly_depreciation_by_function_sched219` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.DEPRECIATION_BY_FUNCTION\n    has_unique_record_ids: bool = False\n\n    def convert_xbrl_metadata_json_to_df(\n        self: Self,\n        xbrl_metadata_json: dict[Literal[\"instant\", \"duration\"], list[dict[str, Any]]],\n    ) -> pd.DataFrame:\n        \"\"\"Create a metadata table with the one factoid we've assigned to this table.\n\n        Instead of adding facts to the metdata like a lot of the other table-specific\n        :meth:`convert_xbrl_metadata_json_to_df`, this method creates a metadata table\n        with one singular ``xbrl_factoid``. We assign that factoid to the table in\n        :meth:`transform_main`.\n        \"\"\"\n        single_table_fact = [\n            {\n                \"xbrl_factoid\": fact,\n                \"calculations\": \"[]\",\n                \"balance\": \"credit\",\n                \"ferc_account\": pd.NA,\n                \"xbrl_factoid_original\": fact,\n                \"is_within_table_calc\": True,\n                \"row_type_xbrl\": \"reported_value\",\n            }\n            for fact in [\"accumulated_depreciation\"]\n        ]\n        tbl_meta = pd.DataFrame(single_table_fact).convert_dtypes()\n        return tbl_meta\n\n    def raw_xbrl_factoid_to_pudl_name(\n        self,\n        col_name_xbrl: str,\n    ) -> str:\n        \"\"\"Return the one fact name for this table.\n\n        We've artificially assigned this table to have one ``xbrl_factoid`` during\n        :meth:`transform_main`. Because this table only has one value for its\n        ``xbrl_factoid`` column, all ``col_name_xbrl`` should be converted to\n        \"accumulated_depreciation\".\n        \"\"\"\n        return \"accumulated_depreciation\"\n\n    @cache_df(\"dbf\")\n    def process_dbf(self, raw_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Accumulated Depreciation table specific DBF cleaning operations.\n\n        The XBRL reports a utility_type which is always electric in this table, but\n        which may be necessary for differentiating between different values when this\n        data is combined with other tables. The DBF data doesn't report this value so we\n        are adding it here for consistency across the two data sources.\n        \"\"\"\n        return super().process_dbf(raw_df).assign(utility_type=\"electric\")\n\n    @cache_df(\"process_instant_xbrl\")\n    def process_instant_xbrl(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Pre-processing required to make the instant and duration tables compatible.\n\n        This table has a rename that needs to take place in an unusual spot -- after the\n        starting / ending balances have been usntacked, but before the instant &\n        duration tables are merged. This method reverses the order in which these\n        operations happen comapared to the inherited method. We also want to strip the\n        ``accumulated_depreciation`` that appears on every plant functional class.\n        \"\"\"\n        df = self.unstack_balances_to_report_year_instant_xbrl(df).pipe(\n            self.rename_columns, rename_stage=\"instant_xbrl\"\n        )\n        return df\n\n    @cache_df(\"main\")\n    def transform_main(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Add ``depreciation_type`` then run default :meth:`transform_main`.\n\n        We are adding ``depreciation_type`` as the ``xbrl_factoid`` column for this\n        table with one value (\"accumulated_depreciation\") across the whole table. This\n        table has multiple \"dimension\" columns such as ``utility_type`` and\n        ``plant_function`` which differentiate what slice of a utility's assets each\n        record pertains to. We added this new column as the ``xbrl_factoid`` of the\n        table instead of using one of the dimensions of the table so that the table can\n        conform to the same patern of treatment for these dimension columns.\n        \"\"\"\n        df = df.assign(depreciation_type=\"accumulated_depreciation\").pipe(\n            super().transform_main\n        )\n        # convert this **one** utility's depreciation $$ from negative -> +\n        # this was found through checking the inter-table calculations in the explosion\n        # process. The one factoid in this table is linked with\n        # depreciation_utility_plant_in_service in the utility_plant_summary_ferc1 table.\n        # the values in both tables are almost always postive. Not always & there are\n        # some logical reasons why depreciation can sometimes be negative. Nonetheless,\n        # for this one utility, all of its values in utility_plant_summary_ferc1 are\n        # postive while nearly all of the $s over here are negative. No other utility\n        # has as many -$ which tells me this is a data entry error.\n        # see https://github.com/catalyst-cooperative/pudl/issues/2703 for more details\n        negative_util_mask = df.utility_id_ferc1 == 211\n        df.loc[negative_util_mask, \"ending_balance\"] = abs(\n            df.loc[negative_util_mask, \"ending_balance\"]\n        )\n        return df\n\n\nclass OperatingExpensesTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for :ref:`core_ferc1__yearly_operating_expenses_sched320` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.OPERATING_EXPENSES\n    has_unique_record_ids: bool = False\n\n    def targeted_drop_duplicates_dbf(self, raw_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Drop incorrect duplicate from 2002.\n\n        In 2002, utility_id_ferc1_dbf 96 reported two values for\n        administrative_and_general_operation_expense. I found the correct value by\n        looking at the prev_yr_amt value in 2003. This removes the incorrect row.\n        \"\"\"\n        start_len = len(raw_df)\n        raw_df = raw_df[\n            ~((raw_df[\"report_year\"] == 2002) & (raw_df[\"crnt_yr_amt\"] == 35990321))\n        ]\n        if (dropped := start_len - len(raw_df)) > 1:\n            raise AssertionError(f\"More rows dropped than expected: {dropped}\")\n        logger.info(\"Heyyyy dropping that one row\")\n        return raw_df\n\n    def convert_xbrl_metadata_json_to_df(\n        self: Self,\n        xbrl_metadata_json: dict[Literal[\"instant\", \"duration\"], list[dict[str, Any]]],\n    ) -> pd.DataFrame:\n        \"\"\"Default XBRL metadata processing and add a DBF-only xblr factoid.\n\n        Note: we should probably parameterize this and add it into the standard\n        :meth:`process_xbrl_metadata`.\n        \"\"\"\n        tbl_meta = super().convert_xbrl_metadata_json_to_df(xbrl_metadata_json)\n        dbf_only_facts = [\n            {\n                \"xbrl_factoid\": dbf_only_fact,\n                \"calculations\": \"[]\",\n                \"balance\": \"credit\",\n                \"ferc_account\": pd.NA,\n                \"xbrl_factoid_original\": dbf_only_fact,\n                \"is_within_table_calc\": True,\n                \"row_type_xbrl\": \"reported_value\",\n            }\n            for dbf_only_fact in [\"load_dispatching_transmission_expense\"]\n        ]\n        dbf_only_facts = pd.DataFrame(dbf_only_facts).convert_dtypes()\n        return pd.concat([tbl_meta, dbf_only_facts]).assign(utility_type=\"electric\")\n\n    @cache_df(key=\"dbf\")\n    def process_dbf(self, raw_dbf: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Process DBF but drop a bad row that is flagged by drop_duplicates.\"\"\"\n        return super().process_dbf(self.targeted_drop_duplicates_dbf(raw_dbf))\n\n\nclass OperatingRevenuesTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for :ref:`core_ferc1__yearly_operating_revenues_sched300` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.OPERATING_REVENUES\n    has_unique_record_ids: bool = False\n\n    def deduplicate_xbrl_factoid_xbrl_metadata(\n        self, tbl_meta: pd.DataFrame\n    ) -> pd.DataFrame:\n        \"\"\"Transform the metadata to reflect the transformed data.\n\n        Employ the standard process for processing metadata. Then remove duplication on\n        the basis of the ``xbrl_factoid``. This table used :meth:`wide_to_tidy` with three\n        seperate value columns. Which results in one ``xbrl_factoid`` referencing three\n        seperate data columns. This method grabs only one piece of metadata for each\n        renamed ``xbrl_factoid``, preferring the calculated value or the factoid\n        referencing the dollar columns.\n\n        In an ideal world, we would have multiple pieces of metadata information (like\n        calucations and ferc account #'s), for every single :meth:`wide_to_tidy` value\n        column. We would probably want to employ that across the board - adding suffixes\n        or something like that to stack the metadata in a similar fashion that we stack\n        the data.\n        \"\"\"\n        dupes_masks = tbl_meta.duplicated(subset=[\"xbrl_factoid\"], keep=False)\n        non_dupes = tbl_meta[~dupes_masks]\n        dupes = tbl_meta[dupes_masks]\n        # the metadata relating to dollar_value column *generally* had the same name as\n        # the renamed xbrl_factoid. the outliers here are these two that have calcs for\n        # the factoid we want to keep (we could also id them w/ their og factoid names\n        # if that would be more straightforward)\n        deduped = dupes[\n            (dupes.xbrl_factoid == dupes.xbrl_factoid_original)\n            | (\n                dupes.xbrl_factoid.isin([\"small_or_commercial\", \"large_or_industrial\"])\n                & (dupes.calculations != \"[]\")\n            )\n        ]\n        tbl_meta_cleaned = pd.concat([non_dupes, deduped])\n        assert ~tbl_meta_cleaned.duplicated(subset=[\"xbrl_factoid\"]).all()\n\n        # double check that we're getting only the guys we want\n        missing = {\n            factoid\n            for factoid in tbl_meta.xbrl_factoid.unique()\n            if factoid not in tbl_meta_cleaned.xbrl_factoid.unique()\n        }\n        if missing:\n            raise AssertionError(\n                \"We expected to find no missing xbrl_factoid's after deduplication \"\n                f\"but found {missing}\"\n            )\n        return tbl_meta_cleaned\n\n    @cache_df(\"main\")\n    def transform_main(self, df):\n        \"\"\"Add duplicate removal after standard transform_main & assign utility type.\"\"\"\n        return super().transform_main(df).pipe(self.targeted_drop_duplicates)\n\n    @cache_df(\"main\")\n    def targeted_drop_duplicates(self, df):\n        \"\"\"Drop one duplicate records from 2011, utility_id_ferc1 295.\"\"\"\n        dupe_mask = (\n            (df.utility_id_ferc1 == 295)\n            & (df.report_year == 2011)\n            & ((df.dollar_value == 3.33e8) | (df.dollar_value == 3.333e9))\n        )\n\n        return df[~dupe_mask].copy()\n\n\nclass CashFlowsTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transform class for :ref:`core_ferc1__yearly_cash_flows_sched120` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.CASH_FLOWS\n    has_unique_record_ids: bool = False\n\n    @cache_df(\"process_instant_xbrl\")\n    def process_instant_xbrl(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Pre-processing required to make the instant and duration tables compatible.\n\n        This table has a rename that needs to take place in an unusual spot -- after the\n        starting / ending balances have been usntacked, but before the instant &\n        duration tables are merged. This method just reversed the order in which these\n        operations happen, comapared to the inherited method.\n        \"\"\"\n        df = self.unstack_balances_to_report_year_instant_xbrl(df).pipe(\n            self.rename_columns, rename_stage=\"instant_xbrl\"\n        )\n        return df\n\n    @cache_df(\"main\")\n    def transform_main(self, df):\n        \"\"\"Add duplicate removal and validation after standard transform_main.\"\"\"\n        return (\n            super()\n            .transform_main(df)\n            .pipe(self.targeted_drop_duplicates)\n            .pipe(self.validate_start_end_balance)\n        )\n\n    @cache_df(\"main\")\n    def targeted_drop_duplicates(self, df):\n        \"\"\"Drop one duplicate record from 2020, utility_id_ferc1 2037.\n\n        Note: This step could be avoided if we employed a :meth:`drop_invalid_rows`\n        transform step with ``required_valid_cols = [\"amount\"]``\n        \"\"\"\n        dupe_mask = (\n            (df.utility_id_ferc1 == 237)\n            & (df.report_year == 2020)\n            & (df.amount_type == \"dividends_on_common_stock\")\n            & (df.amount.isnull())\n        )\n        if (len_dupes := dupe_mask.value_counts().loc[True]) != 1:\n            raise ValueError(f\"Expected to find 1 duplicate record. Found {len_dupes}\")\n        return df[~dupe_mask].copy()\n\n    @cache_df(\"main\")\n    def validate_start_end_balance(self, df):\n        \"\"\"Validate of start balance + net = end balance.\n\n        Add a quick check to ensure the vast majority of the ending balances are\n        calculable from the net change + the starting balance = the ending balance.\n        \"\"\"\n        # calculate ending balance\n        df.amount = pd.to_numeric(df.amount)\n        end_bal_calc = (\n            df[\n                df.amount_type.isin(\n                    [\n                        \"starting_balance\",\n                        \"net_increase_decrease_in_cash_and_cash_equivalents\",\n                    ]\n                )\n            ]\n            .groupby([\"utility_id_ferc1\", \"report_year\"])[[\"amount\"]]\n            .sum(min_count=2, numeric_only=True)\n            .add_suffix(\"_ending_balace\")\n        )\n        # grab reported ending balance & squish with the calculated version\n        end_bal = df[df.amount_type == \"ending_balance\"].set_index(\n            [\"utility_id_ferc1\", \"report_year\"]\n        )\n        logger.info(end_bal_calc.columns)\n        end_bal.loc[:, \"amount_ending_balace\"] = end_bal_calc.amount_ending_balace\n\n        # when both exist, are they close?\n        end_bal_off = end_bal[\n            ~np.isclose(end_bal.amount, end_bal.amount_ending_balace)\n            & end_bal[[\"amount\", \"amount_ending_balace\"]].notnull().all(axis=\"columns\")\n        ]\n        if (end_bal_off_ratio := len(end_bal_off) / len(end_bal)) > 0.005:\n            raise ValueError(\n                f\"Ahhh!! The ending balance isn't calculable in {end_bal_off_ratio:.2%}\"\n                \" of records. Expected under 0.5%.\"\n            )\n        return df\n\n    def convert_xbrl_metadata_json_to_df(\n        self: Self,\n        xbrl_metadata_json: dict[Literal[\"instant\", \"duration\"], list[dict[str, Any]]],\n    ) -> pd.DataFrame:\n        \"\"\"Transform the metadata to reflect the transformed data.\n\n        Replace the name of the balance column reported in the XBRL Instant table with\n        starting_balance / ending_balance since we pull those two values into their own\n        separate labeled rows, each of which should get the original metadata for the\n        Instant column.\n        \"\"\"\n        meta = super().convert_xbrl_metadata_json_to_df(xbrl_metadata_json)\n        ending_balance = meta[meta.xbrl_factoid == \"starting_balance\"].assign(\n            xbrl_factoid=\"ending_balance\"\n        )\n        return pd.concat([meta, ending_balance])\n\n\nclass SalesByRateSchedulesTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transform class for :ref:`core_ferc1__yearly_sales_by_rate_schedules_sched304` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.SALES_BY_RATE_SCHEDULES\n    has_unique_record_ids: bool = False\n\n    def add_axis_to_total_table_rows(self, df: pd.DataFrame):\n        \"\"\"Add total to the axis column for rows from the total table.\n\n        Because we're adding the\n        sales_of_electricity_by_rate_schedules_account_totals_304 table into the mix,\n        we have a bunch of total values that get mixed in with all the _billed columns\n        from the individual tables. If left alone, these totals aren't labeled in any\n        way becuse they don't have the same _axis columns explaining what each of the\n        values are. In order to distinguish them from the rest of the sub-total data we\n        use this function to create an _axis value for them noting that they are totals.\n\n        It's worth noting that there are also some total values in there already.\n        Those would be hard to clean. The idea is that if you want the actual totals,\n        don't try and sum the sub-components, look at the actual labeled total rows.\n\n        This function relies on the ``sched_table_name`` column, so it must be called\n        before that gets dropped.\n\n        Args:\n            df: The sales table with a ``sched_table_name`` column.\n        \"\"\"\n        logger.info(f\"{self.table_id.value}: Labeling total values.\")\n        df.loc[\n            df[\"sched_table_name\"]\n            == \"sales_of_electricity_by_rate_schedules_account_totals_304\",\n            [\"sales_axis\", \"rate_schedule_description\"],\n        ] = \"total\"\n        return df\n\n    @cache_df(key=\"xbrl\")\n    def process_xbrl(\n        self,\n        raw_xbrl_instant: pd.DataFrame,\n        raw_xbrl_duration: pd.DataFrame,\n    ) -> pd.DataFrame:\n        \"\"\"Rename columns before running wide_to_tidy.\"\"\"\n        logger.info(f\"{self.table_id.value}: Processing XBRL data pre-concatenation.\")\n        return (\n            self.merge_instant_and_duration_tables_xbrl(\n                raw_xbrl_instant, raw_xbrl_duration\n            )\n            .pipe(self.rename_columns, rename_stage=\"xbrl\")\n            .pipe(self.combine_axis_columns_xbrl)\n            .pipe(self.add_axis_to_total_table_rows)\n            .pipe(self.wide_to_tidy, source_ferc1=SourceFerc1.XBRL)\n            .pipe(self.assign_record_id, source_ferc1=SourceFerc1.XBRL)\n            .pipe(self.assign_utility_id_ferc1, source_ferc1=SourceFerc1.XBRL)\n        )\n\n\nclass OtherRegulatoryLiabilitiesTableTransformer(Ferc1AbstractTableTransformer):\n    \"\"\"Transformer class for :ref:`core_ferc1__yearly_other_regulatory_liabilities_sched278` table.\"\"\"\n\n    table_id: TableIdFerc1 = TableIdFerc1.OTHER_REGULATORY_LIABILITIES\n    has_unique_record_ids = False\n\n\nFERC1_TFR_CLASSES: Mapping[str, type[Ferc1AbstractTableTransformer]] = {\n    \"core_ferc1__yearly_steam_plants_fuel_sched402\": SteamPlantsFuelTableTransformer,\n    \"core_ferc1__yearly_steam_plants_sched402\": SteamPlantsTableTransformer,\n    \"core_ferc1__yearly_small_plants_sched410\": SmallPlantsTableTransformer,\n    \"core_ferc1__yearly_hydroelectric_plants_sched406\": HydroelectricPlantsTableTransformer,\n    \"core_ferc1__yearly_plant_in_service_sched204\": PlantInServiceTableTransformer,\n    \"core_ferc1__yearly_pumped_storage_plants_sched408\": PumpedStoragePlantsTableTransformer,\n    \"core_ferc1__yearly_transmission_lines_sched422\": TransmissionLinesTableTransformer,\n    \"core_ferc1__yearly_purchased_power_and_exchanges_sched326\": PurchasedPowerAndExchangesTableTransformer,\n    \"core_ferc1__yearly_energy_sources_sched401\": EnergySourcesTableTransformer,\n    \"core_ferc1__yearly_energy_dispositions_sched401\": EnergyDispositionsTableTransformer,\n    \"core_ferc1__yearly_utility_plant_summary_sched200\": UtilityPlantSummaryTableTransformer,\n    \"core_ferc1__yearly_operating_expenses_sched320\": OperatingExpensesTableTransformer,\n    \"core_ferc1__yearly_balance_sheet_liabilities_sched110\": BalanceSheetLiabilitiesTableTransformer,\n    \"core_ferc1__yearly_depreciation_summary_sched336\": DepreciationSummaryTableTransformer,\n    \"core_ferc1__yearly_balance_sheet_assets_sched110\": BalanceSheetAssetsTableTransformer,\n    \"core_ferc1__yearly_income_statements_sched114\": IncomeStatementsTableTransformer,\n    \"core_ferc1__yearly_depreciation_changes_sched219\": DepreciationChangesTableTransformer,\n    \"core_ferc1__yearly_depreciation_by_function_sched219\": DepreciationByFunctionTableTransformer,\n    \"core_ferc1__yearly_retained_earnings_sched118\": RetainedEarningsTableTransformer,\n    \"core_ferc1__yearly_operating_revenues_sched300\": OperatingRevenuesTableTransformer,\n    \"core_ferc1__yearly_cash_flows_sched120\": CashFlowsTableTransformer,\n    \"core_ferc1__yearly_sales_by_rate_schedules_sched304\": SalesByRateSchedulesTableTransformer,\n    \"core_ferc1__yearly_other_regulatory_liabilities_sched278\": OtherRegulatoryLiabilitiesTableTransformer,\n}\n\n\ndef ferc1_transform_asset_factory(\n    table_name: str,\n    tfr_class: Ferc1AbstractTableTransformer,\n    io_manager_key: str = \"pudl_io_manager\",\n    convert_dtypes: bool = True,\n    generic: bool = False,\n) -> AssetsDefinition:\n    \"\"\"Create an asset that pulls in raw ferc Form 1 assets and applies transformations.\n\n    This is a convenient way to create assets for tables that only depend on raw dbf,\n    raw xbrl instant and duration tables and xbrl metadata.\n\n    Args:\n        table_name: The name of the table to create an asset for.\n        tfr_class: A transformer class cooresponding to the table_name.\n        io_manager_key: the dagster io_manager key to use. None defaults\n            to the fs_io_manager.\n        convert_dtypes: convert dtypes of transformed dataframes.\n        generic: If using GenericPlantFerc1TableTransformer pass table_id to constructor.\n\n    Return:\n        An asset for the clean table.\n    \"\"\"\n    ins: Mapping[str, AssetIn] = {}\n\n    listify = lambda x: x if isinstance(x, list) else [x]  # noqa: E731\n    dbf_tables = listify(TABLE_NAME_MAP_FERC1[table_name][\"dbf\"])\n    xbrl_tables = listify(TABLE_NAME_MAP_FERC1[table_name][\"xbrl\"])\n\n    ins = {f\"raw_dbf__{tn}\": AssetIn(f\"raw_ferc1_dbf__{tn}\") for tn in dbf_tables}\n    ins |= {\n        f\"raw_xbrl_instant__{tn}\": AssetIn(f\"raw_ferc1_xbrl__{tn}_instant\")\n        for tn in xbrl_tables\n    }\n    ins |= {\n        f\"raw_xbrl_duration__{tn}\": AssetIn(f\"raw_ferc1_xbrl__{tn}_duration\")\n        for tn in xbrl_tables\n    }\n    ins[\"_core_ferc1_xbrl__metadata_json\"] = AssetIn(\"_core_ferc1_xbrl__metadata_json\")\n\n    table_id = TableIdFerc1(table_name)\n\n    @asset(name=table_name, ins=ins, io_manager_key=io_manager_key)\n    def ferc1_transform_asset(**kwargs: dict[str, pd.DataFrame]) -> pd.DataFrame:\n        \"\"\"Transform a FERC Form 1 table.\n\n        Args:\n            raw_dbf: raw dbf table.\n            raw_xbrl_instant: raw XBRL instant table.\n            raw_xbrl_duration: raw XBRL duration table.\n            _core_ferc1_xbrl__metadata_json: XBRL metadata json for all tables.\n\n        Returns:\n            transformed FERC Form 1 table.\n        \"\"\"\n        # TODO: split the key by __, then groupby, then concatenate\n        _core_ferc1_xbrl__metadata_json = kwargs[\"_core_ferc1_xbrl__metadata_json\"]\n        if generic:\n            transformer = tfr_class(\n                xbrl_metadata_json=_core_ferc1_xbrl__metadata_json[table_name],\n                table_id=table_id,\n            )\n        else:\n            transformer = tfr_class(\n                xbrl_metadata_json=_core_ferc1_xbrl__metadata_json[table_name]\n            )\n\n        raw_dbf = pd.concat(\n            [df for key, df in kwargs.items() if key.startswith(\"raw_dbf__\")]\n        )\n        raw_xbrl_instant = pd.concat(\n            [df for key, df in kwargs.items() if key.startswith(\"raw_xbrl_instant__\")]\n        )\n        raw_xbrl_duration = pd.concat(\n            [df for key, df in kwargs.items() if key.startswith(\"raw_xbrl_duration__\")]\n        )\n        df = transformer.transform(\n            raw_dbf=raw_dbf,\n            raw_xbrl_instant=raw_xbrl_instant,\n            raw_xbrl_duration=raw_xbrl_duration,\n        )\n        if convert_dtypes:\n            df = convert_cols_dtypes(df, data_source=\"ferc1\")\n        return df\n\n    return ferc1_transform_asset\n\n\ndef create_ferc1_transform_assets() -> list[AssetsDefinition]:\n    \"\"\"Create a list of transformed FERC Form 1 assets.\n\n    Returns:\n        A list of AssetsDefinitions where each asset is a clean ferc form 1 table.\n    \"\"\"\n    assets = []\n    for table_name, tfr_class in FERC1_TFR_CLASSES.items():\n        assets.append(ferc1_transform_asset_factory(table_name, tfr_class))\n    return assets\n\n\nferc1_assets = create_ferc1_transform_assets()\n\n\ndef other_dimensions(table_names: list[str]) -> list[str]:\n    \"\"\"Get a list of the other dimension columns across all of the transformers.\"\"\"\n    # grab all of the dimensions columns that we are currently verifying as a part of\n    # reconcile_table_calculations\n    return pudl.helpers.dedupe_n_flatten_list_of_lists(\n        [\n            FERC1_TFR_CLASSES[table_name]().params.dimension_columns\n            for table_name in table_names\n        ]\n    )\n\n\ndef table_to_xbrl_factoid_name() -> dict[str, str]:\n    \"\"\"Build a dictionary of table name (keys) to ``xbrl_factoid`` column name.\"\"\"\n    return {\n        table_name: transformer().params.xbrl_factoid_name\n        for (table_name, transformer) in FERC1_TFR_CLASSES.items()\n    }\n\n\ndef table_to_column_to_check() -> dict[str, list[str]]:\n    \"\"\"Build a dictionary of table name (keys) to column_to_check from reconcile_table_calculations.\"\"\"\n    return {\n        table_name: transformer().params.reconcile_table_calculations.column_to_check\n        for (table_name, transformer) in FERC1_TFR_CLASSES.items()\n        if transformer().params.reconcile_table_calculations.column_to_check\n    }\n\n\n@asset(\n    ins={\n        table_name: AssetIn(table_name)\n        for table_name in FERC1_TFR_CLASSES\n        if table_name != \"plants_steam_ferc1\"\n    }\n)\ndef _core_ferc1__table_dimensions(**kwargs) -> pd.DataFrame:\n    \"\"\"Build a table of values of dimensions observed in the transformed data tables.\n\n    Compile a dataframe indicating what distinct values are observed in the data for\n    each dimension column in association with each unique combination of ``table_name``\n    and ``xbrl_factoid``. E.g. for all factoids found in the\n    :ref:`core_ferc1__yearly_depreciation_by_function_sched219` table,\n    the only value observed for ``utility_type`` is ``electric`` and the values observed\n    for ``plant_status`` include: ``future``, ``in_service``, ``leased`` and ``total``.\n\n    We need to include the ``xbrl_factoid`` column because these dimensions can differ\n    based on the ``xbrl_factoid``. So we first rename all of the columns which\n    contain the ``xbrl_factoid`` using :func:`table_to_xbrl_factoid_name` rename\n    dictionary. Then we concatenate all of the tables together and drop duplicates so\n    we have unique instances of observed ``table_name`` and ``xbrl_factoid`` and the\n    other dimension columns found in :func:`other_dimensions`.\n    \"\"\"\n    table_to_xbrl_factoid_name_dict = table_to_xbrl_factoid_name()\n    tbls = {\n        name: df.assign(table_name=name).rename(\n            columns={table_to_xbrl_factoid_name_dict[name]: \"xbrl_factoid\"}\n        )\n        for (name, df) in kwargs.items()\n    }\n    dimensions = (\n        pd.concat(tbls.values())[\n            [\"table_name\", \"xbrl_factoid\"]\n            + other_dimensions(table_names=list(FERC1_TFR_CLASSES))\n        ]\n        .drop_duplicates()\n        .reset_index(drop=True)\n    )\n    return dimensions\n\n\n@asset(\n    ins={\n        \"_core_ferc1_xbrl__metadata_json\": AssetIn(\"_core_ferc1_xbrl__metadata_json\"),\n        \"_core_ferc1__table_dimensions\": AssetIn(\"_core_ferc1__table_dimensions\"),\n    },\n    io_manager_key=None,  # Change to sqlite_io_manager...\n)\ndef _core_ferc1_xbrl__metadata(**kwargs) -> pd.DataFrame:\n    \"\"\"Build a table of all of the tables' XBRL metadata.\"\"\"\n    _core_ferc1_xbrl__metadata_json = kwargs[\"_core_ferc1_xbrl__metadata_json\"]\n    _core_ferc1__table_dimensions = kwargs[\"_core_ferc1__table_dimensions\"]\n    tbl_metas = []\n    for table_name, trans in FERC1_TFR_CLASSES.items():\n        tbl_meta = (\n            trans(xbrl_metadata_json=_core_ferc1_xbrl__metadata_json[table_name])\n            .xbrl_metadata[\n                [\n                    \"xbrl_factoid\",\n                    \"xbrl_factoid_original\",\n                    \"is_within_table_calc\",\n                ]\n            ]\n            .assign(table_name=table_name)\n        )\n        tbl_metas.append(tbl_meta)\n    dimensions = other_dimensions(table_names=list(FERC1_TFR_CLASSES))\n    metadata_all = (\n        pd.concat(tbl_metas)\n        .reset_index(drop=True)\n        .assign(**{dim: pd.NA for dim in dimensions})\n        .pipe(\n            make_xbrl_factoid_dimensions_explicit,\n            table_dimensions_ferc1=_core_ferc1__table_dimensions,\n            dimensions=dimensions,\n        )\n    )\n    return metadata_all\n\n\n@asset(\n    ins={\n        \"_core_ferc1_xbrl__metadata_json\": AssetIn(\"_core_ferc1_xbrl__metadata_json\"),\n        \"_core_ferc1__table_dimensions\": AssetIn(\"_core_ferc1__table_dimensions\"),\n        \"_core_ferc1_xbrl__metadata\": AssetIn(\"_core_ferc1_xbrl__metadata\"),\n    },\n    io_manager_key=None,  # Change to sqlite_io_manager...\n)\ndef _core_ferc1_xbrl__calculation_components(**kwargs) -> pd.DataFrame:\n    \"\"\"Create calculation-component table from table-level metadata.\"\"\"\n    _core_ferc1_xbrl__metadata_json = kwargs[\"_core_ferc1_xbrl__metadata_json\"]\n    _core_ferc1__table_dimensions = kwargs[\"_core_ferc1__table_dimensions\"]\n    _core_ferc1_xbrl__metadata = kwargs[\"_core_ferc1_xbrl__metadata\"]\n    # compile all of the calc comp tables.\n    calc_metas = []\n    for table_name, transformer in FERC1_TFR_CLASSES.items():\n        calc_meta = transformer(\n            xbrl_metadata_json=_core_ferc1_xbrl__metadata_json[table_name]\n        ).xbrl_calculations.convert_dtypes()\n        calc_metas.append(calc_meta)\n    # squish all of the calc comp tables then add in the implicit table dimensions\n    dimensions = other_dimensions(table_names=list(FERC1_TFR_CLASSES))\n    calc_components = (\n        pd.concat(calc_metas)\n        .astype({dim: pd.StringDtype() for dim in dimensions})\n        .pipe(\n            make_xbrl_factoid_dimensions_explicit,\n            _core_ferc1__table_dimensions,\n            dimensions=dimensions,\n        )\n        .pipe(\n            assign_parent_dimensions,\n            table_dimensions=_core_ferc1__table_dimensions,\n            dimensions=dimensions,\n        )\n        .pipe(\n            infer_intra_factoid_totals,\n            meta_w_dims=_core_ferc1_xbrl__metadata,\n            table_dimensions=_core_ferc1__table_dimensions,\n            dimensions=dimensions,\n        )\n    )\n\n    child_cols = [\"table_name\", \"xbrl_factoid\"]\n    calc_cols = child_cols + dimensions\n    calc_and_parent_cols = calc_cols + [f\"{col}_parent\" for col in calc_cols]\n\n    # Defensive testing on this table!\n    assert calc_components[[\"table_name\", \"xbrl_factoid\"]].notnull().all(axis=1).all()\n\n    # Let's check that all calculated components that show up in our data are\n    # getting calculated.\n    def check_calcs_vs_table(\n        calcs: pd.DataFrame,\n        checked_table: pd.DataFrame,\n        idx_calcs: list[str],\n        idx_table: list[str],\n        how: Literal[\"in\", \"not_in\"],\n    ) -> pd.DataFrame:\n        if how == \"in\":\n            idx = calcs.set_index(idx_calcs).index.intersection(\n                checked_table.set_index(idx_table).index\n            )\n        elif how == \"not_in\":\n            idx = calcs.set_index(idx_calcs).index.difference(\n                checked_table.set_index(idx_table).index\n            )\n        calcs_vs_table = calcs.set_index(idx_calcs).loc[idx]\n        return calcs_vs_table.reset_index()\n\n    # which calculations are missing from the metadata table?\n    missing_calcs = check_calcs_vs_table(\n        calcs=calc_components[\n            calc_components.table_name.isin(FERC1_TFR_CLASSES.keys())\n        ],\n        checked_table=_core_ferc1_xbrl__metadata,\n        idx_calcs=calc_cols,\n        idx_table=calc_cols,\n        how=\"not_in\",\n    )\n    # ensure that none of the calculation components that are missing from the metadata\n    # table are from any of the exploded tables.\n    if not missing_calcs.empty:\n        logger.warning(\n            \"Calculations found in calculation components table are missing from the \"\n            \"_core_ferc1_xbrl__metadata table.\"\n        )\n        # which of these missing calculations actually show up in the transformed tables?\n        # This handles dbf-only calculation components, whic are added to the\n        # _core_ferc1_xbrl__metadata table as part of each table's transformations but aren't\n        # observed (or therefore present in _core_ferc1__table_dimensions) in the fast ETL or\n        # in all subsets of years. We only want to flag calculation components as\n        # missing when they're actually observed in the data.\n        actually_missing_kids = check_calcs_vs_table(\n            calcs=missing_calcs,\n            checked_table=_core_ferc1__table_dimensions,\n            idx_calcs=child_cols,\n            idx_table=child_cols,\n            how=\"in\",\n        )\n        logger.warning(\n            f\"{len(actually_missing_kids)} of {len(missing_calcs)} missing calculation components observed in transformed FERC1 data.\"\n        )\n        if not actually_missing_kids.empty:\n            raise AssertionError(\n                f\"Found missing calculations from the exploded tables:\\n{actually_missing_kids=}\"\n            )\n\n    check_for_calc_components_duplicates(\n        calc_components,\n        table_names_known_dupes=[\"core_ferc1__yearly_sales_by_rate_schedules_sched304\"],\n        idx=calc_and_parent_cols,\n    )\n\n    # check for parent/child duplicates. again need to remove the\n    # core_ferc1__yearly_sales_by_rate_schedules_sched304 table. Null hack bc comparing pandas\n    # nulls\n    self_refs_mask = calc_components[calc_and_parent_cols].fillna(\"NULL HACK\").apply(\n        lambda x: all(x[col] == x[f\"{col}_parent\"] for col in calc_cols), axis=1\n    ) & (\n        calc_components.table_name\n        != \"core_ferc1__yearly_sales_by_rate_schedules_sched304\"\n    )\n    if not (parent_child_dupes := calc_components.loc[self_refs_mask]).empty:\n        raise AssertionError(\n            f\"Found {len(parent_child_dupes)} calcuations where the parent and child \"\n            f\"columns are identical and expected 0.\\n{parent_child_dupes=}\"\n        )\n\n    if not (\n        unexpected_totals := unexpected_total_components(\n            calc_components.convert_dtypes(), dimensions\n        )\n    ).empty:\n        raise AssertionError(f\"Found unexpected total records: {unexpected_totals}\")\n    # Remove convert_dtypes() once we're writing to the DB using enforce_schema()\n    return calc_components.convert_dtypes()\n\n\ndef unexpected_total_components(\n    calc_comps: pd.DataFrame, dimensions: list[str]\n) -> pd.DataFrame:\n    \"\"\"Find unexpected components in within-fact total calculations.\n\n    This doesn't check anything about the calcs we get from the metadata, we\n    are only looking at within-fact totals which we've added ourselves.\n\n    Finds calculation relationships where:\n\n    - child components that do not match with parent in non-total dimensions.\n\n      - For example, if utility_type_parent is not \"total\", then utility_type\n        must be the same as utility_type_parent.\n\n    - child components, that share table_name/xbrl_factoid with their parent,\n      that have \"total\" for any dimension - these should be represented by\n      *their* child components\n\n    Args:\n        calc_comps: calculation component join table\n        dimensions: list of dimensions we resolved \"total\" values for\n    \"\"\"\n    parent_dimensions = [f\"{dim}_parent\" for dim in dimensions]\n    totals_mask = (\n        (calc_comps[parent_dimensions] == \"total\").any(axis=\"columns\")\n        & (calc_comps[\"table_name_parent\"] == calc_comps[\"table_name\"])\n        & (calc_comps[\"xbrl_factoid_parent\"] == calc_comps[\"xbrl_factoid\"])\n    )\n    calcs_with_totals = calc_comps[totals_mask]\n\n    unexpected_links = []\n    for child_dim in dimensions:\n        mismatched_non_total = (\n            calcs_with_totals[f\"{child_dim}_parent\"] != calcs_with_totals[child_dim]\n        ) & (calcs_with_totals[f\"{child_dim}_parent\"] != \"total\")\n        children_with_totals = calcs_with_totals[child_dim] == \"total\"\n        unexpected_links.append(\n            calcs_with_totals[mismatched_non_total | children_with_totals][\n                [\"table_name_parent\", \"xbrl_factoid_parent\"]\n                + parent_dimensions\n                + [\"table_name\", \"xbrl_factoid\"]\n                + dimensions\n            ]\n        )\n    return pd.concat(unexpected_links)\n\n\ndef check_for_calc_components_duplicates(\n    calc_components: pd.DataFrame, table_names_known_dupes: list[str], idx: list[str]\n) -> None:\n    \"\"\"Check for duplicates calculation records.\n\n    We need to remove the core_ferc1__yearly_sales_by_rate_schedules_sched304 bc there are\n    duplicate renamed factoids in that table (originally billed/unbilled).\n    \"\"\"\n    calc_components_test = (\n        calc_components[\n            ~calc_components.table_name_parent.isin(table_names_known_dupes)\n        ]\n        .set_index(idx)\n        .sort_index()\n    )\n    if not calc_components_test.index.is_unique:\n        raise AssertionError(\n            f\"Found duplicates based on {idx=} when expected none.\\n\"\n            f\"{calc_components_test[calc_components_test.index.duplicated(keep=False)]}\"\n        )\n\n\ndef make_xbrl_factoid_dimensions_explicit(\n    df_w_xbrl_factoid: pd.DataFrame,\n    table_dimensions_ferc1: pd.DataFrame,\n    dimensions: list[str],\n    parent: bool = False,\n) -> pd.DataFrame:\n    \"\"\"Fill in null dimensions w/ the values observed in :func:`_core_ferc1__table_dimensions`.\n\n    In the raw XBRL metadata's calculations, there is an implicit assumption that\n    calculated values are aggregated within categorical columns called Axes or\n    dimensions, in addition to being grouped by date, utility, table, and fact. The\n    dimensions and their values don't need to be specified explicitly in the calculation\n    components because the same calculation is assumed to apply in all cases.\n\n    We have extended this calculation system to allow independent calculations to be\n    specified for different values within a given dimension. For example, the\n    :ref:`core_ferc1__yearly_utility_plant_summary_sched200` table contains records with a variety of\n    different ``utility_type`` values (gas, electric, etc.). For many combinations of\n    fact and ``utility_type``, no more detailed information about the soruce of the data\n    is available, but for some, and only in the case of electric utilities, much more\n    detail can be found in the :ref:`core_ferc1__yearly_plant_in_service_sched204` table.\n    In order to use this additional information when it is available, we sometimes\n    explicitly specify different calculations for different values of additional\n    dimension columns.\n\n    This function uses the observed associations between ``table_name``,\n    ``xbrl_factoid`` and the other dimension columns compiled by\n    :func:`_core_ferc1__table_dimensions` to fill in missing (previously implied) dimension\n    values in the calculation components table.\n\n    This is often a broadcast merge because many tables contain many values within these\n    dimension columns, so it is expected that new calculation component table will have\n    many more records than the input calculation components table.\n\n    Any dimension that was already specified in the calculation fixes will be left\n    unchanged. If no value of a particular dimension has ever been observed in\n    association with a given combination of ``table_name`` and ``xbrl_factoid`` it will\n    remain null.\n\n    Args:\n        calculation_components: a table of calculation component records which have had\n            some manual calculation fixes applied.\n        table_dimensions_ferc1: table with all observed values of\n            :func:`other_dimensions` for each ``table_name`` and ``xbrl_factoid``\n        dimensions: list of dimension columns to check.\n        parent: boolean to indicate whether or not the dimensions to be added are\n            the parental dimensions or the child dimensions.\n    \"\"\"\n    logger.info(f\"Adding {dimensions=} into calculation component table.\")\n    # even though we don't actually get correction records for all of the\n    # factiods, we are still going to force them to exist here so all of the\n    # downstream processes have them all. All table dim facts get correction\n    # records regardless of whether they are calculated values.\n    # The non-calcualted _correction records will be dropped in the left\n    # merge below while assigning df_w_implied_dims\n    non_correction_mask = ~table_dimensions_ferc1.xbrl_factoid.str.endswith(\n        \"_correction\"\n    )\n    table_dimensions_ferc1 = pd.concat(\n        [\n            table_dimensions_ferc1[non_correction_mask],\n            (\n                table_dimensions_ferc1[non_correction_mask].assign(\n                    xbrl_factoid=lambda x: x.xbrl_factoid + \"_correction\"\n                )\n            ),\n        ]\n    )\n    df_w_dims = df_w_xbrl_factoid.copy()\n    on_cols = [\"table_name\", \"xbrl_factoid\"]\n    if parent:\n        table_dimensions_ferc1 = table_dimensions_ferc1.rename(\n            columns={col: f\"{col}_parent\" for col in on_cols}\n            | {dim: f\"{dim}_parent\" for dim in dimensions}\n        )\n        on_cols = [f\"{col}_parent\" for col in on_cols]\n        dimensions = [f\"{dim}_parent\" for dim in dimensions]\n    # for each dimension, use split/apply/combine. when there are no dims explict in\n    # the calc components, merge in all of the dims.\n    for dim_col in dimensions:\n        # extract the unique observed instances of this one dimension column & add the\n        # _calc suffix so we can merge onto the calculation components.\n        observed_dim = (\n            table_dimensions_ferc1[on_cols + [dim_col]]\n            .drop_duplicates()\n            .dropna(subset=dim_col)\n        )  # bc there are dupes after we removed the other dim cols\n\n        null_dim_mask = df_w_dims[dim_col].isnull()\n        null_dim = df_w_dims[null_dim_mask].drop(columns=[dim_col])\n        df_w_implied_dims = pd.merge(\n            null_dim,\n            observed_dim,\n            on=on_cols,\n            how=\"left\",\n        )\n        df_w_explicit_dims = df_w_dims[~null_dim_mask]\n        # astypes dealing w/ future warning regarding all null columns\n        df_w_dims = pd.concat(\n            [\n                df_w_implied_dims,\n                df_w_explicit_dims.astype(df_w_implied_dims.dtypes, errors=\"ignore\"),\n            ]\n        )\n    return df_w_dims\n\n\ndef assign_parent_dimensions(\n    calc_components: pd.DataFrame, table_dimensions: pd.DataFrame, dimensions: list[str]\n) -> pd.DataFrame:\n    \"\"\"Add dimensions to calculation parents.\n\n    We now add in parent-dimension values for all of the original calculation component\n    records using the observed dimensions.\n\n    Args:\n        calc_components: a table of calculation component records which have had some\n            manual calculation fixes applied.\n        table_dimensions: table with all observed values of :func:`other_dimensions` for\n            each ``table_name`` and ``xbrl_factoid``.\n        dimensions: list of dimension columns to check.\n    \"\"\"\n    if calc_components.empty:\n        return calc_components.assign(**{f\"{dim}_parent\": pd.NA for dim in dimensions})\n    # desired: add parental dimension columns\n    for dim in dimensions:\n        # split the nulls and non-nulls. If the child dim is null, then we can run the\n        # parent factoid through make_xbrl_factoid_dimensions_explicit to get it's dims.\n        # if a child fact has dims, we need to merge the dimensions using the dim of the\n        # child and the dim of the parent bc we don't want to broadcast merge all parent\n        # dims to all child dims. We are assuming here that if a child is has a dim\n        null_dim_mask = calc_components[dim].isnull()\n        calc_components_null = make_xbrl_factoid_dimensions_explicit(\n            df_w_xbrl_factoid=calc_components[null_dim_mask].assign(\n                **{f\"{dim}_parent\": pd.NA}\n            ),\n            table_dimensions_ferc1=table_dimensions,\n            dimensions=[dim],\n        )\n        parent_dim_idx = [\"table_name_parent\", \"xbrl_factoid_parent\", f\"{dim}_parent\"]\n        calc_components_non_null = calc_components[~null_dim_mask]\n        table_dimensions_non_null = table_dimensions.rename(\n            columns={col: f\"{col}_parent\" for col in table_dimensions}\n        )[parent_dim_idx].drop_duplicates()\n        calc_components_non_null = pd.merge(\n            left=calc_components_non_null,\n            right=table_dimensions_non_null,\n            left_on=[\"table_name_parent\", \"xbrl_factoid_parent\", dim],\n            right_on=parent_dim_idx,\n            how=\"left\",\n        )\n        # astypes dealing w/ future warning regarding empty or all null dfs\n        calc_components = pd.concat(\n            [\n                calc_components_null.astype(calc_components_non_null.dtypes),\n                calc_components_non_null.astype(calc_components_null.dtypes),\n            ]\n        ).reset_index(drop=True)\n\n    return calc_components\n\n\ndef infer_intra_factoid_totals(\n    calc_components: pd.DataFrame,\n    meta_w_dims: pd.DataFrame,\n    table_dimensions: pd.DataFrame,\n    dimensions: list[str],\n) -> pd.DataFrame:\n    \"\"\"Define dimension total calculations.\n\n    Some factoids are marked as a total along some dimension in the metadata,\n    which means that they are the sum of all the non-total factoids along that\n    dimension.\n\n    We match the parent factoids from the metadata to child factoids from the\n    table_dimensions. We treat \"total\" as a wildcard value.\n\n    We exclude child factoids that are themselves totals, because that would\n    result in a double-count.\n\n    Here are a few examples:\n\n    Imagine a factoid with the following dimensions & values:\n\n    - utility types: \"total\", \"gas\", \"electric\";\n    - plant status: \"total\", \"in_service\", \"future\"\n\n    Then the following parents would match/not-match:\n\n    - parent: \"total\", \"in_service\"\n\n      - child: \"gas\", \"in_service\" WOULD match.\n      - child: \"electric\", \"in_service\" WOULD match.\n      - child: \"electric\", \"future\" WOULD NOT match.\n\n    - parent: \"total\", \"total\"\n\n      - child: \"gas\", \"in_service\" WOULD match.\n      - child: \"electric\", \"future\" WOULD match.\n\n    See the unit test in ferc1_test.py for more details.\n\n    To be able to define these within-dimension calculations we also add dimension\n    columns to all of the parent factoids in the table.\n\n    Args:\n        calc_components: a table of calculation component records which have had some\n            manual calculation fixes applied. Passed through unmodified.\n        meta_w_dims: metadata table with the dimensions.\n        table_dimensions: table with all observed values of :func:`other_dimensions` for\n            each ``table_name`` and ``xbrl_factoid``.\n        dimensions: list of dimension columns to check.\n\n    Returns:\n        An table associating calculation components with the parents they will be\n        aggregated into. The components and the parents are each identified by\n        ``table_name``, ``xbrl_factoid``, and columns defining the additional dimensions\n        (``utility_type``, ``plant_status``, ``plant_function``). The parent columns\n        have a ``_parent`` suffix.\n    \"\"\"\n    # grab all of the non-total and non-correction records\n    child_candidates = table_dimensions[\n        ~(table_dimensions[dimensions] == \"total\").any(axis=\"columns\")\n        & ~(table_dimensions.xbrl_factoid.str.endswith(\"_correction\"))\n    ]\n\n    total_comps = []\n\n    # check *every* combination of dimensions that could have any total values\n    dim_combos = itertools.chain.from_iterable(\n        itertools.combinations(dimensions, i + 1) for i in range(len(dimensions))\n    )\n    for _total_dims in dim_combos:\n        total_dims = list(_total_dims)\n        parents = meta_w_dims.dropna(subset=total_dims).loc[\n            (meta_w_dims[total_dims] == \"total\").all(axis=\"columns\")\n        ]\n        if parents.empty:\n            continue\n\n        # There's no wildcard merge in Pandas, so we just ignore whichever\n        # columns have \"total\"\n        non_total_cols = [\"table_name\", \"xbrl_factoid\"] + [\n            d for d in dimensions if d not in total_dims\n        ]\n        total_comps.append(\n            pd.merge(\n                left=parents,\n                right=child_candidates,\n                on=non_total_cols,\n                how=\"inner\",\n                suffixes=(\"_parent\", \"\"),\n            ).assign(\n                is_within_table_calc=True,\n                weight=1,\n                table_name_parent=lambda x: x.table_name,\n                xbrl_factoid_parent=lambda x: x.xbrl_factoid,\n            )\n        )\n\n    child_node_pk = [\"table_name\", \"xbrl_factoid\"] + dimensions\n    parent_node_pk = [f\"{col}_parent\" for col in child_node_pk]\n    relationship_cols = [\"is_within_table_calc\", \"weight\"]\n    all_expected_cols = parent_node_pk + child_node_pk + relationship_cols\n    inferred_totals = (\n        pd.concat(total_comps).reindex(columns=all_expected_cols).reset_index(drop=True)\n    )\n\n    # merge() will have dropped shared columns, so re-fill with child values:\n    child_values = inferred_totals[[\"table_name\", \"xbrl_factoid\"] + dimensions].rename(\n        lambda dim: f\"{dim}_parent\", axis=\"columns\"\n    )\n    inferred_totals = inferred_totals.fillna(child_values)\n    calcs_with_totals = pd.concat(\n        [\n            calc_components.assign(is_total_to_subdimensions_calc=False),\n            inferred_totals.assign(is_total_to_subdimensions_calc=True),\n        ]\n    )\n\n    # verification + deduping below.\n\n    check_for_calc_components_duplicates(\n        calcs_with_totals,\n        table_names_known_dupes=[\n            \"core_ferc1__yearly_sales_by_rate_schedules_sched304\",\n        ],\n        idx=parent_node_pk + child_node_pk,\n    )\n\n    # only drop duplicates if the table_name is in known dupes list.\n    calcs_with_totals = calcs_with_totals.drop_duplicates(\n        parent_node_pk + child_node_pk, keep=\"first\"\n    )\n    assert calcs_with_totals[calcs_with_totals.duplicated()].empty\n    return calcs_with_totals\n\n\n@asset(\n    ins={\n        table_name: AssetIn(table_name)\n        # list of tables that have reconcile_table_calculations params\n        # minus electric_plant_depreciation_changes_ferc1 bc that table is messy and\n        # not actually in the explosion work\n        for table_name in [\n            \"core_ferc1__yearly_plant_in_service_sched204\",\n            \"core_ferc1__yearly_utility_plant_summary_sched200\",\n            \"core_ferc1__yearly_operating_expenses_sched320\",\n            \"core_ferc1__yearly_balance_sheet_liabilities_sched110\",\n            \"core_ferc1__yearly_depreciation_summary_sched336\",\n            \"core_ferc1__yearly_balance_sheet_assets_sched110\",\n            \"core_ferc1__yearly_income_statements_sched114\",\n            \"core_ferc1__yearly_depreciation_by_function_sched219\",\n            \"core_ferc1__yearly_retained_earnings_sched118\",\n            \"core_ferc1__yearly_operating_revenues_sched300\",\n        ]\n    }\n    | {\n        \"_core_ferc1_xbrl__calculation_components\": AssetIn(\n            \"_core_ferc1_xbrl__calculation_components\"\n        )\n    },\n)\ndef _core_ferc1__calculation_metric_checks(**kwargs):\n    \"\"\"Check calculation metrics for all transformed tables which have reconciled calcs.\"\"\"\n    calculation_components = kwargs[\"_core_ferc1_xbrl__calculation_components\"]\n    transformed_ferc1_dfs = {\n        name: df\n        for (name, df) in kwargs.items()\n        if name not in [\"_core_ferc1_xbrl__calculation_components\"]\n    }\n    # standardize the two key columns we are going to use into generic names\n    xbrl_factoid_name = table_to_xbrl_factoid_name()\n    columns_to_check = table_to_column_to_check()\n    tbls = [\n        df.assign(table_name=name).rename(\n            columns={\n                xbrl_factoid_name[name]: \"xbrl_factoid\",\n                columns_to_check[name]: \"column_to_check\",\n            }\n        )\n        for (name, df) in transformed_ferc1_dfs.items()\n    ]\n    transformed_ferc1 = pd.concat(tbls)\n    # restrict the calculation components to only the bits we want\n    calculation_components = calculation_components[\n        # remove total to subdimensions\n        ~calculation_components.is_total_to_subdimensions_calc\n        # only the intra table calcs\n        & calculation_components.is_within_table_calc\n        # remove corrections (bc they would clean up the calcs so the errors wouldn't show up)\n        & ~calculation_components.xbrl_factoid.str.contains(\"_correction\")\n        # remove all of the tables that aren't in this check\n        & calculation_components.table_name_parent.isin(transformed_ferc1_dfs.keys())\n    ]\n\n    calc_idx = [\"xbrl_factoid\", \"table_name\"] + other_dimensions(\n        table_names=list(FERC1_TFR_CLASSES)\n    )\n    calculated_df = calculate_values_from_components(\n        data=transformed_ferc1,\n        calculation_components=calculation_components,\n        calc_idx=calc_idx,\n        value_col=\"column_to_check\",\n    )\n    calculation_metrics = check_calculation_metrics_by_group(\n        calculated_df=calculated_df,\n        group_metric_checks=GroupMetricChecks(\n            groups_to_check=[\n                \"ungrouped\",\n                \"table_name\",\n                \"xbrl_factoid\",\n                \"utility_id_ferc1\",\n                \"report_year\",\n            ]\n        ),\n    )\n\n    errors = calculation_metrics[\n        calculation_metrics.filter(like=\"is_error\").any(axis=1)\n    ]\n    if len(errors) > 42:\n        raise AssertionError(\n            f\"Found {len(errors)} from the results of check_calculation_metrics_by_group\"\n            f\"with default group values when less than 41 was expected.\\n{errors}\"\n        )\n    return calculation_metrics\n", "import_text": ["enum", "importlib.resources", "itertools", "json", "re", "abc.abstractmethod", "collections.namedtuple", "collections.abc.Mapping", "typing.Annotated", "typing.Any", "typing.Literal", "typing.Self", "numpy", "pandas", "sqlalchemy", "dagster.AssetIn", "dagster.AssetsDefinition", "dagster.asset", "pandas.core.groupby.DataFrameGroupBy", "pydantic.BaseModel", "pydantic.Field", "pydantic.field_validator", "pudl", "pudl.extract.ferc1.TABLE_NAME_MAP_FERC1", "pudl.helpers.assert_cols_areclose", "pudl.helpers.convert_cols_dtypes", "pudl.metadata.fields.apply_pudl_dtypes", "pudl.settings.Ferc1Settings", "pudl.transform.classes.AbstractTableTransformer", "pudl.transform.classes.InvalidRows", "pudl.transform.classes.RenameColumns", "pudl.transform.classes.TableTransformParams", "pudl.transform.classes.TransformParams", "pudl.transform.classes.cache_df", "pudl.transform.classes.enforce_snake_case"], "prompt": "\"\"\"\nDescription: This function is used to fill a DataFrame with a DBF-XBRL map.\n\nArgs:\n    df (pd.DataFrame): The DataFrame to be filled.\n    dbf_years (list[int] | None): The list of years to be used in the DBF-XBRL map. If None, the list of years is taken from Ferc1Settings().dbf_years.\n\nReturns:\n    pd.DataFrame: The filled DataFrame.\n\nRaises:\n    ValueError: If the first year that we're trying to produce isn't mapped, or if there are any NA values in the DBF-XBRL map.\n\nNotes:\n    The function uses pandas.MultiIndex.from_product to create an index containing all combinations of report_year and row_number.\n    It then concatenates the row map with the empty index, so we have blank spaces to fill.\n    It uses the transform method to forward fill missing XBRL column names, until a new definition for the row number is encountered.\n    It drops NA values produced in the broadcasting merge onto the exhaustive index.\n    If there are any NA values left at this point, it raises a ValueError.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Forward-fill missing years in the minimal, manually compiled DBF to XBRL mapping.\n\n    The relationship between a DBF row and XBRL column/fact/entity/whatever is mostly\n    consistent from year to year. To minimize the amount of manual mapping work we have\n    to do, we only map the years in which the relationship changes. In the end we do\n    need a complete correspondence for all years though, and this function uses the\n    minimal information we've compiled to fill in all the gaps, producing a complete\n    mapping across all requested years.\n\n    One complication is that we need to explicitly indicate which DBF rows have headers\n    in them (which don't exist in XBRL), to differentiate them from null values in the\n    exhaustive index we create below. We set a ``HEADER_ROW`` sentinel value so we can\n    distinguish between two different reasons that we might find NULL values in the\n    ``xbrl_factoid`` field:\n\n    1. It's NULL because it's between two valid mapped values (the NULL was created\n       in our filling of the time series) and should thus be filled in, or\n\n    2. It's NULL because it was a header row in the DBF data, which means it should\n       NOT be filled in. Without the ``HEADER_ROW`` value, when a row number from year X\n       becomes associated with a non-header row in year X+1 the ffill will keep right on\n       filling, associating all of the new header rows with the value of\n       ``xbrl_factoid`` that was associated with the old row number.\n\n    Args:\n        df: A dataframe containing a DBF row to XBRL mapping for a single FERC 1 DBF\n            table.\n        dbf_years: The list of years that should have their DBF row to XBRL mapping\n            filled in. This defaults to all available years of DBF data for FERC 1. In\n            general this parameter should only be set to a non-default value for testing\n            purposes.\n\n    Returns:\n        A complete mapping of DBF row number to XBRL columns for all years of data\n        within a single FERC 1 DBF table. Has columns of\n        ``[report_year, row_number, xbrl_factoid]``\n    \"\"\"", "function_dependencies": ["pudl.settings.Ferc1Settings", "pandas.MultiIndex.from_product", "pandas.concat", "pandas.concat.reset_index", "pandas.DataFrame", "pandas.concat.reset_index.set_index", "pandas.concat.reset_index.groupby", "pandas.concat.reset_index.groupby.transform", "pandas.concat.reset_index.dropna", "pandas.concat.reset_index.dropna.isnull", "pandas.concat.reset_index.dropna.isnull.any"], "project_create_time": "2017-02-01T17:45:40+00:00", "project_update_time": "2024-04-11T14:14:17+00:00", "file_create_time": "2021-09-22T10:35:52Z", "file_update_time": "2024-02-20T16:51:17Z", "function_update_time": "2024-02-20T16:51:17Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["pandas.MultiIndex.from_product"], "test_function": [{"file_path": "/pudl-v2024.2.6/pudl-2024.2.6/test/unit/transform/ferc1_test.py", "class_name": null, "function_name": "test_dbf_to_xbrl_mapping_is_unique", "code": "def test_dbf_to_xbrl_mapping_is_unique(dbf_table_name):\n    dbf_xbrl_map = fill_dbf_to_xbrl_map(\n        df=read_dbf_to_xbrl_map(dbf_table_names=[dbf_table_name]),\n        dbf_years=Ferc1Settings().dbf_years,\n    )\n    dbf_xbrl_map = dbf_xbrl_map[dbf_xbrl_map.xbrl_factoid != \"HEADER_ROW\"]\n    dbf_to_xbrl_mapping_is_unique = (\n        dbf_xbrl_map.groupby([\"report_year\", \"xbrl_factoid\"])[\"row_number\"]\n        .nunique()\n        .le(1)\n        .all()\n    )\n\n    assert dbf_to_xbrl_mapping_is_unique  # nosec: B101"}, {"file_path": "/pudl-v2024.2.6/pudl-2024.2.6/test/unit/transform/ferc1_test.py", "class_name": null, "function_name": "test_fill_dbf_to_xbrl_map", "code": "def test_fill_dbf_to_xbrl_map():\n    expected = pd.read_csv(\n        StringIO(\n            \"\"\"\nsched_table_name,report_year,row_literal,row_number,xbrl_factoid\ntest_table1,2000,\"Account A\",2,account_a\ntest_table1,2000,\"Account B\",3,account_b\ntest_table1,2000,\"Account C\",5,account_c\ntest_table1,2001,\"Account A\",2,account_a\ntest_table1,2001,\"Account B\",3,account_b\ntest_table1,2001,\"Account C\",5,account_c\ntest_table1,2002,\"Account A\",2,account_a\ntest_table1,2002,\"Account B\",3,account_b\ntest_table1,2002,\"Account B1\",4,account_b1\ntest_table1,2002,\"Account C\",6,account_c\ntest_table1,2003,\"Account A\",2,account_a\ntest_table1,2003,\"Account B\",3,account_b\ntest_table1,2003,\"Account B1\",4,account_b1\ntest_table1,2003,\"Account C\",6,account_c\n\"\"\"\n        )\n    )\n    test_map = TEST_DBF_XBRL_MAP.reset_index(drop=True)\n    actual = fill_dbf_to_xbrl_map(df=test_map, dbf_years=sorted(range(2000, 2004)))\n    actual = actual[actual.xbrl_factoid != \"HEADER_ROW\"].reset_index(drop=True)\n    pd.testing.assert_frame_equal(actual, expected, check_like=True)"}, {"file_path": "/pudl-v2024.2.6/pudl-2024.2.6/test/unit/transform/ferc1_test.py", "class_name": null, "function_name": "test_two_table_fill_dbf_to_xbrl_map", "code": "def test_two_table_fill_dbf_to_xbrl_map():\n    expected = pd.read_csv(\n        StringIO(\n            \"\"\"\nsched_table_name,report_year,row_number,xbrl_factoid,row_literal\ntest_table1,2000,2,account_a,\"Account A\"\ntest_table1,2000,3,account_b,\"Account B\"\ntest_table1,2000,5,account_c,\"Account C\"\ntest_table2,2000,7,account_d,\"Account D\"\ntest_table2,2000,8,account_e,\"Account E\"\ntest_table1,2001,2,account_a,\"Account A\"\ntest_table1,2001,3,account_b,\"Account B\"\ntest_table1,2001,5,account_c,\"Account C\"\ntest_table2,2001,7,account_d,\"Account D\"\ntest_table2,2001,8,account_e,\"Account E\"\ntest_table1,2002,2,account_a,\"Account A\"\ntest_table1,2002,3,account_b,\"Account B\"\ntest_table1,2002,4,account_b1,\"Account B1\"\ntest_table1,2002,6,account_c,\"Account C\"\ntest_table2,2002,8,account_d,\"Account D\"\ntest_table2,2002,9,account_e,\"Account E\"\ntest_table1,2003,2,account_a,\"Account A\"\ntest_table1,2003,3,account_b,\"Account B\"\ntest_table1,2003,4,account_b1,\"Account B1\"\ntest_table1,2003,6,account_c,\"Account C\"\ntest_table2,2003,8,account_d,\"Account D\"\ntest_table2,2003,9,account_e,\"Account E\"\n\"\"\"\n        )\n    )\n    test_map = pd.concat(\n        [TEST_DBF_XBRL_MAP, TEST_MUTLI_TABLE_DBF_XBRL_MAP]\n    ).reset_index(drop=True)\n    actual = fill_dbf_to_xbrl_map(df=test_map, dbf_years=sorted(range(2000, 2004)))\n    actual = actual[actual.xbrl_factoid != \"HEADER_ROW\"].reset_index(drop=True)\n    pd.testing.assert_frame_equal(actual, expected, check_like=True)"}]}, {"git_group": "baal-org", "git_name": "baal", "version": "v1.9.2", "language": "Python", "project_name": "baal-v1.9.2.zip", "file_path": "/baal-v1.9.2/baal-1.9.2/baal/utils/equality.py", "file_name": "equality.py", "focal_class": null, "focal_name": "deep_check", "focal_parameter": ["obj1", "obj2"], "solution": "\ndef deep_check(obj1, obj2) -> bool:\n    if type(obj1) != type(obj2):\n        return False\n    elif isinstance(obj1, str):\n        return bool(obj1 == obj2)\n    elif isinstance(obj1, Sequence):\n        return all(deep_check(i1, i2) for i1, i2 in zip(obj1, obj2))\n    elif isinstance(obj1, Mapping):\n        return all(deep_check(val1, obj2[key1]) for key1, val1 in obj1.items())\n    elif isinstance(obj1, Tensor):\n        return torch.equal(obj1, obj2)\n    elif isinstance(obj1, np.ndarray):\n        return bool((obj1 == obj2).all())\n    else:\n        return bool(obj1 == obj2)", "function_signature": "def deep_check(obj1, obj2) -> bool :", "left_context": "from typing import Sequence, Mapping\n\nimport numpy as np\nimport torch\nfrom torch import Tensor\n\n", "right_context": "", "import_text": ["typing.Sequence", "typing.Mapping", "numpy", "torch", "torch.Tensor"], "prompt": "\"\"\"\nDescription: This function performs a deep comparison between two objects.\n\nArgs:\n    obj1 (any): The first object to compare.\n    obj2 (any): The second object to compare.\n\nReturns:\n    bool: True if the objects are deeply equal, False otherwise.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["torch.equal"], "project_create_time": "2019-09-30T20:16:26+00:00", "project_update_time": "2024-04-14T13:11:15+00:00", "file_create_time": "2022-10-02T16:20:32Z", "file_update_time": "2022-10-02T16:20:32Z", "function_update_time": "2022-10-02T16:20:32Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["torch.equal"], "test_function": [{"file_path": "/baal-v1.9.2/baal-1.9.2/tests/utils/test_equality.py", "class_name": null, "function_name": "test_deep_check", "code": "\ndef test_deep_check():\n    arr1, arr2 = np.random.rand(10), np.random.rand(10)\n    tensor1, tensor2 = torch.rand([10]), torch.rand([10])\n    s1, s2 = \"string1\", \"string2\"\n    p1, p2 = Point(x=1, y=2), Point(x=2, y=1)\n\n    assert not deep_check(arr1, arr2)\n    assert not deep_check(tensor1, tensor2)\n    assert not deep_check(s1, s2)\n    assert not deep_check(p1, p2)\n    assert not deep_check([arr1, tensor1], [arr2, tensor2])\n    assert not deep_check([arr1, tensor1], (arr1, tensor1))\n    assert not deep_check([arr1, tensor1], [tensor1, arr1])\n    assert not deep_check({'x': arr1, 'y': tensor1}, {'x': arr2, 'y': tensor2})\n    assert not deep_check({'x': arr1, 'y': tensor1}, {'x': tensor1, 'y': arr1})\n\n    assert deep_check(arr1, arr1)\n    assert deep_check(tensor1, tensor1)\n    assert deep_check(s1, s1)\n    assert deep_check(p1, p1)\n    assert deep_check([arr1, tensor1], [arr1, tensor1])\n    assert deep_check((arr1, tensor1), (arr1, tensor1))\n    assert deep_check({'x': arr1, 'y': tensor1}, {'x': arr1, 'y': tensor1})"}]}, {"git_group": "beta-team", "git_name": "beta-recsys", "version": "v0.3.3", "language": "Python", "project_name": "beta-recsys-v0.3.3.zip", "file_path": "/beta-recsys-v0.3.3/beta-recsys-0.3.3/beta_rec/utils/evaluation.py", "file_name": "evaluation.py", "focal_class": null, "focal_name": "logloss", "focal_parameter": ["rating_true", "rating_pred"], "solution": "def logloss(\n    rating_true,\n    rating_pred,\n    col_user=DEFAULT_USER_COL,\n    col_item=DEFAULT_ITEM_COL,\n    col_rating=DEFAULT_RATING_COL,\n    col_prediction=DEFAULT_PREDICTION_COL,\n):\n    y_true, y_pred = merge_rating_true_pred(\n        rating_true=rating_true,\n        rating_pred=rating_pred,\n        col_user=col_user,\n        col_item=col_item,\n        col_rating=col_rating,\n        col_prediction=col_prediction,\n    )\n    return log_loss(y_true, y_pred)", "function_signature": "def logloss(\n    rating_true,\n    rating_pred,\n    col_user=DEFAULT_USER_COL,\n    col_item=DEFAULT_ITEM_COL,\n    col_rating=DEFAULT_RATING_COL,\n    col_prediction=DEFAULT_PREDICTION_COL,\n) :", "left_context": "from functools import lru_cache, wraps\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import (\n    explained_variance_score,\n    log_loss,\n    mean_absolute_error,\n    mean_squared_error,\n    r2_score,\n    roc_auc_score,\n)\n\nfrom ..utils.constants import (\n    DEFAULT_ITEM_COL,\n    DEFAULT_K,\n    DEFAULT_PREDICTION_COL,\n    DEFAULT_RATING_COL,\n    DEFAULT_THRESHOLD,\n    DEFAULT_USER_COL,\n)\n\n\nclass PandasHash:\n    \"\"\"Wrapper class to allow pandas objects (DataFrames or Series) to be hashable.\"\"\"\n\n    # reserve space just for a single pandas object\n    __slots__ = \"pandas_object\"\n\n    def __init__(self, pandas_object):\n        \"\"\"Initialize PandasHash class.\n\n        Args:\n            pandas_object (pd.DataFrame|pd.Series): pandas object.\n        \"\"\"\n        if not isinstance(pandas_object, (pd.DataFrame, pd.Series)):\n            raise TypeError(\"Can only wrap pandas DataFrame or Series objects\")\n        self.pandas_object = pandas_object\n\n    def __eq__(self, other):\n        \"\"\"Overwrite equality comparison.\n\n        Args:\n            other (pd.DataFrame|pd.Series): pandas object to compare.\n\n        Returns:\n            bool: whether other object is the same as this one.\n        \"\"\"\n        return hash(self) == hash(other)\n\n    def __hash__(self):\n        \"\"\"Overwrite hash operator for use with pandas objects.\n\n        Returns:\n            int: hashed value of object.\n        \"\"\"\n        hashable = tuple(self.pandas_object.values.tobytes())\n        if isinstance(self.pandas_object, pd.DataFrame):\n            hashable += tuple(self.pandas_object.columns)\n        else:\n            hashable += tuple(self.pandas_object.name)\n        return hash(hashable)\n\n\ndef has_columns(df, columns):\n    \"\"\"Check if DataFrame has necessary columns.\n\n    Args:\n        df (pd.DataFrame): DataFrame.\n        columns (list(str): columns to check for.\n\n    Returns:\n        bool: True if DataFrame has specified columns.\n    \"\"\"\n    result = True\n    for column in columns:\n        if column not in df.columns:\n            print(\"Missing column: {} in DataFrame\".format(column))\n            result = False\n\n    return result\n\n\ndef has_same_base_dtype(df_1, df_2, columns=None):\n    \"\"\"Check if specified columns have the same base dtypes across both DataFrames.\n\n    Args:\n        df_1 (pd.DataFrame): first DataFrame.\n        df_2 (pd.DataFrame): second DataFrame.\n        columns (list(str)): columns to check, None checks all columns.\n\n    Returns:\n        bool: True if DataFrames columns have the same base dtypes.\n    \"\"\"\n    if columns is None:\n        if any(set(df_1.columns).symmetric_difference(set(df_2.columns))):\n            print(\n                \"Cannot test all columns because they are not all shared across DataFrames\"\n            )\n            return False\n        columns = df_1.columns\n\n    if not (\n        has_columns(df=df_1, columns=columns) and has_columns(df=df_2, columns=columns)\n    ):\n        return False\n\n    result = True\n    for column in columns:\n        if df_1[column].dtype.type.__base__ != df_2[column].dtype.type.__base__:\n            print(\"Columns {} do not have the same base datatype\".format(column))\n            result = False\n\n    return result\n\n\ndef check_column_dtypes(func):\n    \"\"\"Check columns of DataFrame inputs.\n\n    This includes the checks on\n        1. whether the input columns exist in the input DataFrames.\n        2. whether the data types of col_user as well as col_item are matched in the two input DataFrames.\n\n    Args:\n        func (function): function that will be wrapped.\n    \"\"\"\n\n    @wraps(func)\n    def check_column_dtypes_wrapper(\n        rating_true,\n        rating_pred,\n        col_user=DEFAULT_USER_COL,\n        col_item=DEFAULT_ITEM_COL,\n        col_rating=DEFAULT_RATING_COL,\n        col_prediction=DEFAULT_PREDICTION_COL,\n        *args,\n        **kwargs\n    ):\n        \"\"\"Check columns of DataFrame inputs.\n\n        Args:\n            rating_true (pd.DataFrame): True data.\n            rating_pred (pd.DataFrame): Predicted data.\n            col_user (str): column name for user.\n            col_item (str): column name for item.\n            col_rating (str): column name for rating.\n            col_prediction (str): column name for prediction.\n        \"\"\"\n        if not has_columns(rating_true, [col_user, col_item, col_rating]):\n            raise ValueError(\"Missing columns in true rating DataFrame\")\n        if not has_columns(rating_pred, [col_user, col_item, col_prediction]):\n            raise ValueError(\"Missing columns in predicted rating DataFrame\")\n        if not has_same_base_dtype(\n            rating_true, rating_pred, columns=[col_user, col_item]\n        ):\n            raise ValueError(\"Columns in provided DataFrames are not the same datatype\")\n\n        return func(\n            rating_true=rating_true,\n            rating_pred=rating_pred,\n            col_user=col_user,\n            col_item=col_item,\n            col_rating=col_rating,\n            col_prediction=col_prediction,\n            *args,\n            **kwargs\n        )\n\n    return check_column_dtypes_wrapper\n\n\ndef lru_cache_df(maxsize, typed=False):\n    \"\"\"Least-recently-used cache decorator.\n\n    Args:\n        maxsize (int|None): max size of cache, if set to None cache is boundless.\n        typed (bool): arguments of different types are cached separately.\n    \"\"\"\n\n    def to_pandas_hash(val):\n        \"\"\"Return PandaHash object if input is a DataFrame otherwise return input unchanged.\"\"\"\n        return PandasHash(val) if isinstance(val, pd.DataFrame) else val\n\n    def from_pandas_hash(val):\n        \"\"\"Extract DataFrame if input is PandaHash object otherwise return input unchanged.\"\"\"\n        return val.pandas_object if isinstance(val, PandasHash) else val\n\n    def decorating_function(user_function):\n        @wraps(user_function)\n        def wrapper(*args, **kwargs):\n            # convert DataFrames in args and kwargs to PandaHash objects\n            args = tuple([to_pandas_hash(a) for a in args])\n            kwargs = {k: to_pandas_hash(v) for k, v in kwargs.items()}\n            return cached_wrapper(*args, **kwargs)\n\n        @lru_cache(maxsize=maxsize, typed=typed)\n        def cached_wrapper(*args, **kwargs):\n            # get DataFrames from PandaHash objects in args and kwargs\n            args = tuple([from_pandas_hash(a) for a in args])\n            kwargs = {k: from_pandas_hash(v) for k, v in kwargs.items()}\n            return user_function(*args, **kwargs)\n\n        # retain lru_cache attributes\n        wrapper.cache_info = cached_wrapper.cache_info\n        wrapper.cache_clear = cached_wrapper.cache_clear\n\n        return wrapper\n\n    return decorating_function\n\n\n@check_column_dtypes\n@lru_cache_df(maxsize=1)\ndef merge_rating_true_pred(\n    rating_true,\n    rating_pred,\n    col_user=DEFAULT_USER_COL,\n    col_item=DEFAULT_ITEM_COL,\n    col_rating=DEFAULT_RATING_COL,\n    col_prediction=DEFAULT_PREDICTION_COL,\n):\n    \"\"\"Join truth and prediction data frames on userID and itemID.\n\n    Joint truth and prediction DataFrames on userID and itemID and return the true and predicted rated with the\n    correct index.\n\n    Args:\n        rating_true (pd.DataFrame): True data.\n        rating_pred (pd.DataFrame): Predicted data.\n        col_user (str): column name for user.\n        col_item (str): column name for item.\n        col_rating (str): column name for rating.\n        col_prediction (str): column name for prediction.\n\n    Returns:\n        np.array: Array with the true ratings.\n        np.array: Array with the predicted ratings.\n    \"\"\"\n    # pd.merge will apply suffixes to columns which have the same name across both dataframes\n    suffixes = [\"_true\", \"_pred\"]\n    rating_true_pred = pd.merge(\n        rating_true, rating_pred, on=[col_user, col_item], suffixes=suffixes\n    )\n    if col_rating in rating_pred.columns:\n        col_rating = col_rating + suffixes[0]\n    if col_prediction in rating_true.columns:\n        col_prediction = col_prediction + suffixes[1]\n    return rating_true_pred[col_rating], rating_true_pred[col_prediction]\n\n\ndef rmse(\n    rating_true,\n    rating_pred,\n    col_user=DEFAULT_USER_COL,\n    col_item=DEFAULT_ITEM_COL,\n    col_rating=DEFAULT_RATING_COL,\n    col_prediction=DEFAULT_PREDICTION_COL,\n):\n    \"\"\"Calculate Root Mean Squared Error.\n\n    Args:\n        rating_true (pd.DataFrame): True data. There should be no duplicate (userID, itemID) pairs.\n        rating_pred (pd.DataFrame): Predicted data. There should be no duplicate (userID, itemID) pairs.\n        col_user (str): column name for user.\n        col_item (str): column name for item.\n        col_rating (str): column name for rating.\n        col_prediction (str): column name for prediction.\n\n    Returns:\n        float: Root mean squared error.\n    \"\"\"\n    y_true, y_pred = merge_rating_true_pred(\n        rating_true=rating_true,\n        rating_pred=rating_pred,\n        col_user=col_user,\n        col_item=col_item,\n        col_rating=col_rating,\n        col_prediction=col_prediction,\n    )\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n\ndef mae(\n    rating_true,\n    rating_pred,\n    col_user=DEFAULT_USER_COL,\n    col_item=DEFAULT_ITEM_COL,\n    col_rating=DEFAULT_RATING_COL,\n    col_prediction=DEFAULT_PREDICTION_COL,\n):\n    \"\"\"Calculate Mean Absolute Error.\n\n    Args:\n        rating_true (pd.DataFrame): True data. There should be no duplicate (userID, itemID) pairs.\n        rating_pred (pd.DataFrame): Predicted data. There should be no duplicate (userID, itemID) pairs.\n        col_user (str): column name for user.\n        col_item (str): column name for item.\n        col_rating (str): column name for rating.\n        col_prediction (str): column name for prediction.\n\n    Returns:\n        float: Mean Absolute Error.\n    \"\"\"\n    y_true, y_pred = merge_rating_true_pred(\n        rating_true=rating_true,\n        rating_pred=rating_pred,\n        col_user=col_user,\n        col_item=col_item,\n        col_rating=col_rating,\n        col_prediction=col_prediction,\n    )\n    return mean_absolute_error(y_true, y_pred)\n\n\ndef rsquared(\n    rating_true,\n    rating_pred,\n    col_user=DEFAULT_USER_COL,\n    col_item=DEFAULT_ITEM_COL,\n    col_rating=DEFAULT_RATING_COL,\n    col_prediction=DEFAULT_PREDICTION_COL,\n):\n    \"\"\"Calculate R squared.\n\n    Args:\n        rating_true (pd.DataFrame): True data. There should be no duplicate (userID, itemID) pairs.\n        rating_pred (pd.DataFrame): Predicted data. There should be no duplicate (userID, itemID) pairs.\n        col_user (str): column name for user.\n        col_item (str): column name for item.\n        col_rating (str): column name for rating.\n        col_prediction (str): column name for prediction.\n\n    Returns:\n        float: R squared (min=0, max=1).\n    \"\"\"\n    y_true, y_pred = merge_rating_true_pred(\n        rating_true=rating_true,\n        rating_pred=rating_pred,\n        col_user=col_user,\n        col_item=col_item,\n        col_rating=col_rating,\n        col_prediction=col_prediction,\n    )\n    return r2_score(y_true, y_pred)\n\n\ndef exp_var(\n    rating_true,\n    rating_pred,\n    col_user=DEFAULT_USER_COL,\n    col_item=DEFAULT_ITEM_COL,\n    col_rating=DEFAULT_RATING_COL,\n    col_prediction=DEFAULT_PREDICTION_COL,\n):\n    \"\"\"Calculate explained variance.\n\n    Args:\n        rating_true (pd.DataFrame): True data. There should be no duplicate (userID, itemID) pairs.\n        rating_pred (pd.DataFrame): Predicted data. There should be no duplicate (userID, itemID) pairs.\n        col_user (str): column name for user.\n        col_item (str): column name for item.\n        col_rating (str): column name for rating.\n        col_prediction (str): column name for prediction.\n\n    Returns:\n        float: Explained variance (min=0, max=1).\n    \"\"\"\n    y_true, y_pred = merge_rating_true_pred(\n        rating_true=rating_true,\n        rating_pred=rating_pred,\n        col_user=col_user,\n        col_item=col_item,\n        col_rating=col_rating,\n        col_prediction=col_prediction,\n    )\n    return explained_variance_score(y_true, y_pred)\n\n\ndef auc(\n    rating_true,\n    rating_pred,\n    col_user=DEFAULT_USER_COL,\n    col_item=DEFAULT_ITEM_COL,\n    col_rating=DEFAULT_RATING_COL,\n    col_prediction=DEFAULT_PREDICTION_COL,\n):\n    \"\"\"Calculate the Area-Under-Curve metric.\n\n    Calculate the Aread-Under-Curve metric for implicit feedback typed recommender, where rating is binary and\n    prediction is float number ranging from 0 to 1.\n\n    https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve\n\n    Note:\n        The evaluation does not require a leave-one-out scenario.\n        This metric does not calculate group-based AUC which considers the AUC scores\n        averaged across users. It is also not limited to k. Instead, it calculates the\n        scores on the entire prediction results regardless the users.\n\n    Args:\n        rating_true (pd.DataFrame): True data.\n        rating_pred (pd.DataFrame): Predicted data.\n        col_user (str): column name for user.\n        col_item (str): column name for item.\n        col_rating (str): column name for rating.\n        col_prediction (str): column name for prediction.\n\n    Returns:\n        float: auc_score (min=0, max=1).\n    \"\"\"\n    y_true, y_pred = merge_rating_true_pred(\n        rating_true=rating_true,\n        rating_pred=rating_pred,\n        col_user=col_user,\n        col_item=col_item,\n        col_rating=col_rating,\n        col_prediction=col_prediction,\n    )\n    return roc_auc_score(y_true, y_pred)\n\n", "right_context": "\n\n@check_column_dtypes\n@lru_cache_df(maxsize=1)\ndef merge_ranking_true_pred(\n    rating_true,\n    rating_pred,\n    col_user,\n    col_item,\n    col_rating,\n    col_prediction,\n    relevancy_method,\n    k=DEFAULT_K,\n    threshold=DEFAULT_THRESHOLD,\n):\n    \"\"\"Filter truth and prediction data frames on common users.\n\n    Args:\n        rating_true (pd.DataFrame): True DataFrame.\n        rating_pred (pd.DataFrame): Predicted DataFrame.\n        col_user (str): column name for user.\n        col_item (str): column name for item.\n        col_rating (str): column name for rating.\n        col_prediction (str): column name for prediction.\n        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold'].\n        k (int): number of top k items per user (optional).\n        threshold (float): threshold of top items per user (optional).\n\n    Returns:\n        pd.DataFrame, pd.DataFrame, int:\n            DataFrame of recommendation hits\n            DataFrmae of hit counts vs actual relevant items per user\n            number of unique user ids.\n    \"\"\"\n    # make sure the true data have no negative items\n    rating_true = rating_true[rating_true[col_rating] >= 1]\n\n    # Make sure the prediction and true data frames have the same set of users\n    common_users = set(rating_true[col_user]).intersection(set(rating_pred[col_user]))\n    rating_true_common = rating_true[rating_true[col_user].isin(common_users)]\n    rating_pred_common = rating_pred[rating_pred[col_user].isin(common_users)]\n    n_users = len(common_users)\n\n    # Return hit items in prediction data frame with ranking information. This is used for calculating NDCG and MAP.\n    # Use first to generate unique ranking values for each item. This is to align with the implementation in\n    # Spark evaluation metrics, where index of each recommended items (the indices are unique to items) is used\n    # to calculate penalized precision of the ordered items.\n    if relevancy_method == \"top_k\":\n        top_k = k\n    elif relevancy_method == \"by_threshold\":\n        top_k = threshold\n    else:\n        raise NotImplementedError(\"Invalid relevancy_method\")\n    df_hit = get_top_k_items(\n        dataframe=rating_pred_common,\n        col_user=col_user,\n        col_rating=col_prediction,\n        k=top_k,\n    )\n    df_hit[\"rank\"] = df_hit.groupby(col_user)[col_prediction].rank(\n        method=\"first\", ascending=False\n    )\n    df_hit = pd.merge(df_hit, rating_true_common, on=[col_user, col_item])[\n        [col_user, col_item, \"rank\"]\n    ]\n\n    # count the number of hits vs actual relevant items per user\n    df_hit_count = pd.merge(\n        df_hit.groupby(col_user, as_index=False)[col_user].agg({\"hit\": \"count\"}),\n        rating_true_common.groupby(col_user, as_index=False)[col_user].agg(\n            {\"actual\": \"count\"}\n        ),\n        on=col_user,\n    )\n\n    return df_hit, df_hit_count, n_users\n\n\ndef precision_at_k(\n    rating_true,\n    rating_pred,\n    col_user=DEFAULT_USER_COL,\n    col_item=DEFAULT_ITEM_COL,\n    col_rating=DEFAULT_RATING_COL,\n    col_prediction=DEFAULT_PREDICTION_COL,\n    relevancy_method=\"top_k\",\n    k=DEFAULT_K,\n    threshold=DEFAULT_THRESHOLD,\n):\n    \"\"\"Precision at K.\n\n    Note:\n    We use the same formula to calculate precision@k as that in Spark.\n    More details can be found at\n    http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.precisionAt\n    In particular, the maximum achievable precision may be < 1, if the number of items for a\n    user in rating_pred is less than k.\n\n    Args:\n        rating_true (pd.DataFrame): True DataFrame.\n        rating_pred (pd.DataFrame): Predicted DataFrame.\n        col_user (str): column name for user.\n        col_item (str): column name for item.\n        col_rating (str): column name for rating.\n        col_prediction (str): column name for prediction.\n        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold'].\n        k (int): number of top k items per user.\n        threshold (float): threshold of top items per user (optional).\n\n    Returns:\n        float: precision at k (min=0, max=1).\n    \"\"\"\n    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n        rating_true=rating_true,\n        rating_pred=rating_pred,\n        col_user=col_user,\n        col_item=col_item,\n        col_rating=col_rating,\n        col_prediction=col_prediction,\n        relevancy_method=relevancy_method,\n        k=k,\n        threshold=threshold,\n    )\n\n    if df_hit.shape[0] == 0:\n        return 0.0\n    return (df_hit_count[\"hit\"] / k).sum() / n_users\n\n\ndef recall_at_k(\n    rating_true,\n    rating_pred,\n    col_user=DEFAULT_USER_COL,\n    col_item=DEFAULT_ITEM_COL,\n    col_rating=DEFAULT_RATING_COL,\n    col_prediction=DEFAULT_PREDICTION_COL,\n    relevancy_method=\"top_k\",\n    k=DEFAULT_K,\n    threshold=DEFAULT_THRESHOLD,\n):\n    \"\"\"Recall at K.\n\n    Args:\n        rating_true (pd.DataFrame): True DataFrame.\n        rating_pred (pd.DataFrame): Predicted DataFrame.\n        col_user (str): column name for user.\n        col_item (str): column name for item.\n        col_rating (str): column name for rating.\n        col_prediction (str): column name for prediction.\n        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold'].\n        k (int): number of top k items per user.\n        threshold (float): threshold of top items per user (optional).\n\n    Returns:\n        float: recall at k (min=0, max=1). The maximum value is 1 even when fewer than\n            k items exist for a user in rating_true.\n    \"\"\"\n    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n        rating_true=rating_true,\n        rating_pred=rating_pred,\n        col_user=col_user,\n        col_item=col_item,\n        col_rating=col_rating,\n        col_prediction=col_prediction,\n        relevancy_method=relevancy_method,\n        k=k,\n        threshold=threshold,\n    )\n\n    if df_hit.shape[0] == 0:\n        return 0.0\n\n    return (df_hit_count[\"hit\"] / df_hit_count[\"actual\"]).sum() / n_users\n\n\ndef ndcg_at_k(\n    rating_true,\n    rating_pred,\n    col_user=DEFAULT_USER_COL,\n    col_item=DEFAULT_ITEM_COL,\n    col_rating=DEFAULT_RATING_COL,\n    col_prediction=DEFAULT_PREDICTION_COL,\n    relevancy_method=\"top_k\",\n    k=DEFAULT_K,\n    threshold=DEFAULT_THRESHOLD,\n):\n    \"\"\"Compute Normalized Discounted Cumulative Gain (nDCG).\n\n    Info: https://en.wikipedia.org/wiki/Discounted_cumulative_gain\n\n    Args:\n        rating_true (pd.DataFrame): True DataFrame.\n        rating_pred (pd.DataFrame): Predicted DataFrame.\n        col_user (str): column name for user.\n        col_item (str): column name for item.\n        col_rating (str): column name for rating.\n        col_prediction (str): column name for prediction.\n        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold'].\n        k (int): number of top k items per user.\n        threshold (float): threshold of top items per user (optional).\n\n    Returns:\n        float: nDCG at k (min=0, max=1).\n    \"\"\"\n    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n        rating_true=rating_true,\n        rating_pred=rating_pred,\n        col_user=col_user,\n        col_item=col_item,\n        col_rating=col_rating,\n        col_prediction=col_prediction,\n        relevancy_method=relevancy_method,\n        k=k,\n        threshold=threshold,\n    )\n\n    if df_hit.shape[0] == 0:\n        return 0.0\n\n    # calculate discounted gain for hit items\n    df_dcg = df_hit.copy()\n    # relevance in this case is always 1\n    df_dcg[\"dcg\"] = 1 / np.log1p(df_dcg[\"rank\"])\n    # sum up discount gained to get discount cumulative gain\n    df_dcg = df_dcg.groupby(col_user, as_index=False, sort=False).agg({\"dcg\": \"sum\"})\n    # calculate ideal discounted cumulative gain\n    df_ndcg = pd.merge(df_dcg, df_hit_count, on=[col_user])\n    df_ndcg[\"idcg\"] = df_ndcg[\"actual\"].apply(\n        lambda x: sum(1 / np.log1p(range(1, min(x, k) + 1)))\n    )\n\n    # DCG over IDCG is the normalized DCG\n    return (df_ndcg[\"dcg\"] / df_ndcg[\"idcg\"]).sum() / n_users\n\n\ndef map_at_k(\n    rating_true,\n    rating_pred,\n    col_user=DEFAULT_USER_COL,\n    col_item=DEFAULT_ITEM_COL,\n    col_rating=DEFAULT_RATING_COL,\n    col_prediction=DEFAULT_PREDICTION_COL,\n    relevancy_method=\"top_k\",\n    k=DEFAULT_K,\n    threshold=DEFAULT_THRESHOLD,\n):\n    \"\"\"Mean Average Precision at k.\n\n    The implementation of MAP is referenced from Spark MLlib evaluation metrics.\n    https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems\n\n    A good reference can be found at:\n    http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n\n    Note:\n        1. The evaluation function is named as 'MAP is at k' because the evaluation class takes top k items for\n        the prediction items. The naming is different from Spark.\n        2. The MAP is meant to calculate Avg. Precision for the relevant items, so it is normalized by the number of\n        relevant items in the ground truth data, instead of k.\n\n    Args:\n        rating_true (pd.DataFrame): True DataFrame.\n        rating_pred (pd.DataFrame): Predicted DataFrame.\n        col_user (str): column name for user.\n        col_item (str): column name for item.\n        col_rating (str): column name for rating.\n        col_prediction (str): column name for prediction.\n        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold'].\n        k (int): number of top k items per user.\n        threshold (float): threshold of top items per user (optional).\n\n    Returns:\n        float: MAP at k (min=0, max=1).\n    \"\"\"\n    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n        rating_true=rating_true,\n        rating_pred=rating_pred,\n        col_user=col_user,\n        col_item=col_item,\n        col_rating=col_rating,\n        col_prediction=col_prediction,\n        relevancy_method=relevancy_method,\n        k=k,\n        threshold=threshold,\n    )\n\n    if df_hit.shape[0] == 0:\n        return 0.0\n\n    # calculate reciprocal rank of items for each user and sum them up\n    df_hit_sorted = df_hit.sort_values([col_user, \"rank\"])\n    df_hit_sorted[\"rr\"] = (df_hit.groupby(col_user).cumcount() + 1) / df_hit[\"rank\"]\n    df_hit_sorted = df_hit_sorted.groupby(col_user).agg({\"rr\": \"sum\"}).reset_index()\n\n    df_merge = pd.merge(df_hit_sorted, df_hit_count, on=col_user)\n    return (df_merge[\"rr\"] / df_merge[\"actual\"]).sum() / n_users\n\n\ndef get_top_k_items(\n    dataframe, col_user=DEFAULT_USER_COL, col_rating=DEFAULT_RATING_COL, k=DEFAULT_K\n):\n    \"\"\"Get the input customer-item-rating tuple in the format of Pandas.\n\n    DataFrame, output a Pandas DataFrame in the dense format of top k items\n    for each user.\n\n    Note:\n        if it is implicit rating, just append a column of constants to be\n        ratings.\n\n    Args:\n        dataframe (pandas.DataFrame): DataFrame of rating data (in the format\n        customerID-itemID-rating).\n        col_user (str): column name for user.\n        col_rating (str): column name for rating.\n        k (int): number of items for each user.\n\n    Returns:\n        pd.DataFrame: DataFrame of top k items for each user.\n    \"\"\"\n    # Sort dataframe by col_user and (top k) col_rating\n    top_k_items = (\n        dataframe.groupby(col_user, as_index=False)\n        .apply(lambda x: x.nlargest(k, col_rating))\n        .reset_index(drop=True)\n    )\n    # Add ranks\n    top_k_items[\"rank\"] = top_k_items.groupby(col_user, sort=False).cumcount() + 1\n    return top_k_items\n", "import_text": ["functools.lru_cache", "functools.wraps", "numpy", "pandas", "sklearn.metrics.explained_variance_score", "sklearn.metrics.log_loss", "sklearn.metrics.mean_absolute_error", "sklearn.metrics.mean_squared_error", "sklearn.metrics.r2_score", "sklearn.metrics.roc_auc_score"], "prompt": "\"\"\"\nDescription: This function calculates the logarithmic loss (logloss) between the true ratings and the predicted ratings.\n\nArgs:\n    rating_true (pandas.DataFrame): DataFrame containing true ratings.\n    rating_pred (pandas.DataFrame): DataFrame containing predicted ratings.\n    col_user (str): Name of the column in the DataFrame that contains user IDs.\n    col_item (str): Name of the column in the DataFrame that contains item IDs.\n    col_rating (str): Name of the column in the DataFrame that contains true ratings.\n    col_prediction (str): Name of the column in the DataFrame that contains predicted ratings.\n\nReturns:\n    float: The logarithmic loss between the true ratings and the predicted ratings.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Calculate the logloss metric.\n\n    Calculate the logloss metric for implicit feedback typed recommender, where rating is binary and prediction is\n    float number ranging from 0 to 1.\n\n    https://en.wikipedia.org/wiki/Loss_functions_for_classification#Cross_entropy_loss_(Log_Loss)\n\n    Args:\n        rating_true (pd.DataFrame): True data.\n        rating_pred (pd.DataFrame): Predicted data.\n        col_user (str): column name for user.\n        col_item (str): column name for item.\n        col_rating (str): column name for rating.\n        col_prediction (str): column name for prediction.\n\n    Returns:\n        float: log_loss_score (min=-inf, max=inf).\n    \"\"\"", "function_dependencies": ["sklearn.metrics.log_loss"], "project_create_time": "2020-03-15T17:03:32+00:00", "project_update_time": "2024-04-13T00:28:01+00:00", "file_create_time": "2020-04-08T08:18:09Z", "file_update_time": "2020-10-04T21:52:13Z", "function_update_time": "2020-06-11T21:22:52Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["sklearn.metrics.log_loss"], "test_function": [{"file_path": "/beta-recsys-v0.3.3/beta-recsys-0.3.3/tests/test_evaluation.py", "class_name": null, "function_name": "test_python_logloss", "code": "def test_python_logloss(rating_true_binary, rating_pred_binary):\n    assert (\n        logloss(\n            rating_true=rating_true_binary,\n            rating_pred=rating_true_binary,\n            col_prediction=DEFAULT_RATING_COL,\n        )\n        == pytest.approx(0, TOL)\n    )\n\n    assert (\n        logloss(\n            rating_true=rating_true_binary,\n            rating_pred=rating_pred_binary,\n            col_rating=DEFAULT_RATING_COL,\n            col_prediction=DEFAULT_PREDICTION_COL,\n        )\n        == pytest.approx(0.7835, TOL)\n    )"}]}, {"git_group": "amundsen-io", "git_name": "amundsen", "version": "search-4.2.0", "language": "Python", "project_name": "amundsen-search-4.2.0.zip", "file_path": "/amundsen-search-4.2.0/amundsen-search-4.2.0/metadata/metadata_service/proxy/mysql_proxy.py", "file_name": "mysql_proxy.py", "focal_class": "MySQLProxy", "focal_name": "get_table_by_user_relation", "focal_parameter": [], "solution": "    def get_table_by_user_relation(self, *, user_email: str, relation_type: UserResourceRel) -> Dict[str, Any]:\n        if relation_type not in resource_relation_model[ResourceType.Table]:\n            raise NotImplementedError(f'The relation type {relation_type} is not defined!')\n\n        relation_model = resource_relation_model[ResourceType.Table][relation_type]\n        table_attr = getattr(relation_model, 'table_rk')\n        user_attr = getattr(relation_model, 'user_rk')\n        with self.client.create_session() as session:\n            table_subquery = session.query(table_attr).filter(user_attr == user_email).subquery()\n\n            query = session.query(RDSTable).filter(RDSTable.rk.in_(table_subquery)).options(\n                load_only(RDSTable.rk, RDSTable.name, RDSTable.schema_rk),\n                subqueryload(RDSTable.description).options(\n                    load_only(RDSTableDescription.description)\n                ),\n                subqueryload(RDSTable.schema).options(\n                    load_only(RDSSchema.name, RDSSchema.cluster_rk),\n                    subqueryload(RDSSchema.cluster).options(\n                        load_only(RDSCluster.name, RDSCluster.database_rk),\n                        subqueryload(RDSCluster.database).options(\n                            load_only(RDSDatabase.name)\n                        )\n                    )\n                )\n            )\n\n            tables = query.all()\n\n        results = []\n        for table in tables:\n            description = table.description\n            schema = table.schema\n            cluster = schema.cluster\n            database = cluster.database\n\n            results.append(PopularTable(database=database.name,\n                                        cluster=cluster.name,\n                                        schema=schema.name,\n                                        name=table.name,\n                                        description=description.description if description else None))\n\n        return {ResourceType.Table.name.lower(): results}", "function_signature": "def get_table_by_user_relation(self, *, user_email: str, relation_type: UserResourceRel) -> Dict[str, Any] :", "left_context": "# Copyright Contributors to the Amundsen project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport time\nfrom collections import namedtuple\nfrom random import randint\nfrom typing import Any, Dict, Iterator, List, Optional, Tuple, Type, Union\n\nfrom amundsen_common.entity.resource_type import ResourceType, to_resource_type\nfrom amundsen_common.models.dashboard import DashboardSummary\nfrom amundsen_common.models.feature import Feature\nfrom amundsen_common.models.generation_code import GenerationCode\nfrom amundsen_common.models.lineage import Lineage, LineageItem\nfrom amundsen_common.models.popular_table import PopularTable\nfrom amundsen_common.models.table import Application\nfrom amundsen_common.models.table import Badge\nfrom amundsen_common.models.table import Badge as TableBadge\nfrom amundsen_common.models.table import (Column, ProgrammaticDescription,\n                                          Reader, Source, Stat, Table,\n                                          TableSummary, Tag, User, Watermark)\nfrom amundsen_common.models.user import User as UserEntity\nfrom amundsen_common.models.user import UserSchema\nfrom amundsen_rds.models import RDSModel\nfrom amundsen_rds.models.badge import Badge as RDSBadge\nfrom amundsen_rds.models.base import Base\nfrom amundsen_rds.models.cluster import Cluster as RDSCluster\nfrom amundsen_rds.models.column import \\\n    ColumnDescription as RDSColumnDescription\nfrom amundsen_rds.models.column import TableColumn as RDSColumn\nfrom amundsen_rds.models.dashboard import Dashboard as RDSDashboard\nfrom amundsen_rds.models.dashboard import DashboardChart as RDSDashboardChart\nfrom amundsen_rds.models.dashboard import \\\n    DashboardCluster as RDSDashboardCluster\nfrom amundsen_rds.models.dashboard import \\\n    DashboardDescription as RDSDashboardDescription\nfrom amundsen_rds.models.dashboard import \\\n    DashboardExecution as RDSDashboardExecution\nfrom amundsen_rds.models.dashboard import \\\n    DashboardFollower as RDSDashboardFollower\nfrom amundsen_rds.models.dashboard import DashboardGroup as RDSDashboardGroup\nfrom amundsen_rds.models.dashboard import DashboardOwner as RDSDashboardOwner\nfrom amundsen_rds.models.dashboard import DashboardQuery as RDSDashboardQuery\nfrom amundsen_rds.models.dashboard import DashboardTable as RDSDashboardTable\nfrom amundsen_rds.models.dashboard import DashboardTag as RDSDashboardTag\nfrom amundsen_rds.models.dashboard import DashboardUsage as RDSDashboardUsage\nfrom amundsen_rds.models.database import Database as RDSDatabase\nfrom amundsen_rds.models.schema import Schema as RDSSchema\nfrom amundsen_rds.models.table import Table as RDSTable\nfrom amundsen_rds.models.table import TableDescription as RDSTableDescription\nfrom amundsen_rds.models.table import TableFollower as RDSTableFollower\nfrom amundsen_rds.models.table import TableOwner as RDSTableOwner\nfrom amundsen_rds.models.table import TableTag as RDSTableTag\nfrom amundsen_rds.models.table import TableUsage as RDSTableUsage\nfrom amundsen_rds.models.tag import Tag as RDSTag\nfrom amundsen_rds.models.updated_timestamp import \\\n    UpdatedTimestamp as RDSUpdatedTimestamp\nfrom amundsen_rds.models.user import User as RDSUser\nfrom beaker.cache import CacheManager\nfrom beaker.util import parse_cache_config_options\nfrom flask import current_app as app\nfrom sqlalchemy import func, literal\nfrom sqlalchemy.orm import Session, load_only, subqueryload\n\nfrom metadata_service.client.rds_client import RDSClient\nfrom metadata_service.entity.dashboard_detail import \\\n    DashboardDetail as DashboardDetailEntity\nfrom metadata_service.entity.dashboard_query import \\\n    DashboardQuery as DashboardQueryEntity\nfrom metadata_service.entity.description import Description\nfrom metadata_service.entity.tag_detail import TagDetail\nfrom metadata_service.exception import NotFoundException\nfrom metadata_service.proxy.base_proxy import BaseProxy\nfrom metadata_service.proxy.statsd_utilities import timer_with_counter\nfrom metadata_service.util import UserResourceRel\n\n_CACHE = CacheManager(**parse_cache_config_options({'cache.type': 'memory'}))\n\n# Expire cache every 11 hours + jitter\n_GET_POPULAR_RESOURCES_CACHE_EXPIRY_SEC = 11 * 60 * 60 + randint(0, 3600)\n\nresource_relation_model = {\n    ResourceType.Table: {\n        UserResourceRel.read: RDSTableUsage,\n        UserResourceRel.own: RDSTableOwner,\n        UserResourceRel.follow: RDSTableFollower\n    },\n    ResourceType.Dashboard: {\n        UserResourceRel.read: RDSDashboardUsage,\n        UserResourceRel.own: RDSDashboardOwner,\n        UserResourceRel.follow: RDSDashboardFollower\n    }\n}\n\nEdge = namedtuple('Edge', ['in_node', 'out_node'])\nEdgePair = namedtuple('EdgePair', ['in_edge', 'out_edge'])\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass MySQLProxy(BaseProxy):\n    \"\"\"\n    A proxy to MySQL using SQLAlchemy ORM and Amundsen RDS\n    See https://docs.sqlalchemy.org/en/13/orm/\n    See https://github.com/amundsen-io/amundsenrds\n    \"\"\"\n    def __init__(self, *,\n                 host: Optional[str] = None,\n                 port: Optional[int] = None,\n                 user: Optional[str] = None,\n                 password: Optional[str] = None,\n                 client_kwargs: Dict = dict(),\n                 **kwargs: Dict\n                 ) -> None:\n        endpoint = app.config['SQLALCHEMY_DATABASE_URI']\n        if not endpoint:\n            database = app.config['PROXY_DATABASE']\n            endpoint = f'mysql://{user}:{password}@{host}:{port}/{database}'\n\n        self.client = RDSClient(sql_alchemy_url=endpoint, client_kwargs=client_kwargs)\n\n        is_the_latest_schema = self.client.validate_schema_version()\n        if not is_the_latest_schema:\n            LOGGER.warning('Warning: DB Schema is not up to date and it may cause some features not supported. '\n                           'Please run rds command to upgrade the schema.')\n\n    def is_healthy(self) -> None:\n        with self.client.create_session() as session:\n            session.execute('SELECT 1 as is_alive')\n\n    @timer_with_counter\n    def get_user(self, *, id: str) -> Union[User, None]:\n        \"\"\"\n        Retrieve user detail based on id(email).\n        :param id: the email for the given user\n        :return:\n        \"\"\"\n        with self.client.create_session() as session:\n            user_record = session.query(RDSUser).filter(RDSUser.rk == id).first()\n            if not user_record:\n                return user_record\n\n            manager_record = user_record.manager\n\n            manager_name = ''\n            if manager_record and manager_record.full_name:\n                manager_name = manager_record.full_name\n\n        return self._build_user_from_record(user_record=user_record, manager_name=manager_name)\n\n    @timer_with_counter\n    def get_users(self) -> List[User]:\n        \"\"\"\n        Retrieve all the user details.\n        :return:\n        \"\"\"\n        with self.client.create_session() as session:\n            users = session.query(RDSUser).filter(RDSUser.is_active.is_(True)).all()\n\n        return [self._build_user_from_record(user_record=user) for user in users]\n\n    @staticmethod\n    def _build_user_from_record(user_record: RDSUser, manager_name: str = '') -> UserEntity:\n        return UserEntity(email=user_record.email,\n                          first_name=user_record.first_name,\n                          last_name=user_record.last_name,\n                          full_name=user_record.full_name,\n                          is_active=user_record.is_active if user_record.is_active else False,\n                          profile_url=user_record.profile_url,\n                          github_username=user_record.github_username,\n                          team_name=user_record.team_name,\n                          slack_id=user_record.slack_id,\n                          employee_type=user_record.employee_type,\n                          role_name=user_record.role_name,\n                          manager_fullname=manager_name)\n\n    @timer_with_counter\n    def create_update_user(self, *, user: User) -> Tuple[User, bool]:\n        \"\"\"\n        Create a user if it does not exist, otherwise update the user.\n        :param user:\n        :return:\n        \"\"\"\n        user_data = UserSchema().dump(user)\n        user_record = RDSUser(rk=user.user_id,\n                              manager_rk=user.manager_id,\n                              published_tag='api_create_update_user',\n                              publisher_last_updated_epoch_ms=int(time.time() * 1000))\n        for attr, value in user_data.items():\n            if hasattr(user_record, attr):\n                user_record.__setattr__(attr, value)\n\n        with self.client.create_session() as session:\n            existed_user = session.query(RDSUser).filter(RDSUser.rk == user.user_id).first()\n            is_new = False if existed_user else True\n\n            session.merge(user_record)\n            session.commit()\n\n        user_result = self._build_user_from_record(user_record)\n\n        return user_result, is_new\n\n    @timer_with_counter\n    def get_table(self, *, table_uri: str) -> Table:\n        \"\"\"\n        Retrieve table detail.\n        :param table_uri:\n        :return:\n        \"\"\"\n        with self.client.create_session() as session:\n            # table\n            table = self._get_table_metadata(session=session, table_uri=table_uri)\n            if not table:\n                raise NotFoundException(f'Table URI( {table_uri} ) does not exist')\n\n            # columns\n            cols = self._get_table_columns(session=session, table_uri=table_uri)\n\n            # usage\n            readers = self._get_table_readers(session=session, table_uri=table_uri)\n\n        table_result = Table(database=table['database'].name,\n                             cluster=table['cluster'].name,\n                             schema=table['schema'].name,\n                             name=table['table'].name,\n                             tags=table['tags'],\n                             badges=table['badges'],\n                             description=table['description'].description if table['description'] else None,\n                             columns=cols,\n                             owners=table['owners'],\n                             table_readers=readers,\n                             watermarks=table['watermarks'],\n                             table_writer=table['table_writer'],\n                             last_updated_timestamp=table['last_updated_timestamp'],\n                             source=table['source'],\n                             is_view=table['table'].is_view,\n                             programmatic_descriptions=table['programmatic_descriptions'])\n        return table_result\n\n    @timer_with_counter\n    def _get_table_metadata(self, *, session: Session, table_uri: str) -> Optional[Dict[str, Any]]:\n        table = session.query(RDSTable).filter(RDSTable.rk == table_uri).first()\n        if not table:\n            return None\n\n        schema = table.schema\n        cluster = schema.cluster\n        database = cluster.database\n\n        # watermark\n        watermark_results = []\n        for watermark in table.watermarks:\n            watermark_type = watermark.rk.split('/')[-2]\n            watermark_result = Watermark(watermark_type=watermark_type,\n                                         partition_key=watermark.partition_key,\n                                         partition_value=watermark.partition_value,\n                                         create_time=watermark.create_time)\n            watermark_results.append(watermark_result)\n\n        # tags\n        tag_results = []\n        tags = [tag for tag in table.tags if tag.tag_type == 'default']\n        for tag in tags:\n            tag_result = Tag(tag_name=tag.rk, tag_type=tag.tag_type)\n            tag_results.append(tag_result)\n\n        # badges\n        badge_results = []\n        for badge in table.badges:\n            badge_result = TableBadge(badge_name=badge.rk, category=badge.category)\n            badge_results.append(badge_result)\n\n        # application\n        table_writer = None\n        application = table.application\n        if application is not None:\n            application_id = '' if application.id is None else application.id\n            table_writer = Application(application_url=application.application_url,\n                                       description=application.description,\n                                       name=application.name,\n                                       id=application_id)\n        # timestamp_value\n        timestamp_value = table.timestamp.last_updated_timestamp if table.timestamp else None\n\n        # owners\n        owner_results = []\n        for owner in table.owners:\n            owner_results.append(User(email=owner.email))\n\n        # source\n        source_result = None\n        source = table.source\n        if source is not None:\n            source_result = Source(source_type=source.source_type, source=source.source)\n\n        # description\n        description_result = table.description\n\n        # programmatic descriptions\n        prog_description_results = []\n        for prog_desc in table.programmatic_descriptions:\n            source = prog_desc.description_source\n            if source is None:\n                LOGGER.error(\"A programmatic description with no source was found... skipping.\")\n            else:\n                prog_description_results.append(ProgrammaticDescription(source=source,\n                                                                        text=prog_desc.description))\n        prog_description_results.sort(key=lambda x: x.source)\n\n        table_metadata_result = dict(database=database,\n                                     cluster=cluster,\n                                     schema=schema,\n                                     table=table,\n                                     tags=tag_results,\n                                     badges=badge_results,\n                                     description=description_result,\n                                     owners=owner_results,\n                                     watermarks=watermark_results,\n                                     table_writer=table_writer,\n                                     last_updated_timestamp=timestamp_value,\n                                     source=source_result,\n                                     programmatic_descriptions=prog_description_results)\n\n        return table_metadata_result\n\n    @timer_with_counter\n    def _get_table_columns(self, *, session: Session, table_uri: str) -> List[Column]:\n        # column\n        query = session.query(RDSColumn).filter(RDSColumn.table_rk == table_uri)\n\n        # description, stats, badges\n        query = query.options(\n            subqueryload(RDSColumn.description),\n            subqueryload(RDSColumn.stats),\n            subqueryload(RDSColumn.badges)\n        )\n\n        columns = query.all()\n\n        col_results = []\n        for column in columns:\n            col_stat_results = []\n            for stat in column.stats:\n                col_stat_result = Stat(\n                    stat_type=stat.stat_type,\n                    stat_val=stat.stat_val,\n                    start_epoch=int(float(stat.start_epoch)),\n                    end_epoch=int(float(stat.end_epoch))\n                )\n                col_stat_results.append(col_stat_result)\n\n            col_badge_results = []\n            for badge in column.badges:\n                col_badge_results.append(\n                    TableBadge(badge_name=badge.rk, category=badge.category)\n                )\n\n            col_result = Column(name=column.name,\n                                description=column.description.description\n                                if column.description else None,\n                                col_type=column.type,\n                                sort_order=int(column.sort_order),\n                                stats=col_stat_results,\n                                badges=col_badge_results)\n            col_results.append(col_result)\n\n        return col_results\n\n    @timer_with_counter\n    def _get_table_readers(self, *, session: Session, table_uri: str) -> List[Reader]:\n        readers = session.query(RDSTableUsage).filter(\n            RDSTableUsage.table_rk == table_uri\n        ).order_by(RDSTableUsage.read_count).limit(5).all()\n\n        reader_results = []\n        for reader in readers:\n            reader_result = Reader(user=User(email=reader.user_rk),\n                                   read_count=reader.read_count)\n            reader_results.append(reader_result)\n\n        return reader_results\n\n    @timer_with_counter\n    def delete_owner(self, *, table_uri: str, owner: str) -> None:\n        \"\"\"\n        Delete relation between the given table and owner.\n        :param table_uri:\n        :param owner:\n        :return:\n        \"\"\"\n        try:\n            with self.client.create_session() as session:\n                session.query(RDSTableOwner).filter(\n                    RDSTableOwner.table_rk == table_uri,\n                    RDSTableOwner.user_rk == owner\n                ).delete()\n                session.commit()\n        except Exception as e:\n            LOGGER.exception(f'Failed to delete owner {owner} for table {table_uri}')\n            raise e\n\n    @timer_with_counter\n    def add_owner(self, *, table_uri: str, owner: str) -> None:\n        \"\"\"\n        Add the owner for the given table.\n        :param table_uri:\n        :param owner:\n        :return:\n        \"\"\"\n        user = RDSUser(rk=owner, email=owner)\n        table_owner = RDSTableOwner(table_rk=table_uri, user_rk=owner)\n        try:\n            with self.client.create_session() as session:\n                session.merge(user)\n                session.merge(table_owner)\n                session.commit()\n        except Exception as e:\n            LOGGER.exception(f'Failed to add owner {owner} for table {table_uri}')\n            raise e\n\n    @timer_with_counter\n    def get_table_description(self, *, table_uri: str) -> Union[str, None]:\n        \"\"\"\n        Get the table description based on table uri.\n        :param table_uri:\n        :return:\n        \"\"\"\n        with self.client.create_session() as session:\n            description = session.query(RDSTableDescription.description).filter(\n                RDSTableDescription.table_rk == table_uri\n            ).scalar()\n\n        return description\n\n    @timer_with_counter\n    def put_table_description(self, *, table_uri: str, description: str) -> None:\n        \"\"\"\n        Update table description with one from user\n        :param table_uri:\n        :param description:\n        :return:\n        \"\"\"\n        desc_key = table_uri + '/_description'\n        description = RDSTableDescription(rk=desc_key,\n                                          description_source='description',\n                                          description=description,\n                                          table_rk=table_uri)\n        try:\n            with self.client.create_session() as session:\n                session.merge(description)\n                session.commit()\n        except Exception as e:\n            LOGGER.exception(f'Failed to add description for table {table_uri}')\n            raise e\n\n    @timer_with_counter\n    def add_tag(self, *,\n                id: str,\n                tag: str,\n                tag_type: str = 'default',\n                resource_type: ResourceType = ResourceType.Table) -> None:\n        \"\"\"\n        Add a new tag if it does not exist and add the relation between the given resource and tag.\n        :param id:\n        :param tag:\n        :param tag_type:\n        :param resource_type:\n        :return:\n        \"\"\"\n        LOGGER.info(f'New tag {tag} for id {id} with type {tag_type} and resource type {resource_type.name}')\n\n        resource_table = f'{resource_type.name.lower()}_tag'\n        resource_model = self._get_model_from_table_name(resource_table)\n        if not resource_model:\n            raise NotImplementedError(f'The resource type {resource_type.name} is not defined!')\n\n        resource_key = f'{resource_type.name.lower()}_rk'\n\n        tag_record = RDSTag(rk=tag, tag_type=tag_type)\n        resource_tag_record = resource_model(tag_rk=tag)\n        resource_tag_record.__setattr__(resource_key, id)\n        try:\n            with self.client.create_session() as session:\n                session.merge(tag_record)\n                session.merge(resource_tag_record)\n                session.commit()\n        except Exception as e:\n            LOGGER.exception(f'Failed to add tag {tag} for {id}')\n            raise e\n\n    @timer_with_counter\n    def add_badge(self, *,\n                  id: str,\n                  badge_name: str,\n                  category: str = '',\n                  resource_type: ResourceType = ResourceType.Table) -> None:\n        \"\"\"\n        Add a new badge if it does not exist and add the relation between the given resource and badge.\n        :param id:\n        :param badge_name:\n        :param category:\n        :param resource_type:\n        :return:\n        \"\"\"\n        LOGGER.info(f'New badge {badge_name} for id {id} with category {category} '\n                    f'and resource type {resource_type.name}')\n\n        resource_table = f'{resource_type.name.lower()}_badge'\n        resource_model = self._get_model_from_table_name(resource_table)\n        if not resource_model:\n            raise NotImplementedError(f'The resource type {resource_type.name} is not defined!')\n\n        resource_key = f'{resource_type.name.lower()}_rk'\n\n        badge_record = RDSBadge(rk=badge_name, category=category)\n        resource_badge_record = resource_model(badge_rk=badge_name)\n        resource_badge_record.__setattr__(resource_key, id)\n        try:\n            with self.client.create_session() as session:\n                session.merge(badge_record)\n                session.merge(resource_badge_record)\n                session.commit()\n        except Exception as e:\n            LOGGER.exception(f'Failed to add badge {badge_name} for {id}')\n            raise e\n\n    @timer_with_counter\n    def delete_tag(self, *,\n                   id: str,\n                   tag: str,\n                   tag_type: str = 'default',\n                   resource_type: ResourceType = ResourceType.Table) -> None:\n        \"\"\"\n        Delete the relation between the resource and the tag.\n        :param id:\n        :param tag:\n        :param tag_type:\n        :param resource_type:\n        :return:\n        \"\"\"\n        LOGGER.info(f'Delete tag {tag} for {id} with type {tag_type} and resource_type: {resource_type.name}')\n\n        resource_table = f'{resource_type.name.lower()}_tag'\n        resource_model = self._get_model_from_table_name(resource_table)\n        if not resource_model:\n            raise NotImplementedError(f'{resource_type.name} is not defined!')\n\n        resource_key = f'{resource_type.name.lower()}_rk'\n        resource_attr = getattr(resource_model, resource_key)\n        tag_attr = getattr(resource_model, 'tag_rk')\n        try:\n            with self.client.create_session() as session:\n                session.query(resource_model).filter(resource_attr == id, tag_attr == tag).delete()\n                session.commit()\n        except Exception as e:\n            LOGGER.exception(f'Failed to delete tag {tag} for {id}')\n            raise e\n\n    @timer_with_counter\n    def delete_badge(self, *,\n                     id: str,\n                     badge_name: str,\n                     category: str,\n                     resource_type: ResourceType = ResourceType.Table) -> None:\n        \"\"\"\n        Delete the relation between the resource and the badge.\n        :param id:\n        :param badge_name:\n        :param category:\n        :param resource_type:\n        :return:\n        \"\"\"\n        LOGGER.info(f'Delete badge {badge_name} for {id} with {category}')\n\n        resource_table = f'{resource_type.name.lower()}_badge'\n        resource_model = self._get_model_from_table_name(resource_table)\n        if not resource_model:\n            raise NotImplementedError(f'{resource_type.name} is not defined!')\n\n        resource_key = f'{resource_type.name.lower()}_rk'\n        resource_attr = getattr(resource_model, resource_key)\n        badge_attr = getattr(resource_model, 'badge_rk')\n        try:\n            with self.client.create_session() as session:\n                session.query(resource_model).filter(resource_attr == id, badge_attr == badge_name).delete()\n                session.commit()\n        except Exception as e:\n            LOGGER.exception(f'Failed to delete badge {badge_name} for {id}')\n            raise e\n\n    @staticmethod\n    def _get_model_from_table_name(table_name: str) -> Optional[Type[RDSModel]]:\n        \"\"\"\n        Get rds model for the given table name.\n        :param table_name:\n        :return:\n        \"\"\"\n        table_model = None\n        try:\n            if hasattr(Base, '_decl_class_registry'):\n                models = Base._decl_class_registry.values()  # sqlalchemy < 1.4\n            else:\n                models = Base.registry._class_registry.values()\n\n            for model in models:\n                if hasattr(model, '__tablename__') and model.__tablename__ == table_name:\n                    table_model = model\n        except Exception as e:\n            LOGGER.exception(f'Failed to get model for the table: {table_name} from rds model base')\n            raise e\n\n        return table_model\n\n    @timer_with_counter\n    def put_column_description(self, *, table_uri: str, column_name: str, description: str) -> None:\n        \"\"\"\n        Update column description with input from user.\n        :param table_uri:\n        :param column_name:\n        :param description:\n        :return:\n        \"\"\"\n        column_uri = table_uri + '/' + column_name\n        desc_key = column_uri + '/_description'\n        description = RDSColumnDescription(rk=desc_key,\n                                           description_source='description',\n                                           description=description,\n                                           column_rk=column_uri)\n        try:\n            with self.client.create_session() as session:\n                session.merge(description)\n                session.commit()\n        except Exception as e:\n            LOGGER.exception(f'Failed to update the table {table_uri} column {column_name} description')\n            raise e\n\n    @timer_with_counter\n    def get_column_description(self, *, table_uri: str, column_name: str) -> Union[str, None]:\n        \"\"\"\n        Get the column description based on table uri.\n        :param table_uri:\n        :param column_name:\n        :return:\n        \"\"\"\n        column_uri = table_uri + '/' + column_name\n        desc_key = column_uri + '/_description'\n        with self.client.create_session() as session:\n            description = session.query(RDSColumnDescription.description).filter(\n                RDSColumnDescription.rk == desc_key\n            ).scalar()\n\n        return description\n\n    @timer_with_counter\n    def get_popular_tables(self, *, num_entries: int, user_id: Optional[str] = None) -> List[PopularTable]:\n        \"\"\"\n        Retrieve popular tables. As popular table computation requires full scan of table usage,\n        it will utilize cached method _get_popular_tables_uris.\n        :param num_entries:\n        :param user_id:\n        :return:\n        \"\"\"\n        if user_id is None:\n            table_uris = self._get_global_popular_resources_uris(num_entries=num_entries)\n        else:\n            table_uris = self._get_personal_popular_resources_uris(num_entries=num_entries, user_id=user_id)\n\n        if not table_uris:\n            return []\n\n        with self.client.create_session() as session:\n            # table\n            query = session.query(RDSTable).filter(RDSTable.rk.in_(table_uris))\n\n            # description\n            query = query.options(\n                subqueryload(RDSTable.description).options(\n                    load_only(RDSTableDescription.description)\n                )\n            )\n\n            # schema, cluster, database\n            query = query.options(\n                subqueryload(RDSTable.schema).options(\n                    load_only(RDSSchema.name, RDSSchema.cluster_rk),\n                    subqueryload(RDSSchema.cluster).options(\n                        load_only(RDSCluster.name, RDSCluster.database_rk),\n                        subqueryload(RDSCluster.database).options(\n                            load_only(RDSDatabase.name)\n                        )\n                    )\n                )\n            )\n\n            tables = query.all()\n\n        popular_tables = []\n        for table in tables:\n            schema = table.schema\n            cluster = schema.cluster\n            database = cluster.database\n            description = table.description\n            popular_table = PopularTable(database=database.name,\n                                         cluster=cluster.name,\n                                         schema=schema.name,\n                                         name=table.name,\n                                         description=description.description if description else None)\n            popular_tables.append(popular_table)\n\n        return popular_tables\n\n    @timer_with_counter\n    def get_popular_resources(self, *,\n                              num_entries: int,\n                              resource_types: List[str],\n                              user_id: Optional[str] = None) -> Dict[str, List]:\n        \"\"\"\n        Retrieve popular resources. As popular resource computation requires full scan of resource usage,\n        it will cached popular resources uris.\n        :param num_entries:\n        :param resource_types:\n        :param user_id:\n        :return:\n        \"\"\"\n        popular_resources: Dict[str, List] = dict()\n        for resource in resource_types:\n            resource_type = to_resource_type(label=resource)\n            popular_resources[resource_type.name] = list()\n            if user_id is None:\n                # Get global popular Table/Dashboard URIs\n                resource_uris = self._get_global_popular_resources_uris(num_entries=num_entries,\n                                                                        resource_type=resource_type)\n            else:\n                # Get personalized popular Table/Dashboard URIs\n                resource_uris = self._get_personal_popular_resources_uris(num_entries=num_entries,\n                                                                          user_id=user_id,\n                                                                          resource_type=resource_type)\n\n            if resource_type == ResourceType.Table:\n                popular_resources[resource_type.name] = self._get_popular_tables(table_uris=resource_uris)\n            elif resource_type == ResourceType.Dashboard:\n                popular_resources[resource_type.name] = self._get_popular_dashboards(dashboard_uris=resource_uris)\n\n        return popular_resources\n\n    @_CACHE.cache('_get_global_popular_resources_uris', expire=_GET_POPULAR_RESOURCES_CACHE_EXPIRY_SEC)\n    def _get_global_popular_resources_uris(self,\n                                           num_entries: int,\n                                           resource_type: ResourceType = ResourceType.Table) -> List[str]:\n        \"\"\"\n        Retrieve popular resources uris. Will provide resources with top x popularity score.\n        Popularity score = number of distinct readers * log(total number of reads)\n        The result of this method will be cached based on the key (num_entries),\n        and the cache will be expired based on _GET_POPULAR_RESOURCES_CACHE_EXPIRY_SEC\n        :param num_entries:\n        :param resource_type:\n        :return:\n        \"\"\"\n        LOGGER.info('Querying global popular resources URIs')\n\n        num_readers = app.config['POPULAR_RESOURCES_MINIMUM_READER_COUNT']\n\n        relation_model = resource_relation_model[resource_type][UserResourceRel.read]\n        res_key = f'{resource_type.name.lower()}_rk'\n        res_attr = getattr(relation_model, res_key)\n        user_attr = getattr(relation_model, 'user_rk')\n        read_count_attr = getattr(relation_model, 'read_count')\n\n        with self.client.create_session() as session:\n            readers = func.count(user_attr).label('readers')\n            usage_subquery = session.query(\n                res_attr.label('res_key'),\n                readers,\n                func.sum(read_count_attr).label('total_reads')\n            ).group_by(res_attr).having(readers >= num_readers).subquery()\n\n            popular_usage = session.query(usage_subquery.c.res_key).order_by(\n                (usage_subquery.c.readers * func.log(usage_subquery.c.total_reads)).desc()\n            ).limit(num_entries).all()\n\n        return [usage.res_key for usage in popular_usage]\n\n    @timer_with_counter\n    @_CACHE.cache('_get_personal_popular_resources_uris', _GET_POPULAR_RESOURCES_CACHE_EXPIRY_SEC)\n    def _get_personal_popular_resources_uris(self,\n                                             num_entries: int,\n                                             user_id: str,\n                                             resource_type: ResourceType = ResourceType.Table) -> List[str]:\n        \"\"\"\n        Retrieve personalized popular resources uris. Will provide resources with top\n        popularity score that have been read by a peer of the user_id provided.\n        The popularity score is defined in the same way as `_get_global_popular_resources_uris`\n        The result of this method will be cached based on the key (num_entries, user_id),\n        and the cache will be expired based on _GET_POPULAR_RESOURCES_CACHE_EXPIRY_SEC\n        :param num_entries:\n        :param user_id:\n        :param resource_type:\n        :return:\n        \"\"\"\n        LOGGER.info('Querying personal popular resources URIs')\n\n        num_readers = app.config['POPULAR_RESOURCES_MINIMUM_READER_COUNT']\n\n        relation_model = resource_relation_model[resource_type][UserResourceRel.read]\n        res_key = f'{resource_type.name.lower()}_rk'\n        res_attr = getattr(relation_model, res_key)\n        user_attr = getattr(relation_model, 'user_rk')\n        read_count_attr = getattr(relation_model, 'read_count')\n\n        with self.client.create_session() as session:\n            readers = func.count(user_attr).label('readers')\n\n            usage_subquery = session.query(\n                res_attr.label('res_key'),\n                readers,\n                func.sum(read_count_attr).label('total_reads')\n            ).filter(\n                user_attr == user_id\n            ).group_by(res_attr).having(readers >= num_readers).subquery()\n\n            popular_usage = session.query(usage_subquery.c.res_key).order_by(\n                (usage_subquery.c.readers * func.log(usage_subquery.c.total_reads)).desc()\n            ).limit(num_entries).all()\n\n        return [usage.res_key for usage in popular_usage]\n\n    def _get_popular_tables(self, *, table_uris: List[str]) -> List[TableSummary]:\n        \"\"\"\n        Retrieve popular table with the given table uris\n        :param table_uris:\n        :return:\n        \"\"\"\n        if not table_uris:\n            return []\n\n        with self.client.create_session() as session:\n            # table\n            query = session.query(RDSTable).filter(RDSTable.rk.in_(table_uris))\n\n            # description\n            query = query.options(\n                subqueryload(RDSTable.description).options(\n                    load_only(RDSTableDescription.description)\n                )\n            )\n\n            # schema, cluster, database\n            query = query.options(\n                subqueryload(RDSTable.schema).options(\n                    load_only(RDSSchema.name, RDSSchema.cluster_rk),\n                    subqueryload(RDSSchema.cluster).options(\n                        load_only(RDSCluster.name, RDSCluster.database_rk),\n                        subqueryload(RDSCluster.database).options(\n                            load_only(RDSDatabase.name)\n                        )\n                    )\n                )\n            )\n\n            tables = query.all()\n\n        popular_tables = []\n        for table in tables:\n            schema = table.schema\n            cluster = schema.cluster\n            database = cluster.database\n            description = table.description\n            popular_table = TableSummary(database=database.name,\n                                         cluster=cluster.name,\n                                         schema=schema.name,\n                                         name=table.name,\n                                         description=description.description if description else None)\n            popular_tables.append(popular_table)\n\n        return popular_tables\n\n    def _get_popular_dashboards(self, *, dashboard_uris: List[str]) -> List[DashboardSummary]:\n        \"\"\"\n        Retrieve popular dashboards with the given dashboard uris\n        :param dashboard_uris:\n        :return:\n        \"\"\"\n        if not dashboard_uris:\n            return []\n\n        with self.client.create_session() as session:\n            # dashboard\n            query = session.query(RDSDashboard).filter(RDSDashboard.rk.in_(dashboard_uris))\n\n            # description, execution\n            query = query.options(\n                subqueryload(RDSDashboard.description).options(\n                    load_only(RDSDashboardDescription.description)\n                ),\n                subqueryload(RDSDashboard.execution).options(\n                    load_only(RDSDashboardExecution.rk, RDSDashboardExecution.timestamp)\n                )\n            )\n\n            # group, cluster\n            query = query.options(\n                subqueryload(RDSDashboard.group).options(\n                    subqueryload(RDSDashboardGroup.cluster).options(\n                        load_only(RDSDashboardCluster.name)\n                    )\n                )\n            )\n\n            dashboards = query.all()\n\n        popular_dashboards = []\n        for dashboard in dashboards:\n            product = dashboard.rk.split('_')[0]\n            execution = dashboard.execution\n            description = dashboard.description\n            group = dashboard.group\n            cluster = group.cluster\n            last_exec = next((execution for execution in execution\n                              if execution.rk.endswith('_last_successful_execution')), None)\n            popular_dashboard = DashboardSummary(uri=dashboard.rk,\n                                                 cluster=cluster.name,\n                                                 group_name=group.name,\n                                                 group_url=group.dashboard_group_url,\n                                                 product=product,\n                                                 name=dashboard.name,\n                                                 url=dashboard.dashboard_url,\n                                                 description=description.description if description else None,\n                                                 last_successful_run_timestamp=last_exec.timestamp\n                                                 if last_exec else None)\n            popular_dashboards.append(popular_dashboard)\n\n        return popular_dashboards\n\n    @timer_with_counter\n    def get_latest_updated_ts(self) -> Optional[int]:\n        \"\"\"\n        Fetch last updated / index timestamp for mysql.\n        :return:\n        \"\"\"\n        with self.client.create_session() as session:\n            latest_updated_ts_value = session.query(RDSUpdatedTimestamp.latest_timestamp).scalar()\n\n        return latest_updated_ts_value\n\n    @timer_with_counter\n    def get_tags(self) -> List:\n        \"\"\"\n        Get all existing tags.\n        :return:\n        \"\"\"\n        LOGGER.info('Get all the tags')\n\n        with self.client.create_session() as session:\n            tag_count = (func.count(RDSTableTag.table_rk)\n                         + func.count(RDSDashboardTag.dashboard_rk)).label('tag_count')\n\n            records = session.query(\n                RDSTag.rk.label('tag_name'),\n                tag_count\n            )\\\n                .outerjoin(RDSTableTag)\\\n                .outerjoin(RDSDashboardTag)\\\n                .filter(RDSTag.tag_type == 'default')\\\n                .group_by(RDSTag.rk)\\\n                .having(tag_count > 0)\\\n                .all()\n\n        results = []\n        for record in records:\n            results.append(TagDetail(tag_name=record.tag_name,\n                                     tag_count=record.tag_count))\n\n        return results\n\n    @timer_with_counter\n    def get_badges(self) -> List:\n        \"\"\"\n        Get all existing badges.\n        :return:\n        \"\"\"\n        LOGGER.info('Get all badges')\n\n        with self.client.create_session() as session:\n            badges = session.query(RDSBadge).all()\n\n        results = []\n        for badge in badges:\n            results.append(Badge(badge_name=badge.rk,\n                                 category=badge.category))\n\n        return results\n\n    @timer_with_counter\n    def get_dashboard_by_user_relation(self, *, user_email: str, relation_type: UserResourceRel) -> \\\n            Dict[str, List[DashboardSummary]]:\n        \"\"\"\n        Retrieve all dashboards based on the given user and relation.\n        :param user_email:\n        :param relation_type:\n        :return:\n        \"\"\"\n        if relation_type not in resource_relation_model[ResourceType.Dashboard]:\n            raise NotImplementedError(f'The relation type {relation_type} is not defined!')\n\n        relation_model = resource_relation_model[ResourceType.Dashboard][relation_type]\n        dashboard_attr = getattr(relation_model, 'dashboard_rk')\n        user_attr = getattr(relation_model, 'user_rk')\n        with self.client.create_session() as session:\n            dashboard_subquery = session.query(dashboard_attr).filter(user_attr == user_email).subquery()\n\n            query = session.query(RDSDashboard).filter(RDSDashboard.rk.in_(dashboard_subquery)).options(\n                subqueryload(RDSDashboard.group).options(\n                    subqueryload(RDSDashboardGroup.cluster).options(\n                        load_only(RDSDashboardCluster.name)\n                    )\n                ),\n                subqueryload(RDSDashboard.description).options(\n                    load_only(RDSDashboardDescription.description)\n                ),\n                subqueryload(RDSDashboard.execution).options(\n                    load_only(RDSDashboardExecution.rk, RDSDashboardExecution.timestamp)\n                )\n            )\n\n            dashboards = query.all()\n\n        results = []\n        for dashboard in dashboards:\n            product = dashboard.rk.split('_')[0]\n            description = dashboard.description\n            group = dashboard.group\n            last_exec = next((execution for execution in dashboard.execution\n                              if execution.rk.endswith('_last_successful_execution')), None)\n            results.append(DashboardSummary(uri=dashboard.rk,\n                                            cluster=group.cluster.name,\n                                            group_name=group.name,\n                                            group_url=group.dashboard_group_url,\n                                            product=product,\n                                            name=dashboard.name,\n                                            url=dashboard.dashboard_url,\n                                            description=description.description if description else None,\n                                            last_successful_run_timestamp=last_exec.timestamp\n                                            if last_exec else None))\n\n        return {ResourceType.Dashboard.name.lower(): results}\n\n    @timer_with_counter", "right_context": "\n    @timer_with_counter\n    def get_frequently_used_tables(self, *, user_email: str) -> Dict[str, Any]:\n        \"\"\"\n        Retrieve all the tables from usage records.\n        :param user_email:\n        :return:\n        \"\"\"\n        with self.client.create_session() as session:\n            # usage\n            frequently_used_tables_uris = self._get_frequently_used_tables_uris(session=session,\n                                                                                user_email=user_email)\n\n            # table\n            query = session.query(RDSTable).filter(RDSTable.rk.in_(frequently_used_tables_uris)).options(\n                load_only(RDSTable.rk, RDSTable.name, RDSTable.schema_rk)\n            )\n\n            # description, schema, cluster, database\n            query = query.options(\n                subqueryload(RDSTable.description).options(\n                    load_only(RDSTableDescription.description)\n                ),\n                subqueryload(RDSTable.schema).options(\n                    load_only(RDSSchema.name, RDSSchema.cluster_rk),\n                    subqueryload(RDSSchema.cluster).options(\n                        load_only(RDSCluster.name, RDSCluster.database_rk),\n                        subqueryload(RDSCluster.database).options(\n                            load_only(RDSDatabase.name)\n                        )\n                    )\n                )\n            )\n\n            tables = query.all()\n\n        results = []\n        for table in tables:\n            description = table.description\n            schema = table.schema\n            cluster = schema.cluster\n            database = cluster.database\n\n            results.append(PopularTable(database=database.name,\n                                        cluster=cluster.name,\n                                        schema=schema.name,\n                                        name=table.name,\n                                        description=description.description if description else None))\n        return {'table': results}\n\n    @timer_with_counter\n    def _get_frequently_used_tables_uris(self, *, session: Session, user_email: str) -> List[str]:\n        records = session.query(RDSTableUsage.table_rk).filter(\n            RDSTableUsage.user_rk == user_email,\n            RDSTableUsage.published_tag.isnot(None)\n        ).order_by(\n            RDSTableUsage.published_tag.desc(),\n            RDSTableUsage.read_count.desc()\n        ).limit(50).all()\n\n        table_uris = []\n        for record in records:\n            table_uris.append(record.table_rk)\n\n        return table_uris\n\n    @timer_with_counter\n    def add_resource_relation_by_user(self, *, id: str,\n                                      user_id: str,\n                                      relation_type: UserResourceRel,\n                                      resource_type: ResourceType) -> None:\n        \"\"\"\n        Add a new user if it does not exist and add the relation between the given resource and user.\n        :param id:\n        :param user_id:\n        :param relation_type:\n        :param resource_type:\n        :return:\n        \"\"\"\n        if resource_type not in resource_relation_model:\n            raise NotImplementedError(f'The resource_type {resource_type.name} is not defined!')\n\n        if relation_type not in resource_relation_model[resource_type]:\n            raise NotImplementedError(f'the relation type {relation_type} is not defined!')\n\n        res_rel_model = resource_relation_model[resource_type][relation_type]\n        res_key = f'{resource_type.name.lower()}_rk'\n\n        user_record = RDSUser(rk=user_id, email=user_id)\n        res_rel_record = res_rel_model(user_rk=user_id)\n        res_rel_record.__setattr__(res_key, id)\n        try:\n            with self.client.create_session() as session:\n                session.merge(user_record)\n                session.merge(res_rel_record)\n                session.commit()\n        except Exception as e:\n            LOGGER.exception(f'Failed to create relation between user {user_id} and resource {id}')\n            raise e\n\n    @timer_with_counter\n    def delete_resource_relation_by_user(self, *,\n                                         id: str,\n                                         user_id: str,\n                                         relation_type: UserResourceRel,\n                                         resource_type: ResourceType) -> None:\n        \"\"\"\n        Delete the relation between the given user and resource.\n        :param id:\n        :param user_id:\n        :param relation_type:\n        :param resource_type:\n        :return:\n        \"\"\"\n        if resource_type not in resource_relation_model:\n            raise NotImplementedError(f'The resource_type {resource_type.name} is not define!')\n\n        if relation_type not in resource_relation_model[resource_type]:\n            raise NotImplementedError(f'the relation type {relation_type} is not defined!')\n\n        res_rel_model = resource_relation_model[resource_type][relation_type]\n        res_key = f'{resource_type.name.lower()}_rk'\n        user_attr = getattr(res_rel_model, 'user_rk')\n        res_attr = getattr(res_rel_model, res_key)\n        try:\n            with self.client.create_session() as session:\n                session.query(res_rel_model).filter(user_attr == user_id, res_attr == id).delete()\n                session.commit()\n        except Exception as e:\n            LOGGER.exception(f'Failed to delete relation between user {user_id} and resource {id}')\n            raise e\n\n    @timer_with_counter\n    def get_dashboard(self, id: str) -> DashboardDetailEntity:\n        \"\"\"\n        Retrieve dashboard detail.\n        :param id:\n        :return:\n        \"\"\"\n        with self.client.create_session() as session:\n            dashboard_result = self._get_dashboard_metadata(session=session, id=id)\n            if not dashboard_result:\n                raise NotFoundException(f'No dashboard exist with URI: {id}')\n\n            dashboard_query_result = self._get_dashboard_queries(session=session, id=id)\n            dashboard_table_result = self._get_dashboard_tables(session=session, id=id)\n\n        return DashboardDetailEntity(uri=dashboard_result['uri'],\n                                     cluster=dashboard_result['cluster'],\n                                     url=dashboard_result['url'],\n                                     name=dashboard_result['name'],\n                                     product=dashboard_result['product'],\n                                     created_timestamp=dashboard_result['created_timestamp'],\n                                     description=dashboard_result['description'],\n                                     group_name=dashboard_result['group_name'],\n                                     group_url=dashboard_result['group_url'],\n                                     last_successful_run_timestamp=dashboard_result['last_successful_run_timestamp'],\n                                     last_run_timestamp=dashboard_result['last_run_timestamp'],\n                                     last_run_state=dashboard_result['last_run_state'],\n                                     updated_timestamp=dashboard_result['updated_timestamp'],\n                                     owners=dashboard_result['owners'],\n                                     tags=dashboard_result['tags'],\n                                     badges=dashboard_result['badges'],\n                                     recent_view_count=dashboard_result['recent_view_count'],\n                                     chart_names=dashboard_query_result['chart_names'],\n                                     query_names=dashboard_query_result['query_names'],\n                                     queries=dashboard_query_result['queries'],\n                                     tables=dashboard_table_result)\n\n    @timer_with_counter\n    def _get_dashboard_metadata(self, session: Session, id: str) -> Optional[Dict[str, Any]]:\n        dashboard = session.query(RDSDashboard).filter(RDSDashboard.rk == id).first()\n        if not dashboard:\n            return None\n\n        product = dashboard.rk.split('_')[0]\n        execution = dashboard.execution\n        last_suc_exec = next((execution for execution in execution\n                              if execution.rk.endswith('_last_successful_execution')), None)\n\n        last_exec = next((execution for execution in execution\n                          if execution.rk.endswith('_last_execution')), None)\n\n        updated_timestamp = dashboard.timestamp\n        description = dashboard.description\n        group = dashboard.group\n        cluster = group.cluster\n        owners = [self._build_user_from_record(owner) for owner in dashboard.owners]\n\n        tags = [Tag(tag_type=tag.tag_type, tag_name=tag.rk) for tag in dashboard.tags if tag.tag_type == 'default']\n        badges = [TableBadge(badge_name=badge.rk, category=badge.category) for badge in dashboard.badges]\n        recent_view_count = sum(usage.read_count for usage in dashboard.usage) if dashboard.usage else 0\n\n        return dict(uri=dashboard.rk,\n                    cluster=cluster.name,\n                    url=dashboard.dashboard_url,\n                    name=dashboard.name,\n                    product=product,\n                    created_timestamp=dashboard.created_timestamp,\n                    description=description.description if description else None,\n                    group_name=group.name,\n                    group_url=group.dashboard_group_url,\n                    last_successful_run_timestamp=int(last_suc_exec.timestamp) if last_suc_exec else None,\n                    last_run_timestamp=int(last_exec.timestamp) if last_exec else None,\n                    last_run_state=last_exec.state if last_exec else None,\n                    updated_timestamp=int(updated_timestamp.timestamp) if updated_timestamp else None,\n                    owners=owners,\n                    tags=tags,\n                    badges=badges,\n                    recent_view_count=recent_view_count)\n\n    @timer_with_counter\n    def _get_dashboard_queries(self, session: Session, id: str) -> Dict[str, Any]:\n        dashboard_queries = session.query(RDSDashboardQuery).filter(RDSDashboardQuery.dashboard_rk == id).options(\n            subqueryload(RDSDashboardQuery.charts).options(\n                load_only(RDSDashboardChart.name)\n            )\n        ).all()\n\n        chart_names = [chart.name for query in dashboard_queries for chart in query.charts]\n        query_names = [query.name for query in dashboard_queries if query.name]\n        queries = [DashboardQueryEntity(name=query.name, url=query.url, query_text=query.query_text)\n                   for query in dashboard_queries if query.name or query.url or query.query_text]\n\n        return dict(chart_names=chart_names, query_names=query_names, queries=queries)\n\n    @timer_with_counter\n    def _get_dashboard_tables(self, session: Session, id: str) -> List[PopularTable]:\n        table_subquery = session.query(RDSDashboardTable.table_rk).filter(\n            RDSDashboardTable.dashboard_rk == id\n        ).subquery()\n\n        tables_query = session.query(RDSTable).filter(RDSTable.rk.in_(table_subquery)).options(\n            load_only(RDSTable.rk, RDSTable.name, RDSTable.schema_rk),\n            subqueryload(RDSTable.description).options(\n                load_only(RDSTableDescription.description)\n            ),\n            subqueryload(RDSTable.schema).options(\n                load_only(RDSSchema.name, RDSSchema.cluster_rk),\n                subqueryload(RDSSchema.cluster).options(\n                    load_only(RDSCluster.name, RDSCluster.database_rk),\n                    subqueryload(RDSCluster.database).options(\n                        load_only(RDSDatabase.name)\n                    )\n                )\n            )\n        )\n\n        tables = tables_query.all()\n\n        table_results = []\n        for table in tables:\n            table_description = table.description\n            table_schema = table.schema\n            table_cluster = table_schema.cluster\n            table_database = table_cluster.database\n            table_results.append(\n                PopularTable(\n                    database=table_database.name,\n                    cluster=table_cluster.name,\n                    schema=table_schema.name,\n                    name=table.name,\n                    description=table_description.description if table_description else None\n                )\n            )\n\n        return table_results\n\n    @timer_with_counter\n    def get_dashboard_description(self, *, id: str) -> Description:\n        \"\"\"\n        Get the dashboard description based on dashboard uri.\n        :param id:\n        :return:\n        \"\"\"\n        with self.client.create_session() as session:\n            description = session.query(RDSDashboardDescription.description).filter(\n                RDSDashboardDescription.dashboard_rk == id\n            ).scalar()\n\n        return Description(description=description)\n\n    @timer_with_counter\n    def put_dashboard_description(self, *, id: str, description: str) -> None:\n        \"\"\"\n        Update dashboard description.\n        :param id:\n        :param description:\n        :return:\n        \"\"\"\n        desc_key = id + '/_description'\n        description = RDSDashboardDescription(rk=desc_key, description=description, dashboard_rk=id)\n        try:\n            with self.client.create_session() as session:\n                session.merge(description)\n                session.commit()\n        except Exception as e:\n            LOGGER.exception(f'Failed to add description for dashboard {id}')\n            raise e\n\n    @timer_with_counter\n    def get_resources_using_table(self, *, id: str, resource_type: ResourceType) -> Dict[str, List[DashboardSummary]]:\n        \"\"\"\n        Fetch resources related to the given table\n        :param id:\n        :param resource_type:\n        :return:\n        \"\"\"\n        if resource_type != ResourceType.Dashboard:\n            raise NotImplementedError(f'{resource_type.name} is not supported')\n\n        with self.client.create_session() as session:\n            dashboard_subquery = session.query(RDSDashboardTable.dashboard_rk).filter(\n                RDSDashboardTable.table_rk == id\n            ).subquery()\n\n            usage_subquery = session.query(\n                RDSDashboardUsage.dashboard_rk,\n                func.sum(RDSDashboardUsage.read_count).label('recent_view_count')\n            ).group_by(RDSDashboardUsage.dashboard_rk).filter(\n                RDSDashboardUsage.dashboard_rk.in_(dashboard_subquery)\n            ).subquery()\n\n            query = session.query(RDSDashboard).join(usage_subquery).filter(\n                RDSDashboard.rk == usage_subquery.c.dashboard_rk\n            ).order_by(usage_subquery.c.recent_view_count.desc())\n\n            query = query.options(\n                subqueryload(RDSDashboard.group).options(\n                    subqueryload(RDSDashboardGroup.cluster).options(\n                        load_only(RDSDashboardCluster.name)\n                    )\n                ),\n                subqueryload(RDSDashboard.description).options(\n                    load_only(RDSDashboardDescription.description)\n                ),\n                subqueryload(RDSDashboard.execution).options(\n                    load_only(RDSDashboardExecution.rk, RDSDashboardExecution.timestamp)\n                )\n            )\n\n            dashboards = query.all()\n\n        results = []\n        for dashboard in dashboards:\n            product = dashboard.rk.split('_')[0]\n            description = dashboard.description\n            group = dashboard.group\n            last_exec = next((execution for execution in dashboard.execution\n                              if execution.rk.endswith('_last_successful_execution')), None)\n            results.append(DashboardSummary(uri=dashboard.rk,\n                                            cluster=group.cluster.name,\n                                            group_name=group.name,\n                                            group_url=group.dashboard_group_url,\n                                            product=product,\n                                            name=dashboard.name,\n                                            url=dashboard.dashboard_url,\n                                            description=description.description if description else None,\n                                            last_successful_run_timestamp=int(last_exec.timestamp)\n                                            if last_exec else None))\n        return {'dashboards': results}\n\n    def get_lineage(self, *, id: str, resource_type: ResourceType, direction: str, depth: int) -> Lineage:\n        \"\"\"\n        Retrieves the lineage information for the specified resource type.\n        :param id: key of a table or a column\n        :param resource_type: type of the entity for which lineage is being retrieved\n        :param direction: whether to get the upstream/downstream or both directions\n        :param depth: depth or level of lineage information\n        :return: the Lineage object with upstream & downstream lineage items\n        \"\"\"\n        LOGGER.info(f'Fetching {resource_type.name.lower()} lineage.')\n\n        res_name = resource_type.name.lower()\n        res_table = f'{res_name}_metadata'\n        res_lng_table = f'{res_name}_lineage'\n        res_usage_table = f'{res_name}_usage'\n\n        res_model = self._get_model_from_table_name(res_table)\n        res_lng_model = self._get_model_from_table_name(res_lng_table)\n        res_usage_model = self._get_model_from_table_name(res_usage_table)\n        if not res_model or not res_lng_model:\n            raise NotImplementedError(f'The resource type {resource_type.name} is not defined!')\n\n        res_src_key = f'{res_name}_source_rk'\n        res_tgt_key = f'{res_name}_target_rk'\n\n        res_attr = getattr(res_model, 'rk')\n        res_src_attr = getattr(res_lng_model, res_src_key)\n        res_tgt_attr = getattr(res_lng_model, res_tgt_key)\n        res_badge_attr = getattr(res_model, 'badges')\n        res_usage_attr = getattr(res_model, 'usage') if hasattr(res_model, 'usage') else None\n        res_read_count_attr = getattr(res_usage_model, 'read_count') if res_usage_model is not None else None\n\n        with self.client.create_session() as session:\n            # build upstream query\n            us_cte = session \\\n                .query(res_src_attr, res_tgt_attr, literal(1).label('level')) \\\n                .filter(res_tgt_attr == id) \\\n                .cte(name='upstream', recursive=True)\n            us_cte_query = us_cte \\\n                .union_all(session\n                           .query(res_src_attr, res_tgt_attr, us_cte.c.level + 1)\n                           .filter(res_tgt_attr == getattr(us_cte.c, res_src_key), us_cte.c.level < depth))\n            us_subquery = session.query(getattr(us_cte_query.c, res_src_key),\n                                        getattr(us_cte_query.c, res_tgt_key),\n                                        us_cte_query.c.level).subquery()\n            us_query = session \\\n                .query(res_model,\n                       literal('upstream').label('direction'),\n                       us_subquery.c.level,\n                       getattr(us_subquery.c, res_tgt_key).label('parent_key')) \\\n                .join(us_subquery, res_attr == getattr(us_subquery.c, res_src_key)) \\\n                .options(load_only(res_attr), subqueryload(res_badge_attr)\n                         .options(load_only(RDSBadge.rk, RDSBadge.category)))\n            if res_usage_attr is not None:\n                us_query.options(subqueryload(res_usage_attr).options(load_only(res_read_count_attr)))\n\n            # build downstream query\n            ds_cte = session \\\n                .query(res_src_attr, res_tgt_attr, literal(1).label('level')) \\\n                .filter(res_src_attr == id).cte(name='downstream', recursive=True)\n            ds_cte_query = ds_cte\\\n                .union_all(session\n                           .query(res_src_attr, res_tgt_attr, ds_cte.c.level + 1)\n                           .filter(res_src_attr == getattr(ds_cte.c, res_tgt_key), ds_cte.c.level < depth))\n            ds_subquery = session.query(getattr(ds_cte_query.c, res_src_key),\n                                        getattr(ds_cte_query.c, res_tgt_key),\n                                        ds_cte_query.c.level).subquery()\n            ds_query = session \\\n                .query(res_model,\n                       literal('downstream').label('direction'),\n                       ds_subquery.c.level,\n                       getattr(ds_subquery.c, res_src_key).label('parent_key')) \\\n                .join(ds_subquery, res_attr == getattr(ds_subquery.c, res_tgt_key)) \\\n                .options(load_only(res_attr), subqueryload(res_badge_attr)\n                         .options(load_only(RDSBadge.rk, RDSBadge.category)))\n            if res_usage_attr is not None:\n                ds_query.options(subqueryload(res_usage_attr).options(load_only(res_read_count_attr)))\n\n            # apply direction\n            if direction == 'upstream':\n                records = us_query.all()\n            elif direction == 'downstream':\n                records = ds_query.all()\n            else:\n                records = us_query.union_all(ds_query).all()\n\n            tables: Dict[str, List[LineageItem]] = {'upstream': [], 'downstream': []}\n            for record in records:\n                record_res = getattr(record, res_model.__name__)\n                tables[record.direction] \\\n                    .append(LineageItem(**{'key': record_res.rk,\n                                           'source': record_res.rk.split('://')[0],\n                                           'level': record.level,\n                                           'badges': [Badge(badge_name=badge.rk, category=badge.category)\n                                                      for badge in record_res.badges],\n                                           'usage': sum(usage.read_count for usage in record_res.usage)\n                                           if res_usage_attr is not None else 0,\n                                           'parent': record.parent_key}))\n\n            return Lineage(**{'key': id,\n                              'upstream_entities': self._sort_lineage_items(tables['upstream'], id),\n                              'downstream_entities': self._sort_lineage_items(tables['downstream'], id),\n                              'direction': direction,\n                              'depth': depth})\n\n    @staticmethod\n    def _sort_lineage_items(lineage_items: List[LineageItem], id: str) -> List[LineageItem]:\n        \"\"\"\n        Return lineage item in topological order.\n        \"\"\"\n        def get_next_edge(node: str) -> Iterator[Edge]:\n            \"\"\"\n            Return edge(in_node, out_node) in tuple\n            \"\"\"\n            if node not in node_to_edges:\n                return\n                yield\n\n            for edge in node_to_edges[node]:\n                yield edge\n\n        node_to_edges = {parent: [Edge(in_node=tgt_item.parent, out_node=tgt_item.key)\n                                  for tgt_item in lineage_items if tgt_item.parent == parent]\n                         for parent in [item.parent for item in lineage_items]}\n        edge_to_lng_item = {Edge(in_node=item.parent, out_node=item.key): item for item in lineage_items}\n        edge_unexplored = [EdgePair(in_edge=Edge(in_node=None, out_node=id), out_edge=get_next_edge(id))]\n        edge_explored = set()\n        lineage_item_sorted = []\n\n        while edge_unexplored:\n            edge, next_edge_iter = edge_unexplored.pop()\n            edge_explored.add(edge)\n\n            for next_edge in next_edge_iter:\n                if next_edge not in edge_explored:\n                    edge_unexplored.append(EdgePair(\n                        in_edge=Edge(in_node=edge.in_node, out_node=edge.out_node),\n                        out_edge=next_edge_iter\n                    ))\n                    edge_unexplored.append(EdgePair(\n                        in_edge=Edge(in_node=next_edge.in_node, out_node=next_edge.out_node),\n                        out_edge=get_next_edge(next_edge.out_node)\n                    ))\n                    break\n            else:\n                if edge.in_node:\n                    lineage_item_sorted.append(edge_to_lng_item[edge])\n\n        lineage_item_sorted.reverse()\n        return lineage_item_sorted\n\n    def get_statistics(self) -> Dict[str, Any]:\n        pass\n\n    def get_feature(self, *, feature_uri: str) -> Feature:\n        pass\n\n    def get_resource_description(self, *, resource_type: ResourceType, uri: str) -> Description:\n        pass\n\n    def put_resource_description(self, *, resource_type: ResourceType, uri: str, description: str) -> None:\n        pass\n\n    def add_resource_owner(self, *, uri: str, resource_type: ResourceType, owner: str) -> None:\n        pass\n\n    def delete_resource_owner(self, *, uri: str, resource_type: ResourceType, owner: str) -> None:\n        pass\n\n    def get_resource_generation_code(self, *, uri: str, resource_type: ResourceType) -> GenerationCode:\n        pass\n\n    def put_type_metadata_description(self, *,\n                                      type_metadata_key: str,\n                                      description: str) -> None:\n        pass\n\n    def get_type_metadata_description(self, *,\n                                      type_metadata_key: str) -> Union[str, None]:\n        pass\n", "import_text": ["logging", "time", "collections.namedtuple", "random.randint", "typing.Any", "typing.Dict", "typing.Iterator", "typing.List", "typing.Optional", "typing.Tuple", "typing.Type", "typing.Union", "amundsen_common.entity.resource_type.ResourceType", "amundsen_common.entity.resource_type.to_resource_type", "amundsen_common.models.dashboard.DashboardSummary", "amundsen_common.models.feature.Feature", "amundsen_common.models.generation_code.GenerationCode", "amundsen_common.models.lineage.Lineage", "amundsen_common.models.lineage.LineageItem", "amundsen_common.models.popular_table.PopularTable", "amundsen_common.models.table.Application", "amundsen_common.models.table.Badge", "amundsen_common.models.table.Badge", "amundsen_common.models.table.Column", "amundsen_common.models.table.ProgrammaticDescription", "amundsen_common.models.table.Reader", "amundsen_common.models.table.Source", "amundsen_common.models.table.Stat", "amundsen_common.models.table.Table", "amundsen_common.models.table.TableSummary", "amundsen_common.models.table.Tag", "amundsen_common.models.table.User", "amundsen_common.models.table.Watermark", "amundsen_common.models.user.User", "amundsen_common.models.user.UserSchema", "amundsen_rds.models.RDSModel", "amundsen_rds.models.badge.Badge", "amundsen_rds.models.base.Base", "amundsen_rds.models.cluster.Cluster", "amundsen_rds.models.column.ColumnDescription", "amundsen_rds.models.column.TableColumn", "amundsen_rds.models.dashboard.Dashboard", "amundsen_rds.models.dashboard.DashboardChart", "amundsen_rds.models.dashboard.DashboardCluster", "amundsen_rds.models.dashboard.DashboardDescription", "amundsen_rds.models.dashboard.DashboardExecution", "amundsen_rds.models.dashboard.DashboardFollower", "amundsen_rds.models.dashboard.DashboardGroup", "amundsen_rds.models.dashboard.DashboardOwner", "amundsen_rds.models.dashboard.DashboardQuery", "amundsen_rds.models.dashboard.DashboardTable", "amundsen_rds.models.dashboard.DashboardTag", "amundsen_rds.models.dashboard.DashboardUsage", "amundsen_rds.models.database.Database", "amundsen_rds.models.schema.Schema", "amundsen_rds.models.table.Table", "amundsen_rds.models.table.TableDescription", "amundsen_rds.models.table.TableFollower", "amundsen_rds.models.table.TableOwner", "amundsen_rds.models.table.TableTag", "amundsen_rds.models.table.TableUsage", "amundsen_rds.models.tag.Tag", "amundsen_rds.models.updated_timestamp.UpdatedTimestamp", "amundsen_rds.models.user.User", "beaker.cache.CacheManager", "beaker.util.parse_cache_config_options", "flask.current_app", "sqlalchemy.func", "sqlalchemy.literal", "sqlalchemy.orm.Session", "sqlalchemy.orm.load_only", "sqlalchemy.orm.subqueryload", "metadata_service.client.rds_client.RDSClient", "metadata_service.entity.dashboard_detail.DashboardDetail", "metadata_service.entity.dashboard_query.DashboardQuery", "metadata_service.entity.description.Description", "metadata_service.entity.tag_detail.TagDetail", "metadata_service.exception.NotFoundException", "metadata_service.proxy.base_proxy.BaseProxy", "metadata_service.proxy.statsd_utilities.timer_with_counter", "metadata_service.util.UserResourceRel"], "prompt": "\"\"\"\nDescription: This function retrieves a table by user relation.\n\nArgs:\n    self: The instance of the class.\n    user_email (str): The email of the user.\n    relation_type (UserResourceRel): The type of relation.\n\nRaises:\n    NotImplementedError: If the relation type is not defined.\n\nReturns:\n    Dict[str, Any]: A dictionary containing the resource type and the list of tables.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"\n        Retrieve all the tables based on the given user and relation.\n        :param user_email:\n        :param relation_type:\n        :return:\n        \"\"\"", "function_dependencies": ["amundsen_rds.models.table.Table.rk.in_", "sqlalchemy.orm.load_only", "sqlalchemy.orm.subqueryload", "sqlalchemy.orm.subqueryload.options", "amundsen_common.models.popular_table.PopularTable", "amundsen_common.entity.resource_type.ResourceType.Table.name.lower"], "project_create_time": "2019-05-14T15:12:40+00:00", "project_update_time": "2024-04-17T09:08:12+00:00", "file_create_time": "2021-06-03T20:39:08Z", "file_update_time": "2023-01-12T19:48:54Z", "function_update_time": "2021-06-03T20:39:08Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["sqlalchemy.orm.load_only"], "test_function": [{"file_path": "/amundsen-search-4.2.0/amundsen-search-4.2.0/metadata/tests/unit/proxy/test_mysql_proxy.py", "class_name": "TestMySQLProxy", "function_name": "test_get_table_by_user_relation", "code": "\n    def test_get_table_by_user_relation(self, mock_rds_client: Any) -> None:\n        table = RDSTable(name='test_table')\n        table_description = RDSTableDescription(description='test_description')\n        schema = RDSSchema(name='test_schema')\n        cluster = RDSCluster(name='test_cluster')\n        database = RDSDatabase(name='test_database')\n        cluster.database = database\n        schema.cluster = cluster\n        table.schema = schema\n        table.description = table_description\n\n        mock_client = MagicMock()\n        mock_rds_client.return_value = mock_client\n\n        mock_create_session = MagicMock()\n        mock_client.create_session.return_value = mock_create_session\n\n        mock_session = MagicMock()\n        mock_create_session.__enter__.return_value = mock_session\n\n        mock_session_query = MagicMock()\n        mock_session.query.return_value = mock_session_query\n\n        mock_session_query_filter = MagicMock()\n        mock_session_query.filter.return_value = mock_session_query_filter\n\n        mock_session_query_filter_options = MagicMock()\n        mock_session_query_filter.options.return_value = mock_session_query_filter_options\n\n        mock_session_query_filter_options.all.return_value = [table]\n\n        expected = {'table': [PopularTable(database='test_database',\n                                           cluster='test_cluster',\n                                           schema='test_schema',\n                                           name='test_table',\n                                           description='test_description')]}\n        proxy = MySQLProxy()\n        actual = proxy.get_table_by_user_relation(user_email='test_user',\n                                                  relation_type=UserResourceRel.follow)\n\n        self.assertEqual(len(actual['table']), 1)\n        self.assertEqual(expected, actual)"}]}, {"git_group": "MushroomRL", "git_name": "mushroom-rl", "version": "1.10.1", "language": "Python", "project_name": "mushroom-rl-1.10.1.zip", "file_path": "/mushroom-rl-1.10.1/mushroom-rl-1.10.1/mushroom_rl/solvers/dynamic_programming.py", "file_name": "dynamic_programming.py", "focal_class": null, "focal_name": "policy_iteration", "focal_parameter": ["prob", "reward", "gamma"], "solution": "def policy_iteration(prob, reward, gamma):\n    n_states = prob.shape[0]\n    n_actions = prob.shape[1]\n\n    policy = np.zeros(n_states, dtype=int)\n    value = np.zeros(n_states)\n\n    changed = True\n    while changed:\n        p_pi = np.zeros((n_states, n_states))\n        r_pi = np.zeros(n_states)\n        i = np.eye(n_states)\n\n        for state in range(n_states):\n            action = policy[state]\n            p_pi_s = prob[state, action, :]\n            r_pi_s = reward[state, action, :]\n\n            p_pi[state, :] = p_pi_s.T\n            r_pi[state] = p_pi_s.T.dot(r_pi_s)\n\n        value = np.linalg.solve(i - gamma * p_pi, r_pi)\n\n        changed = False\n\n        for state in range(n_states):\n            vmax = value[state]\n            for action in range(n_actions):\n                if action != policy[state]:\n                    p_sa = prob[state, action]\n                    r_sa = reward[state, action]\n                    va = p_sa.T.dot(r_sa + gamma * value)\n                    if va > vmax and not np.isclose(va, vmax):\n                        policy[state] = action\n                        vmax = va\n                        changed = True\n\n    return value, policy", "function_signature": "def policy_iteration(prob, reward, gamma) :", "left_context": "import numpy as np\nfrom copy import deepcopy\n\n\ndef value_iteration(prob, reward, gamma, eps):\n    \"\"\"\n    Value iteration algorithm to solve a dynamic programming problem.\n\n    Args:\n        prob (np.ndarray): transition probability matrix;\n        reward (np.ndarray): reward matrix;\n        gamma (float): discount factor;\n        eps (float): accuracy threshold.\n\n    Returns:\n        The optimal value of each state.\n\n    \"\"\"\n    n_states = prob.shape[0]\n    n_actions = prob.shape[1]\n\n    value = np.zeros(n_states)\n\n    while True:\n        value_old = deepcopy(value)\n\n        for state in range(n_states):\n            vmax = -np.inf\n            for action in range(n_actions):\n                prob_state_action = prob[state, action, :]\n                reward_state_action = reward[state, action, :]\n                va = prob_state_action.T.dot(\n                    reward_state_action + gamma * value_old)\n                vmax = max(va, vmax)\n\n            value[state] = vmax\n        if np.linalg.norm(value - value_old) <= eps:\n            break\n\n    return value\n\n", "right_context": "", "import_text": ["numpy", "copy.deepcopy"], "prompt": "\"\"\"\nDescription: This function implements the policy iteration algorithm for solving Markov Decision Processes (MDPs).\n\nArgs:\n    prob (numpy.ndarray): A 3D numpy array representing the transition probabilities of the MDP. The dimensions are (n_states, n_actions, n_states).\n    reward (numpy.ndarray): A 3D numpy array representing the rewards of the MDP. The dimensions are (n_states, n_actions, n_states).\n    gamma (float): The discount factor of the MDP.\n\nReturns:\n    tuple: A tuple containing two numpy arrays. The first array is the value function of the MDP, and the second array is the optimal policy.\n\nRaises:\n    ValueError: If the dimensions of the input arrays do not match the expected dimensions.\n\nNotes:\n    This function uses the numpy.eye and numpy.isclose functions. The numpy.eye function is used to create an identity matrix of the appropriate size. The numpy.isclose function is used to compare two values for closeness.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Policy iteration algorithm to solve a dynamic programming problem.\n\n    Args:\n        prob (np.ndarray): transition probability matrix;\n        reward (np.ndarray): reward matrix;\n        gamma (float): discount factor.\n\n    Returns:\n        The optimal value of each state and the optimal policy.\n\n    \"\"\"", "function_dependencies": ["numpy.zeros", "numpy.eye", "numpy.linalg.solve", "numpy.isclose"], "project_create_time": "2017-02-25T19:59:57+00:00", "project_update_time": "2024-04-17T09:51:49+00:00", "file_create_time": "2020-01-03T16:53:59Z", "file_update_time": "2022-07-19T10:47:22Z", "function_update_time": "2020-01-03T16:53:59Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["numpy.eye", "numpy.isclose"], "test_function": [{"file_path": "/mushroom-rl-1.10.1/mushroom-rl-1.10.1/tests/solvers/test_dynamic_programming.py", "class_name": null, "function_name": "test_policy_iteration", "code": "\ndef test_policy_iteration():\n    p = np.array([[[1., 0., 0., 0.],\n                   [0.1, 0., 0.9, 0.],\n                   [1., 0., 0., 0.],\n                   [0.1, 0.9, 0., 0.]],\n                  [[0., 1., 0., 0.],\n                   [0., 0.1, 0., 0.9],\n                   [0.9, 0.1, 0., 0.],\n                   [0., 1., 0., 0.]],\n                  [[0.9, 0., 0.1, 0.],\n                   [0., 0., 1., 0.],\n                   [0., 0., 1., 0.],\n                   [0., 0., 0.1, 0.9]],\n                  [[0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.]]])\n    r = np.array([[[0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.]],\n                  [[0., 0., 0., 0.],\n                   [0., 0., 0., 1.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.]],\n                  [[0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 1.]],\n                  [[0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.]]])\n    gamma = .95\n\n    q, p = policy_iteration(p, r, gamma)\n    q_test = np.array([0.93953176, 0.99447514, 0.99447514, 0.])\n    p_test = np.array([1, 1, 3, 0])\n\n    assert np.allclose(q, q_test) and np.allclose(p, p_test)"}]}, {"git_group": "quantumlib", "git_name": "OpenFermion", "version": "v1.6.1", "language": "Python", "project_name": "OpenFermion-v1.6.1.zip", "file_path": "/OpenFermion-v1.6.1/OpenFermion-1.6.1/src/openfermion/hamiltonians/hartree_fock.py", "file_name": "hartree_fock.py", "focal_class": null, "focal_name": "rhf_params_to_matrix", "focal_parameter": [], "solution": "def rhf_params_to_matrix(\n    parameters: np.ndarray,\n    num_orbitals: int,\n    occ: Optional[Union[None, List[int]]] = None,\n    virt: Optional[Union[None, List[int]]] = None,\n) -> np.ndarray:\n    if occ is None:\n        occ = range(num_orbitals // 2)\n    if virt is None:\n        virt = range(num_orbitals // 2, num_orbitals)\n\n    # check that parameters are a real array\n    if not np.allclose(parameters.imag, 0):\n        raise ValueError(\"parameters input must be real valued\")\n\n    kappa = np.zeros((len(occ) + len(virt), len(occ) + len(virt)))\n    for idx, (v, o) in enumerate(product(virt, occ)):\n        kappa[v, o] = parameters[idx].real\n        kappa[o, v] = -parameters[idx].real\n    return kappa", "function_signature": "def rhf_params_to_matrix(\n    parameters: np.ndarray,\n    num_orbitals: int,\n    occ: Optional[Union[None, List[int]]] = None,\n    virt: Optional[Union[None, List[int]]] = None,\n) -> np.ndarray :", "left_context": "\"\"\"\nModule performs gradient based RHF, [WIP] UHF, [WIP] GHF\n\nModule needs AO integrals\n\"\"\"\n# pylint: disable=C\nfrom typing import Callable, Dict, List, Optional, Tuple, Union\nfrom itertools import product\nimport numpy as np\nimport scipy as sp\nfrom scipy.optimize import OptimizeResult\nfrom openfermion.ops.representations import (\n    InteractionOperator,\n    InteractionRDM,\n    general_basis_change,\n)\nimport openfermion.linalg as linalg\n\n\ndef get_matrix_of_eigs(w: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Transform the eigenvalues into a matrix corresponding\n    to summing the adjoint rep.\n\n    Args:\n        w: eigenvalues of C-matrix\n\n    Returns: new array of transformed eigenvalues\n    \"\"\"\n    transform_eigs = np.zeros((w.shape[0], w.shape[0]), dtype=np.complex128)\n    for i, j in product(range(w.shape[0]), repeat=2):\n        if np.isclose(abs(w[i] - w[j]), 0):\n            transform_eigs[i, j] = 1\n        else:\n            transform_eigs[i, j] = (np.exp(1j * (w[i] - w[j])) - 1) / (1j * (w[i] - w[j]))\n    return transform_eigs\n\n\nclass InputError(Exception):\n    pass\n\n\nclass HartreeFockFunctional:\n    \"\"\"\n    Implementation of the objective function code for Restricted Hartree-Fock\n\n    The object transforms a variety of input types into the appropriate output.\n    It does this by analyzing the type and size of the input based on its\n    knowledge of each type.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        one_body_integrals: np.ndarray,\n        two_body_integrals: np.ndarray,\n        overlap: np.ndarray,\n        n_electrons: int,\n        model='rhf',\n        nuclear_repulsion: Optional[float] = 0.0,\n        initial_orbitals: Optional[Union[None, Callable]] = None,\n    ):\n        \"\"\"\n        Initialize functional\n\n        Args:\n            one_body_integrals: integrals in the atomic orbital basis for the\n                                one-body potential.\n            two_body_integrals: integrals in the  atomic obrital basis for the\n                                two-body potential ordered according to\n                                phi_{p}(r1)^{*}phi_{q}^{*}(r2) x\n                                phi_{r}(r2)phi_{s}(r1)\n            overlap:  overlap integrals in the atomic orbital basis\n            n_electrons:  number of electrons total\n            model: Optional flag for performing restricted-, unrestricted-,\n                   or generalized- hartree-fock.\n            nuclear_repulsion: Optional nuclear repulsion term.  Energy is\n                               shifted by this amount. default is 0.\n            initial_orbitals:  Method for producing the initial orbitals from\n                               the atomic orbitals. Default is defining the\n                               core orbitals.\n        \"\"\"\n        if model not in ['rhf', 'uhf', 'ghf']:\n            raise InputError(\"{} is not rhf, uhf, or ghf\".format(model))\n        self.model = model\n        self.obi = one_body_integrals\n        self.tbi = two_body_integrals\n        self.overlap = overlap\n        self.num_orbitals = one_body_integrals.shape[0]\n        self.num_electrons = n_electrons\n        self.constant_offset = nuclear_repulsion\n        self.hamiltonian = None\n\n        self.nocc = None\n        self.nvirt = None\n        self.occ = None\n        self.virt = None\n        if model == 'rhf':\n            self.nocc = self.num_electrons // 2\n            self.nvirt = self.num_orbitals - self.nocc\n            self.occ = list(range(self.nocc))\n            self.virt = list(range(self.nocc, self.nocc + self.nvirt))\n        elif model == 'uhf' or model == 'ghf':\n            self.nocc = self.num_electrons\n            self.nvirt = 2 * self.num_orbitals - self.nocc\n            self.occ = list(range(self.nocc))\n            self.virt = list(range(self.nocc, self.nocc + self.nvirt))\n\n        if initial_orbitals is None:\n            # use core orbitals\n            _, core_orbs = sp.linalg.eigh(one_body_integrals, b=overlap)\n\n            molecular_hamiltonian = generate_hamiltonian(\n                one_body_integrals=general_basis_change(self.obi, core_orbs, (1, 0)),\n                two_body_integrals=general_basis_change(self.tbi, core_orbs, (1, 1, 0, 0)),\n                constant=self.constant_offset,\n            )\n            self.hamiltonian = molecular_hamiltonian\n        else:\n            self.hamiltonian = initial_orbitals(self.obi, self.tbi, self.num_electrons)\n\n    def rdms_from_rhf_opdm(self, opdm_aa: np.ndarray) -> InteractionRDM:\n        \"\"\"\n        Generate spin-orbital InteractionRDM object from the alpha-spin\n        opdm.\n\n        Args:\n            opdm_aa: single spin sector of the 1-particle denstiy matrix\n\n        Returns:  InteractionRDM object for full spin-orbital 1-RDM and 2-RDM\n        \"\"\"\n\n        opdm = np.zeros((2 * self.num_orbitals, 2 * self.num_orbitals), dtype=np.complex128)\n        opdm[::2, ::2] = opdm_aa\n        opdm[1::2, 1::2] = opdm_aa\n        tpdm = linalg.wedge(opdm, opdm, (1, 1), (1, 1))\n        rdms = InteractionRDM(opdm, 2 * tpdm)\n        return rdms\n\n    def energy_from_rhf_opdm(self, opdm_aa: np.ndarray) -> float:\n        \"\"\"\n        Compute the energy given a spin-up opdm\n\n        Args:\n            opdm_aa: spin-up opdm.  Should be an n x n matrix where n is\n                     the number of spatial orbitals\n\n        Returns: RHF energy\n        \"\"\"\n        rdms = self.rdms_from_rhf_opdm(opdm_aa)\n        return rdms.expectation(self.hamiltonian).real\n\n    def rhf_global_gradient(self, params: np.ndarray, alpha_opdm: np.ndarray):\n        \"\"\"\n        Compute rhf global gradient\n\n        Args:\n            params: rhf-parameters for rotation matrix.\n            alpha_opdm: 1-RDM corresponding to results of basis rotation\n                        parameterized by `params'.\n\n        Returns: gradient vector the same size as the input `params'\n        \"\"\"\n        opdm = np.zeros((2 * self.num_orbitals, 2 * self.num_orbitals), dtype=np.complex128)\n        opdm[::2, ::2] = alpha_opdm\n        opdm[1::2, 1::2] = alpha_opdm\n        tpdm = 2 * linalg.wedge(opdm, opdm, (1, 1), (1, 1))\n\n        # now go through and generate all the necessary Z, Y, Y_kl matrices\n        kappa_matrix = rhf_params_to_matrix(\n            params, len(self.occ) + len(self.virt), self.occ, self.virt\n        )\n        kappa_matrix_full = np.kron(kappa_matrix, np.eye(2))\n        w_full, v_full = np.linalg.eigh(-1j * kappa_matrix_full)  # so that kappa = i U lambda U^\n        eigs_scaled_full = get_matrix_of_eigs(w_full)\n\n        grad = np.zeros(self.nocc * self.nvirt, dtype=np.complex128)\n        # kdelta = np.eye(2 * self.num_orbitals)\n\n        # NOW GENERATE ALL TERMS ASSOCIATED WITH THE GRADIENT!!!!!!\n        for p in range(self.nocc * self.nvirt):\n            grad_params = np.zeros_like(params)\n            grad_params[p] = 1\n            Y = rhf_params_to_matrix(\n                grad_params, len(self.occ) + len(self.virt), self.occ, self.virt\n            )\n            Y_full = np.kron(Y, np.eye(2))\n\n            # Now rotate Y int othe basis that diagonalizes Z\n            Y_kl_full = v_full.conj().T.dot(Y_full).dot(v_full)\n            # now rotate Y_{kl} * (exp(i(l_{k} - l_{l})) - 1) / (i(l_{k} - l_{l}))\n            # into the original basis\n            pre_matrix_full = v_full.dot(eigs_scaled_full * Y_kl_full).dot(v_full.conj().T)\n\n            grad_expectation = (\n                -1.0\n                * np.einsum(\n                    'ab,pa,pb',\n                    self.hamiltonian.one_body_tensor,\n                    pre_matrix_full,\n                    opdm,\n                    optimize='optimal',\n                ).real\n            )\n\n            grad_expectation += (\n                1.0\n                * np.einsum(\n                    'ab,bq,aq',\n                    self.hamiltonian.one_body_tensor,\n                    pre_matrix_full,\n                    opdm,\n                    optimize='optimal',\n                ).real\n            )\n\n            grad_expectation += (\n                1.0\n                * np.einsum(\n                    'ijkl,pi,jpkl',\n                    self.hamiltonian.two_body_tensor,\n                    pre_matrix_full,\n                    tpdm,\n                    optimize='optimal',\n                ).real\n            )\n\n            grad_expectation += (\n                -1.0\n                * np.einsum(\n                    'ijkl,pj,ipkl',\n                    self.hamiltonian.two_body_tensor,\n                    pre_matrix_full,\n                    tpdm,\n                    optimize='optimal',\n                ).real\n            )\n\n            grad_expectation += (\n                -1.0\n                * np.einsum(\n                    'ijkl,kq,ijlq',\n                    self.hamiltonian.two_body_tensor,\n                    pre_matrix_full,\n                    tpdm,\n                    optimize='optimal',\n                ).real\n            )\n\n            grad_expectation += (\n                1.0\n                * np.einsum(\n                    'ijkl,lq,ijkq',\n                    self.hamiltonian.two_body_tensor,\n                    pre_matrix_full,\n                    tpdm,\n                    optimize='optimal',\n                ).real\n            )\n            grad[p] = grad_expectation\n\n        return grad\n\n\ndef generate_hamiltonian(\n    one_body_integrals: np.ndarray,\n    two_body_integrals: np.ndarray,\n    constant: float,\n    EQ_TOLERANCE: Optional[float] = 1.0e-12,\n) -> InteractionOperator:\n    n_qubits = 2 * one_body_integrals.shape[0]\n    # Initialize Hamiltonian coefficients.\n    one_body_coefficients = np.zeros((n_qubits, n_qubits))\n    two_body_coefficients = np.zeros((n_qubits, n_qubits, n_qubits, n_qubits))\n    # Loop through integrals.\n    for p in range(n_qubits // 2):\n        for q in range(n_qubits // 2):\n            # Populate 1-body coefficients. Require p and q have same spin.\n            one_body_coefficients[2 * p, 2 * q] = one_body_integrals[p, q]\n            one_body_coefficients[2 * p + 1, 2 * q + 1] = one_body_integrals[p, q]\n            # Continue looping to prepare 2-body coefficients.\n            for r in range(n_qubits // 2):\n                for s in range(n_qubits // 2):\n                    # Mixed spin\n                    two_body_coefficients[2 * p, 2 * q + 1, 2 * r + 1, 2 * s] = (\n                        two_body_integrals[p, q, r, s] / 2.0\n                    )\n                    two_body_coefficients[2 * p + 1, 2 * q, 2 * r, 2 * s + 1] = (\n                        two_body_integrals[p, q, r, s] / 2.0\n                    )\n\n                    # Same spin\n                    two_body_coefficients[2 * p, 2 * q, 2 * r, 2 * s] = (\n                        two_body_integrals[p, q, r, s] / 2.0\n                    )\n                    two_body_coefficients[2 * p + 1, 2 * q + 1, 2 * r + 1, 2 * s + 1] = (\n                        two_body_integrals[p, q, r, s] / 2.0\n                    )\n\n    # Truncate.\n    one_body_coefficients[np.absolute(one_body_coefficients) < EQ_TOLERANCE] = 0.0\n    two_body_coefficients[np.absolute(two_body_coefficients) < EQ_TOLERANCE] = 0.0\n\n    # Cast to InteractionOperator class and return.\n    molecular_hamiltonian = InteractionOperator(\n        constant, one_body_coefficients, two_body_coefficients\n    )\n    return molecular_hamiltonian\n\n", "right_context": "\n\ndef rhf_func_generator(\n    rhf_func: HartreeFockFunctional,\n    init_occ_vec: Optional[Union[None, np.ndarray]] = None,\n    get_opdm_func: Optional[bool] = False,\n) -> Union[Tuple[Callable, Callable, Callable], Tuple[Callable, Callable, Callable, Callable]]:\n    \"\"\"\n    Generate the energy, gradient, and unitary functions\n\n    Args:\n        rhf_func: objective function object.\n        init_occ_vec: (optional) vector for occupation numbers of\n                      the alpha-opdm.\n        get_opdm_func: (optional) flag for returning Callable that returns\n                       the final opdm.\n    Returns: functions for unitary, energy, gradient (in that order)\n    \"\"\"\n    if init_occ_vec is None:\n        initial_opdm = np.diag([1] * rhf_func.nocc + [0] * rhf_func.nvirt)\n    else:\n        initial_opdm = np.diag(init_occ_vec)\n\n    def energy(params):\n        u = unitary(params)\n        final_opdm_aa = u.dot(initial_opdm).dot(np.conjugate(u).T)\n        tenergy = rhf_func.energy_from_rhf_opdm(final_opdm_aa)\n        return tenergy\n\n    def gradient(params):\n        u = unitary(params)\n        final_opdm_aa = u.dot(initial_opdm).dot(np.conjugate(u).T)\n        return rhf_func.rhf_global_gradient(params, final_opdm_aa).real\n\n    def unitary(params):\n        kappa = rhf_params_to_matrix(\n            params, rhf_func.nocc + rhf_func.nvirt, rhf_func.occ, rhf_func.virt\n        )\n        return sp.linalg.expm(kappa)\n\n    def get_opdm(params):\n        u = unitary(params)\n        return u.dot(initial_opdm).dot(np.conjugate(u).T)\n\n    if get_opdm_func:\n        return unitary, energy, gradient, get_opdm\n    return unitary, energy, gradient\n\n\ndef rhf_minimization(\n    rhf_object: HartreeFockFunctional,\n    method: Optional[str] = 'CG',\n    initial_guess: Optional[Union[None, np.ndarray]] = None,\n    verbose: Optional[bool] = True,\n    sp_options: Optional[Union[None, Dict]] = None,\n) -> OptimizeResult:\n    \"\"\"\n    Perform Hartree-Fock energy minimization\n\n    Args:\n        rhf_object: An instantiation of the HartreeFockFunctional\n        method: (optional) scipy optimization method\n        initial_guess: (optional) initial rhf parameter vector.  If None\n                       zero vector is used.\n        verbose: (optional) turn on printing.  This is passed to the\n                 scipy 'disp' option.\n        sp_options:\n    Returns: scipy.optimize result object\n    \"\"\"\n    _, energy, gradient = rhf_func_generator(rhf_object)\n    if initial_guess is None:\n        init_guess = np.zeros(rhf_object.nocc * rhf_object.nvirt)\n    else:\n        init_guess = np.asarray(initial_guess).flatten()\n\n    sp_optimizer_options = {'disp': verbose}\n    if sp_options is not None:\n        sp_optimizer_options.update(sp_options)\n\n    return sp.optimize.minimize(\n        energy, init_guess, jac=gradient, method=method, options=sp_optimizer_options\n    )\n", "import_text": ["typing.Callable", "typing.Dict", "typing.List", "typing.Optional", "typing.Tuple", "typing.Union", "itertools.product", "numpy", "scipy", "scipy.optimize.OptimizeResult", "openfermion.ops.representations.InteractionOperator", "openfermion.ops.representations.InteractionRDM", "openfermion.ops.representations.general_basis_change", "openfermion.linalg"], "prompt": "\"\"\"\nDescription: This function converts restricted Hartree-Fock (RHF) parameters to a matrix.\n\nArgs:\n    parameters (np.ndarray): The parameters to be converted.\n    num_orbitals (int): The total number of orbitals.\n    occ (Optional[Union[None, List[int]]]): The occupied orbitals. Defaults to the first half of the orbitals.\n    virt (Optional[Union[None, List[int]]]): The virtual orbitals. Defaults to the second half of the orbitals.\n\nReturns:\n    np.ndarray: The matrix representation of the parameters.\n\nRaises:\n    ValueError: If the parameters are not real valued.\n\nNotes:\n    This function uses the numpy.allclose API to check if the parameters are real valued.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    For restricted Hartree-Fock we have nocc * nvirt parameters.  These are\n    provided as a list that is ordered by (virtuals) \\times (occupied).\n\n    For example, for H4 we have 2 orbitals occupied and 2 virtuals\n\n    occupied = [0, 1]  virtuals = [2, 3]\n\n    parameters = [(v_{0}, o_{0}), (v_{0}, o_{1}), (v_{1}, o_{0}), (v_{1}, o_{1})]\n               = [(2, 0), (2, 1), (3, 0), (3, 1)]\n\n    You can think of the tuples of elements of the upper right triangle of the\n    antihermitian matrix that specifies the c_{b, i} coefficients.\n\n    coefficient matrix\n    [[ c_{0, 0}, -c_{1, 0}, -c_{2, 0}, -c_{3, 0}],\n     [ c_{1, 0},  c_{1, 1}, -c_{2, 1}, -c_{3, 1}],\n     [ c_{2, 0},  c_{2, 1},  c_{2, 2}, -c_{3, 2}],\n     [ c_{3, 0},  c_{3, 1},  c_{3, 2},  c_{3, 3}]]\n\n    Since we are working with only non-redundant operators we know c_{i, i} = 0\n    and any c_{i, j} where i and j are both in occupied or both in virtual = 0.\n\n    Args:\n        parameters: array of parameters for kappa matrix\n        num_orbitals: total number of spatial orbitals\n        occ: (Optional) indices for doubly occupied sector\n        virt: (Optional) indices for virtual sector\n\n    Returns: np.ndarray kappa matrix\n    \"\"\"", "function_dependencies": ["numpy.allclose", "numpy.zeros", "itertools.product"], "project_create_time": "2017-09-21T22:10:28+00:00", "project_update_time": "2024-04-16T10:59:35+00:00", "file_create_time": "2020-07-23T23:38:20Z", "file_update_time": "2023-11-22T18:22:17Z", "function_update_time": "2023-11-22T18:22:17Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["numpy.allclose"], "test_function": [{"file_path": "/OpenFermion-v1.6.1/OpenFermion-1.6.1/src/openfermion/hamiltonians/hartree_fock_test.py", "class_name": null, "function_name": "test_gradient", "code": "\ndef test_gradient():\n    filename = os.path.join(DATA_DIRECTORY, \"H2_sto-3g_singlet_0.7414.hdf5\")\n    molecule = MolecularData(filename=filename)\n\n    overlap = molecule.overlap_integrals\n    mo_obi = molecule.one_body_integrals\n    mo_tbi = molecule.two_body_integrals\n    rotation_mat = molecule.canonical_orbitals.T.dot(overlap)\n    obi = general_basis_change(mo_obi, rotation_mat, (1, 0))\n    tbi = general_basis_change(mo_tbi, rotation_mat, (1, 1, 0, 0))\n    hff = HartreeFockFunctional(\n        one_body_integrals=obi,\n        two_body_integrals=tbi,\n        overlap=overlap,\n        n_electrons=molecule.n_electrons,\n        model='rhf',\n        nuclear_repulsion=molecule.nuclear_repulsion,\n    )\n\n    params = np.random.randn(hff.nocc * hff.nvirt)\n    u = sp.linalg.expm(rhf_params_to_matrix(params, hff.num_orbitals, occ=hff.occ, virt=hff.virt))\n    initial_opdm = np.diag([1] * hff.nocc + [0] * hff.nvirt)\n    final_opdm = u.dot(initial_opdm).dot(u.conj().T)\n    grad = hff.rhf_global_gradient(params, final_opdm)\n    grad_dim = grad.shape[0]\n\n    # get finite difference gradient\n    finite_diff_grad = np.zeros(grad_dim)\n    epsilon = 0.0001\n    for i in range(grad_dim):\n        params_epsilon = params.copy()\n        params_epsilon[i] += epsilon\n        u = sp.linalg.expm(\n            rhf_params_to_matrix(params_epsilon, hff.num_orbitals, occ=hff.occ, virt=hff.virt)\n        )\n        tfinal_opdm = u.dot(initial_opdm).dot(u.conj().T)\n        energy_plus_epsilon = hff.energy_from_rhf_opdm(tfinal_opdm)\n\n        params_epsilon[i] -= 2 * epsilon\n        u = sp.linalg.expm(\n            rhf_params_to_matrix(params_epsilon, hff.num_orbitals, occ=hff.occ, virt=hff.virt)\n        )\n        tfinal_opdm = u.dot(initial_opdm).dot(u.conj().T)\n        energy_minus_epsilon = hff.energy_from_rhf_opdm(tfinal_opdm)\n\n        finite_diff_grad[i] = (energy_plus_epsilon - energy_minus_epsilon) / (2 * epsilon)\n\n    assert np.allclose(finite_diff_grad, grad, atol=epsilon)"}, {"file_path": "/OpenFermion-v1.6.1/OpenFermion-1.6.1/src/openfermion/hamiltonians/hartree_fock_test.py", "class_name": null, "function_name": "test_gradient_lih", "code": "\ndef test_gradient_lih():\n    filename = os.path.join(DATA_DIRECTORY, \"H1-Li1_sto-3g_singlet_1.45.hdf5\")\n    molecule = MolecularData(filename=filename)\n\n    overlap = molecule.overlap_integrals\n    mo_obi = molecule.one_body_integrals\n    mo_tbi = molecule.two_body_integrals\n    rotation_mat = molecule.canonical_orbitals.T.dot(overlap)\n    obi = general_basis_change(mo_obi, rotation_mat, (1, 0))\n    tbi = general_basis_change(mo_tbi, rotation_mat, (1, 1, 0, 0))\n\n    hff = HartreeFockFunctional(\n        one_body_integrals=obi,\n        two_body_integrals=tbi,\n        overlap=overlap,\n        n_electrons=molecule.n_electrons,\n        model='rhf',\n        nuclear_repulsion=molecule.nuclear_repulsion,\n    )\n\n    params = np.random.randn(hff.nocc * hff.nvirt)\n    u = sp.linalg.expm(rhf_params_to_matrix(params, hff.num_orbitals, occ=hff.occ, virt=hff.virt))\n    grad_dim = hff.nocc * hff.nvirt\n    initial_opdm = np.diag([1] * hff.nocc + [0] * hff.nvirt)\n    final_opdm = u.dot(initial_opdm).dot(u.conj().T)\n    grad = hff.rhf_global_gradient(params, final_opdm)\n\n    # get finite difference gradient\n    finite_diff_grad = np.zeros(grad_dim)\n    epsilon = 0.0001\n    for i in range(grad_dim):\n        params_epsilon = params.copy()\n        params_epsilon[i] += epsilon\n        u = sp.linalg.expm(\n            rhf_params_to_matrix(params_epsilon, hff.num_orbitals, occ=hff.occ, virt=hff.virt)\n        )\n        tfinal_opdm = u.dot(initial_opdm).dot(u.conj().T)\n        energy_plus_epsilon = hff.energy_from_rhf_opdm(tfinal_opdm)\n\n        params_epsilon[i] -= 2 * epsilon\n        u = sp.linalg.expm(\n            rhf_params_to_matrix(params_epsilon, hff.num_orbitals, occ=hff.occ, virt=hff.virt)\n        )\n        tfinal_opdm = u.dot(initial_opdm).dot(u.conj().T)\n        energy_minus_epsilon = hff.energy_from_rhf_opdm(tfinal_opdm)\n\n        finite_diff_grad[i] = (energy_plus_epsilon - energy_minus_epsilon) / (2 * epsilon)\n\n    assert np.allclose(finite_diff_grad, grad, atol=epsilon)"}, {"file_path": "/OpenFermion-v1.6.1/OpenFermion-1.6.1/src/openfermion/hamiltonians/hartree_fock_test.py", "class_name": null, "function_name": "test_rhf_params_to_matrix", "code": "\ndef test_rhf_params_to_matrix():\n    params = np.random.randn(4)\n    true_kappa = np.zeros((4, 4))\n    true_kappa[0, 2], true_kappa[2, 0] = -params[0], params[0]\n    true_kappa[1, 2], true_kappa[2, 1] = -params[1], params[1]\n    true_kappa[0, 3], true_kappa[3, 0] = -params[2], params[2]\n    true_kappa[1, 3], true_kappa[3, 1] = -params[3], params[3]\n    test_kappa = rhf_params_to_matrix(params, 4)\n    assert np.allclose(test_kappa, true_kappa)\n\n    test_kappa = rhf_params_to_matrix(params, 4, occ=list(range(2)))\n    assert np.allclose(test_kappa, true_kappa)\n\n    test_kappa = rhf_params_to_matrix(params, 4, virt=list(range(2, 4)))\n    assert np.allclose(test_kappa, true_kappa)\n\n    with pytest.raises(ValueError):\n        rhf_params_to_matrix(params + 1j, 4)"}]}, {"git_group": "kudkudak", "git_name": "word-embeddings-benchmarks", "version": "master", "language": "Python", "project_name": "word-embeddings-benchmarks-master.zip", "file_path": "/word-embeddings-benchmarks-master/word-embeddings-benchmarks-master/web/datasets/analogy.py", "file_name": "analogy.py", "focal_class": null, "focal_name": "fetch_google_analogy", "focal_parameter": [], "solution": "def fetch_google_analogy():\n\n    url = \"https://www.dropbox.com/s/eujtyfb5zem1mim/EN-GOOGLE.txt?dl=1\"\n    with open(_fetch_file(url, \"analogy/EN-GOOGLE\", verbose=0), \"r\") as f:\n        L = f.read().splitlines()\n\n    # Simple 4 word analogy questions with categories\n    questions = []\n    answers = []\n    category = []\n    cat = None\n    for l in L:\n        if l.startswith(\":\"):\n            cat =l.lower().split()[1]\n        else:\n            words =  standardize_string(l).split()\n            questions.append(words[0:3])\n            answers.append(words[3])\n            category.append(cat)\n\n    assert set(category) == set(['gram3-comparative', 'gram8-plural', 'capital-common-countries',\n                                         'city-in-state', 'family', 'gram9-plural-verbs', 'gram2-opposite',\n                                         'currency', 'gram4-superlative', 'gram6-nationality-adjective',\n                                         'gram7-past-tense',\n                                         'gram5-present-participle', 'capital-world', 'gram1-adjective-to-adverb'])\n\n\n    syntactic = set([c for c in set(category) if c.startswith(\"gram\")])\n    category_high_level = []\n    for cat in category:\n         category_high_level.append(\"syntactic\" if cat in syntactic else \"semantic\")\n\n    # dtype=object for memory efficiency\n    return Bunch(X=np.vstack(questions).astype(\"object\"),\n                 y=np.hstack(answers).astype(\"object\"),\n                 category=np.hstack(category).astype(\"object\"),\n                 category_high_level=np.hstack(category_high_level).astype(\"object\"))", "function_signature": "def fetch_google_analogy() :", "left_context": "# -*- coding: utf-8 -*-\n\n\"\"\"\n Functions for fetching analogy datasets\n\"\"\"\n\nfrom collections import defaultdict\nimport glob\nimport os\nimport numpy as np\n\nfrom sklearn.utils import check_random_state\n\nfrom sklearn.datasets.base import Bunch\nfrom .utils import _get_dataset_dir, _fetch_file, _change_list_to_np\nfrom ..utils import standardize_string\n\n\ndef fetch_wordrep(subsample=None, rng=None):\n    \"\"\"\n    Fetch MSR WordRep dataset for testing both syntactic and semantic dataset\n\n    Returns\n    -------\n    data : sklearn.datasets.base.Bunch\n        dictionary-like object. Keys of interest:\n        'X': matrix of word pairs\n        'y': vector of answers\n        'category': name of category\n        'category_high_level': name of high level category (semantic/syntactic)\n\n    References\n    ----------\n    Gao, Bin and Bian, Jiang and Liu, Tie-Yan,\n    \"Wordrep: A benchmark for research on learning word representations\", 2014\n\n\n    Notes\n    -----\n    This dataset is too big to calculate and store all word analogy quadruples, this is\n    why it returns word paris\n\n    \"\"\"\n    path = _fetch_file(url=\"https://www.dropbox.com/sh/5k78h9gllvc44vt/AAALLQq-Bge605OIMlmGBbNJa?dl=1\",\n                       data_dir=\"analogy\",\n                       uncompress=True,\n                       move=\"EN-WORDREP/EN-WORDREP.zip\",\n                       verbose=0)\n\n    wikipedia_dict = glob.glob(os.path.join(path, \"Pairs_from_Wikipedia_and_Dictionary/*.txt\"))\n    wordnet = glob.glob(os.path.join(path, \"Pairs_from_WordNet/*.txt\"))\n\n    # This dataset is too big to calculate and store all word analogy quadruples\n    word_pairs = []\n    category = []\n    category_high_level = []\n\n    files = wikipedia_dict + wordnet\n\n    for file_name in files:\n        c = os.path.basename(file_name).split(\".\")[0]\n        c = c[c.index(\"-\")+1:]\n        with open(file_name, \"r\") as f:\n            for l in f.read().splitlines():\n                word_pairs.append(standardize_string(l).split())\n                category.append(c)\n                category_high_level.append(\"wikipedia-dict\" if file_name in wikipedia_dict else \"wordnet\")\n\n    if subsample:\n        assert 0 <= subsample <= 1.0\n        rng = check_random_state(rng)\n        ids = rng.choice(range(len(word_pairs)), int(subsample * len(word_pairs)), replace=False)\n        word_pairs = [word_pairs[i] for i in ids]\n        category = [category[i] for i in ids]\n        category_high_level = [category_high_level[i] for i in ids]\n\n    wordnet_categories = {'Antonym',\n     'Attribute',\n     'Causes',\n     'DerivedFrom',\n     'Entails',\n     'HasContext',\n     'InstanceOf',\n     'IsA',\n     'MadeOf',\n     'MemberOf',\n     'PartOf',\n     'RelatedTo',\n     'SimilarTo'}\n\n    wikipedia_categories = {'adjective-to-adverb',\n     'all-capital-cities',\n     'city-in-state',\n     'comparative',\n     'currency',\n     'man-woman',\n     'nationality-adjective',\n     'past-tense',\n     'plural-nouns',\n     'plural-verbs',\n     'present-participle',\n     'superlative'}\n\n    return Bunch(category_high_level=np.array(category_high_level),\n                 X=np.array(word_pairs),\n                 category=np.array(category),\n                 wikipedia_categories=wordnet_categories,\n                 wordnet_categories=wikipedia_categories)\n\n", "right_context": "\n\n\ndef fetch_msr_analogy():\n    \"\"\"\n    Fetch MSR dataset for testing performance on syntactic analogies\n\n    Returns\n    -------\n    data : sklearn.datasets.base.Bunch\n        dictionary-like object. Keys of interest:\n        'X': matrix of word questions\n        'y': vector of answers\n        'category': name of category\n        'category_high_level': name of high level category (noun/adjective/verb)\n\n    References\n    ----------\n    Originally published at http://research.microsoft.com/en-us/projects/rnn/.\n\n    Notes\n    -----\n    Authors description: \"more precisely, we tagged 267M words of newspaper text\n    with Treebank POS tags (Marcus et al., 1993). We then selected 100 of the most frequent comparative adjectives\n    (words labeled JJR); 100 of the most frequent plural nouns (NNS); 100 of the most frequent possessive nouns\n    (NN POS); and 100 of the most frequent base form verbs (VB).\n    We then systematically generated analogy questions by randomly matching each of the 100 words with 5 other words\n    from the same category, and creating variants.\n    \"\"\"\n    url = \"https://www.dropbox.com/s/ne0fib302jqbatw/EN-MSR.txt?dl=1\"\n    with open(_fetch_file(url, \"analogy/EN-MSR\", verbose=0), \"r\") as f:\n        L = f.read().splitlines()\n\n    # Typical 4 words analogy questions\n    questions = []\n    answers = []\n    category = []\n    for l in L:\n        words = standardize_string(l).split()\n        questions.append(words[0:3])\n        answers.append(words[4])\n        category.append(words[3])\n\n    verb = set([c for c in set(category) if c.startswith(\"VB\")])\n    noun = set([c for c in set(category) if c.startswith(\"NN\")])\n    category_high_level = []\n    for cat in category:\n         if cat in verb:\n             category_high_level.append(\"verb\")\n         elif cat in noun:\n             category_high_level.append(\"noun\")\n         else:\n             category_high_level.append(\"adjective\")\n\n    assert set([c.upper() for c in category]) == set(['VBD_VBZ', 'VB_VBD', 'VBZ_VBD',\n                                         'VBZ_VB', 'NNPOS_NN', 'JJR_JJS', 'JJS_JJR', 'NNS_NN', 'JJR_JJ',\n                                         'NN_NNS', 'VB_VBZ', 'VBD_VB', 'JJS_JJ', 'NN_NNPOS', 'JJ_JJS', 'JJ_JJR'])\n\n    return Bunch(X=np.vstack(questions).astype(\"object\"),\n                 y=np.hstack(answers).astype(\"object\"),\n                 category=np.hstack(category).astype(\"object\"),\n                 category_high_level=np.hstack(category_high_level).astype(\"object\"))\n\n\n# TODO: rewrite to a more standarized version\ndef fetch_semeval_2012_2(which=\"all\", which_scoring=\"golden\"):\n    \"\"\"\n    Fetch dataset used for SEMEVAL 2012 task 2 competition\n\n    Parameters\n    -------\n    which : \"all\", \"train\" or \"test\"\n    which_scoring: \"golden\" or \"platinium\" (see Notes)\n\n    Returns\n    -------\n    data : sklearn.datasets.base.Bunch\n        dictionary-like object. Keys of interest:\n        'X_prot': dictionary keyed on category. Each entry is a matrix of prototype word pairs (see Notes)\n        'X': dictionary keyed on category. Each entry is a matrix of question word pairs\n        'y': dictionary keyed on category. Each entry is a dictionary word pair -> score\n\n        'categories_names': dictionary keyed on category. Each entry is a human readable name of\n        category.\n        'categories_descriptions': dictionary keyed on category. Each entry is a human readable description of\n        category.\n\n    References\n    ----------\n    DA Jurgens et al.,\n    \"Measuring degrees of relational similarity. In *SEM 2012: The First Joint Conference on Lexical\n    and Computational Semantics\", 2012\n\n    Notes\n    -----\n    Dataset used in competition was scored as in golden scoring (which_scoring) parameter, however\n    organiser have release improved labels afterwards (platinium scoring)\n\n    The task is, given two pairs of words, A:B and C:D, determine the degree to which the semantic relations between\n    A and B are similar to those between C and D. Unlike the more familiar task of semantic relation identification,\n    which assigns each word pair to a discrete semantic relation class, this task recognizes the continuous range of\n    degrees of relational similarity. The challenge is to determine the degrees of relational similarity between a\n    given reference word pair and a variety of other pairs, mostly in the same general semantic relation class as the\n    reference pair.\n    \"\"\"\n    assert which in ['all', 'train', 'test']\n    assert which_scoring in ['golden', 'platinium']\n\n    path = _fetch_file(url=\"https://www.dropbox.com/sh/aarqsfnumx3d8ds/AAB05Mu2HdypP0pudGrNjooaa?dl=1\",\n                       data_dir=\"analogy\",\n                       uncompress=True,\n                       move=\"EN-SEMVAL-2012-2/EN-SEMVAL-2012-2.zip\",\n                       verbose=0)\n\n    train_files = set(glob.glob(os.path.join(path, \"train*.txt\"))) - \\\n                  set(glob.glob(os.path.join(path, \"train*_meta.txt\")))\n    test_files = set(glob.glob(os.path.join(path, \"test*.txt\"))) - \\\n                 set(glob.glob(os.path.join(path, \"test*_meta.txt\")))\n\n    if which == \"train\":\n        files = train_files\n    elif which == \"test\":\n        files = test_files\n    elif which == \"all\":\n        files = train_files.union(test_files)\n\n    # Every question is formed as similarity to analogy category that is\n    # posed as a list of 3 prototype word pairs\n    questions = defaultdict(list)\n    prototypes = {}\n    golden_scores = {}\n    platinium_scores = {}\n    scores = {\"platinium\": platinium_scores, \"golden\": golden_scores}\n    categories_names = {}\n    categories_descriptions = {}\n    for f in files:\n        with open(f[0:-4] + \"_meta.txt\") as meta_f:\n            meta = meta_f.read().splitlines()[1].split(\",\")\n\n        with open(os.path.dirname(f) + \"/pl-\" + os.path.basename(f)) as f_pl:\n            platinium = f_pl.read().splitlines()\n\n        with open(f) as f_gl:\n            golden = f_gl.read().splitlines()\n\n        assert platinium[0] == golden[0], (\"Incorrect file for \", f)\n\n        c = meta[0] + \"_\" + meta[1]\n        categories_names[c] = meta[2] + \"_\" + meta[3]\n        categories_descriptions[c] = meta[4]\n\n        prototypes[c] = [l.split(\":\") for l in \\\n                         platinium[0].replace(\": \", \":\").replace(\" \", \",\").replace(\".\", \"\").split(\",\")]\n        golden_scores[c] = {}\n        platinium_scores[c] = {}\n        questions_raw = []\n        for line_pl in platinium[1:]:\n            word_pair, score = line_pl.split()\n            questions_raw.append(word_pair)\n            questions[c].append([standardize_string(w) for w in word_pair.split(\":\")])\n            platinium_scores[c][word_pair] = score\n\n        for line_g in golden[1:]:\n            word_pair, score = line_g.split()\n            golden_scores[c][word_pair] = score\n\n        # Make scores a list\n        platinium_scores[c] = [platinium_scores[c][w] for w in questions_raw]\n        golden_scores[c] = [golden_scores[c][w] for w in questions_raw]\n\n    return Bunch(X_prot=_change_list_to_np(prototypes),\n                 X=_change_list_to_np(questions),\n                 y=scores[which_scoring],\n                 categories_names=categories_names,\n                 categories_descriptions=categories_descriptions)\n\n\n", "import_text": ["collections.defaultdict", "glob", "os", "numpy", "sklearn.utils.check_random_state", "sklearn.datasets.base.Bunch"], "prompt": "\"\"\"\nDescription: This function fetches a Google Analogy dataset from a URL and processes it into a Bunch object.\n\nArgs:\n    None\n\nReturns:\n    sklearn.datasets.base.Bunch: A Bunch object containing the following attributes:\n        - X (numpy.ndarray): A 2D array of questions. Each row represents a question and each column represents a word in the question.\n        - y (numpy.ndarray): A 1D array of answers. Each element corresponds to the answer for the question at the same index in X.\n        - category (numpy.ndarray): A 1D array of categories. Each element corresponds to the category for the question at the same index in X.\n        - category_high_level (numpy.ndarray): A 1D array of high-level categories. Each element corresponds to the high-level category for the question at the same index in X. High-level categories are either 'syntactic' or 'semantic'.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Fetch Google dataset for testing both semantic and syntactic analogies.\n\n    Returns\n    -------\n    data : sklearn.datasets.base.Bunch\n        dictionary-like object. Keys of interest:\n        'X': matrix of word questions\n        'y': vector of answers\n        'category': name of category\n        'category_high_level': name of high level category (semantic/syntactic)\n\n    References\n    ----------\n    Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff,\n    \"Distributed representations of words and phrases and their compositionality\", 2013\n\n    Notes\n    -----\n    This dataset is a subset of WordRep dataset.\n\n    \"\"\"", "function_dependencies": ["sklearn.datasets.base.Bunch", "numpy.vstack", "numpy.vstack.astype", "numpy.hstack", "numpy.hstack.astype"], "project_create_time": "2015-11-21T09:15:10+00:00", "project_update_time": "2024-04-03T18:01:38+00:00", "file_create_time": "2015-12-01T19:47:46Z", "file_update_time": "2015-12-23T12:30:15Z", "function_update_time": "2015-12-10T09:15:03Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["sklearn.datasets.base.Bunch"], "test_function": [{"file_path": "/word-embeddings-benchmarks-master/word-embeddings-benchmarks-master/web/tests/test_analogy.py", "class_name": null, "function_name": "test_analogy_solver", "code": "\ndef test_analogy_solver():\n    url = \"https://www.dropbox.com/s/5occ4p7k28gvxfj/ganalogy-sg-wiki-en-400.bin?dl=1\"\n    file_name = _fetch_file(url, \"test\")\n\n    w = Embedding.from_word2vec(file_name, binary=True)\n    data = fetch_google_analogy()\n    ids = np.random.RandomState(777).choice(range(data.X.shape[0]), 1000, replace=False)\n    X, y = data.X[ids], data.y[ids]\n    category = data.category_high_level[ids]\n\n    results = evaluate_analogy(w=w, X=X, y=y, category=category)\n    assert results['accuracy']['all'] >= 0.65\n    assert results['accuracy']['semantic'] >= 0.7\n    assert results['accuracy']['syntactic'] >= 0.63\n\n    results = evaluate_analogy(w=w, X=X, y=y, category=category, method=\"mul\")\n    assert results['accuracy']['all'] >= 0.7\n    assert results['accuracy']['semantic'] >= 0.75\n    assert results['accuracy']['syntactic'] >= 0.64\n\n    results_mul = evaluate_analogy(w=w, X=X, y=y, category=category, method=\"mul\", k=400)\n    results_add = evaluate_analogy(w=w, X=X, y=y, category=category, method=\"add\", k=400)\n    assert results_mul['accuracy']['all'] >= results_add['accuracy']['all']\n    assert results_mul['accuracy']['syntactic'] >= results_add['accuracy']['syntactic']\n    assert results_mul['accuracy']['semantic'] >= results_add['accuracy']['semantic']"}]}, {"git_group": "meltano", "git_name": "meltano", "version": "v3.4.0", "language": "Python", "project_name": "meltano-v3.4.0.zip", "file_path": "/meltano-v3.4.0/meltano-3.4.0/src/meltano/core/db.py", "file_name": "db.py", "focal_class": null, "focal_name": "project_engine", "focal_parameter": [], "solution": "def project_engine(\n    project: Project,\n    default: bool = False,\n) -> tuple[Engine, sessionmaker]:\n    if existing_engine := _engines.get(project):\n        return existing_engine\n\n    database_uri = project.settings.get(\"database_uri\")\n    parsed_db_uri = urlparse(database_uri)\n    sanitized_db_uri = parsed_db_uri._replace(  # noqa: WPS437\n        netloc=(\n            f\"{parsed_db_uri.username}:********@\"  # user:pass auth case\n            if parsed_db_uri.password\n            else \"********@\"  # token auth case\n            if parsed_db_uri.username\n            else \"\"  # no auth case\n        )\n        + (parsed_db_uri.hostname or \"\"),\n    ).geturl()\n    logging.debug(\n        f\"Creating DB engine for project at {str(project.root)!r} \"  # noqa: G004\n        f\"with DB URI {sanitized_db_uri!r}\",\n    )\n\n    if database_uri is None:\n        raise NullConnectionStringError\n\n    engine = create_engine(database_uri, poolclass=NullPool, future=True)\n\n    # Connect to the database to ensure it is available.\n    connect(\n        engine,\n        max_retries=project.settings.get(\"database_max_retries\"),\n        retry_timeout=project.settings.get(\"database_retry_timeout\"),\n    )\n\n    check_database_compatibility(engine)\n    init_hook(engine)\n\n    engine_session = (engine, sessionmaker(bind=engine, future=True))\n\n    if default:\n        # register the default engine\n        _engines[project] = engine_session\n\n    return engine_session", "function_signature": "def project_engine(\n    project: Project,\n    default: bool = False,\n) -> tuple[Engine, sessionmaker] :", "left_context": "\"\"\"Defines helpers related to the system database.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport time\nimport typing as t\nfrom urllib.parse import urlparse\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.exc import OperationalError\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.pool import NullPool\nfrom sqlalchemy.sql import text\n\nfrom meltano.core.error import MeltanoError\n\nif t.TYPE_CHECKING:\n    from sqlalchemy.engine import Connection, Engine\n\n    from meltano.core.project import Project\n\n# Keep a Project \u2192 Engine mapping to serve\n# the same engine for the same Project\n_engines = {}\n\n\nclass MeltanoDatabaseCompatibilityError(MeltanoError):\n    \"\"\"Raised when the database is not compatible with Meltano.\"\"\"\n\n    INSTRUCTION = (\n        \"Upgrade your database to be compatible with Meltano or use a different \"\n        \"database\"\n    )\n\n    def __init__(self, reason: str):\n        \"\"\"Initialize the error with a reason.\n\n        Args:\n            reason: The reason why the database is not compatible.\n        \"\"\"\n        super().__init__(reason, self.INSTRUCTION)\n\n\nclass NullConnectionStringError(MeltanoError):\n    \"\"\"Raised when the database is not compatible with Meltano.\"\"\"\n\n    REASON = \"The `database_uri` setting has a null value\"\n    INSTRUCTION = (\n        \"Verify that the `database_uri` setting points to a valid database connection \"\n        \"URI, or use `MELTANO_FF_STRICT_ENV_VAR_MODE=1 meltano config meltano list` \"\n        \"to check for missing environment variables\"\n    )\n\n    def __init__(self):\n        \"\"\"Initialize the exception.\"\"\"\n        super().__init__(self.REASON, self.INSTRUCTION)\n\n", "right_context": "\n\ndef connect(\n    engine: Engine,\n    max_retries: int,\n    retry_timeout: float,\n) -> Connection:\n    \"\"\"Connect to the database.\n\n    Args:\n        engine: The DB engine with which the check will be performed.\n        max_retries: The maximum number of retries that will be attempted.\n        retry_timeout: The number of seconds to wait between retries.\n\n    Raises:\n        OperationalError: Error during DB connection - max retries exceeded.\n\n    Returns:\n        A connection to the database.\n    \"\"\"\n    attempt = 0\n    while True:\n        try:\n            return engine.connect()\n        except OperationalError:\n            if attempt >= max_retries:\n                logging.error(\n                    f\"Could not connect to the database after {attempt} \"  # noqa: G004\n                    \"attempts. Max retries exceeded.\",\n                )\n                raise\n            attempt += 1\n            logging.info(\n                f\"DB connection failed. Will retry after {retry_timeout}s. \"  # noqa: G004\n                f\"Attempt {attempt}/{max_retries}\",\n            )\n            time.sleep(retry_timeout)\n\n\ninit_hooks: dict[str, t.Callable[[Connection], None]] = {\n    \"sqlite\": lambda x: x.execute(text(\"PRAGMA journal_mode=WAL\")),\n}\n\n\ndef init_hook(engine: Engine) -> None:\n    \"\"\"Run the initialization hook for the provided DB engine.\n\n    The initialization hooks are taken from the `meltano.core.db.init_hooks`\n    dictionary, which maps the dialect name of the engine to a unary function\n    which will be called with the provided DB engine.\n\n    Args:\n        engine: The engine for which the init hook will be run.\n\n    Raises:\n        Exception: The init hook raised an exception.\n    \"\"\"\n    if hook := init_hooks.get(engine.dialect.name):\n        with engine.connect() as conn:\n            try:\n                hook(conn)\n            except Exception as ex:\n                raise Exception(f\"Failed to initialize database: {ex!s}\") from ex  # noqa: EM102\n\n\ndef ensure_schema_exists(\n    engine: Engine,\n    schema_name: str,\n    grant_roles: tuple[str] = (),\n) -> None:\n    \"\"\"Ensure the specified `schema_name` exists in the database.\n\n    Args:\n        engine: The DB engine to be used.\n        schema_name: The name of the schema.\n        grant_roles: Roles to grant to the specified schema.\n    \"\"\"\n    group_identifiers = \",\".join(grant_roles)\n\n    create_schema = text(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n    grant_select_schema = text(\n        f\"ALTER DEFAULT PRIVILEGES IN SCHEMA {schema_name} GRANT SELECT ON \"\n        f\"TABLES TO {group_identifiers}\",\n    )\n    grant_usage_schema = text(\n        f\"GRANT USAGE ON SCHEMA {schema_name} TO {group_identifiers}\",\n    )\n\n    with engine.connect() as conn, conn.begin():\n        conn.execute(create_schema)\n        if grant_roles:\n            conn.execute(grant_select_schema)\n            conn.execute(grant_usage_schema)\n\n    logging.info(f\"Schema {schema_name} has been created successfully.\")  # noqa: G004\n    for role in grant_roles:\n        logging.info(f\"Usage has been granted for role: {role}.\")  # noqa: G004\n\n\ndef check_database_compatibility(engine: Engine) -> None:\n    \"\"\"Check that the database is compatible with Meltano.\n\n    Args:\n        engine: The DB engine to be used. This should already be connected to\n            the database.\n\n    Raises:\n        MeltanoDatabaseCompatibilityError: The database is not compatible with\n            Meltano.\n    \"\"\"\n    dialect = engine.dialect.name\n    version = engine.dialect.server_version_info\n\n    if dialect == \"sqlite\" and version < (3, 25, 1):\n        version_string = \".\".join(map(str, version))\n        reason = (\n            f\"Detected SQLite {version_string}, but Meltano requires at least 3.25.1\"\n        )\n        raise MeltanoDatabaseCompatibilityError(reason)\n", "import_text": ["logging", "time", "typing", "urllib.parse.urlparse", "sqlalchemy.create_engine", "sqlalchemy.exc.OperationalError", "sqlalchemy.orm.sessionmaker", "sqlalchemy.pool.NullPool", "sqlalchemy.sql.text", "meltano.core.error.MeltanoError"], "prompt": "\"\"\"\nDescription: This function creates an SQLAlchemy engine and sessionmaker for a given project.\n\nArgs:\n    project (Project): The project for which the engine and sessionmaker are to be created.\n    default (bool): A flag indicating whether the engine and sessionmaker should be registered as the default for the project. Defaults to False.\n\nReturns:\n    tuple[Engine, sessionmaker]: A tuple containing the SQLAlchemy engine and sessionmaker for the project.\n\nRaises:\n    NullConnectionStringError: If the database_uri for the project is None.\n\nNotes:\n    - The function first checks if an engine already exists for the project. If so, it returns the existing engine.\n    - The function then sanitizes the database_uri for logging purposes.\n    - If the database_uri is None, the function raises a NullConnectionStringError.\n    - The function creates an engine with the given database_uri, using a NullPool for connection pooling and enabling the use of future SQLAlchemy features.\n    - The function then connects to the database to ensure it is available, using the provided retry parameters.\n    - The function checks the compatibility of the database with the current version of SQLAlchemy.\n    - The function initializes any necessary hooks for the engine.\n    - The function creates a sessionmaker bound to the engine.\n    - If the default flag is set, the function registers the engine and sessionmaker as the default for the project.\n    - The function returns the engine and sessionmaker as a tuple.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Create and register a SQLAlchemy engine for a Meltano project instance.\n\n    Args:\n        project: The Meltano project that the engine will be connected to.\n        default: Whether the engine created should be stored as the default\n            engine for this project.\n\n    Returns:\n        The engine, and a session maker bound to the engine.\n\n    Raises:\n        NullConnectionStringError: The `database_uri` setting has a null value.\n    \"\"\"", "function_dependencies": ["urllib.parse.urlparse", "urllib.parse.urlparse._replace", "urllib.parse.urlparse._replace.geturl", "logging.debug", "sqlalchemy.create_engine", "sqlalchemy.orm.sessionmaker"], "project_create_time": "2021-06-21T16:35:39+00:00", "project_update_time": "2024-04-18T01:21:28+00:00", "file_create_time": "2018-10-25T17:17:36Z", "file_update_time": "2024-03-19T16:28:49Z", "function_update_time": "2022-08-12T18:46:27Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["sqlalchemy.create_engine", "sqlalchemy.orm.sessionmaker"], "test_function": [{"file_path": "/meltano-v3.4.0/meltano-3.4.0/tests/meltano/core/test_db_connection.py", "class_name": "TestProjectEngine", "function_name": "test_project_engine", "code": "\n    def test_project_engine(\n        self,\n        monkeypatch: pytest.MonkeyPatch,\n        tmp_path: Path,\n    ):\n        with tmp_path.joinpath(\"meltano.yml\").open(\"w\") as meltano_yml:\n            yaml.dump(\n                {\n                    \"project_id\": \"test\",\n                    \"send_anonymous_usage_stat\": False,\n                    \"database_uri\": \"$DATABASE\",\n                },\n                meltano_yml,\n            )\n        monkeypatch.delenv(\"MELTANO_DATABASE_URI\")\n\n        project = Project(tmp_path)\n        with pytest.raises(NullConnectionStringError):\n            project_engine(project)"}]}, {"git_group": "argoverse", "git_name": "argoverse-api", "version": "v1.1.0", "language": "Python", "project_name": "argoverse-api-v1.1.0.zip", "file_path": "/argoverse-api-v1.1.0/argoverse-api-1.1.0/argoverse/utils/cuboid_interior.py", "file_name": "cuboid_interior.py", "focal_class": null, "focal_name": "filter_point_cloud_to_bbox_3D_vectorized", "focal_parameter": [], "solution": "def filter_point_cloud_to_bbox_3D_vectorized(bbox: np.ndarray, pc_raw: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    # get 3 principal directions (edges) of the cuboid\n    u = bbox[2] - bbox[6]\n    v = bbox[2] - bbox[3]\n    w = bbox[2] - bbox[1]\n\n    # point x lies within the box when the following\n    # constraints are respected\n\n    # IN BETWEEN\n\n    # do i need to check the other direction as well?\n    valid_u1 = np.logical_and(u.dot(bbox[2]) <= pc_raw.dot(u), pc_raw.dot(u) <= u.dot(bbox[6]))\n    valid_v1 = np.logical_and(v.dot(bbox[2]) <= pc_raw.dot(v), pc_raw.dot(v) <= v.dot(bbox[3]))\n    valid_w1 = np.logical_and(w.dot(bbox[2]) <= pc_raw.dot(w), pc_raw.dot(w) <= w.dot(bbox[1]))\n\n    valid_u2 = np.logical_and(u.dot(bbox[2]) >= pc_raw.dot(u), pc_raw.dot(u) >= u.dot(bbox[6]))\n    valid_v2 = np.logical_and(v.dot(bbox[2]) >= pc_raw.dot(v), pc_raw.dot(v) >= v.dot(bbox[3]))\n    valid_w2 = np.logical_and(w.dot(bbox[2]) >= pc_raw.dot(w), pc_raw.dot(w) >= w.dot(bbox[1]))\n\n    valid_u = np.logical_or(valid_u1, valid_u2)\n    valid_v = np.logical_or(valid_v1, valid_v2)\n    valid_w = np.logical_or(valid_w1, valid_w2)\n\n    is_valid = np.logical_and(np.logical_and(valid_u, valid_v), valid_w)\n    segment_pc = pc_raw[is_valid]\n    return segment_pc, is_valid", "function_signature": "def filter_point_cloud_to_bbox_3D_vectorized(bbox: np.ndarray, pc_raw: np.ndarray) -> Tuple[np.ndarray, np.ndarray] :", "left_context": "# Copyright (c) 2018 Charles R. Qi from Stanford University and Wei Liu from Nuro Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n# <Modifications copyright (C) 2019, Argo AI, LLC>\n\nimport copy\nfrom typing import Optional, Tuple\n\nimport numpy as np\nfrom scipy.spatial import Delaunay\n\n\ndef filter_point_cloud_to_bbox(bbox: np.ndarray, velodyne_pts: np.ndarray) -> Optional[np.ndarray]:\n    \"\"\"\n    Given 2 orthogonal directions \"u\", \"v\" defined by 3 bbox vertices, s.t.::\n\n        u = P1 - P2\n        v = P1 - P4\n\n    a point \"x\" in R^3 lies within the bbox iff::\n\n        <u,P1> >= <u,x> >= <u,P2>\n        <v,P1> >= <v,x> >= <v,P4>\n\n    Args:\n       bbox: Numpy array of shape (4,3) representing 3D bbox\n       velodyne_pts: NumPy array of shape (N,3) representing Velodyne point cloud\n\n    Returns:\n       interior_pts: Numpy array of shape (N,3) representing velodyne points\n            that fall inside the cuboid\n    \"\"\"\n    P3 = bbox[0, :2]  # xmax, ymax\n    P4 = bbox[1, :2]  # xmax, ymin\n    P2 = bbox[2, :2]  # xmin, ymax\n    P1 = bbox[3, :2]  # xmin, ymin\n\n    u = P1 - P2\n    v = P1 - P4\n\n    pt_indices_to_plot = []\n\n    u_low_bnd = u.dot(P2)\n    u_upp_bnd = u.dot(P1)\n\n    v_low_bnd = v.dot(P4)\n    v_upp_bnd = v.dot(P1)\n\n    for pt_idx in range(velodyne_pts.shape[0]):\n        u_dot_x = u.dot(velodyne_pts[pt_idx, :2])\n        v_dot_x = v.dot(velodyne_pts[pt_idx, :2])\n\n        inside_u = u_low_bnd <= u_dot_x <= u_upp_bnd\n        inside_v = v_low_bnd <= v_dot_x <= v_upp_bnd\n        if inside_u and inside_v:\n            pt_indices_to_plot.append(pt_idx)\n\n    interior_pt_indices = np.array(pt_indices_to_plot)\n    if interior_pt_indices.shape[0] == 0:\n        return None\n    else:\n        interior_pts = velodyne_pts[interior_pt_indices]\n        return interior_pts\n\n\ndef filter_point_cloud_to_bbox_2D_vectorized(bbox: np.ndarray, pc_raw: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Args:\n       bbox: NumPy array of shape (4,2) representing 2D bbox\n       pc_raw: NumPy array of shape (N,3) representing Velodyne point cloud\n\n    Returns:\n       pc_seg: NumPy array of shape (N,3) representing velodyne points\n            that fall inside the cuboid\n    \"\"\"\n    pc_2d = copy.deepcopy(pc_raw[:, :2])\n    P3 = bbox[0]  # xmax, ymax\n    P4 = bbox[1]  # xmax, ymin\n    P2 = bbox[2]  # xmin, ymax\n    P1 = bbox[3]  # xmin, ymin\n\n    u = P1 - P2\n    v = P1 - P4\n\n    U = np.array([u[0:2]])\n    V = np.array([v[0:2]])\n    P1 = np.array([bbox[0][0:2]])\n    P2 = np.array([bbox[1][0:2]])\n    P4 = np.array([bbox[2][0:2]])\n\n    dot1 = np.matmul(U, pc_2d.transpose(1, 0))\n    dot2 = np.matmul(V, pc_2d.transpose(1, 0))\n    u_p1 = np.tile((U * P1).sum(axis=1), (len(pc_2d), 1)).transpose(1, 0)\n    v_p1 = np.tile((V * P1).sum(axis=1), (len(pc_2d), 1)).transpose(1, 0)\n    u_p2 = np.tile((U * P2).sum(axis=1), (len(pc_2d), 1)).transpose(1, 0)\n    v_p4 = np.tile((V * P4).sum(axis=1), (len(pc_2d), 1)).transpose(1, 0)\n\n    flag = np.logical_and(in_between_matrix(dot1, u_p1, u_p2), in_between_matrix(dot2, v_p1, v_p4))\n    flag = flag.squeeze()\n    pc_seg = pc_raw[flag]\n    return pc_seg, flag\n\n\ndef filter_point_cloud_to_bbox_3D(bbox: np.ndarray, pc_raw: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Args:\n       bbox has shape object array: [(3,), (3,), (3,), height]\n       pc_raw\n    \"\"\"\n    u = bbox[1] - bbox[0]\n    v = bbox[2] - bbox[0]\n    w = np.zeros((3, 1))\n    w[2, 0] += bbox[3]\n\n    p5 = w + bbox[0]\n\n    U = np.array([u[0:3, 0]])\n    V = np.array([v[0:3, 0]])\n    W = np.array([w[0:3, 0]])\n    P1 = np.array([bbox[0][0:3, 0]])\n    P2 = np.array([bbox[1][0:3, 0]])\n    P4 = np.array([bbox[2][0:3, 0]])\n    P5 = np.array([p5[0:3, 0]])\n\n    dot1 = np.matmul(U, pc_raw.transpose(1, 0))\n    dot2 = np.matmul(V, pc_raw.transpose(1, 0))\n    dot3 = np.matmul(W, pc_raw.transpose(1, 0))\n    u_p1 = np.tile((U * P1).sum(axis=1), (len(pc_raw), 1)).transpose(1, 0)\n    v_p1 = np.tile((V * P1).sum(axis=1), (len(pc_raw), 1)).transpose(1, 0)\n    w_p1 = np.tile((W * P1).sum(axis=1), (len(pc_raw), 1)).transpose(1, 0)\n    u_p2 = np.tile((U * P2).sum(axis=1), (len(pc_raw), 1)).transpose(1, 0)\n    v_p4 = np.tile((V * P4).sum(axis=1), (len(pc_raw), 1)).transpose(1, 0)\n    w_p5 = np.tile((W * P5).sum(axis=1), (len(pc_raw), 1)).transpose(1, 0)\n\n    flag = np.logical_and(\n        np.logical_and(in_between_matrix(dot1, u_p1, u_p2), in_between_matrix(dot2, v_p1, v_p4)),\n        in_between_matrix(dot3, w_p1, w_p5),\n    )\n\n    pc_seg = pc_raw[flag[0, :]]\n    return pc_seg\n\n\ndef in_between_matrix(x: np.ndarray, v1: np.ndarray, v2: np.ndarray) -> np.ndarray:\n    return np.logical_or(np.logical_and(x <= v1, x >= v2), np.logical_and(x <= v2, x >= v1))\n\n\ndef filter_point_cloud_to_bbox_3D_single_pt(bbox: np.ndarray, x: np.ndarray) -> np.ndarray:  # pc_raw):\n    r\"\"\"\n\n    Args:\n       bbox: Numpy array of shape (8,1)\n       x: Numpy array of shape (3,1)\n\n    https://math.stackexchange.com/questions/1472049/check-if-a-point-is-inside-a-rectangular-shaped-area-3d\n\n    ::\n\n            5------4\n            |\\\\    |\\\\\n            | \\\\   | \\\\\n            6--\\\\--7  \\\\\n            \\\\  \\\\  \\\\ \\\\\n        l    \\\\  1-------0    h\n         e    \\\\ ||   \\\\ ||   e\n          n    \\\\||    \\\\||   i\n           g    \\\\2------3    g\n            t      width.     h\n             h.               t.\n\n    \"\"\"\n    # get 3 principal directions (edges) of the cuboid\n    u = bbox[2] - bbox[6]\n    v = bbox[2] - bbox[3]\n    w = bbox[2] - bbox[1]\n\n    # point x lies within the box when the following\n    # constraints are respected\n\n    # IN BETWEEN\n\n    # do i need to check the other direction as well?\n    valid_u1 = np.logical_and(u.dot(bbox[2]) <= u.dot(x), u.dot(x) <= u.dot(bbox[6]))\n    valid_v1 = np.logical_and(v.dot(bbox[2]) <= v.dot(x), v.dot(x) <= v.dot(bbox[3]))\n    valid_w1 = np.logical_and(w.dot(bbox[2]) <= w.dot(x), w.dot(x) <= w.dot(bbox[1]))\n\n    valid_u2 = np.logical_and(u.dot(bbox[2]) >= u.dot(x), u.dot(x) >= u.dot(bbox[6]))\n    valid_v2 = np.logical_and(v.dot(bbox[2]) >= v.dot(x), v.dot(x) >= v.dot(bbox[3]))\n    valid_w2 = np.logical_and(w.dot(bbox[2]) >= w.dot(x), w.dot(x) >= w.dot(bbox[1]))\n\n    valid_u = np.logical_or(valid_u1, valid_u2)\n    valid_v = np.logical_or(valid_v1, valid_v2)\n    valid_w = np.logical_or(valid_w1, valid_w2)\n\n    valid = np.logical_and(np.logical_and(valid_u, valid_v), valid_w)\n\n    return valid\n\n", "right_context": "\n\ndef extract_pc_in_box3d_hull(pc: np.ndarray, bbox_3d: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Find points that fall within a 3d cuboid, by treating the 3d cuboid as a hull.\n    Scipy.spatial's Delaunay class performs tesselation in N dimensions. By finding\n    the simplices containing the given points, we also can determine which points\n    lie outside the triangulation. Such invalid points obtain the value \"-1\". We\n    threshold these to find the points that fall within the cuboid/hull.\n\n    Please see Apache 2.0 license below, which governs this specific function.\n\n    Args:\n       pc: Numpy array of shape (N,3) representing point cloud\n       bbox_3d: Numpy array of shape (8,3) representing 3D cuboid vertices\n\n    Returns:\n       segment: Numpy array of shape (K,3) representing 3d points that fell\n                within 3d cuboid volume.\n       box3d_roi_inds: Numpy array of shape (N,) of type bool, representing\n            point cloud indices corresponding to points that fall within the\n            3D cuboid.\n    \"\"\"\n    if not isinstance(bbox_3d, Delaunay):\n        hull = Delaunay(bbox_3d)\n\n    box3d_roi_inds = hull.find_simplex(pc) >= 0\n    return pc[box3d_roi_inds, :], box3d_roi_inds\n\n\n\"\"\"\nhttps://github.com/charlesq34/frustum-pointnets/blob/master/LICENSE\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2018 Charles R. Qi from Stanford University and\n    Wei Liu from Nuro Inc.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\"\"\"\n", "import_text": ["copy", "typing.Optional", "typing.Tuple", "numpy", "scipy.spatial.Delaunay"], "prompt": "\"\"\"\nDescription: This function filters a point cloud to a 3D bounding box.\n\nArgs:\n    bbox (np.ndarray): A numpy array representing the bounding box.\n    pc_raw (np.ndarray): A numpy array representing the raw point cloud.\n\nReturns:\n    Tuple[np.ndarray, np.ndarray]: A tuple containing the segmented point cloud and a boolean array indicating which points are valid.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    r\"\"\"\n\n    Args:\n       bbox: Numpy array pf shape (8,3) representing 3d cuboid vertices, ordered\n                as shown below.\n       pc_raw: Numpy array of shape (N,3), representing a point cloud\n\n    Returns:\n       segment: Numpy array of shape (K,3) representing 3d points that fell\n                within 3d cuboid volume.\n       is_valid: Numpy array of shape (N,) of type bool\n\n    https://math.stackexchange.com/questions/1472049/check-if-a-point-is-inside-a-rectangular-shaped-area-3d\n\n    ::\n\n            5------4\n            |\\\\    |\\\\\n            | \\\\   | \\\\\n            6--\\\\--7  \\\\\n            \\\\  \\\\  \\\\ \\\\\n        l    \\\\  1-------0    h\n         e    \\\\ ||   \\\\ ||   e\n          n    \\\\||    \\\\||   i\n           g    \\\\2------3    g\n            t      width.     h\n             h.               t.\n\n    \"\"\"", "function_dependencies": ["numpy.logical_and", "numpy.logical_or"], "project_create_time": "2019-05-17T20:21:18+00:00", "project_update_time": "2024-04-17T06:41:09+00:00", "file_create_time": "2019-06-19T18:48:29Z", "file_update_time": "2021-06-04T00:47:51Z", "function_update_time": "2021-02-11T04:23:42Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["numpy.logical_and"], "test_function": [{"file_path": "/argoverse-api-v1.1.0/argoverse-api-1.1.0/tests/test_cuboid_interior.py", "class_name": null, "function_name": "test_3d_cuboid_interior_test1", "code": "def test_3d_cuboid_interior_test1() -> None:\n    pc_raw, bbox_3d, gt_segment, gt_is_valid = get_scenario_1()\n    segment, is_valid = filter_point_cloud_to_bbox_3D_vectorized(bbox_3d, pc_raw)\n\n    assert np.array_equal(segment, gt_segment)\n    assert np.array_equal(gt_is_valid, is_valid)"}]}, {"git_group": "omaha-consulting", "git_name": "omaha-server", "version": "v0.6.4", "language": "Python", "project_name": "omaha-server-v0.6.4.zip", "file_path": "/omaha-server-v0.6.4/omaha-server-0.6.4/omaha_server/omaha/limitation.py", "file_name": "limitation.py", "focal_class": null, "focal_name": "monitoring_size", "focal_parameter": [], "solution": "\ndef monitoring_size():\n    size = OmahaVersion.objects.get_size()\n    if size > gpm['Version__limit_size'] * 1024 * 1024 * 1024:\n        raven.captureMessage(\"[Limitation]Size limit of omaha versions is exceeded. Current size is %s [%d]\" %\n                             (filters.filesizeformat(size).replace(u'\\xa0', u' '), time.time()),\n                             data={'level': 30, 'logger': 'limitation'})\n    cache.set('omaha_version_size', size)\n\n    size = SparkleVersion.objects.get_size()\n    if size > gpm['SparkleVersion__limit_size'] * 1024 * 1024 * 1024:\n        raven.captureMessage(\"[Limitation]Size limit of sparkle versions is exceeded. Current size is %s [%d]\" %\n                             (filters.filesizeformat(size).replace(u'\\xa0', u' '), time.time()),\n                             data={'level': 30, 'logger': 'limitation'})\n    cache.set('sparkle_version_size', size)\n\n    size = Feedback.objects.get_size()\n    if size > gpm['Feedback__limit_size'] * 1024 * 1024 * 1024:\n        raven.captureMessage(\"[Limitation]Size limit of feedbacks is exceeded. Current size is %s [%d]\" %\n                             (filters.filesizeformat(size).replace(u'\\xa0', u' '), time.time()),\n                             data={'level': 30, 'logger': 'limitation'})\n    cache.set('feedbacks_size', size)\n\n    size = Crash.objects.get_size()\n    if size > gpm['Crash__limit_size'] * 1024 * 1024 * 1024:\n        raven.captureMessage(\"[Limitation]Size limit of crashes is exceeded. Current size is %s [%d]\" %\n                             (filters.filesizeformat(size).replace(u'\\xa0', u' '), time.time()),\n                             data={'level': 30, 'logger': 'limitation'})\n    cache.set('crashes_size', size)\n\n    size = Symbols.objects.get_size()\n    if size > gpm['Symbols__limit_size'] * 1024 * 1024 * 1024:\n        raven.captureMessage(\"[Limitation]Size limit of symbols is exceeded. Current size is %s [%d]\" %\n                             (filters.filesizeformat(size).replace(u'\\xa0', u' '), time.time()),\n                             data={'level': 30, 'logger': 'limitation'})\n    cache.set('symbols_size', size)", "function_signature": "def monitoring_size() :", "left_context": "from itertools import chain\nimport operator\nimport time\nimport logging\n\nfrom django.apps import apps\nfrom django.utils import timezone\nfrom django.conf import settings\nfrom django.db.models import Count\nfrom django.core.cache import cache\nfrom django.template import defaultfilters as filters\n\nimport boto\nfrom boto.s3.key import Key\nfrom raven import Client\n\n\nfrom omaha.models import Version as OmahaVersion\nfrom omaha.utils import valuedispatch\nfrom sparkle.models import SparkleVersion\nfrom crash.models import Crash, Symbols\nfrom feedback.models import Feedback\n\nfrom dynamic_preferences_registry import global_preferences_manager as gpm\n\ndsn = getattr(settings, 'RAVEN_CONFIG', None)\nif dsn:\n    dsn = dsn['dsn']\nraven = Client(dsn, name=getattr(settings, 'HOST_NAME', None), release=getattr(settings, 'APP_VERSION', None))\n\n@valuedispatch\ndef bulk_delete(cls, qs):\n    raise NotImplementedError\n\n\n@bulk_delete.register(Crash)\ndef _(cls, qs):\n    if settings.DEFAULT_FILE_STORAGE == 'omaha_server.s3utils.S3Storage':\n        qs = s3_bulk_delete(qs, file_fields=['archive', 'upload_file_minidump'],\n                            s3_fields=['minidump_archive', 'minidump'])\n\n    result = dict()\n    result['count'] = qs.count()\n    result['size'] = qs.get_size()\n    elements = list(qs.values_list('id', 'created', 'signature', 'userid', 'appid'))\n    result['elements'] = map(lambda x: dict(id=x[0], element_created=x[1].strftime(\"%d. %B %Y %I:%M%p\"), signature=x[2],\n                                            userid=x[3], appid=x[4]), elements)\n    qs.delete()\n    return result\n\n\n@bulk_delete.register(Feedback)\ndef _(cls, qs):\n    if settings.DEFAULT_FILE_STORAGE == 'storages.backends.s3boto.S3BotoStorage':\n        qs = s3_bulk_delete(qs, file_fields=['attached_file', 'blackbox', 'screenshot', 'system_logs'],\n                            s3_fields=['feedback_attach', 'blackbox', 'screenshot', 'system_logs'])\n\n    result = dict()\n    result['count'] = qs.count()\n    result['size'] = qs.get_size()\n    elements = list(qs.values_list('id', 'created'))\n    result['elements'] = map(lambda x: dict(id=x[0], element_created=x[1].strftime(\"%d. %B %Y %I:%M%p\")), elements)\n    qs.delete()\n    return result\n\n\n@bulk_delete.register(Symbols)\ndef _(cls, qs):\n    if settings.DEFAULT_FILE_STORAGE == 'storages.backends.s3boto.S3BotoStorage':\n        qs = s3_bulk_delete(qs, file_fields=['file'], s3_fields=['symbols'])\n\n    result = dict()\n    result['count'] = qs.count()\n    result['size'] = qs.get_size()\n    elements = list(qs.values_list('id', 'created'))\n    result['elements'] = map(lambda x: dict(id=x[0], element_created=x[1].strftime(\"%d. %B %Y %I:%M%p\")), elements)\n    qs.delete()\n    return result\n\n\n@bulk_delete.register(OmahaVersion)\ndef _(cls, qs):\n    if settings.DEFAULT_FILE_STORAGE == 'storages.backends.s3boto.S3BotoStorage':\n        qs = s3_bulk_delete(qs, file_fields=['file'], s3_fields=['build'])\n\n    result = dict()\n    result['count'] = qs.count()\n    result['size'] = qs.get_size()\n    elements = list(qs.values_list('id', 'created'))\n    result['elements'] = map(lambda x: dict(id=x[0], element_created=x[1].strftime(\"%d. %B %Y %I:%M%p\")), elements)\n    qs.delete()\n    return result\n\n\n@bulk_delete.register(SparkleVersion)\ndef _(cls, qs):\n    if settings.DEFAULT_FILE_STORAGE == 'storages.backends.s3boto.S3BotoStorage':\n        qs = s3_bulk_delete(qs, file_fields=['file'], s3_fields=['sparkle'])\n\n    result = dict()\n    result['count'] = qs.count()\n    result['size'] = qs.get_size()\n    result['elements'] = list(qs.values_list('id', 'created'))\n    elements = list(qs.values_list('id', 'created'))\n    result['elements'] = map(lambda x: dict(id=x[0], element_created=x[1].strftime(\"%d. %B %Y %I:%M%p\")), elements)\n    qs.delete()\n    return result\n\n\ndef s3_bulk_delete(qs, file_fields, s3_fields):\n    conn = boto.connect_s3(settings.AWS_ACCESS_KEY_ID, settings.AWS_SECRET_ACCESS_KEY)\n    bucket = conn.get_bucket(settings.AWS_STORAGE_BUCKET_NAME)\n\n    file_keys = qs.values_list(*file_fields)\n    file_keys = [key for key in chain(*file_keys) if key]\n    bucket.delete_keys(file_keys)\n    s3_keys = [x for x in chain(*[bucket.list(prefix=\"%s/\" % field) for field in s3_fields])]\n    error_keys = filter(lambda key: key in s3_keys, file_keys)\n    if error_keys:\n        logging.error(\"Files were not deleted from s3: %r\" % error_keys)\n        exclude_fields = [qs.exclude(**{\"%s__in\" % key: error_keys}) for key in file_fields]\n        qs = reduce(operator.and_, exclude_fields)\n\n    update_kwargs = dict(zip(file_fields, [None for x in file_fields]))\n    qs.update(**update_kwargs)\n    return qs\n\n\ndef delete_older_than(app, model_name, limit=None):\n    if not limit:\n        preference_key = '__'.join([model_name, 'limit_storage_days'])\n        limit = gpm[preference_key]\n    model = apps.get_model(app, model_name)\n    offset = timezone.timedelta(days=limit)\n    limit = timezone.now() - offset\n    old_objects = model.objects.filter(created__lte=limit)\n    result = dict()\n    if old_objects:\n        result = bulk_delete(model, old_objects)\n    return result\n\n\ndef delete_duplicate_crashes(limit=None):\n    logger = logging.getLogger('limitation')\n    full_result = dict(count=0, size=0, elements=[])\n    if not limit:\n        preference_key = '__'.join(['Crash', 'duplicate_number'])\n        limit = gpm[preference_key]\n    duplicated = Crash.objects.values('signature').annotate(count=Count('signature'))\n    duplicated = filter(lambda x: x['count'] > limit, duplicated)\n    logger.info('Duplicated signatures: %r' % duplicated)\n    for group in duplicated:\n        qs = Crash.objects.filter(signature=group['signature']).order_by('created')\n        dup_elements = []\n        dup_count = qs.count()\n        while dup_count > limit:\n            bulk_size = dup_count - limit if dup_count - limit < 1000 else 1000\n            bulk_ids = qs[:bulk_size].values_list('id', flat=True)\n            bulk = qs.filter(id__in=bulk_ids)\n            result = bulk_delete(Crash, bulk)\n            full_result['count'] += result['count']\n            full_result['size'] += result['size']\n            full_result['elements'] += result['elements']\n            dup_elements += result['elements']\n            dup_count -= bulk_size\n    return full_result\n\n\ndef delete_size_is_exceeded(app, model_name, limit=None):\n    if not limit:\n        preference_key = '__'.join([model_name, 'limit_size'])\n        limit = gpm[preference_key] * 1024 * 1024 * 1024\n    else:\n        limit *= 1024*1024*1024\n    model = apps.get_model(app, model_name)\n    group_count = 1000\n    full_result = dict(count=0, size=0, elements=[])\n    objects_size = model.objects.get_size()\n\n    while objects_size > limit:\n        group_objects_ids = list(model.objects.order_by('created').values_list(\"id\", flat=True)[:group_count])\n        group_objects = model.objects.order_by('created').filter(pk__in=group_objects_ids)\n        group_size = group_objects.get_size()\n        diff_size = objects_size - limit\n\n        if group_size > diff_size:\n            group_size = 0\n            low_border = 0\n            for instance in group_objects:\n                group_size += instance.size\n                low_border += 1\n                if group_size >= diff_size:\n                    group_objects = model.objects.order_by('created').filter(pk__in=group_objects_ids[:low_border])\n                    break\n\n        result = bulk_delete(model, group_objects)\n        objects_size -= result['size']\n        full_result['count'] += result['count']\n        full_result['size'] += result['size']\n        full_result['elements'] += result['elements']\n    return full_result\n\n", "right_context": "\n\ndef handle_dangling_files(model, prefix, file_fields):\n    conn = boto.connect_s3(settings.AWS_ACCESS_KEY_ID, settings.AWS_SECRET_ACCESS_KEY)\n    bucket = conn.get_bucket(settings.AWS_STORAGE_BUCKET_NAME)\n    result = dict()\n    dangling_files_in_db, dangling_files_in_s3 = check_dangling_files(model, prefix, file_fields, bucket)\n    if dangling_files_in_db:\n        # send notifications\n        result['mark'] = 'db'\n        result['data'] = dangling_files_in_db\n        result['status'] = 'Send notifications'\n        result['count'] = len(dangling_files_in_db)\n        result['cleaned_space'] = 0\n    elif dangling_files_in_s3:\n        # delete files from s3\n        result['mark'] = 's3'\n        result['data'] = dangling_files_in_s3\n        result['status'] = 'Delete files'\n        result.update(delete_dangling_files_s3(bucket, result['data']))\n    else:\n        # not detected dangling files\n        result['mark'] = 'Nothing'\n        result['data'] = []\n        result['status'] = 'Nothing'\n        result['count'] = 0\n        result['cleaned_space'] = 0\n    return result\n\n\ndef check_dangling_files(model, prefix, file_fields, bucket):\n    keys_from_s3 = list()\n    for pref in prefix:\n        key_from_s3 = bucket.list(prefix=\"%s/\" % pref)\n        keys_from_s3.append(key_from_s3)\n    obj_from_s3 = [x for x in chain(*keys_from_s3)]\n    urls_from_s3 = list()\n    for obj in obj_from_s3:\n        urls_from_s3.append(obj.key)\n    all_objects = model.objects.all()\n    urls = list(filter(None, [filed for filed in chain(*all_objects.values_list(*file_fields))]))\n    dangling_files_in_db = list(set(urls) - set(urls_from_s3))\n    dangling_files_in_s3 = list(set(urls_from_s3) - set(urls))\n    return dangling_files_in_db, dangling_files_in_s3\n\n\ndef delete_dangling_files_s3(bucket, file_paths):\n    result = dict()\n    result['cleaned_space'] = 0\n    result['count'] = 0\n    for path in file_paths:\n        _file = bucket.lookup(path)\n        result['cleaned_space'] += _file.size\n        result['count'] += 1\n        bucket.delete_key(_file.key)\n    return result\n", "import_text": ["itertools.chain", "operator", "time", "logging", "django.apps.apps", "django.utils.timezone", "django.conf.settings", "django.db.models.Count", "django.core.cache.cache", "django.template.defaultfilters", "boto", "boto.s3.key.Key", "raven.Client", "omaha.models.Version", "omaha.utils.valuedispatch", "sparkle.models.SparkleVersion", "crash.models.Crash", "crash.models.Symbols", "feedback.models.Feedback", "dynamic_preferences_registry.global_preferences_manager"], "prompt": "\"\"\"\nDescription: This function is used to monitor the size of different Django models and store them in the cache.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["omaha.models.Version.objects.get_size", "django.template.defaultfilters.filesizeformat", "django.template.defaultfilters.filesizeformat.replace", "time.time", "django.core.cache.cache.set", "sparkle.models.SparkleVersion.objects.get_size", "feedback.models.Feedback.objects.get_size", "crash.models.Crash.objects.get_size", "crash.models.Symbols.objects.get_size"], "project_create_time": "2011-06-12T11:01:47+00:00", "project_update_time": "2024-04-01T07:07:26+00:00", "file_create_time": "2015-07-22T08:18:32Z", "file_update_time": "2017-11-16T05:58:20Z", "function_update_time": "2015-07-22T08:18:32Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["django.core.cache.cache.set"], "test_function": [{"file_path": "/omaha-server-v0.6.4/omaha-server-0.6.4/omaha_server/omaha/tests/test_limitation.py", "class_name": "MonitoringTest", "function_name": "test_monitoring", "code": "\n    def test_monitoring(self):\n        for key in self.cache_keys:\n            self.assertEqual(cache.get(key, 0), 0)\n\n        VersionFactory.create(file_size=100)\n        SparkleVersionFactory.create(file_size=100)\n        Crash.objects.create(archive_size=80, minidump_size=20)\n        Symbols.objects.create(file_size=100)\n        Feedback.objects.create(screenshot_size=25, blackbox_size=25, attached_file_size=25, system_logs_size=25)\n\n        monitoring_size()\n\n        for key in self.cache_keys:\n            self.assertEqual(cache.get(key), 100)"}]}, {"git_group": "betodealmeida", "git_name": "shillelagh", "version": "1.2.19", "language": "Python", "project_name": "shillelagh-1.2.19.zip", "file_path": "/shillelagh-1.2.19/shillelagh-1.2.19/src/shillelagh/console.py", "file_name": "console.py", "focal_class": null, "focal_name": "main", "focal_parameter": [], "solution": "def main():  # pylint: disable=too-many-locals\n    # read args from config file\n    config_dir = Path(user_config_dir(\"shillelagh\"))\n    if not config_dir.exists():\n        config_dir.mkdir(parents=True)\n\n    config_path = config_dir / \"shillelagh.yaml\"\n    history_path = config_dir / \"shillelagh.history\"\n\n    adapter_kwargs = {}\n    if os.path.exists(config_path):\n        try:\n            with open(config_path, encoding=\"utf-8\") as stream:\n                adapter_kwargs = yaml.load(stream, Loader=yaml.SafeLoader)\n        except (PermissionError, yaml.parser.ParserError, yaml.scanner.ScannerError):\n            _logger.exception(\"Unable to load configuration file\")\n\n    connection = connect(\":memory:\", adapter_kwargs=adapter_kwargs)\n    cursor = connection.cursor()\n\n    # non-interactive\n    if not sys.stdin.isatty():\n        for query in emit_statements(sys.stdin.readlines()):\n            cursor.execute(query)\n            results = cursor.fetchall()\n            headers = [t[0] for t in cursor.description or []]\n            sys.stdout.write(tabulate(results, headers=headers))\n            sys.stdout.write(\"\\n\")\n        return\n\n    session = PromptSession(\n        lexer=PygmentsLexer(SqlLexer),\n        completer=sql_completer,\n        style=style,\n        history=FileHistory(history_path),\n    )\n\n    for query in emit_statements(repl(session)):\n        start = time.time()\n        results = None\n        try:\n            cursor.execute(query)\n            results = cursor.fetchall()\n        except Error as ex:\n            print(ex)\n            continue\n\n        headers = [t[0] for t in cursor.description or []]\n        print(tabulate(results, headers=headers))\n        duration = time.time() - start\n        print(\n            f\"({len(results)} row{'s' if len(results) != 1 else ''} \"\n            f\"in {duration:.2f}s)\\n\",\n        )\n\n    connection.close()\n    print(\"GoodBye!\")", "function_signature": "def main() :", "left_context": "#!/usr/bin/env python\n\"\"\"\nA simple REPL for Shillelagh.\n\nTo run the REPL, since run ``shillelagh``. Pressing return will execute the\nquery immediately, and multi-line queries are currently not supported.\n\nConnection arguments can be passed via a ``shillelagh.yaml`` file located in the users\napplication directory (see https://pypi.org/project/appdirs/), eg::\n\n    gsheestapi:\n      service_account_file: /path/to/credentials.json\n      subject: user@example.com\n      catalog:\n        # allows writing ``SELECT * FROM my_sheet``\n        my_sheet:  https://docs.google.com/spreadsheets/d/1/edit#gid=0\n    weatherapi:\n      api_key: XXX\n\n\"\"\"\nimport logging\nimport os.path\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Iterable, Iterator, Optional\n\nimport yaml\nfrom appdirs import user_config_dir\nfrom prompt_toolkit import PromptSession\nfrom prompt_toolkit.completion import WordCompleter\nfrom prompt_toolkit.history import FileHistory\nfrom prompt_toolkit.lexers import PygmentsLexer\nfrom prompt_toolkit.styles.pygments import style_from_pygments_cls\nfrom pygments.lexers.sql import SqlLexer\nfrom pygments.styles import get_style_by_name\nfrom tabulate import tabulate\n\nfrom shillelagh.backends.apsw.db import connect\nfrom shillelagh.exceptions import Error\n\n_logger = logging.getLogger(__name__)\n\nsql_completer = WordCompleter(\n    [\n        \"ABORT\",\n        \"ACTION\",\n        \"ADD\",\n        \"AFTER\",\n        \"ALL\",\n        \"ALTER\",\n        \"ANALYZE\",\n        \"AND\",\n        \"AS\",\n        \"ASC\",\n        \"ATTACH\",\n        \"AUTOINCREMENT\",\n        \"BEFORE\",\n        \"BEGIN\",\n        \"BETWEEN\",\n        \"BY\",\n        \"CASCADE\",\n        \"CASE\",\n        \"CAST\",\n        \"CHECK\",\n        \"COLLATE\",\n        \"COLUMN\",\n        \"COMMIT\",\n        \"CONFLICT\",\n        \"CONSTRAINT\",\n        \"CREATE\",\n        \"CROSS\",\n        \"CURRENT_DATE\",\n        \"CURRENT_TIME\",\n        \"CURRENT_TIMESTAMP\",\n        \"DATABASE\",\n        \"DEFAULT\",\n        \"DEFERRABLE\",\n        \"DEFERRED\",\n        \"DELETE\",\n        \"DESC\",\n        \"DETACH\",\n        \"DISTINCT\",\n        \"DROP\",\n        \"EACH\",\n        \"ELSE\",\n        \"END\",\n        \"ESCAPE\",\n        \"EXCEPT\",\n        \"EXCLUSIVE\",\n        \"EXISTS\",\n        \"EXPLAIN\",\n        \"FAIL\",\n        \"FOR\",\n        \"FOREIGN\",\n        \"FROM\",\n        \"FULL\",\n        \"GET_METADATA\",\n        \"GLOB\",\n        \"GROUP\",\n        \"HAVING\",\n        \"IF\",\n        \"IGNORE\",\n        \"IMMEDIATE\",\n        \"IN\",\n        \"INDEX\",\n        \"INDEXED\",\n        \"INITIALLY\",\n        \"INNER\",\n        \"INSERT\",\n        \"INSTEAD\",\n        \"INTERSECT\",\n        \"INTO\",\n        \"IS\",\n        \"ISNULL\",\n        \"JOIN\",\n        \"KEY\",\n        \"LEFT\",\n        \"LIKE\",\n        \"LIMIT\",\n        \"MATCH\",\n        \"NATURAL\",\n        \"NO\",\n        \"NOT\",\n        \"NOTNULL\",\n        \"NULL\",\n        \"OF\",\n        \"OFFSET\",\n        \"ON\",\n        \"OR\",\n        \"ORDER\",\n        \"OUTER\",\n        \"PLAN\",\n        \"PRAGMA\",\n        \"PRIMARY\",\n        \"QUERY\",\n        \"RAISE\",\n        \"RECURSIVE\",\n        \"REFERENCES\",\n        \"REGEXP\",\n        \"REINDEX\",\n        \"RELEASE\",\n        \"RENAME\",\n        \"REPLACE\",\n        \"RESTRICT\",\n        \"RIGHT\",\n        \"ROLLBACK\",\n        \"ROW\",\n        \"SLEEP\",\n        \"SAVEPOINT\",\n        \"SELECT\",\n        \"SET\",\n        \"TABLE\",\n        \"TEMP\",\n        \"TEMPORARY\",\n        \"THEN\",\n        \"TO\",\n        \"TRANSACTION\",\n        \"TRIGGER\",\n        \"UNION\",\n        \"UNIQUE\",\n        \"UPDATE\",\n        \"USING\",\n        \"VACUUM\",\n        \"VALUES\",\n        \"VERSION\",\n        \"VIEW\",\n        \"VIRTUAL\",\n        \"WHEN\",\n        \"WHERE\",\n        \"WITH\",\n        \"WITHOUT\",\n    ],\n    ignore_case=True,\n)\n\nstyle = style_from_pygments_cls(get_style_by_name(\"friendly\"))\nquote_chars = ('\"', \"'\", \"`\")\n\n\ndef emit_statements(lines: Iterable[str]) -> Iterator[str]:\n    \"\"\"\n    Consume lines and emit complete statements.\n    \"\"\"\n    quote_context: Optional[str] = None\n\n    rest = \"\"\n    for line in lines:\n        start = 0\n        for pos, char in enumerate(line):\n            if quote_context is not None and char == quote_context:\n                # leave context\n                quote_context = None\n            elif quote_context is None and char == \";\":\n                yield (rest + line[start:pos]).strip()\n                rest = \"\"\n                start = pos + 1\n            else:\n                for quote in quote_chars:\n                    if quote_context is None and char == quote:\n                        # enter context\n                        quote_context = quote\n\n        rest += line[start:] + \"\\n\"\n\n\ndef repl(session: PromptSession) -> Iterator[str]:\n    \"\"\"\n    Yield lines.\n    \"\"\"\n    quote_context: Optional[str] = None\n\n    start = True\n    while True:\n        if start:\n            prompt = \"\ud83c\udf40> \"\n        elif quote_context is None:\n            prompt = \"  . \"\n        else:\n            prompt = f\" {quote_context}. \"\n\n        try:\n            line = session.prompt(prompt)\n            yield line\n        except KeyboardInterrupt:\n            continue  # Control-C pressed. Clear and try again.\n        except EOFError:\n            break  # Control-D pressed.\n\n        quote_context = update_quote_context(line, quote_context)\n        start = quote_context is None and line.strip().endswith(\";\")\n\n\ndef update_quote_context(line: str, quote_context: Optional[str]) -> Optional[str]:\n    \"\"\"\n    Update the quote context.\n\n    Inside single quotes, inside double quotes, neither.\n    \"\"\"\n    for char in line:\n        if quote_context is not None and char == quote_context:\n            # leave context\n            quote_context = None\n        else:\n            for quote in quote_chars:\n                if quote_context is None and char == quote:\n                    # enter context\n                    quote_context = quote\n\n    return quote_context\n\n", "right_context": "\n\nif __name__ == \"__main__\":\n    main()\n", "import_text": ["logging", "os.path", "sys", "time", "pathlib.Path", "typing.Iterable", "typing.Iterator", "typing.Optional", "yaml", "appdirs.user_config_dir", "prompt_toolkit.PromptSession", "prompt_toolkit.completion.WordCompleter", "prompt_toolkit.history.FileHistory", "prompt_toolkit.lexers.PygmentsLexer", "prompt_toolkit.styles.pygments.style_from_pygments_cls", "pygments.lexers.sql.SqlLexer", "pygments.styles.get_style_by_name", "tabulate.tabulate", "shillelagh.backends.apsw.db.connect", "shillelagh.exceptions.Error"], "prompt": "\"\"\"\nDescription: This function is the main entry point of the program. It reads configuration from a YAML file, \n             sets up a SQLite connection with a specified adapter, and provides a command-line interface for \n             executing SQL queries.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"", "comment": "    \"\"\"\n    Run a REPL until the user presses Control-D.\n    \"\"\"", "prompt_is_gen_from_api": true, "function_dependencies": ["pathlib.Path", "appdirs.user_config_dir", "pathlib.Path.exists", "pathlib.Path.mkdir", "yaml.load", "shillelagh.backends.apsw.db.connect", "shillelagh.backends.apsw.db.connect.cursor", "sys.stdin.isatty", "sys.stdin.readlines", "shillelagh.backends.apsw.db.connect.cursor.execute", "shillelagh.backends.apsw.db.connect.cursor.fetchall", "sys.stdout.write", "tabulate.tabulate", "prompt_toolkit.PromptSession", "prompt_toolkit.lexers.PygmentsLexer", "prompt_toolkit.history.FileHistory", "time.time", "shillelagh.backends.apsw.db.connect.close"], "project_create_time": "2017-12-08T23:46:37+00:00", "project_update_time": "2024-04-16T13:08:48+00:00", "file_create_time": "2021-06-30T05:34:06Z", "file_update_time": "2024-02-29T01:34:08Z", "function_update_time": "2022-07-25T17:31:20Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["yaml.load"], "test_function": [{"file_path": "/shillelagh-1.2.19/shillelagh-1.2.19/tests/console_test.py", "class_name": null, "function_name": "test_main", "code": "def test_main(mocker: MockerFixture) -> None:\n    mocker.patch(\"sys.stdin.isatty\", return_value=True)\n    stdout = mocker.patch(\"sys.stdout\", new_callable=StringIO)\n    PromptSession = mocker.patch(\"shillelagh.console.PromptSession\")\n\n    PromptSession.return_value.prompt.side_effect = [\"SELECT 1;\", \"\", EOFError()]\n    console.main()\n    result = stdout.getvalue()\n    assert (\n        result\n        == \"\"\"  1\n---\n  1\n(1 row in 0.00s)\n\nGoodBye!\n\"\"\"\n    )"}, {"file_path": "/shillelagh-1.2.19/shillelagh-1.2.19/tests/console_test.py", "class_name": null, "function_name": "test_exception", "code": "def test_exception(mocker: MockerFixture) -> None:\n    mocker.patch(\"sys.stdin.isatty\", return_value=True)\n    stdout = mocker.patch(\"sys.stdout\", new_callable=StringIO)\n    PromptSession = mocker.patch(\"shillelagh.console.PromptSession\")\n\n    PromptSession.return_value.prompt.side_effect = [\"SSELECT 1;\", EOFError()]\n    console.main()\n    result = stdout.getvalue()\n    assert (\n        result\n        == \"\"\"SQLError: near \"SSELECT\": syntax error\nGoodBye!\n\"\"\"\n    )"}, {"file_path": "/shillelagh-1.2.19/shillelagh-1.2.19/tests/console_test.py", "class_name": null, "function_name": "test_ctrl_c", "code": "def test_ctrl_c(mocker: MockerFixture) -> None:\n    mocker.patch(\"sys.stdin.isatty\", return_value=True)\n    stdout = mocker.patch(\"sys.stdout\", new_callable=StringIO)\n    PromptSession = mocker.patch(\"shillelagh.console.PromptSession\")\n\n    PromptSession.return_value.prompt.side_effect = [\n        KeyboardInterrupt(),\n        \"SELECT 1;\",\n        EOFError(),\n    ]\n    console.main()\n    result = stdout.getvalue()\n    assert (\n        result\n        == \"\"\"  1\n---\n  1\n(1 row in 0.00s)\n\nGoodBye!\n\"\"\"\n    )"}, {"file_path": "/shillelagh-1.2.19/shillelagh-1.2.19/tests/console_test.py", "class_name": null, "function_name": "test_configuration", "code": "def test_configuration(mocker: MockerFixture, fs: FakeFilesystem) -> None:\n    mocker.patch(\"sys.stdin.isatty\", return_value=True)\n    config_dir = Path(user_config_dir(\"shillelagh\"))\n    config_path = config_dir / \"shillelagh.yaml\"\n    fs.create_file(config_path, contents=yaml.dump({\"foo\": {\"bar\": \"baz\"}}))\n\n    connect = mocker.patch(\"shillelagh.console.connect\")\n    mocker.patch(\"sys.stdout\", new_callable=StringIO)\n    PromptSession = mocker.patch(\"shillelagh.console.PromptSession\")\n\n    PromptSession.return_value.prompt.side_effect = [EOFError()]\n    console.main()\n\n    connect.assert_called_with(\":memory:\", adapter_kwargs={\"foo\": {\"bar\": \"baz\"}})"}, {"file_path": "/shillelagh-1.2.19/shillelagh-1.2.19/tests/console_test.py", "class_name": null, "function_name": "test_no_configuration", "code": "def test_no_configuration(mocker: MockerFixture, fs: FakeFilesystem) -> None:\n    mocker.patch(\"sys.stdin.isatty\", return_value=True)\n    config_dir = Path(user_config_dir(\"shillelagh\"))\n    fs.create_dir(config_dir)\n\n    connect = mocker.patch(\"shillelagh.console.connect\")\n    mocker.patch(\"sys.stdout\", new_callable=StringIO)\n    PromptSession = mocker.patch(\"shillelagh.console.PromptSession\")\n\n    PromptSession.return_value.prompt.side_effect = [EOFError()]\n    console.main()\n\n    connect.assert_called_with(\":memory:\", adapter_kwargs={})"}, {"file_path": "/shillelagh-1.2.19/shillelagh-1.2.19/tests/console_test.py", "class_name": null, "function_name": "test_configuration_invalid", "code": "def test_configuration_invalid(mocker: MockerFixture, fs: FakeFilesystem) -> None:\n    mocker.patch(\"sys.stdin.isatty\", return_value=True)\n    config_dir = Path(user_config_dir(\"shillelagh\"))\n    config_path = config_dir / \"shillelagh.yaml\"\n    fs.create_file(config_path, contents=\"foo: *\")\n\n    _logger = mocker.patch(\"shillelagh.console._logger\")\n    mocker.patch(\"sys.stdout\", new_callable=StringIO)\n    PromptSession = mocker.patch(\"shillelagh.console.PromptSession\")\n\n    PromptSession.return_value.prompt.side_effect = [EOFError()]\n    console.main()\n\n    _logger.exception.assert_called_with(\"Unable to load configuration file\")"}, {"file_path": "/shillelagh-1.2.19/shillelagh-1.2.19/tests/console_test.py", "class_name": null, "function_name": "test_multiline", "code": "def test_multiline(mocker: MockerFixture, fs: FakeFilesystem) -> None:\n    mocker.patch(\"sys.stdin.isatty\", return_value=True)\n    stdout = mocker.patch(\"sys.stdout\", new_callable=StringIO)\n    PromptSession = mocker.patch(\"shillelagh.console.PromptSession\")\n\n    PromptSession.return_value.prompt.side_effect = [\"SELECT \", \"1;\", \"\", EOFError()]\n    console.main()\n    result = stdout.getvalue()\n    assert (\n        result\n        == \"\"\"  1\n---\n  1\n(1 row in 0.00s)\n\nGoodBye!\n\"\"\"\n    )"}, {"file_path": "/shillelagh-1.2.19/shillelagh-1.2.19/tests/console_test.py", "class_name": null, "function_name": "test_multiline_quoted_semicolon", "code": "def test_multiline_quoted_semicolon(mocker: MockerFixture, fs: FakeFilesystem) -> None:\n    mocker.patch(\"sys.stdin.isatty\", return_value=True)\n    stdout = mocker.patch(\"sys.stdout\", new_callable=StringIO)\n    PromptSession = mocker.patch(\"shillelagh.console.PromptSession\")\n\n    PromptSession.return_value.prompt.side_effect = [\"SELECT ';'=\", \"';';\", EOFError()]\n    console.main()\n    result = stdout.getvalue()\n\n    assert (\n        result\n        == \"\"\"  ';'=\n   ';'\n------\n     1\n(1 row in 0.00s)\n\nGoodBye!\n\"\"\"\n    )"}, {"file_path": "/shillelagh-1.2.19/shillelagh-1.2.19/tests/console_test.py", "class_name": null, "function_name": "test_multiline_quoted_semicolon_on_line_end", "code": "def test_multiline_quoted_semicolon_on_line_end(\n    mocker: MockerFixture,\n    fs: FakeFilesystem,\n) -> None:\n    mocker.patch(\"sys.stdin.isatty\", return_value=True)\n    stdout = mocker.patch(\"sys.stdout\", new_callable=StringIO)\n    PromptSession = mocker.patch(\"shillelagh.console.PromptSession\")\n\n    PromptSession.return_value.prompt.side_effect = [\"SELECT ';'=';\", \"';\", EOFError()]\n    console.main()\n    result = stdout.getvalue()\n\n    assert (\n        result\n        == \"\"\"  ';'=';\n       '\n--------\n       0\n(1 row in 0.00s)\n\nGoodBye!\n\"\"\"\n    )"}, {"file_path": "/shillelagh-1.2.19/shillelagh-1.2.19/tests/console_test.py", "class_name": null, "function_name": "test_multiline_triple_quoted_semicolon_on_line_end", "code": "def test_multiline_triple_quoted_semicolon_on_line_end(\n    mocker: MockerFixture,\n    fs: FakeFilesystem,\n) -> None:\n    mocker.patch(\"sys.stdin.isatty\", return_value=True)\n    stdout = mocker.patch(\"sys.stdout\", new_callable=StringIO)\n    PromptSession = mocker.patch(\"shillelagh.console.PromptSession\")\n\n    PromptSession.return_value.prompt.side_effect = [\n        \"SELECT ''';'''=''';\",\n        \"''';\",\n        EOFError(),\n    ]\n    console.main()\n    result = stdout.getvalue()\n\n    assert (\n        result\n        == \"\"\"  ''';'''=''';\n           '''\n--------------\n             0\n(1 row in 0.00s)\n\nGoodBye!\n\"\"\"\n    )"}, {"file_path": "/shillelagh-1.2.19/shillelagh-1.2.19/tests/console_test.py", "class_name": null, "function_name": "test_non_interactive", "code": "def test_non_interactive(mocker: MockerFixture) -> None:\n    stdin = mocker.patch(\"sys.stdin\")\n    stdin.isatty.return_value = False\n    stdout = mocker.patch(\"sys.stdout\", new_callable=StringIO)\n\n    stdin.readlines.return_value = [\"SELECT\", \"1\", \";\"]\n    console.main()\n    result = stdout.getvalue()\n    assert (\n        result\n        == \"\"\"  1\n---\n  1\n\"\"\"\n    )"}]}, {"git_group": "matthiask", "git_name": "plata", "version": "v1.2.1pre", "language": "Python", "project_name": "plata-v1.2.1pre.zip", "file_path": "/plata-v1.2.1pre/plata-1.2.1pre/plata/discount/models.py", "file_name": "models.py", "focal_class": "Discount", "focal_name": "validate", "focal_parameter": ["order"], "solution": "    def validate(self, order):\n\n        messages = []\n        if not self.is_active:\n            messages.append(_('Discount is inactive.'))\n\n        today = date.today()\n        if today < self.valid_from:\n            messages.append(_('Discount is not active yet.'))\n        if self.valid_until and today > self.valid_until:\n            messages.append(_('Discount is expired.'))\n\n        if self.allowed_uses and self.used >= self.allowed_uses:\n            messages.append(\n                _('Allowed uses for this discount has already been reached.'))\n\n        if (self.currency != order.currency and self.type in (\n                self.AMOUNT_VOUCHER_EXCL_TAX,\n                self.AMOUNT_VOUCHER_INCL_TAX,\n                self.MEANS_OF_PAYMENT)):\n            messages.append(_('Discount and order currencies do not match.'))\n\n        if messages:\n            raise ValidationError(messages)\n\n        return True", "function_signature": "def validate(self, order) :", "left_context": "from __future__ import absolute_import, unicode_literals\n\nfrom datetime import date\nfrom decimal import Decimal\nimport random\n\nfrom django.core.exceptions import ValidationError\nfrom django.db import models\nfrom django.db.models import ObjectDoesNotExist, Q\nfrom django.utils.encoding import python_2_unicode_compatible\nfrom django.utils.translation import ugettext_lazy as _\n\nimport plata\nfrom plata.fields import CurrencyField, JSONField\nfrom plata.shop.models import TaxClass, Order\n\n\n@python_2_unicode_compatible\nclass DiscountBase(models.Model):\n    \"\"\"Base class for discounts and applied discounts\"\"\"\n\n    AMOUNT_VOUCHER_EXCL_TAX = 10\n    AMOUNT_VOUCHER_INCL_TAX = 20\n    PERCENTAGE_VOUCHER = 30\n    MEANS_OF_PAYMENT = 40\n\n    TYPE_CHOICES = (\n        (AMOUNT_VOUCHER_EXCL_TAX,\n            _('amount voucher excl. tax (reduces total tax on order)')),\n        (AMOUNT_VOUCHER_INCL_TAX,\n            _('amount voucher incl. tax (reduces total tax on order)')),\n        (PERCENTAGE_VOUCHER,\n            _('percentage voucher (reduces total tax on order)')),\n        (MEANS_OF_PAYMENT,\n            _('means of payment (does not change total tax on order)')),\n    )\n\n    #: You can add and remove options at will, except for 'all': This option\n    #: must always be available, and it cannot have any form fields\n    CONFIG_OPTIONS = [\n        ('all', {\n            'title': _('All products'),\n        }),\n        ('exclude_sale', {\n            'title': _('Exclude sale prices'),\n            'orderitem_query': lambda **values: Q(is_sale=False),\n        }),\n    ]\n\n    name = models.CharField(_('name'), max_length=100)\n\n    type = models.PositiveIntegerField(_('type'), choices=TYPE_CHOICES)\n    value = models.DecimalField(_('value'), max_digits=18, decimal_places=10)\n\n    currency = CurrencyField(\n        blank=True, null=True,\n        help_text=_('Only required for amount discounts.'))\n    tax_class = models.ForeignKey(\n        TaxClass, verbose_name=_('tax class'),\n        blank=True, null=True,\n        help_text=_('Only required for amount discounts incl. tax.'))\n\n    config = JSONField(\n        _('configuration'), blank=True, help_text=_(\n            'If you edit this field directly, changes below will be'\n            ' ignored.'))\n\n    class Meta:\n        abstract = True\n\n    def __str__(self):\n        return self.name\n\n    def save(self, *args, **kwargs):\n        self.full_clean()\n        super(DiscountBase, self).save(*args, **kwargs)\n    save.alters_data = True\n\n    def clean(self):\n        if self.type == self.PERCENTAGE_VOUCHER:\n            if self.currency or self.tax_class:\n                raise ValidationError(_(\n                    'Percentage discounts cannot have currency and tax'\n                    ' class set.'))\n        elif self.type == self.AMOUNT_VOUCHER_EXCL_TAX:\n            if not self.currency:\n                raise ValidationError(_(\n                    'Amount discounts excl. tax need a currency.'))\n            if self.tax_class:\n                raise ValidationError(_(\n                    'Amount discounts excl. tax cannot have tax class'\n                    ' set.'))\n        elif self.type == self.AMOUNT_VOUCHER_INCL_TAX:\n            if not (self.currency and self.tax_class):\n                raise ValidationError(_(\n                    'Amount discounts incl. tax need a currency and a tax'\n                    ' class.'))\n        elif self.type == self.MEANS_OF_PAYMENT:\n            if not self.currency:\n                raise ValidationError(_('Means of payment need a currency.'))\n            if self.tax_class:\n                raise ValidationError(\n                    _('Means of payment cannot have tax class set.'))\n        else:\n            raise ValidationError(_('Unknown discount type.'))\n\n    def _eligible_products(self, order, items):\n        \"\"\"\n        Return a list of products which are eligible for discounting using\n        the discount configuration.\n        \"\"\"\n\n        product_model = plata.product_model()\n\n        products = product_model._default_manager.filter(\n            id__in=[item.product_id for item in items])\n        orderitems = order.items.model._default_manager.filter(\n            id__in=[item.id for item in items])\n\n        for key, parameters in self.config.items():\n            parameters = dict((str(k), v) for k, v in parameters.items())\n\n            cfg = dict(self.CONFIG_OPTIONS)[key]\n\n            if 'product_query' in cfg:\n                products = products.filter(\n                    cfg['product_query'](**parameters))\n            if 'orderitem_query' in cfg:\n                orderitems = orderitems.filter(\n                    cfg['orderitem_query'](**parameters))\n\n        return products.filter(id__in=orderitems.values('product_id'))\n\n    def apply(self, order, items, **kwargs):\n        if not items:\n            return\n\n        if self.type == self.AMOUNT_VOUCHER_EXCL_TAX:\n            self._apply_amount_discount(order, items, tax_included=False)\n        elif self.type == self.AMOUNT_VOUCHER_INCL_TAX:\n            self._apply_amount_discount(order, items, tax_included=True)\n        elif self.type == self.PERCENTAGE_VOUCHER:\n            self._apply_percentage_discount(order, items)\n        elif self.type == self.MEANS_OF_PAYMENT:\n            self._apply_means_of_payment(order, items)\n        else:\n            raise NotImplementedError('Unknown discount type %s' % self.type)\n\n    def _apply_amount_discount(self, order, items, tax_included):\n        \"\"\"\n        Apply amount discount evenly to all eligible order items\n\n        Aggregates remaining discount (if discount is bigger than order total)\n        \"\"\"\n\n        eligible_products = self._eligible_products(order, items).values_list(\n            'id', flat=True)\n        eligible_items = [\n            item for item in items\n            if item.product_id in eligible_products]\n\n        if tax_included:\n            discount = self.value / (1 + self.tax_class.rate / 100)\n        else:\n            discount = self.value\n\n        items_subtotal = sum([\n            item.discounted_subtotal_excl_tax for item in eligible_items\n        ], Decimal('0.00'))\n\n        # Don't allow bigger discounts than the items subtotal\n        if discount > items_subtotal:\n            self.remaining = discount - items_subtotal\n            self.save()\n            discount = items_subtotal\n\n        for item in eligible_items:\n            item._line_item_discount += (\n                item.discounted_subtotal_excl_tax / items_subtotal * discount)\n\n    def _apply_means_of_payment(self, order, items):\n\n        items_tax = sum(\n                (item._line_item_tax for item in items),\n                Decimal('0.00')\n            )\n\n        for item in items:\n            items_tax\n\n        discount = self.value\n        items_subtotal = order.subtotal if \\\n            order.price_includes_tax else order.subtotal + items_tax\n        # CHECK: this is probably item.subtotal, not items_subtotal!\n\n        # Don't allow bigger discounts than the items subtotal\n        remaining = discount\n        for item in items:\n            if order.price_includes_tax:\n                items_subtotal_inkl_taxes = item.subtotal\n                items_subtotal_excl_taxes = item._unit_price * item.quantity\n            else:\n                items_subtotal_inkl_taxes = item.subtotal + item._line_item_tax\n                items_subtotal_excl_taxes = item.subtotal\n            # CHECK: items_subtotal_excl_taxes is unused!\n\n            if remaining >= items_subtotal_inkl_taxes - item._line_item_discount:\n                if item._line_item_discount < items_subtotal_inkl_taxes:\n                    new_discount = items_subtotal_inkl_taxes - item._line_item_discount\n                    item._line_item_discount += new_discount\n                    remaining -= new_discount\n            else:\n                item._line_item_discount += remaining\n                remaining = 0\n\n        self.remaining = remaining\n        self.save()\n\n    def _apply_percentage_discount(self, order, items):\n        \"\"\"\n        Apply percentage discount evenly to all eligible order items\n        \"\"\"\n\n        eligible_products = self._eligible_products(order, items).values_list(\n            'id', flat=True)\n\n        factor = self.value / 100\n\n        for item in items:\n            if item.product_id not in eligible_products:\n                continue\n\n            item._line_item_discount += (\n                    item.discounted_subtotal_excl_tax * factor)\n\n\n# Nearly all letters and digits, excluding those which can be easily confounded\nRANDOM_CODE_CHARACTERS = (\n    '23456789abcdefghijkmnopqrstuvwxyzABCDEFGHJKLMNPQRSTUVWXYZ')\n\n\ndef generate_random_code():\n    return u''.join(random.sample(RANDOM_CODE_CHARACTERS, 10))\n\n\nclass Discount(DiscountBase):\n    code = models.CharField(\n        _('code'), max_length=30, unique=True, default=generate_random_code)\n\n    is_active = models.BooleanField(_('is active'), default=True)\n    valid_from = models.DateField(_('valid from'), default=date.today)\n    valid_until = models.DateField(_('valid until'), blank=True, null=True)\n\n    allowed_uses = models.IntegerField(\n        _('number of allowed uses'),\n        blank=True, null=True,\n        help_text=_(\n            'Leave empty if there is no limit on the number of uses'\n            ' of this discount.'))\n    used = models.IntegerField(_('number of times already used'), default=0)\n\n    class Meta:\n        verbose_name = _('discount')\n        verbose_name_plural = _('discounts')\n", "right_context": "\n    def add_to(self, order, recalculate=True):\n        \"\"\"\n        Add discount to passed order\n\n        Removes the previous discount if a discount with this code has\n        already been added to the order before.\n        \"\"\"\n\n        self.validate(order)\n\n        try:\n            order.applied_discounts.get(code=self.code).delete()\n        except ObjectDoesNotExist:\n            pass\n\n        instance = order.applied_discounts.create(\n            code=self.code,\n            type=self.type,\n            name=self.name,\n            value=self.value,\n            currency=self.currency,\n            tax_class=self.tax_class,\n            config=self.config,\n        )\n\n        if recalculate:\n            order.recalculate_total()\n\n        return instance\n\n\nclass AppliedDiscountManager(models.Manager):\n    \"\"\"\n    Default manager for the ``AppliedDiscount`` model\n    \"\"\"\n\n    def remaining(self, order=None):\n        \"\"\"\n        Calculate remaining discount excl. tax\n\n        Can either be used as related manager::\n\n            order.applied_discounts.remaining()\n\n        or directly::\n\n            AppliedDiscount.objects.remaining(order)\n        \"\"\"\n\n        queryset = self.all()\n        if order:\n            queryset = queryset.filter(order=order)\n\n        return sum((d.remaining for d in queryset), Decimal('0.00'))\n\n\nclass AppliedDiscount(DiscountBase):\n    \"\"\"\n    Stores an applied discount, so that deletion of discounts does not\n    affect orders.\n    \"\"\"\n\n    order = models.ForeignKey(\n        Order, related_name='applied_discounts', verbose_name=_('order'))\n    # We could make this a ForeignKey to Discount.code, but we do not\n    # want deletions to cascade to this table and we still need the code\n    # for the PDF generation or whatever anyway.\n    code = models.CharField(_('code'), max_length=30)\n    remaining = models.DecimalField(\n        _('remaining'),\n        max_digits=18, decimal_places=10, default=0,\n        help_text=_(\n            'Discount amount excl. tax remaining after discount has'\n            ' been applied.'))\n\n    class Meta:\n        ordering = ['type', 'name']\n        verbose_name = _('applied discount')\n        verbose_name_plural = _('applied discounts')\n\n    objects = AppliedDiscountManager()\n", "import_text": ["datetime.date", "decimal.Decimal", "random", "django.core.exceptions.ValidationError", "django.db.models", "django.db.models.ObjectDoesNotExist", "django.db.models.Q", "django.utils.encoding.python_2_unicode_compatible", "django.utils.translation.ugettext_lazy", "plata", "plata.fields.CurrencyField", "plata.fields.JSONField", "plata.shop.models.TaxClass", "plata.shop.models.Order"], "prompt": "\"\"\"\nDescription: This function validates an order based on various conditions related to the discount.\n\nArgs:\n    self (object): The instance of the class that contains this method.\n    order (object): The order to be validated.\n\nReturns:\n    bool: True if the order is valid, otherwise raises a ValidationError with a list of error messages.\n\nRaises:\n    ValidationError: If the order is not valid based on the conditions.\n\nNotes:\n    This function uses the django.utils.translation.ugettext_lazy for translating messages, and django.core.exceptions.ValidationError for raising validation errors.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"\n        Validate whether this discount can be applied on the given order\n        \"\"\"", "function_dependencies": ["django.utils.translation.ugettext_lazy", "datetime.date.today", "django.core.exceptions.ValidationError"], "project_create_time": "2010-04-20T09:35:17+00:00", "project_update_time": "2024-04-17T13:44:42+00:00", "file_create_time": "2012-01-17T10:01:15Z", "file_update_time": "2016-06-11T12:16:18Z", "function_update_time": "2012-01-17T10:01:15Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["django.utils.translation.ugettext_lazy", "django.core.exceptions.ValidationError"], "test_function": [{"file_path": "/plata-v1.2.1pre/plata-1.2.1pre/tests/testapp/tests/test_models.py", "class_name": "ModelTest", "function_name": "test_10_discount_validation", "code": "    def test_10_discount_validation(self):\n        order = self.create_order()\n        d = Discount(\n            is_active=False,\n            valid_from=date(2100, 1, 1), # far future date\n            valid_until=None,\n            )\n\n        try:\n            d.validate(order)\n        except ValidationError as e:\n            self.assertEqual(len(e.messages), 2)\n\n        d.is_active = True\n        d.valid_until = date(2000, 1, 1)\n\n        try:\n            d.validate(order)\n        except ValidationError as e:\n            self.assertEqual(len(e.messages), 2)"}]}, {"git_group": "recommenders-team", "git_name": "recommenders", "version": "1.1.1", "language": "Python", "project_name": "recommenders-1.1.1.zip", "file_path": "/recommenders-1.1.1/recommenders-1.1.1/recommenders/datasets/sparse.py", "file_name": "sparse.py", "focal_class": "AffinityMatrix", "focal_name": "map_back_sparse", "focal_parameter": ["X", "kind"], "solution": "    def map_back_sparse(self, X, kind):\n        m, n = X.shape\n\n        # 1) Create a DF from a sparse matrix\n        # obtain the non zero items\n        items = [np.asanyarray(np.where(X[i, :] != 0)).flatten() for i in range(m)]\n        ratings = [X[i, items[i]] for i in range(m)]  # obtain the non-zero ratings\n\n        # Creates user ids following the DF format\n        userids = []\n        for i in range(0, m):\n            userids.extend([i] * len(items[i]))\n\n        # Flatten the lists to follow the DF input format\n        items = list(itertools.chain.from_iterable(items))\n        ratings = list(itertools.chain.from_iterable(ratings))\n\n        if kind == \"ratings\":\n            col_out = self.col_rating\n        else:\n            col_out = self.col_pred\n\n        # create a df\n        out_df = pd.DataFrame.from_dict(\n            {self.col_user: userids, self.col_item: items, col_out: ratings}\n        )\n\n        # 2) map back user/item ids to their original value\n\n        out_df[self.col_user] = out_df[self.col_user].map(self.map_back_users)\n        out_df[self.col_item] = out_df[self.col_item].map(self.map_back_items)\n\n        return out_df", "function_signature": "def map_back_sparse(self, X, kind) :", "left_context": "# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License.\n\nimport pandas as pd\nimport numpy as np\nimport itertools\n\nfrom scipy.sparse import coo_matrix\nimport logging\n\n# import default parameters\nfrom recommenders.utils.constants import (\n    DEFAULT_USER_COL,\n    DEFAULT_ITEM_COL,\n    DEFAULT_RATING_COL,\n    DEFAULT_PREDICTION_COL,\n)\n\n\nlog = logging.getLogger(__name__)\n\n\nclass AffinityMatrix:\n    \"\"\"Generate the user/item affinity matrix from a pandas dataframe and vice versa\"\"\"\n\n    def __init__(\n        self,\n        df,\n        items_list=None,\n        col_user=DEFAULT_USER_COL,\n        col_item=DEFAULT_ITEM_COL,\n        col_rating=DEFAULT_RATING_COL,\n        col_pred=DEFAULT_PREDICTION_COL,\n        save_path=None,\n    ):\n        \"\"\"Initialize class parameters\n\n        Args:\n            df (pandas.DataFrame): a dataframe containing the data\n            items_list (numpy.ndarray): a list of unique items to use (if provided)\n            col_user (str): default name for user column\n            col_item (str): default name for item column\n            col_rating (str): default name for rating columns\n            save_path (str): default path to save item/user maps\n        \"\"\"\n        self.df = df  # dataframe\n        self.items_list = items_list  # list of unique items\n\n        # pandas DF parameters\n        self.col_item = col_item\n        self.col_user = col_user\n        self.col_rating = col_rating\n        self.col_pred = col_pred\n\n        # Options to save the model for future use\n        self.save_path = save_path\n\n    def _gen_index(self):\n        \"\"\"\n        Generate the user/item index:\n        map_users, map_items: dictionaries mapping the original user/item index to matrix indices\n        map_back_users, map_back_items: dictionaries to map back the matrix elements to the original\n        dataframe indices\n\n        Basic mechanics:\n        As a first step we retieve the unique elements in the dataset. In this way we can take care\n        of either completely missing rows (a user with no ratings) or completely missing columns\n        (an item that has not being reviewed by anyone). The original indices in the dataframe are\n        then mapped to an ordered, contiguous integer series to generate a compact matrix representation.\n        Functions to map back to the original indices are also provided and can be saved in order to use\n        a pretrained model.\n        \"\"\"\n        # sort entries by user index\n        self.df_ = self.df.sort_values(by=[self.col_user])\n\n        # find unique user and item index\n        unique_users = self.df_[self.col_user].unique()\n\n        if self.items_list is not None:\n            unique_items = self.items_list  # use this list if provided\n        else:\n            unique_items = self.df_[\n                self.col_item\n            ].unique()  # otherwise use unique items from DF\n\n        self.Nusers = len(unique_users)\n        self.Nitems = len(unique_items)\n\n        # create a dictionary to map unique users/items to hashed values to generate the matrix\n        self.map_users = {x: i for i, x in enumerate(unique_users)}\n        self.map_items = {x: i for i, x in enumerate(unique_items)}\n\n        # map back functions used to get back the original dataframe\n        self.map_back_users = {i: x for i, x in enumerate(unique_users)}\n        self.map_back_items = {i: x for i, x in enumerate(unique_items)}\n\n        self.df_.loc[:, \"hashedItems\"] = self.df_[self.col_item].map(self.map_items)\n        self.df_.loc[:, \"hashedUsers\"] = self.df_[self.col_user].map(self.map_users)\n\n        # optionally save the inverse dictionary to work with trained models\n        if self.save_path is not None:\n\n            np.save(self.save_path + \"/user_dict\", self.map_users)\n            np.save(self.save_path + \"/item_dict\", self.map_items)\n\n            np.save(self.save_path + \"/user_back_dict\", self.map_back_users)\n            np.save(self.save_path + \"/item_back_dict\", self.map_back_items)\n\n    def gen_affinity_matrix(self):\n        \"\"\"Generate the user/item affinity matrix.\n\n        As a first step, two new columns are added to the input DF, containing the index maps\n        generated by the gen_index() method. The new indices, together with the ratings, are\n        then used to generate the user/item affinity matrix using scipy's sparse matrix method\n        coo_matrix; for reference see:\n        https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html.\n        The input format is: `coo_matrix((data, (rows, columns)), shape=(rows, columns))`\n\n        Returns:\n            scipy.sparse.coo_matrix: User-affinity matrix of dimensions (Nusers, Nitems) in numpy format.\n            Unrated movies are assigned a value of 0.\n        \"\"\"\n\n        log.info(\"Generating the user/item affinity matrix...\")\n\n        self._gen_index()\n\n        ratings = self.df_[self.col_rating]  # ratings\n        itm_id = self.df_[\"hashedItems\"]  # itm_id serving as columns\n        usr_id = self.df_[\"hashedUsers\"]  # usr_id serving as rows\n\n        # generate a sparse matrix representation using scipy's coo_matrix and convert to array format\n        self.AM = coo_matrix(\n            (ratings, (usr_id, itm_id)), shape=(self.Nusers, self.Nitems)\n        ).toarray()\n\n        zero = (self.AM == 0).sum()  # number of unrated items\n        total = self.AM.shape[0] * self.AM.shape[1]  # number of elements in the matrix\n        sparsness = zero / total * 100  # Percentage of zeros in the matrix\n\n        log.info(\"Matrix generated, sparseness percentage: %d\" % sparsness)\n\n        return self.AM, self.map_users, self.map_items\n", "right_context": "", "import_text": ["pandas", "numpy", "itertools", "scipy.sparse.coo_matrix", "logging", "recommenders.utils.constants.DEFAULT_USER_COL", "recommenders.utils.constants.DEFAULT_ITEM_COL", "recommenders.utils.constants.DEFAULT_RATING_COL", "recommenders.utils.constants.DEFAULT_PREDICTION_COL"], "prompt": "\"\"\"\nDescription: This function maps back sparse matrix data to a pandas DataFrame.\n\nArgs:\n    X (numpy.ndarray): The sparse matrix to be mapped back.\n    kind (str): The type of data to be mapped back. Can be either 'ratings' or 'predictions'.\n\nReturns:\n    pandas.DataFrame: A DataFrame containing the mapped back data. The DataFrame has three columns:\n        - self.col_user: The original user ids.\n        - self.col_item: The original item ids.\n        - col_out: The ratings or predictions, depending on the 'kind' argument.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"Map back the user/affinity matrix to a pd dataframe\n\n        Args:\n            X (numpy.ndarray, int32): user/item affinity matrix\n            kind (string): specify if the output values are ratings or predictions\n        Returns:\n            pandas.DataFrame: the generated pandas dataframe\n        \"\"\"", "function_dependencies": ["numpy.asanyarray", "numpy.asanyarray.flatten", "numpy.where", "itertools.chain.from_iterable", "pandas.DataFrame.from_dict"], "project_create_time": "2018-09-19T10:06:07+00:00", "project_update_time": "2024-04-18T03:16:27+00:00", "file_create_time": "2021-07-22T14:41:01Z", "file_update_time": "2021-10-15T20:23:35Z", "function_update_time": "2021-07-22T14:41:01Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["numpy.where", "pandas.DataFrame.from_dict"], "test_function": [{"file_path": "/recommenders-1.1.1/recommenders-1.1.1/tests/unit/recommenders/datasets/test_sparse.py", "class_name": null, "function_name": "test_sparse_to_df", "code": "\ndef test_sparse_to_df(test_specs, python_dataset):\n    # initialize the splitter\n    header = {\n        \"col_user\": DEFAULT_USER_COL,\n        \"col_item\": DEFAULT_ITEM_COL,\n        \"col_rating\": DEFAULT_RATING_COL,\n    }\n\n    # instantiate the the affinity matrix\n    am = AffinityMatrix(df=python_dataset, **header)\n\n    # generate the sparse matrix representation\n    X, _, _ = am.gen_affinity_matrix()\n\n    # use the inverse function to generate a pandas df from a sparse matrix ordered by userID\n    DF = am.map_back_sparse(X, kind=\"ratings\")\n\n    # tests: check that the two dataframes have the same elements in the same positions.\n    assert (\n        DF.userID.values.all()\n        == python_dataset.sort_values(by=[\"userID\"]).userID.values.all()\n    )\n\n    assert (\n        DF.itemID.values.all()\n        == python_dataset.sort_values(by=[\"userID\"]).itemID.values.all()\n    )\n\n    assert (\n        DF.rating.values.all()\n        == python_dataset.sort_values(by=[\"userID\"]).rating.values.all()\n    )"}]}, {"git_group": "mwaskom", "git_name": "seaborn", "version": "v0.13.2", "language": "Python", "project_name": "seaborn-v0.13.2.zip", "file_path": "/seaborn-v0.13.2/seaborn-0.13.2/seaborn/utils.py", "file_name": "utils.py", "focal_class": null, "focal_name": "desaturate", "focal_parameter": ["color", "prop"], "solution": "def desaturate(color, prop):\n    # Check inputs\n    if not 0 <= prop <= 1:\n        raise ValueError(\"prop must be between 0 and 1\")\n\n    # Get rgb tuple rep\n    rgb = to_rgb(color)\n\n    # Short circuit to avoid floating point issues\n    if prop == 1:\n        return rgb\n\n    # Convert to hls\n    h, l, s = colorsys.rgb_to_hls(*rgb)\n\n    # Desaturate the saturation channel\n    s *= prop\n\n    # Convert back to rgb\n    new_color = colorsys.hls_to_rgb(h, l, s)\n\n    return new_color", "function_signature": "def desaturate(color, prop) :", "left_context": "\"\"\"Utility functions, mostly for internal use.\"\"\"\nimport os\nimport inspect\nimport warnings\nimport colorsys\nfrom contextlib import contextmanager\nfrom urllib.request import urlopen, urlretrieve\nfrom types import ModuleType\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nfrom matplotlib.colors import to_rgb\nimport matplotlib.pyplot as plt\nfrom matplotlib.cbook import normalize_kwargs\n\nfrom seaborn._core.typing import deprecated\nfrom seaborn.external.version import Version\nfrom seaborn.external.appdirs import user_cache_dir\n\n__all__ = [\"desaturate\", \"saturate\", \"set_hls_values\", \"move_legend\",\n           \"despine\", \"get_dataset_names\", \"get_data_home\", \"load_dataset\"]\n\nDATASET_SOURCE = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master\"\nDATASET_NAMES_URL = f\"{DATASET_SOURCE}/dataset_names.txt\"\n\n\ndef ci_to_errsize(cis, heights):\n    \"\"\"Convert intervals to error arguments relative to plot heights.\n\n    Parameters\n    ----------\n    cis : 2 x n sequence\n        sequence of confidence interval limits\n    heights : n sequence\n        sequence of plot heights\n\n    Returns\n    -------\n    errsize : 2 x n array\n        sequence of error size relative to height values in correct\n        format as argument for plt.bar\n\n    \"\"\"\n    cis = np.atleast_2d(cis).reshape(2, -1)\n    heights = np.atleast_1d(heights)\n    errsize = []\n    for i, (low, high) in enumerate(np.transpose(cis)):\n        h = heights[i]\n        elow = h - low\n        ehigh = high - h\n        errsize.append([elow, ehigh])\n\n    errsize = np.asarray(errsize).T\n    return errsize\n\n\ndef _draw_figure(fig):\n    \"\"\"Force draw of a matplotlib figure, accounting for back-compat.\"\"\"\n    # See https://github.com/matplotlib/matplotlib/issues/19197 for context\n    fig.canvas.draw()\n    if fig.stale:\n        try:\n            fig.draw(fig.canvas.get_renderer())\n        except AttributeError:\n            pass\n\n\ndef _default_color(method, hue, color, kws, saturation=1):\n    \"\"\"If needed, get a default color by using the matplotlib property cycle.\"\"\"\n\n    if hue is not None:\n        # This warning is probably user-friendly, but it's currently triggered\n        # in a FacetGrid context and I don't want to mess with that logic right now\n        #  if color is not None:\n        #      msg = \"`color` is ignored when `hue` is assigned.\"\n        #      warnings.warn(msg)\n        return None\n\n    kws = kws.copy()\n    kws.pop(\"label\", None)\n\n    if color is not None:\n        if saturation < 1:\n            color = desaturate(color, saturation)\n        return color\n\n    elif method.__name__ == \"plot\":\n\n        color = normalize_kwargs(kws, mpl.lines.Line2D).get(\"color\")\n        scout, = method([], [], scalex=False, scaley=False, color=color)\n        color = scout.get_color()\n        scout.remove()\n\n    elif method.__name__ == \"scatter\":\n\n        # Matplotlib will raise if the size of x/y don't match s/c,\n        # and the latter might be in the kws dict\n        scout_size = max(\n            np.atleast_1d(kws.get(key, [])).shape[0]\n            for key in [\"s\", \"c\", \"fc\", \"facecolor\", \"facecolors\"]\n        )\n        scout_x = scout_y = np.full(scout_size, np.nan)\n\n        scout = method(scout_x, scout_y, **kws)\n        facecolors = scout.get_facecolors()\n\n        if not len(facecolors):\n            # Handle bug in matplotlib <= 3.2 (I think)\n            # This will limit the ability to use non color= kwargs to specify\n            # a color in versions of matplotlib with the bug, but trying to\n            # work out what the user wanted by re-implementing the broken logic\n            # of inspecting the kwargs is probably too brittle.\n            single_color = False\n        else:\n            single_color = np.unique(facecolors, axis=0).shape[0] == 1\n\n        # Allow the user to specify an array of colors through various kwargs\n        if \"c\" not in kws and single_color:\n            color = to_rgb(facecolors[0])\n\n        scout.remove()\n\n    elif method.__name__ == \"bar\":\n\n        # bar() needs masked, not empty data, to generate a patch\n        scout, = method([np.nan], [np.nan], **kws)\n        color = to_rgb(scout.get_facecolor())\n        scout.remove()\n        # Axes.bar adds both a patch and a container\n        method.__self__.containers.pop(-1)\n\n    elif method.__name__ == \"fill_between\":\n\n        kws = normalize_kwargs(kws, mpl.collections.PolyCollection)\n        scout = method([], [], **kws)\n        facecolor = scout.get_facecolor()\n        color = to_rgb(facecolor[0])\n        scout.remove()\n\n    if saturation < 1:\n        color = desaturate(color, saturation)\n\n    return color\n\n", "right_context": "\n\ndef saturate(color):\n    \"\"\"Return a fully saturated color with the same hue.\n\n    Parameters\n    ----------\n    color : matplotlib color\n        hex, rgb-tuple, or html color name\n\n    Returns\n    -------\n    new_color : rgb tuple\n        saturated color code in RGB tuple representation\n\n    \"\"\"\n    return set_hls_values(color, s=1)\n\n\ndef set_hls_values(color, h=None, l=None, s=None):  # noqa\n    \"\"\"Independently manipulate the h, l, or s channels of a color.\n\n    Parameters\n    ----------\n    color : matplotlib color\n        hex, rgb-tuple, or html color name\n    h, l, s : floats between 0 and 1, or None\n        new values for each channel in hls space\n\n    Returns\n    -------\n    new_color : rgb tuple\n        new color code in RGB tuple representation\n\n    \"\"\"\n    # Get an RGB tuple representation\n    rgb = to_rgb(color)\n    vals = list(colorsys.rgb_to_hls(*rgb))\n    for i, val in enumerate([h, l, s]):\n        if val is not None:\n            vals[i] = val\n\n    rgb = colorsys.hls_to_rgb(*vals)\n    return rgb\n\n\ndef axlabel(xlabel, ylabel, **kwargs):\n    \"\"\"Grab current axis and label it.\n\n    DEPRECATED: will be removed in a future version.\n\n    \"\"\"\n    msg = \"This function is deprecated and will be removed in a future version\"\n    warnings.warn(msg, FutureWarning)\n    ax = plt.gca()\n    ax.set_xlabel(xlabel, **kwargs)\n    ax.set_ylabel(ylabel, **kwargs)\n\n\ndef remove_na(vector):\n    \"\"\"Helper method for removing null values from data vectors.\n\n    Parameters\n    ----------\n    vector : vector object\n        Must implement boolean masking with [] subscript syntax.\n\n    Returns\n    -------\n    clean_clean : same type as ``vector``\n        Vector of data with null values removed. May be a copy or a view.\n\n    \"\"\"\n    return vector[pd.notnull(vector)]\n\n\ndef get_color_cycle():\n    \"\"\"Return the list of colors in the current matplotlib color cycle\n\n    Parameters\n    ----------\n    None\n\n    Returns\n    -------\n    colors : list\n        List of matplotlib colors in the current cycle, or dark gray if\n        the current color cycle is empty.\n    \"\"\"\n    cycler = mpl.rcParams['axes.prop_cycle']\n    return cycler.by_key()['color'] if 'color' in cycler.keys else [\".15\"]\n\n\ndef despine(fig=None, ax=None, top=True, right=True, left=False,\n            bottom=False, offset=None, trim=False):\n    \"\"\"Remove the top and right spines from plot(s).\n\n    fig : matplotlib figure, optional\n        Figure to despine all axes of, defaults to the current figure.\n    ax : matplotlib axes, optional\n        Specific axes object to despine. Ignored if fig is provided.\n    top, right, left, bottom : boolean, optional\n        If True, remove that spine.\n    offset : int or dict, optional\n        Absolute distance, in points, spines should be moved away\n        from the axes (negative values move spines inward). A single value\n        applies to all spines; a dict can be used to set offset values per\n        side.\n    trim : bool, optional\n        If True, limit spines to the smallest and largest major tick\n        on each non-despined axis.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    # Get references to the axes we want\n    if fig is None and ax is None:\n        axes = plt.gcf().axes\n    elif fig is not None:\n        axes = fig.axes\n    elif ax is not None:\n        axes = [ax]\n\n    for ax_i in axes:\n        for side in [\"top\", \"right\", \"left\", \"bottom\"]:\n            # Toggle the spine objects\n            is_visible = not locals()[side]\n            ax_i.spines[side].set_visible(is_visible)\n            if offset is not None and is_visible:\n                try:\n                    val = offset.get(side, 0)\n                except AttributeError:\n                    val = offset\n                ax_i.spines[side].set_position(('outward', val))\n\n        # Potentially move the ticks\n        if left and not right:\n            maj_on = any(\n                t.tick1line.get_visible()\n                for t in ax_i.yaxis.majorTicks\n            )\n            min_on = any(\n                t.tick1line.get_visible()\n                for t in ax_i.yaxis.minorTicks\n            )\n            ax_i.yaxis.set_ticks_position(\"right\")\n            for t in ax_i.yaxis.majorTicks:\n                t.tick2line.set_visible(maj_on)\n            for t in ax_i.yaxis.minorTicks:\n                t.tick2line.set_visible(min_on)\n\n        if bottom and not top:\n            maj_on = any(\n                t.tick1line.get_visible()\n                for t in ax_i.xaxis.majorTicks\n            )\n            min_on = any(\n                t.tick1line.get_visible()\n                for t in ax_i.xaxis.minorTicks\n            )\n            ax_i.xaxis.set_ticks_position(\"top\")\n            for t in ax_i.xaxis.majorTicks:\n                t.tick2line.set_visible(maj_on)\n            for t in ax_i.xaxis.minorTicks:\n                t.tick2line.set_visible(min_on)\n\n        if trim:\n            # clip off the parts of the spines that extend past major ticks\n            xticks = np.asarray(ax_i.get_xticks())\n            if xticks.size:\n                firsttick = np.compress(xticks >= min(ax_i.get_xlim()),\n                                        xticks)[0]\n                lasttick = np.compress(xticks <= max(ax_i.get_xlim()),\n                                       xticks)[-1]\n                ax_i.spines['bottom'].set_bounds(firsttick, lasttick)\n                ax_i.spines['top'].set_bounds(firsttick, lasttick)\n                newticks = xticks.compress(xticks <= lasttick)\n                newticks = newticks.compress(newticks >= firsttick)\n                ax_i.set_xticks(newticks)\n\n            yticks = np.asarray(ax_i.get_yticks())\n            if yticks.size:\n                firsttick = np.compress(yticks >= min(ax_i.get_ylim()),\n                                        yticks)[0]\n                lasttick = np.compress(yticks <= max(ax_i.get_ylim()),\n                                       yticks)[-1]\n                ax_i.spines['left'].set_bounds(firsttick, lasttick)\n                ax_i.spines['right'].set_bounds(firsttick, lasttick)\n                newticks = yticks.compress(yticks <= lasttick)\n                newticks = newticks.compress(newticks >= firsttick)\n                ax_i.set_yticks(newticks)\n\n\ndef move_legend(obj, loc, **kwargs):\n    \"\"\"\n    Recreate a plot's legend at a new location.\n\n    The name is a slight misnomer. Matplotlib legends do not expose public\n    control over their position parameters. So this function creates a new legend,\n    copying over the data from the original object, which is then removed.\n\n    Parameters\n    ----------\n    obj : the object with the plot\n        This argument can be either a seaborn or matplotlib object:\n\n        - :class:`seaborn.FacetGrid` or :class:`seaborn.PairGrid`\n        - :class:`matplotlib.axes.Axes` or :class:`matplotlib.figure.Figure`\n\n    loc : str or int\n        Location argument, as in :meth:`matplotlib.axes.Axes.legend`.\n\n    kwargs\n        Other keyword arguments are passed to :meth:`matplotlib.axes.Axes.legend`.\n\n    Examples\n    --------\n\n    .. include:: ../docstrings/move_legend.rst\n\n    \"\"\"\n    # This is a somewhat hackish solution that will hopefully be obviated by\n    # upstream improvements to matplotlib legends that make them easier to\n    # modify after creation.\n\n    from seaborn.axisgrid import Grid  # Avoid circular import\n\n    # Locate the legend object and a method to recreate the legend\n    if isinstance(obj, Grid):\n        old_legend = obj.legend\n        legend_func = obj.figure.legend\n    elif isinstance(obj, mpl.axes.Axes):\n        old_legend = obj.legend_\n        legend_func = obj.legend\n    elif isinstance(obj, mpl.figure.Figure):\n        if obj.legends:\n            old_legend = obj.legends[-1]\n        else:\n            old_legend = None\n        legend_func = obj.legend\n    else:\n        err = \"`obj` must be a seaborn Grid or matplotlib Axes or Figure instance.\"\n        raise TypeError(err)\n\n    if old_legend is None:\n        err = f\"{obj} has no legend attached.\"\n        raise ValueError(err)\n\n    # Extract the components of the legend we need to reuse\n    # Import here to avoid a circular import\n    from seaborn._compat import get_legend_handles\n    handles = get_legend_handles(old_legend)\n    labels = [t.get_text() for t in old_legend.get_texts()]\n\n    # Handle the case where the user is trying to override the labels\n    if (new_labels := kwargs.pop(\"labels\", None)) is not None:\n        if len(new_labels) != len(labels):\n            err = \"Length of new labels does not match existing legend.\"\n            raise ValueError(err)\n        labels = new_labels\n\n    # Extract legend properties that can be passed to the recreation method\n    # (Vexingly, these don't all round-trip)\n    legend_kws = inspect.signature(mpl.legend.Legend).parameters\n    props = {k: v for k, v in old_legend.properties().items() if k in legend_kws}\n\n    # Delegate default bbox_to_anchor rules to matplotlib\n    props.pop(\"bbox_to_anchor\")\n\n    # Try to propagate the existing title and font properties; respect new ones too\n    title = props.pop(\"title\")\n    if \"title\" in kwargs:\n        title.set_text(kwargs.pop(\"title\"))\n    title_kwargs = {k: v for k, v in kwargs.items() if k.startswith(\"title_\")}\n    for key, val in title_kwargs.items():\n        title.set(**{key[6:]: val})\n        kwargs.pop(key)\n\n    # Try to respect the frame visibility\n    kwargs.setdefault(\"frameon\", old_legend.legendPatch.get_visible())\n\n    # Remove the old legend and create the new one\n    props.update(kwargs)\n    old_legend.remove()\n    new_legend = legend_func(handles, labels, loc=loc, **props)\n    new_legend.set_title(title.get_text(), title.get_fontproperties())\n\n    # Let the Grid object continue to track the correct legend object\n    if isinstance(obj, Grid):\n        obj._legend = new_legend\n\n\ndef _kde_support(data, bw, gridsize, cut, clip):\n    \"\"\"Establish support for a kernel density estimate.\"\"\"\n    support_min = max(data.min() - bw * cut, clip[0])\n    support_max = min(data.max() + bw * cut, clip[1])\n    support = np.linspace(support_min, support_max, gridsize)\n\n    return support\n\n\ndef ci(a, which=95, axis=None):\n    \"\"\"Return a percentile range from an array of values.\"\"\"\n    p = 50 - which / 2, 50 + which / 2\n    return np.nanpercentile(a, p, axis)\n\n\ndef get_dataset_names():\n    \"\"\"Report available example datasets, useful for reporting issues.\n\n    Requires an internet connection.\n\n    \"\"\"\n    with urlopen(DATASET_NAMES_URL) as resp:\n        txt = resp.read()\n\n    dataset_names = [name.strip() for name in txt.decode().split(\"\\n\")]\n    return list(filter(None, dataset_names))\n\n\ndef get_data_home(data_home=None):\n    \"\"\"Return a path to the cache directory for example datasets.\n\n    This directory is used by :func:`load_dataset`.\n\n    If the ``data_home`` argument is not provided, it will use a directory\n    specified by the `SEABORN_DATA` environment variable (if it exists)\n    or otherwise default to an OS-appropriate user cache location.\n\n    \"\"\"\n    if data_home is None:\n        data_home = os.environ.get(\"SEABORN_DATA\", user_cache_dir(\"seaborn\"))\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n\ndef load_dataset(name, cache=True, data_home=None, **kws):\n    \"\"\"Load an example dataset from the online repository (requires internet).\n\n    This function provides quick access to a small number of example datasets\n    that are useful for documenting seaborn or generating reproducible examples\n    for bug reports. It is not necessary for normal usage.\n\n    Note that some of the datasets have a small amount of preprocessing applied\n    to define a proper ordering for categorical variables.\n\n    Use :func:`get_dataset_names` to see a list of available datasets.\n\n    Parameters\n    ----------\n    name : str\n        Name of the dataset (``{name}.csv`` on\n        https://github.com/mwaskom/seaborn-data).\n    cache : boolean, optional\n        If True, try to load from the local cache first, and save to the cache\n        if a download is required.\n    data_home : string, optional\n        The directory in which to cache data; see :func:`get_data_home`.\n    kws : keys and values, optional\n        Additional keyword arguments are passed to passed through to\n        :func:`pandas.read_csv`.\n\n    Returns\n    -------\n    df : :class:`pandas.DataFrame`\n        Tabular data, possibly with some preprocessing applied.\n\n    \"\"\"\n    # A common beginner mistake is to assume that one's personal data needs\n    # to be passed through this function to be usable with seaborn.\n    # Let's provide a more helpful error than you would otherwise get.\n    if isinstance(name, pd.DataFrame):\n        err = (\n            \"This function accepts only strings (the name of an example dataset). \"\n            \"You passed a pandas DataFrame. If you have your own dataset, \"\n            \"it is not necessary to use this function before plotting.\"\n        )\n        raise TypeError(err)\n\n    url = f\"{DATASET_SOURCE}/{name}.csv\"\n\n    if cache:\n        cache_path = os.path.join(get_data_home(data_home), os.path.basename(url))\n        if not os.path.exists(cache_path):\n            if name not in get_dataset_names():\n                raise ValueError(f\"'{name}' is not one of the example datasets.\")\n            urlretrieve(url, cache_path)\n        full_path = cache_path\n    else:\n        full_path = url\n\n    df = pd.read_csv(full_path, **kws)\n\n    if df.iloc[-1].isnull().all():\n        df = df.iloc[:-1]\n\n    # Set some columns as a categorical type with ordered levels\n\n    if name == \"tips\":\n        df[\"day\"] = pd.Categorical(df[\"day\"], [\"Thur\", \"Fri\", \"Sat\", \"Sun\"])\n        df[\"sex\"] = pd.Categorical(df[\"sex\"], [\"Male\", \"Female\"])\n        df[\"time\"] = pd.Categorical(df[\"time\"], [\"Lunch\", \"Dinner\"])\n        df[\"smoker\"] = pd.Categorical(df[\"smoker\"], [\"Yes\", \"No\"])\n\n    elif name == \"flights\":\n        months = df[\"month\"].str[:3]\n        df[\"month\"] = pd.Categorical(months, months.unique())\n\n    elif name == \"exercise\":\n        df[\"time\"] = pd.Categorical(df[\"time\"], [\"1 min\", \"15 min\", \"30 min\"])\n        df[\"kind\"] = pd.Categorical(df[\"kind\"], [\"rest\", \"walking\", \"running\"])\n        df[\"diet\"] = pd.Categorical(df[\"diet\"], [\"no fat\", \"low fat\"])\n\n    elif name == \"titanic\":\n        df[\"class\"] = pd.Categorical(df[\"class\"], [\"First\", \"Second\", \"Third\"])\n        df[\"deck\"] = pd.Categorical(df[\"deck\"], list(\"ABCDEFG\"))\n\n    elif name == \"penguins\":\n        df[\"sex\"] = df[\"sex\"].str.title()\n\n    elif name == \"diamonds\":\n        df[\"color\"] = pd.Categorical(\n            df[\"color\"], [\"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"],\n        )\n        df[\"clarity\"] = pd.Categorical(\n            df[\"clarity\"], [\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\"],\n        )\n        df[\"cut\"] = pd.Categorical(\n            df[\"cut\"], [\"Ideal\", \"Premium\", \"Very Good\", \"Good\", \"Fair\"],\n        )\n\n    elif name == \"taxis\":\n        df[\"pickup\"] = pd.to_datetime(df[\"pickup\"])\n        df[\"dropoff\"] = pd.to_datetime(df[\"dropoff\"])\n\n    elif name == \"seaice\":\n        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n\n    elif name == \"dowjones\":\n        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n\n    return df\n\n\ndef axis_ticklabels_overlap(labels):\n    \"\"\"Return a boolean for whether the list of ticklabels have overlaps.\n\n    Parameters\n    ----------\n    labels : list of matplotlib ticklabels\n\n    Returns\n    -------\n    overlap : boolean\n        True if any of the labels overlap.\n\n    \"\"\"\n    if not labels:\n        return False\n    try:\n        bboxes = [l.get_window_extent() for l in labels]\n        overlaps = [b.count_overlaps(bboxes) for b in bboxes]\n        return max(overlaps) > 1\n    except RuntimeError:\n        # Issue on macos backend raises an error in the above code\n        return False\n\n\ndef axes_ticklabels_overlap(ax):\n    \"\"\"Return booleans for whether the x and y ticklabels on an Axes overlap.\n\n    Parameters\n    ----------\n    ax : matplotlib Axes\n\n    Returns\n    -------\n    x_overlap, y_overlap : booleans\n        True when the labels on that axis overlap.\n\n    \"\"\"\n    return (axis_ticklabels_overlap(ax.get_xticklabels()),\n            axis_ticklabels_overlap(ax.get_yticklabels()))\n\n\ndef locator_to_legend_entries(locator, limits, dtype):\n    \"\"\"Return levels and formatted levels for brief numeric legends.\"\"\"\n    raw_levels = locator.tick_values(*limits).astype(dtype)\n\n    # The locator can return ticks outside the limits, clip them here\n    raw_levels = [l for l in raw_levels if l >= limits[0] and l <= limits[1]]\n\n    class dummy_axis:\n        def get_view_interval(self):\n            return limits\n\n    if isinstance(locator, mpl.ticker.LogLocator):\n        formatter = mpl.ticker.LogFormatter()\n    else:\n        formatter = mpl.ticker.ScalarFormatter()\n        # Avoid having an offset/scientific notation which we don't currently\n        # have any way of representing in the legend\n        formatter.set_useOffset(False)\n        formatter.set_scientific(False)\n    formatter.axis = dummy_axis()\n\n    formatted_levels = formatter.format_ticks(raw_levels)\n\n    return raw_levels, formatted_levels\n\n\ndef relative_luminance(color):\n    \"\"\"Calculate the relative luminance of a color according to W3C standards\n\n    Parameters\n    ----------\n    color : matplotlib color or sequence of matplotlib colors\n        Hex code, rgb-tuple, or html color name.\n\n    Returns\n    -------\n    luminance : float(s) between 0 and 1\n\n    \"\"\"\n    rgb = mpl.colors.colorConverter.to_rgba_array(color)[:, :3]\n    rgb = np.where(rgb <= .03928, rgb / 12.92, ((rgb + .055) / 1.055) ** 2.4)\n    lum = rgb.dot([.2126, .7152, .0722])\n    try:\n        return lum.item()\n    except ValueError:\n        return lum\n\n\ndef to_utf8(obj):\n    \"\"\"Return a string representing a Python object.\n\n    Strings (i.e. type ``str``) are returned unchanged.\n\n    Byte strings (i.e. type ``bytes``) are returned as UTF-8-decoded strings.\n\n    For other objects, the method ``__str__()`` is called, and the result is\n    returned as a string.\n\n    Parameters\n    ----------\n    obj : object\n        Any Python object\n\n    Returns\n    -------\n    s : str\n        UTF-8-decoded string representation of ``obj``\n\n    \"\"\"\n    if isinstance(obj, str):\n        return obj\n    try:\n        return obj.decode(encoding=\"utf-8\")\n    except AttributeError:  # obj is not bytes-like\n        return str(obj)\n\n\ndef _check_argument(param, options, value, prefix=False):\n    \"\"\"Raise if value for param is not in options.\"\"\"\n    if prefix and value is not None:\n        failure = not any(value.startswith(p) for p in options if isinstance(p, str))\n    else:\n        failure = value not in options\n    if failure:\n        raise ValueError(\n            f\"The value for `{param}` must be one of {options}, \"\n            f\"but {repr(value)} was passed.\"\n        )\n    return value\n\n\ndef _assign_default_kwargs(kws, call_func, source_func):\n    \"\"\"Assign default kwargs for call_func using values from source_func.\"\"\"\n    # This exists so that axes-level functions and figure-level functions can\n    # both call a Plotter method while having the default kwargs be defined in\n    # the signature of the axes-level function.\n    # An alternative would be to have a decorator on the method that sets its\n    # defaults based on those defined in the axes-level function.\n    # Then the figure-level function would not need to worry about defaults.\n    # I am not sure which is better.\n    needed = inspect.signature(call_func).parameters\n    defaults = inspect.signature(source_func).parameters\n\n    for param in needed:\n        if param in defaults and param not in kws:\n            kws[param] = defaults[param].default\n\n    return kws\n\n\ndef adjust_legend_subtitles(legend):\n    \"\"\"\n    Make invisible-handle \"subtitles\" entries look more like titles.\n\n    Note: This function is not part of the public API and may be changed or removed.\n\n    \"\"\"\n    # Legend title not in rcParams until 3.0\n    font_size = plt.rcParams.get(\"legend.title_fontsize\", None)\n    hpackers = legend.findobj(mpl.offsetbox.VPacker)[0].get_children()\n    for hpack in hpackers:\n        draw_area, text_area = hpack.get_children()\n        handles = draw_area.get_children()\n        if not all(artist.get_visible() for artist in handles):\n            draw_area.set_width(0)\n            for text in text_area.get_children():\n                if font_size is not None:\n                    text.set_size(font_size)\n\n\ndef _deprecate_ci(errorbar, ci):\n    \"\"\"\n    Warn on usage of ci= and convert to appropriate errorbar= arg.\n\n    ci was deprecated when errorbar was added in 0.12. It should not be removed\n    completely for some time, but it can be moved out of function definitions\n    (and extracted from kwargs) after one cycle.\n\n    \"\"\"\n    if ci is not deprecated and ci != \"deprecated\":\n        if ci is None:\n            errorbar = None\n        elif ci == \"sd\":\n            errorbar = \"sd\"\n        else:\n            errorbar = (\"ci\", ci)\n        msg = (\n            \"\\n\\nThe `ci` parameter is deprecated. \"\n            f\"Use `errorbar={repr(errorbar)}` for the same effect.\\n\"\n        )\n        warnings.warn(msg, FutureWarning, stacklevel=3)\n\n    return errorbar\n\n\ndef _get_transform_functions(ax, axis):\n    \"\"\"Return the forward and inverse transforms for a given axis.\"\"\"\n    axis_obj = getattr(ax, f\"{axis}axis\")\n    transform = axis_obj.get_transform()\n    return transform.transform, transform.inverted().transform\n\n\n@contextmanager\ndef _disable_autolayout():\n    \"\"\"Context manager for preventing rc-controlled auto-layout behavior.\"\"\"\n    # This is a workaround for an issue in matplotlib, for details see\n    # https://github.com/mwaskom/seaborn/issues/2914\n    # The only affect of this rcParam is to set the default value for\n    # layout= in plt.figure, so we could just do that instead.\n    # But then we would need to own the complexity of the transition\n    # from tight_layout=True -> layout=\"tight\". This seems easier,\n    # but can be removed when (if) that is simpler on the matplotlib side,\n    # or if the layout algorithms are improved to handle figure legends.\n    orig_val = mpl.rcParams[\"figure.autolayout\"]\n    try:\n        mpl.rcParams[\"figure.autolayout\"] = False\n        yield\n    finally:\n        mpl.rcParams[\"figure.autolayout\"] = orig_val\n\n\ndef _version_predates(lib: ModuleType, version: str) -> bool:\n    \"\"\"Helper function for checking version compatibility.\"\"\"\n    return Version(lib.__version__) < Version(version)\n\n\ndef _scatter_legend_artist(**kws):\n\n    kws = normalize_kwargs(kws, mpl.collections.PathCollection)\n\n    edgecolor = kws.pop(\"edgecolor\", None)\n    rc = mpl.rcParams\n    line_kws = {\n        \"linestyle\": \"\",\n        \"marker\": kws.pop(\"marker\", \"o\"),\n        \"markersize\": np.sqrt(kws.pop(\"s\", rc[\"lines.markersize\"] ** 2)),\n        \"markerfacecolor\": kws.pop(\"facecolor\", kws.get(\"color\")),\n        \"markeredgewidth\": kws.pop(\"linewidth\", 0),\n        **kws,\n    }\n\n    if edgecolor is not None:\n        if edgecolor == \"face\":\n            line_kws[\"markeredgecolor\"] = line_kws[\"markerfacecolor\"]\n        else:\n            line_kws[\"markeredgecolor\"] = edgecolor\n\n    return mpl.lines.Line2D([], [], **line_kws)\n\n\ndef _get_patch_legend_artist(fill):\n\n    def legend_artist(**kws):\n\n        color = kws.pop(\"color\", None)\n        if color is not None:\n            if fill:\n                kws[\"facecolor\"] = color\n            else:\n                kws[\"edgecolor\"] = color\n                kws[\"facecolor\"] = \"none\"\n\n        return mpl.patches.Rectangle((0, 0), 0, 0, **kws)\n\n    return legend_artist\n", "import_text": ["os", "inspect", "warnings", "colorsys", "contextlib.contextmanager", "urllib.request.urlopen", "urllib.request.urlretrieve", "types.ModuleType", "numpy", "pandas", "matplotlib", "matplotlib.colors.to_rgb", "matplotlib.pyplot", "matplotlib.cbook.normalize_kwargs", "seaborn._core.typing.deprecated", "seaborn.external.version.Version", "seaborn.external.appdirs.user_cache_dir"], "prompt": "\"\"\"\nDescription: This function desaturates a color by a given proportion.\n\nArgs:\n    color (str or tuple): The color to desaturate. It can be a string representing a color name, or a tuple of RGB values.\n    prop (float): The proportion by which to desaturate the color. It must be between 0 and 1.\n\nReturns:\n    tuple: The desaturated color as an RGB tuple.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Decrease the saturation channel of a color by some percent.\n\n    Parameters\n    ----------\n    color : matplotlib color\n        hex, rgb-tuple, or html color name\n    prop : float\n        saturation channel of color will be multiplied by this value\n\n    Returns\n    -------\n    new_color : rgb tuple\n        desaturated color code in RGB tuple representation\n\n    \"\"\"", "function_dependencies": ["matplotlib.colors.to_rgb", "colorsys.rgb_to_hls", "colorsys.hls_to_rgb"], "project_create_time": "2012-06-18T18:41:19+00:00", "project_update_time": "2024-04-18T02:55:57+00:00", "file_create_time": "2012-06-27T22:19:29Z", "file_update_time": "2023-12-30T23:55:56Z", "function_update_time": "2012-06-27T22:19:29Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["matplotlib.colors.to_rgb"], "test_function": [{"file_path": "/seaborn-v0.13.2/seaborn-0.13.2/tests/test_utils.py", "class_name": null, "function_name": "test_desaturate", "code": "def test_desaturate():\n    out1 = utils.desaturate(\"red\", .5)\n    assert out1 == (.75, .25, .25)\n\n    out2 = utils.desaturate(\"#00FF00\", .5)\n    assert out2 == (.25, .75, .25)\n\n    out3 = utils.desaturate((0, 0, 1), .5)\n    assert out3 == (.25, .25, .75)\n\n    out4 = utils.desaturate(\"red\", .5)\n    assert out4 == (.75, .25, .25)\n\n    out5 = utils.desaturate(\"lightblue\", 1)\n    assert out5 == mpl.colors.to_rgb(\"lightblue\")"}]}, {"git_group": "capitalone", "git_name": "DataProfiler", "version": "v0.4.5", "language": "Python", "project_name": "DataProfiler-v0.4.5.zip", "file_path": "/DataProfiler-v0.4.5/DataProfiler-0.4.5/dataprofiler/profilers/base_column_profilers.py", "file_name": "base_column_profilers.py", "focal_class": "BaseColumnProfiler", "focal_name": "_add_helper", "focal_parameter": ["other1", "other2"], "solution": "    def _add_helper(self, other1, other2):\n        if np.isnan(other1.col_index) and np.isnan(other2.col_index):\n            pass\n        elif other1.col_index == other2.col_index:\n            self.col_index = other1.col_index\n        else:\n            raise ValueError(\"Column indexes unmatched: {} != {}\"\n                             .format(other1.col_index, other2.col_index))\n        if other1.name == other2.name:\n            self.name = other1.name\n        else:\n            raise ValueError(\"Column names unmatched: {} != {}\"\n                             .format(other1.name, other2.name))\n\n        self.times = defaultdict(\n            float, {key: (other1.times.get(key, 0)\n                          + other2.times.get(key, 0)\n                          + self.times.get(key, 0))\n                    for key in (set(other1.times) | set(other2.times)\n                                | set(self.times))}\n        )\n\n        self.sample_size = other1.sample_size + other2.sample_size", "function_signature": "def _add_helper(self, other1, other2) :", "left_context": "#!/usr/bin/env python\n\"\"\"\ncoding=utf-8\n\nProfiles the data.\n\"\"\"\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport abc\nimport time\nfrom collections import defaultdict\nfrom future.utils import with_metaclass\nimport functools\nimport warnings\n\nimport numpy as np\n\n\nclass BaseColumnProfiler(with_metaclass(abc.ABCMeta, object)):\n    \"\"\"\n    Abstract class for profiling a column of data.\n    \"\"\"\n    col_type = None\n\n    # This specifies the minimum percent of elements in a column to meet the\n    # matching condition so that the column is classified as that type.\n    _COLUMN_MATCH_THRESHOLD = 0.9\n\n    _SAMPLING_RATIO = 0.20\n    _MIN_SAMPLING_COUNT = 500\n\n    def __init__(self, name):\n        \"\"\"\n        Initialization of base class properties for the subclass.\n\n        :param name: Name of the dataset\n        :type name: String\n        \"\"\"\n        self.name = name\n        self.col_index = np.nan\n        self.sample_size = 0\n        self.metadata = dict()\n        self.times = defaultdict(float)\n        self.thread_safe = True\n\n\n    @staticmethod\n    def _combine_unique_sets(a, b):  # TODO: Not needed for data labeling\n        \"\"\"\n        Method to union two lists.\n        :type a: list\n        :type b: list\n        :rtype: list\n        \"\"\"\n        combined_list = None\n        if not a and not b:\n            combined_list = list()\n        elif not a:\n            combined_list = set(b)\n        elif not b:\n            combined_list = set(a)\n        else:\n            combined_list = set().union(a,b)\n        combined_list = list(combined_list)        \n        return combined_list\n\n        \n    @staticmethod\n    def _timeit(method=None, name=None):\n        \"\"\"\n        Measure execution time of provided method\n        Records time into times dictionary\n\n        :param method: method to time\n        :type method: Callable\n        :param name: key argument for the times dictionary\n        :type name: str\n        \"\"\"\n\n        def decorator(method, name_dec=None):\n            @functools.wraps(method)\n            def wrapper(self, *args, **kw):\n                # necessary bc can't reassign external name\n                name_dec = name\n                if not name_dec:\n                    name_dec = method.__name__\n                ts = time.time()\n                result = method(self, *args, **kw)\n                te = time.time()\n                self.times[name_dec] += (te - ts)\n                return result\n\n            return wrapper\n\n        if callable(method):\n            return decorator(method, name_dec=name)\n        return decorator\n\n    @staticmethod\n    def _filter_properties_w_options(calculations, options):\n        \"\"\"\n        Cycles through the calculations and turns off the ones that are\n        disabled.\n\n        :param calculations: Contains all the column calculations.\n        :type calculations: Dict\n        :param options: Contains all the options.\n        :type options: BaseColumnOptions\n        \"\"\"\n        for prop in list(calculations):\n            if options and not options.is_prop_enabled(prop):\n                del calculations[prop]\n\n    def _perform_property_calcs(self, calculations, df_series,\n                                prev_dependent_properties, subset_properties):\n        \"\"\"\n        Cycles through the properties of the columns and calculate them.\n\n        :param calculations: Contains all the column calculations.\n        :type calculations: dict\n        :param df_series: Data to be profiled\n        :type df_series: pandas.Dataframe\n        :param prev_dependent_properties: Contains all the previous properties \n        that the calculations depend on.\n        :type prev_dependent_properties: dict\n        :param subset_properties: Contains the results of the properties of the\n        subset before they are merged into the main data profile.\n        :type subset_properties: dict\n        :return: None\n        \"\"\"\n        for prop in calculations:\n            calculations[prop](self,\n                               df_series,\n                               prev_dependent_properties,\n                               subset_properties)\n\n    @staticmethod\n    def _merge_calculations(merged_profile_calcs, profile1_calcs, profile2_calcs):\n        \"\"\"\n        Merges the calculations of two profiles to the lowest common\n        denominator.\n\n        :param merged_profile_calcs: default calculations of the merged profile\n        :type merged_profile_calcs: dict\n        :param profile1_calcs: calculations of profile1\n        :type profile1_calcs: dict\n        :param profile2_calcs: calculations of profile2\n        :type profile2_calcs: dict\n        :return: None\n        \"\"\"\n        calcs = list(merged_profile_calcs.keys())\n        for calc in calcs:\n            if calc not in profile1_calcs or calc not in profile2_calcs:\n                del merged_profile_calcs[calc]\n                if calc in profile1_calcs or calc in profile2_calcs:\n                    warnings.warn(\"{} is disabled because it is not enabled in \"\n                                  \"both profiles.\".format(calc), RuntimeWarning)\n", "right_context": "\n    def _update_column_base_properties(self, profile):\n        \"\"\"\n        Updates the base properties with the base schema.\n\n        :param profile: profile dictionary of data type\n        :type profile: dict\n        :return: None\n        \"\"\"\n        self.sample_size += profile.pop(\"sample_size\")\n        self.metadata = profile\n\n    def __getitem__(self, item):\n        \"\"\"\n        Override for the [] operator to allow access to class properties.\n        NOTE: Will be removed when switched over, only used as a method to\n        integrate with current setup.\n        \"\"\"\n        if not hasattr(self, item):\n            raise ValueError(\"The property '{} does not exist.\".format(item))\n        return getattr(self, item)\n\n    @abc.abstractmethod\n    def _update_helper(self, df_series_clean, profile):\n        \"\"\"\n        Private abstract method for updating the profile.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def update(self, df_series):\n        \"\"\"\n        Private abstract method for updating the profile.\n\n        :param df_series: Data to profile.\n        :type df_series: Pandas Dataframe\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    @abc.abstractmethod\n    def profile(self):\n        \"\"\"\n        Property for profile. Returns the profile of the column.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass BaseColumnPrimitiveTypeProfiler(with_metaclass(abc.ABCMeta,\n                                                     BaseColumnProfiler)):\n    \"\"\"\n    Abstract class for profiling the primative data type for a column of data.\n    \"\"\"\n\n    def __init__(self, name):\n        \"\"\"\n        Initialization of base class properties for the subclass.\n\n        :param name: Name of the data\n        :type name: String\n        \"\"\"\n        BaseColumnProfiler.__init__(self, name)\n        # Number of values that match the column type. eg. how many floats match\n        # in the float column\n        self.match_count = 0 \n\n    def _update_column_base_properties(self, profile):\n        \"\"\"\n        Updates the base properties with the base schema.\n\n        :param profile: profile containg base properties\n        :type profile: base data profile dict\n        :return: None\n        \"\"\"\n        self.match_count += profile.pop(\"match_count\")\n        BaseColumnProfiler. \\\n            _update_column_base_properties(self, profile)\n\n    def _add_helper(self, other1, other2):\n        \"\"\"\n        Merges the properties of two objects inputted\n\n        :param other1: first profile\n        :param other2: second profile\n        :type other1: BaseColumnPrimitiveTypeProfiler\n        :type other2: BaseColumnPrimitiveTypeProfiler\n        \"\"\"\n        BaseColumnProfiler._add_helper(self, other1, other2)\n        self.match_count = other1.match_count + other2.match_count\n", "import_text": ["abc", "time", "collections.defaultdict", "future.utils.with_metaclass", "functools", "warnings", "numpy"], "prompt": "\"\"\"\nDescription: This function is used to add two objects together. It checks if the column indexes and names of the two objects match, and if they do, it adds the times and sample sizes of the two objects together.\n\nArgs:\n    other1 (object): The first object to be added.\n    other2 (object): The second object to be added.\n\nRaises:\n    ValueError: If the column indexes or names of the two objects do not match.\n\nReturns:\n    None: The function modifies the object in place and does not return anything.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"\n        Merges the properties of two BaseColumnProfile objects\n\n        :param other1: first BaseColumn profile\n        :param other2: second BaseColumn profile\n        :type other1: BaseColumnProfiler\n        :type other2: BaseColumnProfiler\n        \"\"\"", "function_dependencies": ["numpy.isnan", "collections.defaultdict"], "project_create_time": "2020-11-09T15:20:21+00:00", "project_update_time": "2024-04-16T07:51:12+00:00", "file_create_time": "2021-02-18T21:04:45Z", "file_update_time": "2021-04-06T17:13:02Z", "function_update_time": "2021-02-18T21:04:45Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["numpy.isnan"], "test_function": [{"file_path": "/DataProfiler-v0.4.5/DataProfiler-0.4.5/dataprofiler/tests/profilers/test_base_column_profilers.py", "class_name": "TestBaseColumnProfileClass", "function_name": "test_add_helper", "code": "    def test_add_helper(self):\n        with patch.multiple(BaseColumnProfiler, __abstractmethods__=set()):\n            profile1 = BaseColumnProfiler(name=0)\n        profile1.sample_size = 2\n\n        with patch.multiple(BaseColumnProfiler, __abstractmethods__=set()):\n            profile2 = BaseColumnProfiler(name=0)\n        profile2.sample_size = 3\n\n        with patch.multiple(BaseColumnProfiler, __abstractmethods__=set()):\n            merged_profile = BaseColumnProfiler(name=0)\n\n        @BaseColumnProfiler._timeit\n        def test_time(self):\n            pass\n\n        @BaseColumnProfiler._timeit\n        def test_time2(self):\n            pass\n\n        # Dictionary starts empty\n        self.assertDictEqual({}, profile1.times)\n\n        # Array is popped twice per _timeit call start_time and end_time respectively\n        time_array = [float(i) for i in range(10, -1, -1)]\n        with patch('time.time', side_effect=lambda: time_array.pop()):\n            # add one entry to profile1.times\n            test_time(profile1)\n\n            # add one entry that is the same to profile2.times\n            test_time(profile2)\n            # add unique entry to profile2.times\n            test_time2(profile2)\n\n            merged_profile._add_helper(profile1, profile2)\n\n            # Ensure merge of times dict is done correctly\n            expected3 = {\"test_time\": 2.0, \"test_time2\": 1.0}\n            self.assertDictEqual(expected3, merged_profile.times)\n\n        self.assertEqual(merged_profile.sample_size, 5)\n\n        # Check for name alignment\n        self.assertEqual(merged_profile.name, profile2.name)\n\n        # Check for np.nan column index values for the merged profile\n        self.assertTrue(np.isnan(merged_profile.col_index))\n\n        # Check for same column index values\n        profile1.col_index = 0\n        profile2.col_index = 0\n        merged_profile._add_helper(profile1, profile2)\n        self.assertEqual(merged_profile.col_index, profile2.col_index)\n\n        # Check for different column index but same column name\n        with self.assertRaises(ValueError) as exc:\n            profile1.col_index = 1\n            profile1.name = \"test\"\n            profile2.col_index = 2\n            profile2.name = \"test\"\n            merged_profile._add_helper(profile1, profile2)\n\n        self.assertEqual(str(exc.exception),\n                         \"Column indexes unmatched: {} != {}\"\n                         .format(profile1.col_index, profile2.col_index))\n\n        # Check for different column name but same column index\n        with self.assertRaises(ValueError) as exc:\n            profile1.col_index = 0\n            profile1.name = \"test1\"\n            profile2.col_index = 0\n            profile2.name = \"test2\"\n            merged_profile._add_helper(profile1, profile2)\n\n        self.assertEqual(str(exc.exception),\n                         \"Column names unmatched: {} != {}\"\n                         .format(profile1.name, profile2.name))"}]}, {"git_group": "py-why", "git_name": "dowhy", "version": "v0.11.1", "language": "Python", "project_name": "dowhy-v0.11.1.zip", "file_path": "/dowhy-v0.11.1/dowhy-0.11.1/dowhy/gcm/auto.py", "file_name": "auto.py", "focal_class": null, "focal_name": "has_linear_relationship", "focal_parameter": [], "solution": "\ndef has_linear_relationship(X: np.ndarray, Y: np.ndarray, max_num_samples: int = 3000) -> bool:\n    X, Y = shape_into_2d(X, Y)\n\n    target_is_categorical = is_categorical(Y)\n    # Making sure there are at least 30% test samples.\n    num_trainings_samples = min(max_num_samples, round(X.shape[0] * 0.7))\n    num_test_samples = min(X.shape[0] - num_trainings_samples, max_num_samples)\n\n    if target_is_categorical:\n        all_classes, indices, counts = np.unique(Y, return_counts=True, return_index=True)\n        for i in range(all_classes.size):\n            # Making sure that there are at least 2 samples from one class (here, simply duplicate the point).\n            if counts[i] == 1:\n                X = np.row_stack([X, X[indices[i], :]])\n                Y = np.row_stack([Y, Y[indices[i], :]])\n\n        x_train, x_test, y_train, y_test = train_test_split(\n            X, Y, train_size=num_trainings_samples, test_size=num_test_samples, stratify=Y\n        )\n\n    else:\n        x_train, x_test, y_train, y_test = train_test_split(\n            X, Y, train_size=num_trainings_samples, test_size=num_test_samples\n        )\n\n    encoders = auto_fit_encoders(x_train, y_train)\n    x_train = auto_apply_encoders(x_train, encoders)\n    x_test = auto_apply_encoders(x_test, encoders)\n\n    if target_is_categorical:\n        linear_mdl = LogisticRegression(max_iter=1000)\n        nonlinear_mdl = create_hist_gradient_boost_classifier()\n        linear_mdl.fit(x_train, y_train.squeeze())\n        nonlinear_mdl.fit(x_train, y_train.squeeze())\n\n        # Compare number of correct classifications.\n        return np.sum(shape_into_2d(linear_mdl.predict(x_test)) == y_test) >= np.sum(\n            shape_into_2d(nonlinear_mdl.predict(x_test)) == y_test\n        )\n    else:\n        linear_mdl = LinearRegression()\n        nonlinear_mdl = create_hist_gradient_boost_regressor()\n        linear_mdl.fit(x_train, y_train.squeeze())\n        nonlinear_mdl.fit(x_train, y_train.squeeze())\n\n        return np.mean((y_test - shape_into_2d(linear_mdl.predict(x_test))) ** 2) <= np.mean(\n            (y_test - shape_into_2d(nonlinear_mdl.predict(x_test))) ** 2\n        )", "function_signature": "def has_linear_relationship(X: np.ndarray, Y: np.ndarray, max_num_samples: int = 3000) -> bool :", "left_context": "import warnings\nfrom enum import Enum, auto\nfrom functools import partial\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nfrom joblib import Parallel, delayed\nfrom sklearn import metrics\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nfrom dowhy.gcm import config\nfrom dowhy.gcm.causal_mechanisms import AdditiveNoiseModel, ClassifierFCM, DiscreteAdditiveNoiseModel\nfrom dowhy.gcm.causal_models import CAUSAL_MECHANISM, ProbabilisticCausalModel, validate_causal_model_assignment\nfrom dowhy.gcm.ml import (\n    ClassificationModel,\n    PredictionModel,\n    create_hist_gradient_boost_classifier,\n    create_hist_gradient_boost_regressor,\n    create_lasso_regressor,\n    create_linear_regressor,\n    create_logistic_regression_classifier,\n    create_random_forest_regressor,\n    create_ridge_regressor,\n    create_support_vector_regressor,\n)\nfrom dowhy.gcm.ml.classification import (\n    create_ada_boost_classifier,\n    create_extra_trees_classifier,\n    create_gaussian_nb_classifier,\n    create_knn_classifier,\n    create_polynom_logistic_regression_classifier,\n    create_random_forest_classifier,\n    create_support_vector_classifier,\n)\nfrom dowhy.gcm.ml.regression import (\n    create_ada_boost_regressor,\n    create_extra_trees_regressor,\n    create_knn_regressor,\n    create_polynom_regressor,\n)\nfrom dowhy.gcm.stochastic_models import EmpiricalDistribution\nfrom dowhy.gcm.util.general import (\n    auto_apply_encoders,\n    auto_fit_encoders,\n    is_categorical,\n    is_discrete,\n    set_random_seed,\n    shape_into_2d,\n)\nfrom dowhy.graph import get_ordered_predecessors, is_root_node\n\n_LIST_OF_POTENTIAL_CLASSIFIERS_GOOD = [\n    partial(create_logistic_regression_classifier, max_iter=10000),\n    create_hist_gradient_boost_classifier,\n]\n_LIST_OF_POTENTIAL_REGRESSORS_GOOD = [\n    create_linear_regressor,\n    create_hist_gradient_boost_regressor,\n]\n\n_LIST_OF_POTENTIAL_CLASSIFIERS_BETTER = _LIST_OF_POTENTIAL_CLASSIFIERS_GOOD + [\n    create_random_forest_classifier,\n    create_extra_trees_classifier,\n    create_support_vector_classifier,\n    create_knn_classifier,\n    create_gaussian_nb_classifier,\n    create_ada_boost_classifier,\n]\n_LIST_OF_POTENTIAL_REGRESSORS_BETTER = _LIST_OF_POTENTIAL_REGRESSORS_GOOD + [\n    create_ridge_regressor,\n    partial(create_lasso_regressor, max_iter=10000),\n    create_random_forest_regressor,\n    create_support_vector_regressor,\n    create_extra_trees_regressor,\n    create_knn_regressor,\n    create_ada_boost_regressor,\n]\n\n\nclass AssignmentQuality(Enum):\n    GOOD = auto()\n    BETTER = auto()\n    BEST = auto()\n\n\nclass AutoAssignmentSummary:\n    \"\"\"Summary class for logging and storing information of the auto assignment process.\"\"\"\n\n    def __init__(self):\n        self._nodes: Dict[Dict[Any, Any]] = {}\n\n    def _init_node_entry(self, node: Any):\n        if node not in self._nodes:\n            self._nodes[node] = {\"messages\": [], \"model_performances\": []}\n\n    def add_node_log_message(self, node: Any, message: str):\n        self._init_node_entry(node)\n\n        self._nodes[node][\"messages\"].append(message)\n\n    def add_model_performance(self, node, model: str, performance: str, metric_name: str):\n        self._nodes[node][\"model_performances\"].append((model, performance, metric_name))\n\n    def __str__(self):\n        summary_strings = []\n\n        summary_strings.append(\n            \"When using this auto assignment function, the given data is used to automatically assign a causal \"\n            \"mechanism to each node. Note that causal mechanisms can also be customized and assigned manually.\\n\"\n            \"The following types of causal mechanisms are considered for the automatic selection:\"\n        )\n        summary_strings.append(\"\\nIf root node:\")\n        summary_strings.append(\n            \"An empirical distribution, i.e., the distribution is represented by randomly sampling from the provided \"\n            \"data. This provides a flexible and non-parametric way to model the marginal distribution and is valid for \"\n            \"all types of data modalities.\"\n        )\n        summary_strings.append(\"\\nIf non-root node and the data is continuous:\")\n        summary_strings.append(\n            \"Additive Noise Models (ANM) of the form X_i = f(PA_i) + N_i, where PA_i are the \"\n            \"parents of X_i and the unobserved noise N_i is assumed to be independent of PA_i.\"\n            \"To select the best model for f, different regression models are evaluated and the model \"\n            \"with the smallest mean squared error is selected.\"\n            \"Note that minimizing the mean squared error here is equivalent to selecting the best \"\n            \"choice of an ANM.\"\n        )\n        summary_strings.append(\"\\nIf non-root node and the data is discrete:\")\n        summary_strings.append(\n            \"Discrete Additive Noise Models have almost the same definition as non-discrete ANMs, but come with an \"\n            \"additional constraint for f to only return discrete values.\\n\"\n            \"Note that 'discrete' here refers to numerical values with an order. If the data is categorical, consider \"\n            \"representing them as strings to ensure proper model selection.\"\n        )\n        summary_strings.append(\"\\nIf non-root node and the data is categorical:\")\n        summary_strings.append(\n            \"A functional causal model based on a classifier, i.e., X_i = f(PA_i, N_i).\\n\"\n            \"Here, N_i follows a uniform distribution on [0, 1] and is used to randomly sample a \"\n            \"class (category) using the conditional probability distribution produced by a \"\n            \"classification model.\"\n            \"Here, different model classes are evaluated using the (negative) F1 score and the best\"\n            \" performing model class is selected.\"\n        )\n        summary_strings.append(\"\\nIn total, %d nodes were analyzed:\" % len(list(self._nodes)))\n\n        for node in self._nodes:\n            summary_strings.append(\"\\n--- Node: %s\" % node)\n            summary_strings.extend(self._nodes[node][\"messages\"])\n\n            if len(self._nodes[node][\"model_performances\"]) > 0:\n                summary_strings.append(\n                    \"For the model selection, the following models were evaluated on the %s metric:\"\n                    % self._nodes[node][\"model_performances\"][0][2]\n                )\n\n                for (model, performance, metric_name) in self._nodes[node][\"model_performances\"]:\n                    summary_strings.append(\"%s: %s\" % (str(model()).replace(\"()\", \"\"), str(performance)))\n\n        summary_strings.append(\n            \"\\n===Note===\\nNote, based on the selected auto assignment quality, the set of \" \"evaluated models changes.\"\n        )\n        summary_strings.append(\n            \"For more insights toward the quality of the fitted graphical causal model, consider \"\n            \"using the evaluate_causal_model function after fitting the causal mechanisms.\"\n        )\n        return \"\\n\".join(summary_strings)\n\n\ndef assign_causal_mechanisms(\n    causal_model: ProbabilisticCausalModel,\n    based_on: pd.DataFrame,\n    quality: AssignmentQuality = AssignmentQuality.GOOD,\n    override_models: bool = False,\n) -> AutoAssignmentSummary:\n    \"\"\"Automatically assigns appropriate causal mechanisms to nodes. If causal mechanisms are already assigned to nodes\n    and override_models is set to False, this function only validates the assignments with respect to the graph\n    structure. This is, the validation checks whether root nodes have StochasticModels and non-root\n    ConditionalStochasticModels assigned.\n\n    The following types of causal mechanisms are considered for the automatic selection:\n\n    If root node:\n    An empirical distribution, i.e., the distribution is represented by randomly sampling from the provided data.\n    This provides a flexible and non-parametric way to model the marginal distribution and is valid for all types of\n    data modalities.\n\n    If non-root node and the data is continuous:\n    Additive Noise Models (ANM) of the form X_i = f(PA_i) + N_i, where PA_i are the parents of X_i and the unobserved\n    noise N_i is assumed to be independent of PA_i. To select the best model for f, different regression models are\n    evaluated and the model with the smallest mean squared error is selected. Note that minimizing the mean squared\n    error here is equivalent to selecting the best choice of an ANM. See the following paper for more details:\n        Hoyer, P., Janzing, D., Mooij, J. M., Peters, J., & Sch\u00f6lkopf, B. (2008).\n        Nonlinear causal discovery with additive noise models.\n        Advances in neural information processing systems, 21\n\n    If non-root node and the data is discrete:\n    Discrete Additive Noise Models have almost the same definition as non-discrete ANMs, but come with an additional\n    constraint to return discrete values. Note that 'discrete' here refers to numerical values with an order. If the\n    data is categorical, consider representing them as strings to ensure proper model selection. See the following\n    paper for more details:\n        Peters, J., Janzing, D., & Scholkopf, B. (2011).\n        Causal inference on discrete data using additive noise models.\n        IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(12), 2436-2450.\n\n    If non-root node and the data is categorical:\n    A functional causal model based on a classifier, i.e., X_i = f(PA_i, N_i).\n    Here, N_i follows a uniform distribution on [0, 1] and is used to randomly sample a class (category) using the\n    conditional probability distribution produced by a classification model. Here, different model classes are evaluated\n    using the (negative) F1 score and the best performing model class is selected.\n\n    The current model zoo is:\n\n    With \"GOOD\" quality:\n        Numerical:\n        - Linear Regressor\n        - Linear Regressor with polynomial features\n        - Histogram Gradient Boost Regressor\n\n        Categorical:\n        - Logistic Regressor\n        - Logistic Regressor with polynomial features\n        - Histogram Gradient Boost Classifier\n\n    With \"BETTER\" quality:\n        Numerical:\n        - Linear Regressor\n        - Linear Regressor with polynomial features\n        - Gradient Boost Regressor\n        - Ridge Regressor\n        - Lasso Regressor\n        - Random Forest Regressor\n        - Support Vector Regressor\n        - Extra Trees Regressor\n        - KNN Regressor\n        - Ada Boost Regressor\n\n        Categorical:\n        - Logistic Regressor\n        - Logistic Regressor with polynomial features\n        - Histogram Gradient Boost Classifier\n        - Random Forest Classifier\n        - Extra Trees Classifier\n        - Support Vector Classifier\n        - KNN Classifier\n        - Gaussian Naive Bayes Classifier\n        - Ada Boost Classifier\n\n    With \"BEST\" quality:\n    An auto ML model based on AutoGluon (optional dependency, needs to be installed).\n\n    :param causal_model: The causal model to whose nodes to assign causal models.\n    :param based_on: Jointly sampled data corresponding to the nodes of the given graph.\n    :param quality: AssignmentQuality for the automatic model selection and model accuracy. This changes the type of\n                    prediction model and time spent on the selection. See the docstring for a list of potential models.\n                    The options for the quality are:\n        - AssignmentQuality.GOOD: Only a small set of models are evaluated.\n            Model selection speed: Fast\n            Model training speed: Fast\n            Model inference speed: Fast\n            Model accuracy: Medium\n        - AssignmentQuality.BETTER: A larger set of models are evaluated.\n            Model selection speed: Medium\n            Model training speed: Fast\n            Model inference speed: Fast\n            Model accuracy: Good\n        - AssignmentQuality.BEST: Uses an AutoGluon (auto ML) model with default settings defined by the AutoGluon\n            wrapper. While the model selection itself is fast, the training and inference speed can be significantly\n            slower than in the other options. NOTE: This requires the optional autogluon.tabular dependency.\n            Model selection speed: Instant\n            Model training speed: Slow\n            Model inference speed: Slow-Medium\n            Model accuracy: Best\n    :param override_models: If set to True, existing mechanism assignments are replaced with automatically selected\n                            ones. If set to False, the assigned mechanisms are only validated with respect to the graph\n                            structure.\n    :return: A summary object containing details about the model selection process.\n    \"\"\"\n    auto_assignment_summary = AutoAssignmentSummary()\n\n    for node in nx.topological_sort(causal_model.graph):\n        if not override_models and CAUSAL_MECHANISM in causal_model.graph.nodes[node]:\n            auto_assignment_summary.add_node_log_message(\n                node,\n                \"Node %s already has a causal mechanism assigned and the override parameter is False. Skipping this \"\n                \"node.\" % node,\n            )\n            validate_causal_model_assignment(causal_model.graph, node)\n            continue\n\n        model_performances = assign_causal_mechanism_node(causal_model, node, based_on, quality)\n\n        if is_root_node(causal_model.graph, node):\n            auto_assignment_summary.add_node_log_message(\n                node,\n                \"Node %s is a root node. Therefore, assigning '%s' to the node representing the marginal distribution.\"\n                % (node, causal_model.causal_mechanism(node)),\n            )\n        else:\n            data_type = \"continuous\"\n            if isinstance(causal_model.causal_mechanism(node), ClassifierFCM):\n                data_type = \"categorical\"\n            elif isinstance(causal_model.causal_mechanism(node), DiscreteAdditiveNoiseModel):\n                data_type = \"discrete\"\n\n            auto_assignment_summary.add_node_log_message(\n                node,\n                \"Node %s is a non-root node with %s data. Assigning '%s' to the node.\"\n                % (\n                    node,\n                    data_type,\n                    causal_model.causal_mechanism(node),\n                ),\n            )\n\n        if isinstance(causal_model.causal_mechanism(node), DiscreteAdditiveNoiseModel):\n            auto_assignment_summary.add_node_log_message(\n                node,\n                \"This represents the discrete causal relationship as \"\n                + str(node)\n                + \" := f(\"\n                + \",\".join([str(parent) for parent in get_ordered_predecessors(causal_model.graph, node)])\n                + \") + N.\",\n            )\n        elif isinstance(causal_model.causal_mechanism(node), AdditiveNoiseModel):\n            auto_assignment_summary.add_node_log_message(\n                node,\n                \"This represents the causal relationship as \"\n                + str(node)\n                + \" := f(\"\n                + \",\".join([str(parent) for parent in get_ordered_predecessors(causal_model.graph, node)])\n                + \") + N.\",\n            )\n        elif isinstance(causal_model.causal_mechanism(node), ClassifierFCM):\n            auto_assignment_summary.add_node_log_message(\n                node,\n                \"This represents the causal relationship as \"\n                + str(node)\n                + \" := f(\"\n                + \",\".join([str(parent) for parent in get_ordered_predecessors(causal_model.graph, node)])\n                + \",N).\",\n            )\n\n        for (model, performance, metric_name) in model_performances:\n            auto_assignment_summary.add_model_performance(node, model, performance, metric_name)\n\n    return auto_assignment_summary\n\n\ndef assign_causal_mechanism_node(\n    causal_model: ProbabilisticCausalModel, node: str, based_on: pd.DataFrame, quality: AssignmentQuality\n) -> List[Tuple[Callable[[], PredictionModel], float, str]]:\n    if is_root_node(causal_model.graph, node):\n        causal_model.set_causal_mechanism(node, EmpiricalDistribution())\n        model_performances = []\n    else:\n        node_data = based_on[node].to_numpy()\n\n        best_model, model_performances = select_model(\n            based_on[get_ordered_predecessors(causal_model.graph, node)].to_numpy(),\n            node_data,\n            quality,\n        )\n\n        if isinstance(best_model, ClassificationModel):\n            causal_model.set_causal_mechanism(node, ClassifierFCM(best_model))\n        else:\n            if is_discrete(node_data):\n                causal_model.set_causal_mechanism(node, DiscreteAdditiveNoiseModel(best_model))\n            else:\n                causal_model.set_causal_mechanism(node, AdditiveNoiseModel(best_model))\n\n    return model_performances\n\n\ndef select_model(\n    X: np.ndarray, Y: np.ndarray, model_selection_quality: AssignmentQuality\n) -> Tuple[Union[PredictionModel, ClassificationModel], List[Tuple[Callable[[], PredictionModel], float, str]]]:\n    if model_selection_quality == AssignmentQuality.BEST:\n        try:\n            from dowhy.gcm.ml.autogluon import AutoGluonClassifier, AutoGluonRegressor\n\n            if is_categorical(Y):\n                return AutoGluonClassifier(), []\n            else:\n                return AutoGluonRegressor(), []\n        except ImportError:\n            raise RuntimeError(\n                \"AutoGluon module not found! For the BEST auto assign quality, consider installing the \"\n                \"optional AutoGluon dependency.\"\n            )\n    elif model_selection_quality == AssignmentQuality.GOOD:\n        list_of_regressor = list(_LIST_OF_POTENTIAL_REGRESSORS_GOOD)\n        list_of_classifier = list(_LIST_OF_POTENTIAL_CLASSIFIERS_GOOD)\n        model_selection_splits = 5\n    elif model_selection_quality == AssignmentQuality.BETTER:\n        list_of_regressor = list(_LIST_OF_POTENTIAL_REGRESSORS_BETTER)\n        list_of_classifier = list(_LIST_OF_POTENTIAL_CLASSIFIERS_BETTER)\n        model_selection_splits = 5\n    else:\n        raise ValueError(\"Invalid model selection quality.\")\n\n    if auto_apply_encoders(X, auto_fit_encoders(X)).shape[1] <= 5:\n        # Avoid too many features\n        list_of_regressor += [create_polynom_regressor]\n        list_of_classifier += [partial(create_polynom_logistic_regression_classifier, max_iter=10000)]\n\n    if is_categorical(Y):\n        best_model, model_performances = find_best_model(\n            list_of_classifier, X, Y, model_selection_splits=model_selection_splits\n        )\n        return best_model(), model_performances\n    else:\n        best_model, model_performances = find_best_model(\n            list_of_regressor, X, Y, model_selection_splits=model_selection_splits\n        )\n        return best_model(), model_performances\n\n", "right_context": "\n\ndef find_best_model(\n    prediction_model_factories: List[Callable[[], PredictionModel]],\n    X: np.ndarray,\n    Y: np.ndarray,\n    metric: Optional[Callable[[np.ndarray, np.ndarray], float]] = None,\n    max_samples_per_split: int = 20000,\n    model_selection_splits: int = 5,\n    n_jobs: Optional[int] = None,\n) -> Tuple[Callable[[], PredictionModel], List[Tuple[Callable[[], PredictionModel], float, str]]]:\n    n_jobs = config.default_n_jobs if n_jobs is None else n_jobs\n\n    X, Y = shape_into_2d(X, Y)\n\n    is_classification_problem = isinstance(prediction_model_factories[0](), ClassificationModel)\n\n    metric_name = \"given\"\n\n    if metric is None:\n        metric_name = \"(negative) F1\"\n        if is_classification_problem:\n            metric = lambda y_true, y_preds: -metrics.f1_score(\n                y_true, y_preds, average=\"macro\", zero_division=0\n            )  # Higher score is better\n        else:\n            metric_name = \"mean squared error (MSE)\"\n            metric = metrics.mean_squared_error\n\n    labelBinarizer = None\n    if is_classification_problem:\n        labelBinarizer = MultiLabelBinarizer()\n        labelBinarizer.fit(Y)\n\n    if is_classification_problem:\n        if len(np.unique(Y)) == 1:\n            raise ValueError(\n                \"The given target samples have only one class! To fit a classification model, there \"\n                \"should be at least two classes.\"\n            )\n        kfolds = list(StratifiedKFold(n_splits=model_selection_splits, shuffle=True).split(X, Y))\n    else:\n        kfolds = list(KFold(n_splits=model_selection_splits, shuffle=True).split(range(X.shape[0])))\n\n    def estimate_average_score(prediction_model_factory: Callable[[], PredictionModel], random_seed: int) -> float:\n        set_random_seed(random_seed)\n\n        average_result = []\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n            for train_indices, test_indices in kfolds:\n                if is_classification_problem and len(np.unique(Y[train_indices[:max_samples_per_split]])) == 1:\n                    continue\n\n                model_instance = prediction_model_factory()\n                model_instance.fit(X[train_indices[:max_samples_per_split]], Y[train_indices[:max_samples_per_split]])\n\n                y_true = Y[test_indices[:max_samples_per_split]]\n                y_pred = model_instance.predict(X[test_indices[:max_samples_per_split]])\n                if labelBinarizer is not None:\n                    y_true = labelBinarizer.transform(y_true)\n                    y_pred = labelBinarizer.transform(y_pred)\n\n                average_result.append(metric(y_true, y_pred))\n\n        if len(average_result) == 0:\n            return float(\"inf\")\n        else:\n            return float(np.mean(average_result))\n\n    random_seeds = np.random.randint(np.iinfo(np.int32).max, size=len(prediction_model_factories))\n    average_metric_scores = Parallel(n_jobs=n_jobs)(\n        delayed(estimate_average_score)(prediction_model_factory, int(random_seed))\n        for prediction_model_factory, random_seed in zip(prediction_model_factories, random_seeds)\n    )\n    sorted_results = sorted(\n        zip(prediction_model_factories, average_metric_scores, [metric_name] * len(prediction_model_factories)),\n        key=lambda x: x[1],\n    )\n\n    return sorted_results[0][0], sorted_results\n", "import_text": ["warnings", "enum.Enum", "enum.auto", "functools.partial", "typing.Any", "typing.Callable", "typing.Dict", "typing.List", "typing.Optional", "typing.Tuple", "typing.Union", "networkx", "numpy", "pandas", "joblib.Parallel", "joblib.delayed", "sklearn.metrics", "sklearn.exceptions.ConvergenceWarning", "sklearn.linear_model.LinearRegression", "sklearn.linear_model.LogisticRegression", "sklearn.model_selection.KFold", "sklearn.model_selection.StratifiedKFold", "sklearn.model_selection.train_test_split", "sklearn.preprocessing.MultiLabelBinarizer", "dowhy.gcm.config", "dowhy.gcm.causal_mechanisms.AdditiveNoiseModel", "dowhy.gcm.causal_mechanisms.ClassifierFCM", "dowhy.gcm.causal_mechanisms.DiscreteAdditiveNoiseModel", "dowhy.gcm.causal_models.CAUSAL_MECHANISM", "dowhy.gcm.causal_models.ProbabilisticCausalModel", "dowhy.gcm.causal_models.validate_causal_model_assignment", "dowhy.gcm.ml.ClassificationModel", "dowhy.gcm.ml.PredictionModel", "dowhy.gcm.ml.create_hist_gradient_boost_classifier", "dowhy.gcm.ml.create_hist_gradient_boost_regressor", "dowhy.gcm.ml.create_lasso_regressor", "dowhy.gcm.ml.create_linear_regressor", "dowhy.gcm.ml.create_logistic_regression_classifier", "dowhy.gcm.ml.create_random_forest_regressor", "dowhy.gcm.ml.create_ridge_regressor", "dowhy.gcm.ml.create_support_vector_regressor", "dowhy.gcm.ml.classification.create_ada_boost_classifier", "dowhy.gcm.ml.classification.create_extra_trees_classifier", "dowhy.gcm.ml.classification.create_gaussian_nb_classifier", "dowhy.gcm.ml.classification.create_knn_classifier", "dowhy.gcm.ml.classification.create_polynom_logistic_regression_classifier", "dowhy.gcm.ml.classification.create_random_forest_classifier", "dowhy.gcm.ml.classification.create_support_vector_classifier", "dowhy.gcm.ml.regression.create_ada_boost_regressor", "dowhy.gcm.ml.regression.create_extra_trees_regressor", "dowhy.gcm.ml.regression.create_knn_regressor", "dowhy.gcm.ml.regression.create_polynom_regressor", "dowhy.gcm.stochastic_models.EmpiricalDistribution", "dowhy.gcm.util.general.auto_apply_encoders", "dowhy.gcm.util.general.auto_fit_encoders", "dowhy.gcm.util.general.is_categorical", "dowhy.gcm.util.general.is_discrete", "dowhy.gcm.util.general.set_random_seed", "dowhy.gcm.util.general.shape_into_2d", "dowhy.graph.get_ordered_predecessors", "dowhy.graph.is_root_node"], "prompt": "\"\"\"\nDescription: This function checks if there is a linear relationship between two arrays X and Y.\n\nArgs:\n    X (np.ndarray): The first array to check for linear relationship.\n    Y (np.ndarray): The second array to check for linear relationship.\n    max_num_samples (int): The maximum number of samples to use for training and testing.\n\nReturns:\n    bool: True if the linear model performs better than the non-linear model, False otherwise.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["dowhy.gcm.util.general.shape_into_2d", "dowhy.gcm.util.general.is_categorical", "numpy.unique", "numpy.row_stack", "sklearn.model_selection.train_test_split", "dowhy.gcm.util.general.auto_fit_encoders", "dowhy.gcm.util.general.auto_apply_encoders", "sklearn.linear_model.LogisticRegression", "dowhy.gcm.ml.create_hist_gradient_boost_classifier", "sklearn.linear_model.LogisticRegression.fit", "dowhy.gcm.ml.create_hist_gradient_boost_classifier.fit", "numpy.sum", "sklearn.linear_model.LogisticRegression.predict", "dowhy.gcm.ml.create_hist_gradient_boost_classifier.predict", "sklearn.linear_model.LinearRegression", "dowhy.gcm.ml.create_hist_gradient_boost_regressor", "sklearn.linear_model.LinearRegression.fit", "dowhy.gcm.ml.create_hist_gradient_boost_regressor.fit", "numpy.mean", "sklearn.linear_model.LinearRegression.predict", "dowhy.gcm.ml.create_hist_gradient_boost_regressor.predict"], "project_create_time": "2018-05-31T13:07:04+00:00", "project_update_time": "2024-04-17T23:56:14+00:00", "file_create_time": "2022-06-05T17:52:48Z", "file_update_time": "2023-11-22T18:46:30Z", "function_update_time": "2022-10-25T23:41:23Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["numpy.unique", "sklearn.model_selection.train_test_split", "sklearn.linear_model.LogisticRegression"], "test_function": [{"file_path": "/dowhy-v0.11.1/dowhy-0.11.1/tests/gcm/test_auto.py", "class_name": null, "function_name": "test_given_imbalanced_categorical_data_when_calling_has_linear_relationship_then_does_not_raise_exception", "code": "\ndef test_given_imbalanced_categorical_data_when_calling_has_linear_relationship_then_does_not_raise_exception():\n    X = np.random.normal(0, 1, 1000)\n    Y = np.array([\"OneClass\"] * 1000)\n\n    assert has_linear_relationship(np.append(X, 0), np.append(Y, \"RareClass\"))\n\n    X = np.random.normal(0, 1, 100000)\n    Y = np.array([\"OneClass\"] * 100000)\n\n    assert has_linear_relationship(\n        np.append(X, np.random.normal(0, 0.000001, 100)), np.append(Y, np.array([\"RareClass\"] * 100))\n    )"}]}, {"git_group": "snarfed", "git_name": "bridgy", "version": "main", "language": "Python", "project_name": "bridgy-main.zip", "file_path": "/bridgy-main/bridgy-main/superfeedr.py", "file_name": "superfeedr.py", "focal_class": null, "focal_name": "subscribe", "focal_parameter": ["source"], "solution": "def subscribe(source):\n  if appengine_info.LOCAL_SERVER:\n    logger.info('Running locally, not subscribing to Superfeedr')\n    return\n\n  data = {\n    'hub.mode': 'subscribe',\n    'hub.topic': source.feed_url(),\n    'hub.callback': util.host_url(f'/{source.SHORT_NAME}/notify/{source.key_id()}'),\n    # TODO\n    # 'hub.secret': 'xxx',\n    'format': 'json',\n    'retrieve': 'true',\n  }\n\n  logger.info(f'Adding Superfeedr subscription: {data}')\n  resp = util.requests_post(\n    PUSH_API_URL, data=data,\n    auth=HTTPBasicAuth(SUPERFEEDR_USERNAME, SUPERFEEDR_TOKEN))\n  resp.raise_for_status()\n\n  handle_feed(resp.json(), source)", "function_signature": "def subscribe(source) :", "left_context": "\"\"\"Superfeedr.\n\n* https://superfeedr.com/users/snarfed\n* http://documentation.superfeedr.com/subscribers.html\n* http://documentation.superfeedr.com/schema.html\n\"\"\"\nimport logging\n\nfrom flask import request\nfrom flask.views import View\nfrom google.cloud.ndb.key import _MAX_KEYPART_BYTES\nfrom google.cloud.ndb._datastore_types import _MAX_STRING_LENGTH\nfrom oauth_dropins.webutil import appengine_info\nfrom requests.auth import HTTPBasicAuth\n\nimport models\nimport util\n\nlogger = logging.getLogger(__name__)\n\nSUPERFEEDR_TOKEN = util.read('superfeedr_token')\nSUPERFEEDR_USERNAME = util.read('superfeedr_username')\nPUSH_API_URL = 'https://push.superfeedr.com'\nMAX_BLOGPOST_LINKS = 10\nTRANSIENT_ERROR_HTTP_CODES = ('500', '501', '502', '503', '429')\n", "right_context": "\n\ndef handle_feed(feed, source):\n  \"\"\"Handles a Superfeedr JSON feed.\n\n  Creates :class:`models.BlogPost` entities and adds propagate-blogpost tasks\n  for new items.\n\n  * http://documentation.superfeedr.com/schema.html#json\n  * http://documentation.superfeedr.com/subscribers.html#pubsubhubbubnotifications\n\n  Args:\n    feed (str): Superfeedr JSON feed\n    source (Blogger, Tumblr, or WordPress)\n  \"\"\"\n  logger.info(f'Source: {source.label()} {source.key_id()}')\n  logger.info(f'Raw feed: {feed}')\n\n  if not feed:\n    return\n\n  if source.status != 'enabled':\n    logger.info(f'Dropping because source is {source.status}')\n    return\n  elif 'webmention' not in source.features:\n    logger.info(\"Dropping because source doesn't have webmention feature\")\n    return\n\n  for item in feed.get('items', []):\n    url = item.get('permalinkUrl') or item.get('id')\n    if not url:\n      logger.error('Dropping feed item without permalinkUrl or id!')\n      continue\n\n    # extract links from content, discarding self links.\n    #\n    # i don't use get_webmention_target[s]() here because they follows redirects\n    # and fetch link contents, and this handler should be small and fast and try\n    # to return a response to superfeedr successfully.\n    content = item.get('content') or item.get('summary', '')\n    links = [util.clean_url(util.unwrap_t_umblr_com(url))\n             for url in util.extract_links(content)\n             if util.domain_from_link(url) not in source.domains]\n\n    unique = []\n    for link in util.dedupe_urls(links):\n      if len(link) <= _MAX_STRING_LENGTH:\n        unique.append(link)\n      else:\n        logger.info(f'Giving up on link over {_MAX_STRING_LENGTH} chars! {link}')\n      if len(unique) >= MAX_BLOGPOST_LINKS:\n        logger.info('Stopping at 10 links! Skipping the rest.')\n        break\n\n    logger.info(f'Found links: {unique}')\n    if len(url) > _MAX_KEYPART_BYTES:\n      logger.warning('Blog post URL is too long (over 500 chars)! Giving up.')\n      bp = models.BlogPost(id=url[:_MAX_KEYPART_BYTES], source=source.key,\n                           feed_item=item, failed=unique)\n    else:\n      bp = models.BlogPost(id=url, source=source.key, feed_item=item, unsent=unique)\n\n    bp.get_or_save()\n\n\nclass Notify(View):\n  \"\"\"Handles a Superfeedr notification.\n\n  Abstract; subclasses must set the :attr:`SOURCE_CLS` attr.\n\n  http://documentation.superfeedr.com/subscribers.html#pubsubhubbubnotifications\n  \"\"\"\n  SOURCE_CLS = None\n\n  def dispatch_request(self, id):\n    source = self.SOURCE_CLS.get_by_id(id)\n    if source:\n      handle_feed(request.json, source)\n\n    return ''\n", "import_text": ["logging", "flask.request", "flask.views.View", "google.cloud.ndb.key._MAX_KEYPART_BYTES", "google.cloud.ndb._datastore_types._MAX_STRING_LENGTH", "oauth_dropins.webutil.appengine_info", "requests.auth.HTTPBasicAuth", "models", "util"], "prompt": "\"\"\"\nDescription: This function is used to subscribe to a source using the Superfeedr API.\n\nArgs:\n    source (type): The source to subscribe to.\n\nReturns:\n    None: This function does not return anything.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "  \"\"\"Subscribes to a source.\n\n  Also receives some past posts and adds propagate tasks for them.\n\n  http://documentation.superfeedr.com/subscribers.html#addingfeedswithpubsubhubbub\n\n  Args:\n    source (Blogger Tumblr, or WordPress)\n  \"\"\"", "function_dependencies": ["util.host_url", "util.requests_post", "requests.auth.HTTPBasicAuth", "util.requests_post.raise_for_status", "util.requests_post.json"], "project_create_time": "2011-12-12T22:33:45+00:00", "project_update_time": "2024-04-03T09:35:09+00:00", "file_create_time": "2014-05-02T14:41:03Z", "file_update_time": "2023-10-06T05:12:17Z", "function_update_time": "2021-08-15T19:16:08Z", "license": {"key": "cc0-1.0", "name": "Creative Commons Zero v1.0 Universal", "spdx_id": "CC0-1.0", "url": "https://api.github.com/licenses/cc0-1.0", "node_id": "MDc6TGljZW5zZTY="}, "reference_api": ["requests.auth.HTTPBasicAuth"], "test_function": [{"file_path": "/bridgy-main/bridgy-main/tests/test_superfeedr.py", "class_name": "SuperfeedrTest", "function_name": "test_subscribe", "code": "\n  def test_subscribe(self):\n    expected = {\n      'hub.mode': 'subscribe',\n      'hub.topic': 'fake feed url',\n      'hub.callback': 'http://localhost/fake/notify/foo.com',\n      'format': 'json',\n      'retrieve': 'true',\n      }\n    item_a = {'permalinkUrl': 'A', 'content': 'a http://a.com a'}\n    item_b = {'permalinkUrl': 'B', 'summary': 'b http://b.com b'}\n    feed = {'items': [item_a, {}, item_b]}\n    self.expect_requests_post(superfeedr.PUSH_API_URL, feed,\n                              data=expected, auth=mox.IgnoreArg())\n\n    post_a = BlogPost(id='A', source=self.source.key, feed_item=item_a,\n                      unsent=['http://a.com/'])\n    post_b = BlogPost(id='B', source=self.source.key, feed_item=item_b,\n                      unsent=['http://b.com/'])\n    self.expect_task('propagate-blogpost', key=post_a)\n    self.expect_task('propagate-blogpost', key=post_b)\n    self.mox.ReplayAll()\n\n    with self.app.test_request_context():\n      superfeedr.subscribe(self.source)\n      self.assert_blogposts([post_a, post_b])"}]}, {"git_group": "XanaduAI", "git_name": "strawberryfields", "version": "v0.23.0", "language": "Python", "project_name": "strawberryfields-v0.23.0.zip", "file_path": "/strawberryfields-v0.23.0/strawberryfields-0.23.0/strawberryfields/decompositions.py", "file_name": "decompositions.py", "focal_class": null, "focal_name": "williamson", "focal_parameter": ["V"], "solution": "def williamson(V, tol=1e-11):\n    (n, m) = V.shape\n\n    if n != m:\n        raise ValueError(\"The input matrix is not square\")\n\n    diffn = np.linalg.norm(V - np.transpose(V))\n\n    if diffn >= tol:\n        raise ValueError(\"The input matrix is not symmetric\")\n\n    if n % 2 != 0:\n        raise ValueError(\"The input matrix must have an even number of rows/columns\")\n\n    n = n // 2\n    omega = sympmat(n)\n    vals = np.linalg.eigvalsh(V)\n\n    for val in vals:\n        if val <= 0:\n            raise ValueError(\"Input matrix is not positive definite\")\n\n    Mm12 = sqrtm(np.linalg.inv(V)).real\n    r1 = Mm12 @ omega @ Mm12\n    s1, K = schur(r1)\n    X = np.array([[0, 1], [1, 0]])\n    I = np.identity(2)\n    seq = []\n\n    # In what follows I construct a permutation matrix p  so that the Schur matrix has\n    # only positive elements above the diagonal\n    # Also the Schur matrix uses the x_1,p_1, ..., x_n,p_n  ordering thus I use rotmat to\n    # go to the ordering x_1, ..., x_n, p_1, ... , p_n\n\n    for i in range(n):\n        if s1[2 * i, 2 * i + 1] > 0:\n            seq.append(I)\n        else:\n            seq.append(X)\n\n    p = block_diag(*seq)\n    Kt = K @ p\n    s1t = p @ s1 @ p\n    dd = xpxp_to_xxpp(s1t)\n    perm_indices = xpxp_to_xxpp(np.arange(2 * n))\n    Ktt = Kt[:, perm_indices]\n    Db = np.diag([1 / dd[i, i + n] for i in range(n)] + [1 / dd[i, i + n] for i in range(n)])\n    S = Mm12 @ Ktt @ sqrtm(Db)\n    return Db, np.linalg.inv(S).T", "function_signature": "def williamson(V, tol=1e-11) :", "left_context": "# Copyright 2019 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis module implements common shared matrix decompositions that are\nused to perform gate decompositions.\n\"\"\"\n\nfrom itertools import groupby\nfrom collections import defaultdict\n\nimport numpy as np\nfrom scipy.linalg import block_diag, sqrtm, polar, schur\nfrom thewalrus.quantum import adj_scaling\nfrom thewalrus.symplectic import sympmat, xpxp_to_xxpp\n\n\ndef takagi(N, tol=1e-13, rounding=13):\n    r\"\"\"Autonne-Takagi decomposition of a complex symmetric (not Hermitian!) matrix.\n\n    Note that singular values of N are considered equal if they are equal after np.round(values, tol).\n\n    See :cite:`cariolaro2016` and references therein for a derivation.\n\n    Args:\n        N (array[complex]): square, symmetric matrix N\n        rounding (int): the number of decimal places to use when rounding the singular values of N\n        tol (float): the tolerance used when checking if the input matrix is symmetric: :math:`|N-N^T| <` tol\n\n    Returns:\n        tuple[array, array]: (rl, U), where rl are the (rounded) singular values,\n            and U is the Takagi unitary, such that :math:`N = U \\diag(rl) U^T`.\n    \"\"\"\n    (n, m) = N.shape\n    if n != m:\n        raise ValueError(\"The input matrix must be square\")\n    if np.linalg.norm(N - np.transpose(N)) >= tol:\n        raise ValueError(\"The input matrix is not symmetric\")\n\n    N = np.real_if_close(N)\n\n    if np.allclose(N, 0):\n        return np.zeros(n), np.eye(n)\n\n    if np.isrealobj(N):\n        # If the matrix N is real one can be more clever and use its eigendecomposition\n        l, U = np.linalg.eigh(N)\n        vals = np.abs(l)  # These are the Takagi eigenvalues\n        phases = np.sqrt(np.complex128([1 if i > 0 else -1 for i in l]))\n        Uc = U @ np.diag(phases)  # One needs to readjust the phases\n        list_vals = [(vals[i], i) for i in range(len(vals))]\n        list_vals.sort(reverse=True)\n        sorted_l, permutation = zip(*list_vals)\n        permutation = np.array(permutation)\n        Uc = Uc[:, permutation]\n        # And also rearrange the unitary and values so that they are decreasingly ordered\n        return np.array(sorted_l), Uc\n\n    v, l, ws = np.linalg.svd(N)\n    w = np.transpose(np.conjugate(ws))\n    rl = np.round(l, rounding)\n\n    # Generate list with degenerancies\n    result = []\n    for k, g in groupby(rl):\n        result.append(list(g))\n\n    # Generate lists containing the columns that correspond to degenerancies\n    kk = 0\n    for k in result:\n        for ind, j in enumerate(k):  # pylint: disable=unused-variable\n            k[ind] = kk\n            kk = kk + 1\n\n    # Generate the lists with the degenerate column subspaces\n    vas = []\n    was = []\n    for i in result:\n        vas.append(v[:, i])\n        was.append(w[:, i])\n\n    # Generate the matrices qs of the degenerate subspaces\n    qs = []\n    for i in range(len(result)):\n        qs.append(sqrtm(np.transpose(vas[i]) @ was[i]))\n\n    # Construct the Takagi unitary\n    qb = block_diag(*qs)\n\n    U = v @ np.conj(qb)\n    return rl, U\n\n\ndef graph_embed_deprecated(A, max_mean_photon=1.0, make_traceless=False, rtol=1e-05, atol=1e-08):\n    r\"\"\"Embed a graph into a Gaussian state.\n\n    Note: The default behaviour of graph embedding has been changed; see :func:`~.graph_embed`. This version is deprecated, but has been kept for consistency.\n\n    Given a graph in terms of a symmetric adjacency matrix\n    (in general with arbitrary complex off-diagonal and real diagonal entries),\n    returns the squeezing parameters and interferometer necessary for\n    creating the Gaussian state whose off-diagonal parts are proportional to that matrix.\n\n    Uses :func:`~.takagi`.\n\n    Args:\n        A (array[complex]): square, symmetric (weighted) adjacency matrix of the graph\n        max_mean_photon (float): Threshold value. It guarantees that the mode with\n            the largest squeezing has ``max_mean_photon`` as the mean photon number\n            i.e., :math:`sinh(r_{max})^2 ==` :code:``max_mean_photon``.\n        make_traceless (bool): Removes the trace of the input matrix, by performing the transformation\n            :math:`\\tilde{A} = A-\\mathrm{tr}(A) \\I/n`. This may reduce the amount of squeezing needed to encode\n            the graph but will lead to different photon number statistics for events with more than\n            one photon in any mode.\n        rtol (float): relative tolerance used when checking if the input matrix is symmetric\n        atol (float): absolute tolerance used when checking if the input matrix is symmetric\n\n    Returns:\n        tuple[array, array]: squeezing parameters of the input\n            state to the interferometer, and the unitary matrix representing the interferometer\n    \"\"\"\n    (m, n) = A.shape\n\n    if m != n:\n        raise ValueError(\"The matrix is not square.\")\n\n    if not np.allclose(A, np.transpose(A), rtol=rtol, atol=atol):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n    if make_traceless:\n        A = A - np.trace(A) * np.identity(n) / n\n\n    s, U = takagi(A, tol=atol)\n    sc = np.sqrt(1.0 + 1.0 / max_mean_photon)\n    vals = -np.arctanh(s / (s[0] * sc))\n    return vals, U\n\n\ndef graph_embed(A, mean_photon_per_mode=1.0, make_traceless=False, rtol=1e-05, atol=1e-08):\n    r\"\"\"Embed a graph into a Gaussian state.\n\n    Given a graph in terms of a symmetric adjacency matrix\n    (in general with arbitrary complex entries),\n    returns the squeezing parameters and interferometer necessary for\n    creating the Gaussian state whose off-diagonal parts are proportional to that matrix.\n\n    Uses :func:`~.takagi`.\n\n    Args:\n        A (array[complex]): square, symmetric (weighted) adjacency matrix of the graph\n        mean_photon_per_mode (float): guarantees that the mean photon number in the pure Gaussian state\n            representing the graph satisfies  :math:`\\frac{1}{N}\\sum_{i=1}^N sinh(r_{i})^2 ==` :code:``mean_photon``\n        make_traceless (bool): Removes the trace of the input matrix, by performing the transformation\n            :math:`\\tilde{A} = A-\\mathrm{tr}(A) \\I/n`. This may reduce the amount of squeezing needed to encode\n            the graph but will lead to different photon number statistics for events with more than\n            one photon in any mode.\n        rtol (float): relative tolerance used when checking if the input matrix is symmetric\n        atol (float): absolute tolerance used when checking if the input matrix is symmetric\n\n    Returns:\n        tuple[array, array]: squeezing parameters of the input\n        state to the interferometer, and the unitary matrix representing the interferometer\n    \"\"\"\n    (m, n) = A.shape\n\n    if m != n:\n        raise ValueError(\"The matrix is not square.\")\n\n    if not np.allclose(A, np.transpose(A), rtol=rtol, atol=atol):\n        raise ValueError(\"The matrix is not symmetric.\")\n\n    if make_traceless:\n        A = A - np.trace(A) * np.identity(n) / n\n\n    scale = adj_scaling(A, n * mean_photon_per_mode)\n    A = scale * A\n    s, U = takagi(A, tol=atol)\n    vals = -np.arctanh(s)\n    return vals, U\n\n\ndef bipartite_graph_embed(A, mean_photon_per_mode=1.0, rtol=1e-05, atol=1e-08):\n    r\"\"\"Embed a bipartite graph into a Gaussian state.\n\n    Given a bipartite graph in terms of an adjacency matrix\n    (in general with arbitrary complex entries),\n    returns the two-mode squeezing parameters and interferometers necessary for\n    creating the Gaussian state that encodes such adjacency matrix\n\n    Uses :func:`~.takagi`.\n\n    Args:\n        A (array[complex]): square, (weighted) adjacency matrix of the bipartite graph\n        mean_photon_per_mode (float): guarantees that the mean photon number in the pure Gaussian state\n            representing the graph satisfies  :math:`\\frac{1}{N}\\sum_{i=1}^N sinh(r_{i})^2 ==` :code:``mean_photon``\n        rtol (float): relative tolerance used when checking if the input matrix is symmetric\n        atol (float): absolute tolerance used when checking if the input matrix is symmetric\n\n    Returns:\n        tuple[array, array, array]: squeezing parameters of the input\n        state to the interferometer, and the unitaries matrix representing the interferometer\n    \"\"\"\n    (m, n) = A.shape\n\n    if m != n:\n        raise ValueError(\"The matrix is not square.\")\n\n    B = np.block([[0 * A, A], [A.T, 0 * A]])\n    scale = adj_scaling(B, 2 * n * mean_photon_per_mode)\n    A = scale * A\n\n    if np.allclose(A, A.T, rtol=rtol, atol=atol):\n        s, u = takagi(A, tol=atol)\n        v = u\n    else:\n        u, s, v = np.linalg.svd(A)\n        v = v.T\n\n    vals = -np.arctanh(s)\n    return vals, u, v\n\n\ndef T(m, n, theta, phi, nmax):\n    r\"\"\"The Clements T matrix from Eq 1 of the paper\"\"\"\n    mat = np.identity(nmax, dtype=np.complex128)\n    mat[m, m] = np.exp(1j * phi) * np.cos(theta)\n    mat[m, n] = -np.sin(theta)\n    mat[n, m] = np.exp(1j * phi) * np.sin(theta)\n    mat[n, n] = np.cos(theta)\n    return mat\n\n\ndef Ti(m, n, theta, phi, nmax):\n    r\"\"\"The inverse Clements T matrix\"\"\"\n    return np.transpose(T(m, n, theta, -phi, nmax))\n\n\ndef nullTi(m, n, U):\n    r\"\"\"Nullifies element m,n of U using Ti\"\"\"\n    (nmax, mmax) = U.shape\n\n    if nmax != mmax:\n        raise ValueError(\"U must be a square matrix\")\n\n    if U[m, n] == 0:\n        # no swaps for the identity-like case\n        thetar = 0\n        phir = 0\n    elif U[m, n + 1] == 0:\n        # swap in the divide-by-zero case\n        thetar = np.pi / 2\n        phir = 0\n    else:\n        r = U[m, n] / U[m, n + 1]\n        thetar = np.arctan(np.abs(r))\n        phir = np.angle(r)\n\n    return [n, n + 1, thetar, phir, nmax]\n\n\ndef nullT(n, m, U):\n    r\"\"\"Nullifies element n,m of U using T\"\"\"\n    (nmax, mmax) = U.shape\n\n    if nmax != mmax:\n        raise ValueError(\"U must be a square matrix\")\n\n    if U[n, m] == 0:\n        # no swaps for the identity-like case\n        thetar = 0\n        phir = 0\n    elif U[n - 1, m] == 0:\n        # swap in the divide-by-zero case\n        thetar = np.pi / 2\n        phir = 0\n    else:\n        r = -U[n, m] / U[n - 1, m]\n        thetar = np.arctan(np.abs(r))\n        phir = np.angle(r)\n\n    return [n - 1, n, thetar, phir, nmax]\n\n\ndef rectangular(V, tol=1e-11):\n    r\"\"\"Rectangular decomposition of a unitary matrix, with local\n    phase shifts applied between two interferometers.\n\n    See :ref:`rectangular` or :cite:`clements2016` for more details.\n\n    This function returns a circuit corresponding to an intermediate step in\n    the decomposition as described in Eq. 4 of the article. In this form,\n    the circuit comprises some T matrices (as in Eq. 1), then phases on all modes,\n    and more T matrices.\n\n    The procedure to construct these matrices is detailed in the supplementary\n    material of the article.\n\n    Args:\n        V (array[complex]): unitary matrix of size n_size\n        tol (float): the tolerance used when checking if the matrix is unitary:\n            :math:`|VV^\\dagger-I| \\leq` tol\n\n    Returns:\n        tuple[array]: tuple of the form ``(tilist,np.diag(localV),tlist)``\n            where:\n\n            * ``tilist``: list containing ``[n,m,theta,phi,n_size]`` of the Ti unitaries needed\n            * ``tlist``: list containing ``[n,m,theta,phi,n_size]`` of the T unitaries needed\n            * ``localV``: Diagonal unitary sitting sandwiched by Ti's and the T's\n    \"\"\"\n    localV = V\n    (nsize, _) = localV.shape\n\n    if not np.allclose(V @ V.conj().T, np.identity(nsize), atol=tol, rtol=0):\n        raise ValueError(\"The input matrix is not unitary\")\n\n    tilist = []\n    tlist = []\n    for k, i in enumerate(range(nsize - 2, -1, -1)):\n        if k % 2 == 0:\n            for j in reversed(range(nsize - 1 - i)):\n                tilist.append(nullTi(i + j + 1, j, localV))\n                localV = localV @ Ti(*tilist[-1])\n        else:\n            for j in range(nsize - 1 - i):\n                tlist.append(nullT(i + j + 1, j, localV))\n                localV = T(*tlist[-1]) @ localV\n\n    return tilist, np.diag(localV), tlist\n\n\ndef rectangular_phase_end(V, tol=1e-11):\n    r\"\"\"Rectangular decomposition of a unitary matrix, with all\n    local phase shifts placed after the interferometers.\n\n    See :cite:`clements2016` for more details.\n\n    Final step in the decomposition of a given discrete unitary matrix.\n    The output is of the form given in Eq. 5.\n\n    Args:\n        V (array[complex]): unitary matrix of size n_size\n        tol (float): the tolerance used when checking if the matrix is unitary\n    Returns:\n        tuple[array]: returns a tuple of the form ``(tlist, np.diag(localV), None)``\n            where:\n\n            * ``tlist``: list containing ``[n,m,theta,phi,n_size]`` of the T unitaries needed\n            * ``localV``: Diagonal unitary matrix to be applied at the end of circuit\n    \"\"\"\n    tilist, diags, tlist = rectangular(V, tol)\n    new_tlist, new_diags = tilist.copy(), diags.copy()\n\n    # Push each beamsplitter through the diagonal unitary\n    for i in reversed(tlist):\n        em, en = int(i[0]), int(i[1])\n        alpha, beta = np.angle(new_diags[em]), np.angle(new_diags[en])\n        theta, phi = i[2], i[3]\n\n        # The new parameters required for D',T' st. T^(-1)D = D'T'\n        new_theta = theta\n        new_phi = (alpha - beta + np.pi) % (2 * np.pi)\n        new_alpha = beta - phi + np.pi\n        new_beta = beta\n\n        new_i = [i[0], i[1], new_theta, new_phi, i[4]]\n        new_diags[em], new_diags[en] = np.exp(1j * new_alpha), np.exp(1j * new_beta)\n\n        new_tlist = new_tlist + [new_i]\n\n    return new_tlist, new_diags, None\n\n\ndef mach_zehnder(m, n, internal_phase, external_phase, nmax):\n    r\"\"\"A two-mode Mach-Zehnder interferometer section.\n\n    This section is constructed by an external phase shifter on the input mode\n    m, a symmetric beamsplitter combining modes m and n, an internal phase\n    shifter on mode m, and another symmetric beamsplitter combining modes m\n    and n.\n\n    The resulting matrix is\n\n    .. math::\n\n       M = i e^{i \\phi_{i}/2} \\left[\\begin{matrix}\\sin \\left( \\phi_{i}/2 \\right) e^{i \\phi_{e}} & \\cos \\left( \\phi_{i}/2 \\right) \\\\\n       \\cos \\left( \\phi_{i}/2 \\right) e^{i \\phi_{e}} & - \\sin \\left( \\phi_{i}/2 \\right) \\end{matrix}\\right]\n\n    Args:\n        m (int): mode number on which the phase shifters act\n        n (int): mode number which is combined with mode m by the beamsplitters\n        internal_phase (float): phase in between the symmetric beamsplitters\n        external_phase (float): phase acting before the first beamsplitter\n        nmax (int): maximum number of modes in the circuit\n\n    Returns:\n        array: unitary matrix of the effective transformation the series of phaseshifters\n        and beamsplitters.\n    \"\"\"\n    Rexternal = np.identity(nmax, dtype=np.complex128)\n    Rexternal[m, m] = np.exp(1j * external_phase)\n    Rinternal = np.identity(nmax, dtype=np.complex128)\n    Rinternal[m, m] = np.exp(1j * internal_phase)\n    BS = np.identity(nmax, dtype=np.complex128)\n    BS[m, m] = 1.0 / np.sqrt(2)\n    BS[m, n] = 1.0j / np.sqrt(2)\n    BS[n, m] = 1.0j / np.sqrt(2)\n    BS[n, n] = 1.0 / np.sqrt(2)\n    return np.round(BS @ Rinternal @ BS @ Rexternal, 14)\n\n\ndef mach_zehnder_inv(m, n, phi_int, phi_ext, nmax):\n    r\"\"\"The inverse of the Mach-Zehnder unitary matrix.\n    See :func:`~.mach_zehnder` for more details on the Mach-Zehnder unitary.\n    \"\"\"\n    return mach_zehnder(m, n, phi_int, phi_ext, nmax).conj().T\n\n\ndef nullMZi(m, n, U):\n    r\"\"\"Nullifies element m,n of U using mach_zehnder_inv.\n\n    Args:\n        m (int): row index of element to be nullified\n        n (int): column index of element to be nullified\n        U (array): matrix whose m,n element is to be nullified\n\n    Returns:\n        list: list containing ``[m, n, internal_phase, external_phase, nmax]`` of the\n            mach_zehnder_inv unitaries needed\n    \"\"\"\n    (nmax, mmax) = U.shape\n\n    if nmax != mmax:\n        raise ValueError(\"U must be a square matrix\")\n\n    if U[m, n] == 0:\n        # no swaps for the identity-like case\n        phi_i = np.pi\n        phi_e = 0\n    elif U[m, n + 1] == 0:\n        # swap in the divide-by-zero case\n        phi_i = 0\n        phi_e = 0\n    else:\n        r = -U[m, n + 1] / U[m, n]\n        phi_i = 2 * np.arctan(np.abs(r))\n        phi_e = -np.angle(r)\n\n    return [n, n + 1, phi_i, phi_e, nmax]\n\n\ndef nullMZ(n, m, U):\n    r\"\"\"Nullifies element n,m of U using mach_zehnder.\n\n    Args:\n        n (int): row index of element to be nullified\n        m (int): column index of element to be nullified\n        U (array): matrix whose m,n element is to be nullified\n\n    Returns:\n        list: list containing ``[m, n, internal_phase, external_phase, nmax]`` of the\n            mach_zehnder unitaries needed\n    \"\"\"\n    (nmax, mmax) = U.shape\n\n    if nmax != mmax:\n        raise ValueError(\"U must be a square matrix\")\n\n    if U[n, m] == 0:\n        # no swaps for the identity-like case\n        phi_i = np.pi\n        phi_e = 0\n    elif U[n - 1, m] == 0:\n        # swap in the divide-by-zero case\n        phi_i = 0\n        phi_e = 0\n    else:\n        r = U[n - 1, m] / U[n, m]\n        phi_i = 2 * np.arctan(np.abs(r))\n        phi_e = -np.angle(r)\n\n    return [n - 1, n, phi_i, phi_e, nmax]\n\n\ndef rectangular_MZ(V, tol=1e-11):\n    r\"\"\"Rectangular decomposition of a unitary matrix, with local\n    phase shifts applied between two interferometers.\n\n    Is similar to :func:`~.rectangular` except that it uses Mach Zehnder matrices to null elements of V\n    using the :func:`~.null_MZ` and :func:`~.null_MZi` instead of :func:`~.T` matrices and corresponding :func:`~.nullT`\n    and :func:`~.nullTi` functions.\n\n    Args:\n        V (array[complex]): unitary matrix of size n_size\n        tol (float): the tolerance used when checking if the matrix is unitary\n\n    Returns:\n        tuple[array]: tuple of the form ``(tilist, np.diag(localV), tlist)``\n        where:\n\n        * ``tilist``: list containing ``[n,m,phi_int,phi_ext,n_size]`` of the ``mach_zehnder_inv`` unitaries needed\n        * ``tlist``: list containing ``[n,m,phi_int,phi_ext,n_size]`` of the ``mach_zehnder`` unitaries needed\n        * ``localV``: Diagonal unitary sitting sandwiched by ``mach_zehnder_inv``'s and the ``mach_zehnder``'s\n    \"\"\"\n    localV = V\n    (nsize, _) = localV.shape\n\n    if not np.allclose(V @ V.conj().T, np.identity(nsize), atol=tol, rtol=0):\n        raise ValueError(\"The input matrix is not unitary\")\n\n    tilist = []\n    tlist = []\n    for k, i in enumerate(range(nsize - 2, -1, -1)):\n        if k % 2 == 0:\n            for j in reversed(range(nsize - 1 - i)):\n                tilist.append(nullMZi(i + j + 1, j, localV))\n                tilist[-1][2] %= 2 * np.pi\n                tilist[-1][3] %= 2 * np.pi\n                # repeat modulo operations, otherwise the input unitary\n                # numpy.identity(20) yields an external_phase of exactly 2 * pi\n                tilist[-1][2] %= 2 * np.pi\n                tilist[-1][3] %= 2 * np.pi\n                localV = localV @ mach_zehnder_inv(*tilist[-1])\n        else:\n            for j in range(nsize - 1 - i):\n                tlist.append(nullMZ(i + j + 1, j, localV))\n                tlist[-1][2] %= 2 * np.pi\n                tlist[-1][3] %= 2 * np.pi\n                # repeat modulo operations, otherwise the input unitary\n                # numpy.identity(20) yields an external_phase of exactly 2 * pi\n                tlist[-1][2] %= 2 * np.pi\n                tlist[-1][3] %= 2 * np.pi\n                localV = mach_zehnder(*tlist[-1]) @ localV\n\n    return tilist, np.diag(localV), tlist\n\n\ndef rectangular_symmetric(V, tol=1e-11):\n    r\"\"\"Decomposition of a unitary into an array of symmetric beamsplitters.\n\n    This decomposition starts with the output from :func:`~.rectangular_MZ`\n    and performs the equivalent of :func:`~.rectangular_phase_end` by placing all the\n    local phase shifts after the interferometers.\n\n    If the Mach-Zehnder unitaries are represented as M and the local phase shifts as D, the new\n    parameters to shift the local phases to the end are calculated such that\n\n    .. math::\n\n       M^{-1} D = D_{\\mathrm{new}} M_{\\mathrm{new}}\n\n    Args:\n        V (array): unitary matrix of size n_size\n        tol (int): the number of decimal places to use when determining\n          whether the matrix is unitary\n\n    Returns:\n        tuple[array]: returns a tuple of the form ``(tlist,np.diag(localV), None)``\n            where:\n\n            * ``tlist``: list containing ``[n, m, internal_phase, external_phase, n_size]`` of the T unitaries needed\n            * ``localV``: Diagonal unitary matrix to be applied at the end of circuit\n            * ``None``: the value ``None``, in order to make the return\n              signature identical to :func:`~.rectangular`\n    \"\"\"\n    tilist, diags, tlist = rectangular_MZ(V, tol)\n    new_tlist, new_diags = tilist.copy(), diags.copy()\n\n    # Push each beamsplitter through the diagonal unitary\n    for i in reversed(tlist):\n        em, en = int(i[0]), int(i[1])\n        alpha, beta = np.angle(new_diags[em]), np.angle(new_diags[en])\n        phi_i, phi_e = i[2], i[3]\n\n        # The new parameters required for D', MZ' st. MZ^(-1)D = D'MZ'\n\n        new_phi_e = (alpha - beta) % (2 * np.pi)\n        new_alpha = (beta - phi_e - phi_i + np.pi) % (2 * np.pi)\n        new_beta = (beta - phi_i + np.pi) % (2 * np.pi)\n        new_phi_i = phi_i % (2 * np.pi)\n        # repeat modulo operations , otherwise the input unitary\n        # numpy.identity(20) yields an external_phase of exactly 2 * pi\n        new_phi_i %= 2 * np.pi\n        new_phi_e %= 2 * np.pi\n\n        new_i = [i[0], i[1], new_phi_i, new_phi_e, i[4]]\n        new_diags[em], new_diags[en] = np.exp(1j * new_alpha), np.exp(1j * new_beta)\n\n        new_tlist = new_tlist + [new_i]\n\n    return new_tlist, new_diags, None\n\n\ndef triangular(V, tol=1e-11):\n    r\"\"\"Triangular decomposition of a unitary matrix due to Reck et al.\n\n    See :cite:`reck1994` for more details and :cite:`clements2016` for details on notation.\n\n    Args:\n        V (array[complex]): unitary matrix of size ``n_size``\n        tol (float): the tolerance used when checking if the matrix is unitary:\n            :math:`|VV^\\dagger-I| \\leq` tol\n\n    Returns:\n        tuple[array]: returns a tuple of the form ``(tlist,np.diag(localV), None)``\n            where:\n\n            * ``tlist``: list containing ``[n,m,theta,phi,n_size]`` of the T unitaries needed\n            * ``localV``: Diagonal unitary applied at the beginning of circuit\n    \"\"\"\n    localV = V\n    (nsize, _) = localV.shape\n\n    if not np.allclose(V @ V.conj().T, np.identity(nsize), atol=tol, rtol=0):\n        raise ValueError(\"The input matrix is not unitary\")\n\n    tlist = []\n    for i in range(nsize - 2, -1, -1):\n        for j in range(i + 1):\n            tlist.append(nullT(nsize - j - 1, nsize - i - 2, localV))\n            localV = T(*tlist[-1]) @ localV\n\n    return list(reversed(tlist)), np.diag(localV), None\n\n\ndef M(n, sigma, delta, m):\n    r\"\"\"The symmetric Mach Zehnder interferometer matrix. (Eq 1 of the paper (arXiv:2104.0756).)\n\n    Args:\n        n (int): the starting mode of sMZI\n        sigma (complex): parameter of the sMZI :math:`\\frac{(\\theta_1+\\theta_2)}{2}`, where :math:`\\theta_{1,2}` are the values of the two internal phase-shifts of sMZI\n        delta (complex): parameter of the sMZI :math:`\\frac{(\\theta_1-\\theta_2)}{2}`, where :math:`\\theta_{1,2}` are the values of the two internal phase-shifts of sMZI\n        m (int): the length of the unitary matrix to be decomposed\n\n    Returns:\n        array[complex,complex]: the sMZI matrix between n-th and (n+1)-th mode\n    \"\"\"\n    mat = np.identity(m, dtype=np.complex128)\n    mat[n, n] = np.exp(1j * sigma) * np.sin(delta)\n    mat[n, n + 1] = np.exp(1j * sigma) * np.cos(delta)\n    mat[n + 1, n] = np.exp(1j * sigma) * np.cos(delta)\n    mat[n + 1, n + 1] = -np.exp(1j * sigma) * np.sin(delta)\n    return mat\n\n\ndef P(j, phi, m):\n    r\"\"\"The phase shifter matrix. (Eq 2 of the paper (arXiv:2104.0756).)\n\n    Args:\n        j (int): the starting mode of phase-shifter\n        phi (complex): parameter of the phase-shifter\n        m (int): the length of the unitary matrix to be decomposed\n\n    Returns:\n        array[complex,complex]: the phase-shifter matrix on the j-th mode\n    \"\"\"\n    mat = np.identity(m, dtype=np.complex128)\n    mat[j, j] = np.exp(1j * phi)\n    return mat\n\n\ndef triangular_compact(U, rtol=1e-12, atol=1e-12):\n    r\"\"\"Triangular decomposition of a unitary matrix with sMZIs and phase-shifters, as given in FIG. 2 and \"The Reck Scheme\" section of (arXiv:2104.0756).\n\n    Args:\n        U (array): unitary matrix\n\n    Returns:\n        dict: A dictionary containing the following items:\n\n        * ``m``: the length of the matrix\n        * ``phi_ins``: parameter of the phase-shifter at the beginning of the mode\n        * ``sigmas``: parameter of the sMZI :math:`\\frac{(\\theta_1+\\theta_2)}{2}`, where :math:`\\theta_{1,2}` are the values of the two internal phase-shifts of sMZI\n        * ``deltas``: parameter of the sMZI :math:`\\frac{(\\theta_1-\\theta_2)}{2}`, where :math:`\\theta_{1,2}` are the values of the two internal phase-shifts of sMZI\n        * ``zetas``: parameter of the phase-shifter at the end of the mode\n    \"\"\"\n\n    if not U.shape[0] == U.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n    if not np.allclose(U @ U.conj().T, np.eye(U.shape[0]), rtol=rtol, atol=atol):\n        raise ValueError(\"The input matrix is not unitary\")\n\n    V = U.conj()\n    m = U.shape[0]\n\n    phases = {}\n    phases[\"m\"] = m\n    phases[\"phi_ins\"] = {}  # mode : phi\n    phases[\"deltas\"] = {}  # (mode, layer) : delta\n    phases[\"sigmas\"] = {}  # (mode, layer) : sigma\n    phases[\"zetas\"] = {}  # mode : zeta\n\n    for j in range(m - 1):\n        x = m - 1\n        y = j\n        phi_j = -np.angle(V[x, y + 1]) + np.angle(V[x, y])\n        Pj = P(j + 1, phi_j, m)\n        phases[\"phi_ins\"][j] = phi_j\n        V = V @ Pj\n        for k in range(j + 1):\n            n = j - k\n            if V[x, y] == 0:\n                delta = 0.5 * np.pi\n            else:\n                delta = np.arctan2(-abs(V[x, y + 1]), abs(V[x, y]))\n            V_temp = V @ M(n, 0, delta, m)\n            sigma = np.angle(V_temp[x - 1, y - 1]) - np.angle(V_temp[x - 1, y])\n            phases[\"deltas\"][n, k] = delta\n            phases[\"sigmas\"][n, k] = sigma\n            V = V @ M(n, sigma, delta, m)\n            x -= 1\n            y -= 1\n\n    # these next two lines are just to remove a global phase\n    zeta = -np.angle(V[0, 0])\n    phases[\"zetas\"][0] = zeta\n    V = V @ P(0, zeta, m)\n\n    for j in range(1, m):\n        zeta = np.angle(V[0, 0]) - np.angle(V[j, j])\n        phases[\"zetas\"][j] = zeta\n        V = V @ P(j, zeta, m)\n\n    assert np.allclose(V, np.eye(m), rtol=rtol, atol=atol), \"decomposition failed\"\n\n    return phases\n\n\ndef _rectangular_compact_init(\n    U, rtol=1e-12, atol=1e-12\n):  # pylint: disable=too-many-statements, too-many-branches\n    r\"\"\"Rectangular decomposition of a unitary with sMZIs and phase-shifters, as given in FIG. 3 and \"The Clements Scheme\" section of (arXiv:2104.0756).\n\n    Args:\n        U (array): unitary matrix\n\n    Returns:\n        dict: A dictionary containing the following items:\n\n        * ``m``: the length of the matrix\n        * ``phi_ins``: parameter of the phase-shifter at the beginning of the mode\n        * ``sigmas``: parameter of the sMZI :math:`\\frac{(\\theta_1+\\theta_2)}{2}`, where :math:`\\theta_{1,2}` are the values of the two internal phase-shifts of sMZI\n        * ``deltas``: parameter of the sMZI :math:`\\frac{(\\theta_1-\\theta_2)}{2}`, where :math:`\\theta_{1,2}` are the values of the two internal phase-shifts of sMZI\n        * ``zetas``: parameter of the phase-shifter at the middle of the mode\n        * ``phi_outs``: parameter of the phase-shifter at the end of the mode\n\n    \"\"\"\n\n    V = U.conj()\n    m = U.shape[0]\n\n    phases = {}\n    phases[\"m\"] = m\n    phases[\"phi_ins\"] = {}  # mode : phi\n    phases[\"deltas\"] = {}  # (mode, layer) : delta\n    phases[\"sigmas\"] = {}  # (mode, layer) : sigma\n    phases[\"zetas\"] = {}  # mode : zeta\n    phases[\"phi_outs\"] = {}  # mode : phi\n\n    for j in range(m - 1):\n        if j % 2 == 0:\n            x = m - 1\n            y = j\n            phi_j = np.angle(V[x, y + 1]) - np.angle(V[x, y])  # reversed order from paper\n            V = V @ P(j, phi_j, m)\n            phases[\"phi_ins\"][j] = phi_j\n            for k in range(j + 1):\n                if V[x, y] == 0:\n                    delta = 0.5 * np.pi\n                else:\n                    delta = np.arctan2(-abs(V[x, y + 1]), abs(V[x, y]))\n                n = j - k\n                V_temp = V @ M(n, 0, delta, m)\n                sigma = np.angle(V_temp[x - 1, y - 1]) - np.angle(V_temp[x - 1, y])\n                V = V @ M(n, sigma, delta, m)\n                phases[\"deltas\"][n, k] = delta\n                phases[\"sigmas\"][n, k] = sigma\n                x -= 1\n                y -= 1\n        else:\n            x = m - j - 1\n            y = 0\n            phi_j = np.angle(V[x - 1, y]) - np.angle(V[x, y])\n            V = P(x, phi_j, m) @ V\n            phases[\"phi_outs\"][x] = phi_j\n            for k in range(j + 1):\n                if V[x, y] == 0.0:\n                    delta = 0.5 * np.pi\n                else:\n                    delta = np.arctan2(abs(V[x - 1, y]), abs(V[x, y]))\n                V_temp = M(x - 1, 0, delta, m) @ V\n                n = m + k - j - 2\n                if j != k:\n                    sigma = np.angle(V_temp[x + 1, y + 1]) - np.angle(V_temp[x, y + 1])\n                else:\n                    sigma = 0\n                phases[\"deltas\"][n, m - k - 1] = delta\n                phases[\"sigmas\"][n, m - k - 1] = sigma\n                V = M(n, sigma, delta, m) @ V\n                x += 1\n                y += 1\n\n    # these next two lines are just to remove a global phase\n    zeta = -np.angle(V[0, 0])\n    V = V @ P(0, zeta, m)\n    phases[\"zetas\"][0] = zeta\n\n    for j in range(1, m):\n        zeta = np.angle(V[0, 0]) - np.angle(V[j, j])\n        V = V @ P(j, zeta, m)\n        phases[\"zetas\"][j] = zeta\n\n    assert np.allclose(V, np.eye(m), rtol=rtol, atol=atol), \"decomposition failed\"\n\n    return phases\n\n\ndef _absorb_zeta(phases):\n    r\"\"\"Adjust rectangular decomposition to relocate residual phase-shifters of interferometer to edge-shifters, as given in FIG. 4 and \"Relocating residual phase-shifts\" section of (arXiv:2104.0756).\n\n    Args:\n        phases (dict): output of _rectangular_compact_init\n\n    Returns:\n        dict: A dictionary containing the following items:\n\n        * ``m``: the length of the matrix\n        * ``phi_ins``: parameter of the phase-shifter at the beginning of the mode\n        * ``sigmas``: parameter of the sMZI :math:`\\frac{(\\theta_1+\\theta_2)}{2}`, where :math:`\\theta_{1,2}` are the values of the two internal phase-shifts of sMZI\n        * ``deltas``: parameter of the sMZI :math:`\\frac{(\\theta_1-\\theta_2)}{2}`, where :math:`\\theta_{1,2}` are the values of the two internal phase-shifts of sMZI\n        * ``phi_edges``: parameters of the edge phase shifters\n        * ``phi_outs``: parameter of the phase-shifter at the end of the mode\n\n    \"\"\"\n    m = phases[\"m\"]\n    new_phases = phases.copy()\n    del new_phases[\"zetas\"]\n    new_phases[\"phi_edges\"] = defaultdict(float)  # (mode, layer) : phi\n\n    if m % 2 == 0:\n        new_phases[\"phi_outs\"][0] = phases[\"zetas\"][0]\n        for j in range(1, m):\n            zeta = phases[\"zetas\"][j]\n            layer = m - j\n            for mode in range(j, m - 1, 2):\n                new_phases[\"sigmas\"][mode, layer] += zeta\n            for mode in range(j + 1, m - 1, 2):\n                new_phases[\"sigmas\"][mode, layer - 1] -= zeta\n            if layer % 2 == 1:\n                new_phases[\"phi_edges\"][m - 1, layer] += zeta\n            else:\n                new_phases[\"phi_edges\"][m - 1, layer - 1] -= zeta\n    else:\n        for j in range(m):\n            zeta = phases[\"zetas\"][j]\n            layer = m - j - 1\n            for mode in range(j, m - 1, 2):\n                new_phases[\"sigmas\"][mode, layer] += zeta\n            for mode in range(j + 1, m - 1, 2):\n                new_phases[\"sigmas\"][mode, layer - 1] -= zeta\n            if layer % 2 == 0:\n                new_phases[\"phi_edges\"][m - 1, layer] += zeta\n            else:\n                new_phases[\"phi_edges\"][m - 1, layer - 1] -= zeta\n    return new_phases\n\n\ndef rectangular_compact(U, rtol=1e-12, atol=1e-12):\n    r\"\"\"Rectangular decomposition of a unitary with sMZIs and phase-shifters, as given in FIG. 3+4 and \"The Clements Scheme\" section of (arXiv:2104.0756).\n\n    Args:\n        U (array): unitary matrix\n\n    Returns:\n        dict: A dictionary containing the following items:\n\n        * ``m``: the length of the matrix\n        * ``phi_ins``: parameter of the phase-shifter at the beginning of the mode\n        * ``sigmas``: parameter of the sMZI :math:`\\frac{(\\theta_1+\\theta_2)}{2}`, where :math:`\\theta_{1,2}` are the values of the two internal phase-shifts of sMZI\n        * ``deltas``: parameter of the sMZI :math:`\\frac{(\\theta_1-\\theta_2)}{2}`, where :math:`\\theta_{1,2}` are the values of the two internal phase-shifts of sMZI\n        * ``phi_edges``: parameters of the edge phase shifters\n        * ``phi_outs``: parameter of the phase-shifter at the end of the mode\n    \"\"\"\n\n    if not U.shape[0] == U.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n\n    if not np.allclose(U @ U.conj().T, np.eye(U.shape[0]), rtol=rtol, atol=atol):\n        raise ValueError(\"The input matrix is not unitary\")\n\n    phases_temp = _rectangular_compact_init(U, rtol=rtol, atol=atol)\n    return _absorb_zeta(phases_temp)\n\n", "right_context": "\n\ndef bloch_messiah(S, tol=1e-10, rounding=9):\n    r\"\"\"Bloch-Messiah decomposition of a symplectic matrix.\n\n    See :ref:`bloch_messiah`.\n\n    Decomposes a symplectic matrix into two symplectic unitaries and squeezing transformation.\n    It automatically sorts the squeezers so that they respect the canonical symplectic form.\n\n    Note that it is assumed that the symplectic form is\n\n    .. math:: \\Omega = \\begin{bmatrix}0&I\\\\-I&0\\end{bmatrix}\n\n    where :math:`I` is the identity matrix and :math:`0` is the zero matrix.\n\n    As in the Takagi decomposition, the singular values of N are considered\n    equal if they are equal after np.round(values, rounding).\n\n    If S is a passive transformation, then return the S as the first passive\n    transformation, and set the the squeezing and second unitary matrices to\n    identity. This choice is not unique.\n\n    For more info see:\n    https://math.stackexchange.com/questions/1886038/finding-euler-decomposition-of-a-symplectic-matrix\n\n    Args:\n        S (array[float]): symplectic matrix\n        tol (float): the tolerance used when checking if the matrix is symplectic:\n            :math:`|S^T\\Omega S-\\Omega| \\leq tol`\n        rounding (int): the number of decimal places to use when rounding the singular values\n\n    Returns:\n        tuple[array]: Returns the tuple ``(ut1, st1, vt1)``. ``ut1`` and ``vt1`` are symplectic orthogonal,\n            and ``st1`` is diagonal and of the form :math:`= \\text{diag}(s1,\\dots,s_n, 1/s_1,\\dots,1/s_n)`\n            such that :math:`S = ut1  st1  v1`\n    \"\"\"\n    (n, m) = S.shape\n\n    if n != m:\n        raise ValueError(\"The input matrix is not square\")\n    if n % 2 != 0:\n        raise ValueError(\"The input matrix must have an even number of rows/columns\")\n\n    n = n // 2\n    omega = sympmat(n)\n    if np.linalg.norm(np.transpose(S) @ omega @ S - omega) >= tol:\n        raise ValueError(\"The input matrix is not symplectic\")\n\n    if np.linalg.norm(np.transpose(S) @ S - np.eye(2 * n)) >= tol:\n\n        u, sigma = polar(S, side=\"left\")\n        ss, uss = takagi(sigma, tol=tol, rounding=rounding)\n\n        # Apply a permutation matrix so that the squeezers appear in the order\n        # s_1,...,s_n, 1/s_1,...1/s_n\n        perm = np.array(list(range(0, n)) + list(reversed(range(n, 2 * n))))\n\n        pmat = np.identity(2 * n)[perm, :]\n        ut = uss @ pmat\n\n        # Apply a second permutation matrix to permute s\n        # (and their corresonding inverses) to get the canonical symplectic form\n        qomega = np.transpose(ut) @ (omega) @ ut\n        st = pmat @ np.diag(ss) @ pmat\n\n        # Identifying degenerate subspaces\n        result = []\n        for _k, g in groupby(np.round(np.diag(st), rounding)[:n]):\n            result.append(list(g))\n\n        stop_is = list(np.cumsum([len(res) for res in result]))\n        start_is = [0] + stop_is[:-1]\n\n        # Rotation matrices (not permutations) based on svd.\n        # See Appendix B2 of Serafini's book for more details.\n        u_list, v_list = [], []\n\n        for start_i, stop_i in zip(start_is, stop_is):\n            x = qomega[start_i:stop_i, n + start_i : n + stop_i].real\n            u_svd, _s_svd, v_svd = np.linalg.svd(x)\n            u_list = u_list + [u_svd]\n            v_list = v_list + [v_svd.T]\n\n        pmat1 = block_diag(*(u_list + v_list))\n\n        st1 = pmat1.T @ pmat @ np.diag(ss) @ pmat @ pmat1\n        ut1 = uss @ pmat @ pmat1\n        v1 = np.transpose(ut1) @ u\n\n    else:\n        ut1 = S\n        st1 = np.eye(2 * n)\n        v1 = np.eye(2 * n)\n\n    return ut1.real, st1.real, v1.real\n\n\ndef sun_compact(U, rtol=1e-12, atol=1e-12):\n    r\"\"\"Recursive factorization of unitary transfomations.\n\n    Decomposes elements of :math:`\\mathrm{SU}(n)` as a sequence of :math:`\\mathrm{SU}(2)`\n    transformations and entangling beamsplitters, see :cite:`deguise2018simple`.\n    This sequence of :math:`\\mathrm{SU}(2)` transformations can then be mapped to an operation\n    on optical modes including two phase plates and one beam splitter.\n\n    This implementation is based on the authors' code at `github:glassnotes/Caspar\n    <https://github.com/glassnotes/Caspar>`_.\n\n    Args:\n        U (array): unitary matrix\n        rtol (float): relative tolerance used when checking if the matrix is unitary\n        atol (float): absolute tolerance used when checking if the matrix is unitary\n\n    Returns:\n        tuple[list[tuple,list], float]: Returns a list of operations with elements in\n        the form ``(i,i+1), [a, b, g]`` where the ``(i,i+1)`` indicates the modes of an\n        :math:`\\mathrm{SU}(2)` transformation and ``[a, b, g]`` are the transformation parameters.\n\n    .. details::\n\n    Note that any unitary can be written in terms of an special unitary as\n\n    .. math:: U = e^{i \\phi/n} S\n\n    where :math:`S \\in \\mathrm{SU}(n)` and :math:`e^{i\\phi} = \\mathrm{det}\\,U`.\n\n    Here any :math:`S \\in \\mathrm{SU}(n)` is parametrized in terms of the Euler angles and written as\n\n    .. math::\n        S(\\alpha, \\beta, \\gamma) =\n        \\begin{pmatrix}\n            e^{i\\alpha/2} & 0              \\\\\n            0             & e^{-i\\alpha/2}\n        \\end{pmatrix}\n        \\begin{pmatrix}\n            \\cos{\\beta/2} & -\\sin{\\beta/2}  \\\\\n            \\sin{\\beta/2}  & \\cos{\\beta/2}\n        \\end{pmatrix}\n        \\begin{pmatrix}\n            e^{i\\gamma/2} & 0              \\\\\n            0             & e^{-i\\gamma/2}\n        \\end{pmatrix}.\n\n    This factorization then determines the constructions of the :math:`\\mathrm{SU}(2)` device\n    acting on the respective optical modes\n\n    .. math::\n         S(\\alpha, \\beta, \\gamma) =\n            \\left[ R(\\alpha/2) \\otimes R(-\\alpha/2) \\right] \\,\n            BS(\\beta/2) \\,\n            \\left[ R(\\gamma/2) \\otimes R(-\\gamma/2) \\right].\n    \"\"\"\n\n    n = U.shape[0]\n    parameters = []\n    global_phase = None\n    det = np.linalg.det(U)\n\n    if n < 3:\n        raise ValueError(\"Input matrix for decomposition must be at least 3x3.\")\n    if not np.allclose(U @ U.conj().T, np.identity(n), rtol=rtol, atol=atol):\n        raise ValueError(\"The input matrix is not unitary.\")\n\n    # if Unitary, factorize into phase times Special Unitary\n    SU = U.copy()\n    if not np.isclose(det, 1, rtol=rtol, atol=atol):\n        SU *= det ** (-1 / n)\n        global_phase = np.angle(det)\n\n    # Decompose the matrix\n    parameters_no_modes = _sun_parameters(SU, rtol, atol)\n\n    # Add the info about which modes each transformation is on\n    param_idx = 0\n    for md2 in range(2, n + 1):\n        for md1 in range(n - 1, md2 - 2, -1):\n            parameters.append([(md1 - 1, md1), parameters_no_modes[param_idx]])\n            param_idx += 1\n\n    return parameters, global_phase\n\n\ndef _sun_parameters(U, rtol=1e-12, atol=1e-12):\n    r\"\"\"Compute the set of parameters of the :math:`\\mathrm{SU}(2)` transforms in the\n    factorization scheme.\n\n    Args:\n        U (array): unitary matrix\n\n    Returns:\n        list: a list of parameters ``[a, b, g]`` of an :math:`\\mathrm{SU}(2)` operation\n\n    .. details::\n\n    This is a recursive process. The first step is to produce a\n    \"staircase\" of transformations on adjacent modes :math:`(d-1, d), (d-2, d-1), \\dots`\n    so that what's left is an :math:`\\mathrm{SU}(n-1)` transformation embedded in the lower\n    portion of the original system.\n    This is performed recursively down to the case of :math:`\\mathrm{SU}(3)` where the\n    Rowe et al algorithm :cite:`rowe1999representations` is used to get the rest of the\n    transformation.\n    \"\"\"\n    if U.shape == (3, 3):\n        return _su3_parameters(U)\n\n    staircase_transformation, new_U = _build_staircase(U, rtol, atol)\n    Unm1 = new_U[1:, 1:]\n    return staircase_transformation + _sun_parameters(Unm1, rtol, atol)\n\n\n# pylint: disable=too-many-branches\ndef _build_staircase(U, rtol=1e-12, atol=1e-12):\n    r\"\"\"Take a matrix in :math:`\\mathrm{SU}(n)` and find the staircase of :math:`\\mathrm{SU}(n)`\n    transformations which turns it into an :math:`\\mathrm{SU}(n-1)` transformation on all but\n    the first mode.\n\n    Args:\n        U (array): unitary matrix\n\n    Returns:\n        list: Returns the list of parameters in the order in which they appear\n            graphically, e.g. for :math:`\\mathrm{SU}(5)` will return parameters for a staircase\n            order as transformations on modes `(4,5)`, `(3,4)`, `(2,3)`, and finally `(1,2)`.\n    \"\"\"\n    n = U.shape[0]\n\n    # We need to do n - 1 transformations, starting from the bottom up.\n    transformations = []\n    running_prod = U\n\n    # There are a number of special cases to consider which occur when the\n    # left-most column contains all 0s except for one entry.\n    moduli = np.abs(U[:, 0])\n    if np.allclose(sorted(moduli), [0.0] * (n - 1) + [1], rtol, atol):\n        # In the special case where the top-most entry is a 1, or within some\n        # small tolerance of it, we basically already have an SU(n-1) transformation\n        # in there so just fill with empty parameters\n        if np.isclose(running_prod[0, 0], 1, rtol, atol):\n            transformations = [[0.0, 0.0, 0.0]] * (n - 1)\n        # Another special case is when the top left entry has modulus 1 (or close\n        # to it). Now we need to add a separate phase shift as well.\n        elif np.isclose(np.abs(running_prod[0, 0]), 1, rtol, atol):\n            # \"Phase shift\" by applying an SU(2) transformation to cancel out the\n            # top-most phase. Do nothing to everything else.\n            phase_su2 = np.array([[running_prod[0, 0].conjugate(), 0], [0, running_prod[0, 0]]])\n            transformations = [[0.0, 0.0, 0.0]] * (n - 2) + [_su2_parameters(phase_su2.conj().T)]\n\n            full_phase_su2 = np.identity(n, dtype=complex)\n            full_phase_su2[0:2, 0:2] = phase_su2\n            running_prod = full_phase_su2 @ running_prod\n        else:\n            # If the non-zero entry is lower down, permute until it\n            # reaches the top and then apply a phase transformation.\n            for rot_idx in range(n - 1, 0, -1):\n                if not np.isclose(running_prod[rot_idx, 0], 0, rtol, atol):\n                    permmat = np.array([[0, -1], [1, 0]])\n\n                    full_permmat = np.identity(n, dtype=complex)\n                    full_permmat[rot_idx - 1 : rot_idx + 1, rot_idx - 1 : rot_idx + 1] = permmat\n                    temp_product = full_permmat @ running_prod\n\n                    if rot_idx == 1:  # If we're at the top, add the phase too\n                        phase_su2 = np.array(\n                            [[temp_product[0, 0].conjugate(), 0], [0, temp_product[0, 0]]]\n                        )\n                        permmat = phase_su2 @ permmat\n\n                    transformations.append(_su2_parameters(permmat.conj().T))\n\n                    full_trans = np.identity(n, dtype=complex)\n                    full_trans[rot_idx - 1 : rot_idx + 1, rot_idx - 1 : rot_idx + 1] = permmat\n\n                    running_prod = full_trans @ running_prod\n\n                else:  # Otherwise do nothing between these modes\n                    transformations.append([0, 0, 0])\n    else:\n        for rot_idx in range(n - 1):\n            # Start at the bottom\n            i, j = n - rot_idx - 2, n - rot_idx - 1\n\n            # Initially we work with the inverses in order to \"0 out\" entries\n            # from the left; later we'll get the parameters from the \"true\" matrices.\n            Rij_inv = np.identity(2, dtype=complex)\n            full_Rij_inv = np.identity(n, dtype=complex)\n\n            if rot_idx != n - 2:\n                # The denominator of the transformation is the difference of\n                # absolute values of all columns *up* to this point.\n                sum_of_column = 0\n                for k in range(i):\n                    sum_of_column += pow(np.absolute(running_prod[k, 0]), 2)\n                cf = np.sqrt(1 - sum_of_column)\n\n                y, z = running_prod[i, 0], running_prod[j, 0]\n                capY, capZ = y / cf, z / cf\n\n                # Build the SU(2) transformation and embed it into the larger matrix\n                Rij_inv = np.array([[np.conj(capY), np.conj(capZ)], [-capZ, capY]])\n            else:\n                # The last transformation, R12 is special and the rotation has\n                # a different form\n                x = U[0, 0]\n                cf = np.sqrt(1 - pow(np.absolute(x), 2))\n                Rij_inv = np.array([[np.conj(x), cf], [-cf, x]])\n\n            # Add the transformation to the sequence and update the product\n            Rij = Rij_inv.conj().T\n            transformations.append(_su2_parameters(Rij))\n\n            # Embed into larger space\n            full_Rij_inv[i : j + 1, i : j + 1] = Rij_inv\n            running_prod = full_Rij_inv @ running_prod\n\n    return transformations, running_prod\n\n\ndef _su2_parameters(U, tol=1e-11):\n    r\"\"\"Compute and return the parameters ``[a, b, g]`` of an :math:`\\mathrm{SU}(2)` matrix.\n\n    Args:\n        U (array): unitary matrix of shape ``(2,2)`` with :math:`\\det U = 1`\n\n    Returns:\n        list: a list of parameters ``[a, b, g]`` of the :math:`\\mathrm{SU}(2)` matrix\n\n    .. details::\n\n    Given a matrix in :math:`\\mathrm{SU}(2)`, parametrized as\n\n    .. math:\n\n        U(a, b, g) =\n        \\begin{pmatrix}\n            e^{i(\\alpha+\\gamma)/2} \\cos(\\beta/2)   & -e^{i(\\alpha-\\gamma)/2} \\sin(\\beta/2) \\\\\n            e^{-i(\\alpha-\\gamma)/2} \\sin(\\beta/2)  & e^{-i(\\alpha+\\gamma)/2} \\cos(\\beta/2)\n        \\end{pmatrix}\n\n    compute and return the parameters :math:`\\alpha, \\beta, \\gamma`.\n    \"\"\"\n    if U.shape != (2, 2):\n        raise ValueError(\"Input matrix dimensions of _su2_parameters must be 2x2.\")\n    if not np.isclose(np.linalg.det(U), 1, atol=tol, rtol=0):\n        raise ValueError(\n            \"Input matrix must have determinant 1 to be decomposed into SU(2) parameters.\"\n        )\n\n    # Sometimes the absolute value of the matrix entry is very, very close to\n    # 1 and slightly above, when it should be 1 exactly. Isolate these cases\n    # to prevent us from getting NaN.\n    b = None\n    if np.isclose(np.absolute(U[0, 1]), 1, atol=tol, rtol=0):\n        b = 2 * np.arcsin(1)\n    else:\n        b = 2 * np.arcsin(np.absolute(U[0, 1]))\n\n    arg_pos = np.angle(U[0, 0])  # (a + g)/2\n    arg_neg = -np.angle(U[1, 0])  # (a - g)/2\n    a, g = arg_pos + arg_neg, arg_pos - arg_neg\n    return [a, b, g]\n\n\ndef _su3_parameters(U):\n    r\"\"\"Factorizes an :math:`\\mathrm{SU}(3)` transformation into 3 :math:`\\mathrm{SU}(2)`\n    transformations.\n\n    Args:\n        U (array): unitary matrix of shape ``(3,3)`` with :math:`\\det U = 1`\n\n    Returns:\n        list[list]: a list containing three entries of the form ``[a, b, g]``, where every\n        entry has the parameters of an :math:`\\mathrm{SU}(2)` matrix.\n\n    .. details::\n\n        Uses the factorization on :cite:`rowe1999representations` to factorize an\n        :math:`\\mathrm{SU}(3)` transformation into 3 :math:`\\mathrm{SU}(2)` transformations.\n        Parameters for each :math:`\\mathrm{SU}(3)` transformation are returned as a list\n        :math:`[\\alpha, \\beta, \\gamma]` (three-parameter transformation) or\n        :math:`[\\alpha, \\beta, \\alpha]` (two-parameter transformation) where the matrices\n        are to be parametrized as\n\n    .. math::\n\n        SU_{ij}(\\alpha, \\beta, \\gamma) =\n        \\begin{pmatrix}\n            e^{i(\\alpha+\\gamma)/2} \\cos(\\beta/2)   & -e^{i(\\alpha-\\gamma)/2} \\sin(\\beta/2) \\\\\n            e^{-i(\\alpha-\\gamma)/2} \\sin(\\beta/2)  & e^{-i(\\alpha+\\gamma)/2} \\cos(\\beta/2)\n        \\end{pmatrix}\n\n    The `ij` subscript indicates that the matrix should be embedded into modes `i` and `j` of the full\n    `n`-dimensional transformation.\n\n    The resultant matrix is expressed as\n\n    .. math::\n\n        U = SU_{23}(\\alpha_1, \\beta_1, \\gamma_1)\n        SU_{12}(\\alpha_2, \\beta_2, \\alpha_2)\n        SU_{23}(\\alpha_3, \\beta_3, \\gamma_3).\n    \"\"\"\n    if U.shape != (3, 3):\n        raise ValueError(\"Input matrix dimensions of _su3_parameters must be 3x3.\")\n    if not np.isclose(np.linalg.det(U), 1):\n        raise ValueError(\n            \"Input matrix must have determinant 1 to be decomposed into SU(2) parameters.\"\n        )\n\n    # Grab the entries of the first row\n    x, y, z = U[0, 0], U[1, 0], U[2, 0]\n\n    # Special case: if the top left element is 1, then we essentially\n    # already have an SU(2) transformation embedded in an SU(3) transform,\n    # so all we need to do is get the parameters of that SU(2) transform.\n    if np.isclose(x, 1):\n        params = [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], _su2_parameters(U[1:, 1:])]\n    # Another special case: the modulus of the top left element is 1.\n    # Then we need to do a transformation on modes 1 and 2 to make the top\n    # entry 1, then an SU(2) transformation on modes 2 and 3 with what's left.\n    elif np.isclose(np.abs(x), 1):\n        # Compute the required phase matrix and embed into SU(3)\n        phase_su2 = np.array([[np.conj(x), 0], [0, x]])\n\n        full_phase_su2 = np.asarray(np.identity(3)) + 0j\n        full_phase_su2[0:2, 0:2] = phase_su2\n\n        # Compute what's left of the product, and the parameters\n        running_product = full_phase_su2 @ U\n        remainder_su2 = running_product[1:, 1:]\n\n        params = [\n            [0.0, 0.0, 0.0],\n            _su2_parameters(phase_su2.conj().T),\n            _su2_parameters(remainder_su2),\n        ]\n\n    else:\n        # Typical case\n        cf = np.sqrt(1 - pow(np.absolute(x), 2))\n        capY, capZ = y / cf, z / cf\n\n        # Build the SU(2) transformation matrices\n        # SU_23(3) - three parameters\n        left = np.array([[1, 0, 0], [0, capY, -np.conj(capZ)], [0, capZ, np.conj(capY)]])\n        left_params = _su2_parameters(left[1:, 1:])\n\n        # SU_12(2) - only two parameters\n        middle = np.array([[x, -cf, 0], [cf, np.conj(x), 0], [0, 0, 1]])\n        middle_params = _su2_parameters(middle[0:2, 0:2])\n\n        # SU_23(3) - again three parameters\n        right = middle.conj().T @ left.conj().T @ U\n        right_params = _su2_parameters(right[1:, 1:])\n\n        params = [left_params, middle_params, right_params]\n\n    return params\n\n\ndef covmat_to_hamil(V, tol=1e-10):  # pragma: no cover\n    r\"\"\"Converts a covariance matrix to a Hamiltonian.\n\n    Given a covariance matrix V of a Gaussian state :math:`\\rho` in the xp ordering,\n    finds a positive matrix :math:`H` such that\n\n    .. math:: \\rho = \\exp(-Q^T H Q/2)/Z\n\n    where :math:`Q = (x_1,\\dots,x_n,p_1,\\dots,p_n)` are the canonical\n    operators, and Z is the partition function.\n\n    For more details, see https://arxiv.org/abs/1507.01941\n\n    Args:\n        V (array): Gaussian covariance matrix\n        tol (int): the number of decimal places to use when determining if the matrix is symmetric\n\n    Returns:\n        array: positive definite Hamiltonian matrix\n    \"\"\"\n    (n, m) = V.shape\n    if n != m:\n        raise ValueError(\"Input matrix must be square\")\n    if np.linalg.norm(V - np.transpose(V)) >= tol:\n        raise ValueError(\"The input matrix is not symmetric\")\n\n    n = n // 2\n    omega = sympmat(n)\n\n    vals = np.linalg.eigvalsh(V)\n    for val in vals:\n        if val <= 0:\n            raise ValueError(\"Input matrix is not positive definite\")\n\n    W = 1j * V @ omega\n    l, v = np.linalg.eig(W)\n    H = (1j * omega @ (v @ np.diag(np.arctanh(1.0 / l.real)) @ np.linalg.inv(v))).real\n\n    return H\n\n\ndef hamil_to_covmat(H, tol=1e-10):  # pragma: no cover\n    r\"\"\"Converts a Hamiltonian matrix to a covariance matrix.\n\n    Given a Hamiltonian matrix of a Gaussian state H, finds the equivalent covariance matrix\n    V in the xp ordering.\n\n    For more details, see https://arxiv.org/abs/1507.01941\n\n    Args:\n        H (array): positive definite Hamiltonian matrix\n        tol (int): the number of decimal places to use when determining if the Hamiltonian is symmetric\n\n    Returns:\n        array: Gaussian covariance matrix\n    \"\"\"\n    (n, m) = H.shape\n    if n != m:\n        raise ValueError(\"Input matrix must be square\")\n    if np.linalg.norm(H - np.transpose(H)) >= tol:\n        raise ValueError(\"The input matrix is not symmetric\")\n\n    vals = np.linalg.eigvalsh(H)\n    for val in vals:\n        if val <= 0:\n            raise ValueError(\"Input matrix is not positive definite\")\n\n    n = n // 2\n    omega = sympmat(n)\n\n    Wi = 1j * omega @ H\n    l, v = np.linalg.eig(Wi)\n    V = (1j * (v @ np.diag(1.0 / np.tanh(l.real)) @ np.linalg.inv(v)) @ omega).real\n    return V\n", "import_text": ["itertools.groupby", "collections.defaultdict", "numpy", "scipy.linalg.block_diag", "scipy.linalg.sqrtm", "scipy.linalg.polar", "scipy.linalg.schur", "thewalrus.quantum.adj_scaling", "thewalrus.symplectic.sympmat", "thewalrus.symplectic.xpxp_to_xxpp"], "prompt": "\"\"\"\nDescription: This function performs a Williamson decomposition of a given positive definite matrix.\n\nArgs:\n    V (numpy.ndarray): The input matrix to be decomposed. It should be a square, symmetric, and positive definite matrix with an even number of rows/columns.\n    tol (float, optional): The tolerance for checking if the matrix is symmetric. Defaults to 1e-11.\n\nReturns:\n    tuple: A tuple containing two elements:\n        1. Db (numpy.ndarray): A diagonal matrix of size n x n.\n        2. S_inv_transpose (numpy.ndarray): The transpose of the inverse of the matrix S.\n\nRaises:\n    ValueError: If the input matrix is not square, not symmetric, does not have an even number of rows/columns, or is not positive definite.\n\nNotes:\n    This function uses the scipy.linalg.sqrtm and numpy.linalg.inv functions to perform the matrix square root and matrix inversion operations, respectively.\n\"\"\"", "comment": "    r\"\"\"Williamson decomposition of positive-definite (real) symmetric matrix.\n\n    See :ref:`williamson`.\n\n    Note that it is assumed that the symplectic form is\n\n    .. math:: \\Omega = \\begin{bmatrix}0&I\\\\-I&0\\end{bmatrix}\n\n    where :math:`I` is the identity matrix and :math:`0` is the zero matrix.\n\n    See https://math.stackexchange.com/questions/1171842/finding-the-symplectic-matrix-in-williamsons-theorem/2682630#2682630\n\n    Args:\n        V (array[float]): positive definite symmetric (real) matrix\n        tol (float): the tolerance used when checking if the matrix is symmetric: :math:`|V-V^T| \\leq` tol\n\n    Returns:\n        tuple[array,array]: ``(Db, S)`` where ``Db`` is a diagonal matrix\n            and ``S`` is a symplectic matrix such that :math:`V = S^T Db S`\n    \"\"\"", "prompt_is_gen_from_api": true, "function_dependencies": ["numpy.linalg.norm", "numpy.transpose", "thewalrus.symplectic.sympmat", "numpy.linalg.eigvalsh", "scipy.linalg.sqrtm", "numpy.linalg.inv", "scipy.linalg.schur", "numpy.array", "numpy.identity", "scipy.linalg.block_diag", "thewalrus.symplectic.xpxp_to_xxpp", "numpy.arange", "numpy.diag"], "project_create_time": "2018-03-26T14:38:39+00:00", "project_update_time": "2024-04-16T10:59:39+00:00", "file_create_time": "2019-01-23T23:14:33Z", "file_update_time": "2022-02-24T21:45:57Z", "function_update_time": "2019-01-23T23:14:33Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["scipy.linalg.sqrtm", "numpy.linalg.inv"], "test_function": [{"file_path": "/strawberryfields-v0.23.0/strawberryfields-0.23.0/tests/frontend/test_decompositions.py", "class_name": "TestWilliamsonDecomposition", "function_name": "test_pure_state", "code": "    def test_pure_state(self, create_cov, hbar, tol):\n        n = 3\n        O = omega(n)\n\n        cov, _ = create_cov(np.zeros([n]))\n\n        Db, S = dec.williamson(cov)\n        nbar = np.diag(Db) / hbar - 0.5\n\n        # check decomposition is correct\n        assert np.allclose(S @ Db @ S.T, cov, atol=tol, rtol=0)\n        # check nbar = 0\n        assert np.allclose(nbar, 0, atol=tol, rtol=0)\n        # check S is symplectic\n        assert np.allclose(S @ O @ S.T, O, atol=tol, rtol=0)"}, {"file_path": "/strawberryfields-v0.23.0/strawberryfields-0.23.0/tests/frontend/test_decompositions.py", "class_name": "TestWilliamsonDecomposition", "function_name": "test_mixed_state", "code": "    def test_mixed_state(self, create_cov, hbar, tol):\n        n = 3\n        O = omega(n)\n        nbar_in = np.abs(np.random.random(n))\n\n        cov, _ = create_cov(nbar_in)\n\n        Db, S = dec.williamson(cov)\n        nbar = np.diag(Db) / hbar - 0.5\n\n        # check decomposition is correct\n        assert np.allclose(S @ Db @ S.T, cov, atol=tol, rtol=0)\n        # check nbar\n        assert np.allclose(sorted(nbar[:n]), sorted(nbar_in), atol=tol, rtol=0)\n        # check S is symplectic\n        assert np.allclose(S @ O @ S.T, O, atol=tol, rtol=0)"}]}, {"git_group": "reflex-dev", "git_name": "reflex", "version": "v0.4.9", "language": "Python", "project_name": "reflex-v0.4.9.zip", "file_path": "/reflex-v0.4.9/reflex-0.4.9/reflex/model.py", "file_name": "model.py", "focal_class": "Model", "focal_name": "alembic_autogenerate", "focal_parameter": ["cls"], "solution": "    def alembic_autogenerate(\n        cls,\n        connection: sqlalchemy.engine.Connection,\n        message: str | None = None,\n        write_migration_scripts: bool = True,\n    ) -> bool:\n        if not Path(constants.ALEMBIC_CONFIG).exists():\n            return False\n\n        config, script_directory = cls._alembic_config()\n        revision_context = alembic.autogenerate.api.RevisionContext(\n            config=config,\n            script_directory=script_directory,\n            command_args=defaultdict(\n                lambda: None,\n                autogenerate=True,\n                head=\"head\",\n                message=message,\n            ),\n        )\n        writer = alembic.autogenerate.rewriter.Rewriter()\n\n        @writer.rewrites(alembic.operations.ops.AddColumnOp)\n        def render_add_column_with_server_default(context, revision, op):\n            # Carry the sqlmodel default as server_default so that newly added\n            # columns get the desired default value in existing rows.\n            if op.column.default is not None and op.column.server_default is None:\n                op.column.server_default = sqlalchemy.DefaultClause(\n                    sqlalchemy.sql.expression.literal(op.column.default.arg),\n                )\n            return op\n\n        def run_autogenerate(rev, context):\n            revision_context.run_autogenerate(rev, context)\n            return []\n\n        with alembic.runtime.environment.EnvironmentContext(\n            config=config,\n            script=script_directory,\n            fn=run_autogenerate,\n        ) as env:\n            env.configure(\n                connection=connection,\n                target_metadata=ModelRegistry.get_metadata(),\n                render_item=cls._alembic_render_item,\n                process_revision_directives=writer,  # type: ignore\n                compare_type=False,\n            )\n            env.run_migrations()\n        changes_detected = False\n        if revision_context.generated_revisions:\n            upgrade_ops = revision_context.generated_revisions[-1].upgrade_ops\n            if upgrade_ops is not None:\n                changes_detected = bool(upgrade_ops.ops)\n        if changes_detected and write_migration_scripts:\n            # Must iterate the generator to actually write the scripts.\n            _ = tuple(revision_context.generate_scripts())\n        return changes_detected", "function_signature": "def alembic_autogenerate(\n        cls,\n        connection: sqlalchemy.engine.Connection,\n        message: str | None = None,\n        write_migration_scripts: bool = True,\n    ) -> bool :", "left_context": "\"\"\"Database built into Reflex.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Any, ClassVar, Optional, Type, Union\n\nimport alembic.autogenerate\nimport alembic.command\nimport alembic.config\nimport alembic.operations.ops\nimport alembic.runtime.environment\nimport alembic.script\nimport alembic.util\nimport sqlalchemy\nimport sqlalchemy.orm\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.config import get_config\nfrom reflex.utils import console\nfrom reflex.utils.compat import sqlmodel\n\n\ndef get_engine(url: str | None = None):\n    \"\"\"Get the database engine.\n\n    Args:\n        url: the DB url to use.\n\n    Returns:\n        The database engine.\n\n    Raises:\n        ValueError: If the database url is None.\n    \"\"\"\n    conf = get_config()\n    url = url or conf.db_url\n    if url is None:\n        raise ValueError(\"No database url configured\")\n    if not Path(constants.ALEMBIC_CONFIG).exists():\n        console.warn(\n            \"Database is not initialized, run [bold]reflex db init[/bold] first.\"\n        )\n    # Print the SQL queries if the log level is INFO or lower.\n    echo_db_query = os.environ.get(\"SQLALCHEMY_ECHO\") == \"True\"\n    # Needed for the admin dash on sqlite.\n    connect_args = {\"check_same_thread\": False} if url.startswith(\"sqlite\") else {}\n    return sqlmodel.create_engine(url, echo=echo_db_query, connect_args=connect_args)\n\n\nSQLModelOrSqlAlchemy = Union[\n    Type[sqlmodel.SQLModel], Type[sqlalchemy.orm.DeclarativeBase]\n]\n\n\nclass ModelRegistry:\n    \"\"\"Registry for all models.\"\"\"\n\n    models: ClassVar[set[SQLModelOrSqlAlchemy]] = set()\n\n    # Cache the metadata to avoid re-creating it.\n    _metadata: ClassVar[sqlalchemy.MetaData | None] = None\n\n    @classmethod\n    def register(cls, model: SQLModelOrSqlAlchemy):\n        \"\"\"Register a model. Can be used directly or as a decorator.\n\n        Args:\n            model: The model to register.\n\n        Returns:\n            The model passed in as an argument (Allows decorator usage)\n        \"\"\"\n        cls.models.add(model)\n        return model\n\n    @classmethod\n    def get_models(cls, include_empty: bool = False) -> set[SQLModelOrSqlAlchemy]:\n        \"\"\"Get registered models.\n\n        Args:\n            include_empty: If True, include models with empty metadata.\n\n        Returns:\n            The registered models.\n        \"\"\"\n        if include_empty:\n            return cls.models\n        return {\n            model for model in cls.models if not cls._model_metadata_is_empty(model)\n        }\n\n    @staticmethod\n    def _model_metadata_is_empty(model: SQLModelOrSqlAlchemy) -> bool:\n        \"\"\"Check if the model metadata is empty.\n\n        Args:\n            model: The model to check.\n\n        Returns:\n            True if the model metadata is empty, False otherwise.\n        \"\"\"\n        return len(model.metadata.tables) == 0\n\n    @classmethod\n    def get_metadata(cls) -> sqlalchemy.MetaData:\n        \"\"\"Get the database metadata.\n\n        Returns:\n            The database metadata.\n        \"\"\"\n        if cls._metadata is not None:\n            return cls._metadata\n\n        models = cls.get_models(include_empty=False)\n\n        if len(models) == 1:\n            metadata = next(iter(models)).metadata\n        else:\n            # Merge the metadata from all the models.\n            # This allows mixing bare sqlalchemy models with sqlmodel models in one database.\n            metadata = sqlalchemy.MetaData()\n            for model in cls.get_models():\n                for table in model.metadata.tables.values():\n                    table.to_metadata(metadata)\n\n        # Cache the metadata\n        cls._metadata = metadata\n\n        return metadata\n\n\nclass Model(Base, sqlmodel.SQLModel):\n    \"\"\"Base class to define a table in the database.\"\"\"\n\n    # The primary key for the table.\n    id: Optional[int] = sqlmodel.Field(default=None, primary_key=True)\n\n    def __init_subclass__(cls):\n        \"\"\"Drop the default primary key field if any primary key field is defined.\"\"\"\n        non_default_primary_key_fields = [\n            field_name\n            for field_name, field in cls.__fields__.items()\n            if field_name != \"id\"\n            and getattr(field.field_info, \"primary_key\", None) is True\n        ]\n        if non_default_primary_key_fields:\n            cls.__fields__.pop(\"id\", None)\n\n        super().__init_subclass__()\n\n    @classmethod\n    def _dict_recursive(cls, value):\n        \"\"\"Recursively serialize the relationship object(s).\n\n        Args:\n            value: The value to serialize.\n\n        Returns:\n            The serialized value.\n        \"\"\"\n        if hasattr(value, \"dict\"):\n            return value.dict()\n        elif isinstance(value, list):\n            return [cls._dict_recursive(item) for item in value]\n        return value\n\n    def dict(self, **kwargs):\n        \"\"\"Convert the object to a dictionary.\n\n        Args:\n            kwargs: Ignored but needed for compatibility.\n\n        Returns:\n            The object as a dictionary.\n        \"\"\"\n        base_fields = {name: getattr(self, name) for name in self.__fields__}\n        relationships = {}\n        # SQLModel relationships do not appear in __fields__, but should be included if present.\n        for name in self.__sqlmodel_relationships__:\n            try:\n                relationships[name] = self._dict_recursive(getattr(self, name))\n            except sqlalchemy.orm.exc.DetachedInstanceError:\n                # This happens when the relationship was never loaded and the session is closed.\n                continue\n        return {\n            **base_fields,\n            **relationships,\n        }\n\n    @staticmethod\n    def create_all():\n        \"\"\"Create all the tables.\"\"\"\n        engine = get_engine()\n        ModelRegistry.get_metadata().create_all(engine)\n\n    @staticmethod\n    def get_db_engine():\n        \"\"\"Get the database engine.\n\n        Returns:\n            The database engine.\n        \"\"\"\n        return get_engine()\n\n    @staticmethod\n    def _alembic_config():\n        \"\"\"Get the alembic configuration and script_directory.\n\n        Returns:\n            tuple of (config, script_directory)\n        \"\"\"\n        config = alembic.config.Config(constants.ALEMBIC_CONFIG)\n        return config, alembic.script.ScriptDirectory(\n            config.get_main_option(\"script_location\", default=\"version\"),\n        )\n\n    @staticmethod\n    def _alembic_render_item(\n        type_: str,\n        obj: Any,\n        autogen_context: \"alembic.autogenerate.api.AutogenContext\",\n    ):\n        \"\"\"Alembic render_item hook call.\n\n        This method is called to provide python code for the given obj,\n        but currently it is only used to add `sqlmodel` to the import list\n        when generating migration scripts.\n\n        See https://alembic.sqlalchemy.org/en/latest/api/runtime.html\n\n        Args:\n            type_: One of \"schema\", \"table\", \"column\", \"index\",\n                \"unique_constraint\", or \"foreign_key_constraint\".\n            obj: The object being rendered.\n            autogen_context: Shared AutogenContext passed to each render_item call.\n\n        Returns:\n            False - Indicating that the default rendering should be used.\n        \"\"\"\n        autogen_context.imports.add(\"import sqlmodel\")\n        return False\n\n    @classmethod\n    def alembic_init(cls):\n        \"\"\"Initialize alembic for the project.\"\"\"\n        alembic.command.init(\n            config=alembic.config.Config(constants.ALEMBIC_CONFIG),\n            directory=str(Path(constants.ALEMBIC_CONFIG).parent / \"alembic\"),\n        )\n\n    @classmethod", "right_context": "\n    @classmethod\n    def _alembic_upgrade(\n        cls,\n        connection: sqlalchemy.engine.Connection,\n        to_rev: str = \"head\",\n    ) -> None:\n        \"\"\"Apply alembic migrations up to the given revision.\n\n        Args:\n            connection: SQLAlchemy connection to use when performing upgrade.\n            to_rev: Revision to migrate towards.\n        \"\"\"\n        config, script_directory = cls._alembic_config()\n\n        def run_upgrade(rev, context):\n            return script_directory._upgrade_revs(to_rev, rev)\n\n        with alembic.runtime.environment.EnvironmentContext(\n            config=config,\n            script=script_directory,\n            fn=run_upgrade,\n        ) as env:\n            env.configure(connection=connection)\n            env.run_migrations()\n\n    @classmethod\n    def migrate(cls, autogenerate: bool = False) -> bool | None:\n        \"\"\"Execute alembic migrations for all sqlmodel Model classes.\n\n        If alembic is not installed or has not been initialized for the project,\n        then no action is performed.\n\n        If there are no revisions currently tracked by alembic, then\n        an initial revision will be created based on sqlmodel metadata.\n\n        If models in the app have changed in incompatible ways that alembic\n        cannot automatically generate revisions for, the app may not be able to\n        start up until migration scripts have been corrected by hand.\n\n        Args:\n            autogenerate: If True, generate migration script and use it to upgrade schema\n                (otherwise, just bring the schema to current \"head\" revision).\n\n        Returns:\n            True - indicating the process was successful.\n            None - indicating the process was skipped.\n        \"\"\"\n        if not Path(constants.ALEMBIC_CONFIG).exists():\n            return\n\n        with cls.get_db_engine().connect() as connection:\n            cls._alembic_upgrade(connection=connection)\n            if autogenerate:\n                changes_detected = cls.alembic_autogenerate(connection=connection)\n                if changes_detected:\n                    cls._alembic_upgrade(connection=connection)\n            connection.commit()\n        return True\n\n    @classmethod\n    def select(cls):\n        \"\"\"Select rows from the table.\n\n        Returns:\n            The select statement.\n        \"\"\"\n        return sqlmodel.select(cls)\n\n\nModelRegistry.register(Model)\n\n\ndef session(url: str | None = None) -> sqlmodel.Session:\n    \"\"\"Get a session to interact with the database.\n\n    Args:\n        url: The database url.\n\n    Returns:\n        A database session.\n    \"\"\"\n    return sqlmodel.Session(get_engine(url))\n", "import_text": ["os", "collections.defaultdict", "pathlib.Path", "typing.Any", "typing.ClassVar", "typing.Optional", "typing.Type", "typing.Union", "alembic.autogenerate", "alembic.command", "alembic.config", "alembic.operations.ops", "alembic.runtime.environment", "alembic.script", "alembic.util", "sqlalchemy", "sqlalchemy.orm", "reflex.constants", "reflex.base.Base", "reflex.config.get_config", "reflex.utils.console", "reflex.utils.compat.sqlmodel"], "prompt": "\"\"\"\nDescription: This function is used to autogenerate alembic migrations.\n\nArgs:\n    cls: The class that the function is called on.\n    connection (sqlalchemy.engine.Connection): The database connection to use.\n    message (str | None, optional): The message to include in the alembic revision. Defaults to None.\n    write_migration_scripts (bool, optional): Whether to write the migration scripts. Defaults to True.\n\nReturns:\n    bool: Whether changes were detected in the autogeneration process.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"Generate migration scripts for alembic-detectable changes.\n\n        Args:\n            connection: SQLAlchemy connection to use when detecting changes.\n            message: Human readable identifier describing the generated revision.\n            write_migration_scripts: If True, write autogenerated revisions to script directory.\n\n        Returns:\n            True when changes have been detected.\n        \"\"\"", "function_dependencies": ["pathlib.Path", "pathlib.Path.exists", "collections.defaultdict", "sqlalchemy.DefaultClause", "sqlalchemy.sql.expression.literal"], "project_create_time": "2022-10-25T03:08:48+00:00", "project_update_time": "2024-04-18T03:15:58+00:00", "file_create_time": "2023-06-25T23:56:55Z", "file_update_time": "2024-04-11T20:42:30Z", "function_update_time": "2023-06-25T23:56:55Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["sqlalchemy.DefaultClause"], "test_function": [{"file_path": "/reflex-v0.4.9/reflex-0.4.9/tests/test_model.py", "class_name": null, "function_name": "test_automigration", "code": "def test_automigration(tmp_working_dir, monkeypatch):\n    alembic_ini = tmp_working_dir / \"alembic.ini\"\n    versions = tmp_working_dir / \"alembic\" / \"versions\"\n    monkeypatch.setattr(reflex.constants, \"ALEMBIC_CONFIG\", str(alembic_ini))\n\n    config_mock = mock.Mock()\n    config_mock.db_url = f\"sqlite:///{tmp_working_dir}/reflex.db\"\n    monkeypatch.setattr(reflex.model, \"get_config\", mock.Mock(return_value=config_mock))\n\n    Model.alembic_init()\n    assert alembic_ini.exists()\n    assert versions.exists()\n\n    # initial table\n    class AlembicThing(Model, table=True):  # type: ignore\n        t1: str\n\n    with Model.get_db_engine().connect() as connection:\n        Model.alembic_autogenerate(connection=connection, message=\"Initial Revision\")\n    Model.migrate()\n    version_scripts = list(versions.glob(\"*.py\"))\n    assert len(version_scripts) == 1\n    assert version_scripts[0].name.endswith(\"initial_revision.py\")\n\n    with reflex.model.session() as session:\n        session.add(AlembicThing(id=None, t1=\"foo\"))\n        session.commit()\n\n    sqlmodel.SQLModel.metadata.clear()\n\n    # Create column t2\n    class AlembicThing(Model, table=True):  # type: ignore\n        t1: str\n        t2: str = \"bar\"\n\n    Model.migrate(autogenerate=True)\n    assert len(list(versions.glob(\"*.py\"))) == 2\n\n    with reflex.model.session() as session:\n        result = session.exec(sqlmodel.select(AlembicThing)).all()\n        assert len(result) == 1\n        assert result[0].t2 == \"bar\"\n\n    sqlmodel.SQLModel.metadata.clear()\n\n    # Drop column t1\n    class AlembicThing(Model, table=True):  # type: ignore\n        t2: str = \"bar\"\n\n    Model.migrate(autogenerate=True)\n    assert len(list(versions.glob(\"*.py\"))) == 3\n\n    with reflex.model.session() as session:\n        result = session.exec(sqlmodel.select(AlembicThing)).all()\n        assert len(result) == 1\n        assert result[0].t2 == \"bar\"\n\n    # Add table\n    class AlembicSecond(Model, table=True):  # type: ignore\n        a: int = 42\n        b: float = 4.2\n\n    Model.migrate(autogenerate=True)\n    assert len(list(versions.glob(\"*.py\"))) == 4\n\n    with reflex.model.session() as session:\n        session.add(AlembicSecond(id=None))\n        session.commit()\n        result = session.exec(sqlmodel.select(AlembicSecond)).all()\n        assert len(result) == 1\n        assert result[0].a == 42\n        assert result[0].b == 4.2\n\n    # No-op\n    Model.migrate(autogenerate=True)\n    assert len(list(versions.glob(\"*.py\"))) == 4\n\n    # drop table (AlembicSecond)\n    sqlmodel.SQLModel.metadata.clear()\n\n    class AlembicThing(Model, table=True):  # type: ignore\n        t2: str = \"bar\"\n\n    Model.migrate(autogenerate=True)\n    assert len(list(versions.glob(\"*.py\"))) == 5\n\n    with reflex.model.session() as session:\n        with pytest.raises(sqlalchemy.exc.OperationalError) as errctx:  # type: ignore\n            session.exec(sqlmodel.select(AlembicSecond)).all()\n        assert errctx.match(r\"no such table: alembicsecond\")\n        # first table should still exist\n        result = session.exec(sqlmodel.select(AlembicThing)).all()\n        assert len(result) == 1\n        assert result[0].t2 == \"bar\"\n\n    sqlmodel.SQLModel.metadata.clear()\n\n    class AlembicThing(Model, table=True):  # type: ignore\n        # changing column type not supported by default\n        t2: int = 42\n\n    Model.migrate(autogenerate=True)\n    assert len(list(versions.glob(\"*.py\"))) == 5\n\n    # clear all metadata to avoid influencing subsequent tests\n    sqlmodel.SQLModel.metadata.clear()\n\n    # drop remaining tables\n    Model.migrate(autogenerate=True)\n    assert len(list(versions.glob(\"*.py\"))) == 6"}]}, {"git_group": "motional", "git_name": "nuplan-devkit", "version": "nuplan-devkit-v1.2", "language": "Python", "project_name": "nuplan-devkit-nuplan-devkit-v1.2.zip", "file_path": "/nuplan-devkit-nuplan-devkit-v1.2/nuplan-devkit-nuplan-devkit-v1.2/nuplan/planning/training/preprocessing/utils/vector_preprocessing.py", "file_name": "vector_preprocessing.py", "focal_class": null, "focal_name": "interpolate_points", "focal_parameter": [], "solution": "def interpolate_points(coords: torch.Tensor, max_points: int, interpolation: str) -> torch.Tensor:\n    if len(coords.shape) != 2 or coords.shape[1] != 2:\n        raise ValueError(f\"Unexpected coords shape: {coords.shape}. Expected shape: (*, 2)\")\n\n    x_coords = coords[:, 0].unsqueeze(0).unsqueeze(0)\n    y_coords = coords[:, 1].unsqueeze(0).unsqueeze(0)\n    align_corners = True if interpolation == 'linear' else None\n    x_coords = torch.nn.functional.interpolate(x_coords, max_points, mode=interpolation, align_corners=align_corners)\n    y_coords = torch.nn.functional.interpolate(y_coords, max_points, mode=interpolation, align_corners=align_corners)\n    coords = torch.stack((x_coords, y_coords), dim=-1).squeeze()\n\n    return coords", "function_signature": "def interpolate_points(coords: torch.Tensor, max_points: int, interpolation: str) -> torch.Tensor :", "left_context": "from typing import List, Optional, Tuple\n\nimport torch\n\n", "right_context": "\n\ndef convert_feature_layer_to_fixed_size(\n    feature_coords: List[torch.Tensor],\n    feature_tl_data_over_time: Optional[List[List[torch.Tensor]]],\n    max_elements: int,\n    max_points: int,\n    traffic_light_encoding_dim: int,\n    interpolation: Optional[str],\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], torch.Tensor]:\n    \"\"\"\n    Converts variable sized map features to fixed size tensors. Map elements are padded/trimmed to max_elements size.\n        Points per feature are interpolated to maintain max_points size.\n    :param feature_coords: Vector set of coordinates for collection of elements in map layer.\n        [num_elements, num_points_in_element (variable size), 2]\n    :param feature_tl_data_over_time: Optional traffic light status corresponding to map elements at given index in coords.\n        [num_frames, num_elements, traffic_light_encoding_dim (4)]\n    :param max_elements: Number of elements to pad/trim to.\n    :param max_points: Number of points to interpolate or pad/trim to.\n    :param traffic_light_encoding_dim: Dimensionality of traffic light data.\n    :param interpolation: Optional interpolation mode for maintaining fixed number of points per element.\n        None indicates trimming and zero-padding to take place in lieu of interpolation. Interpolation options:\n        'linear' and 'area'.\n    :return\n        coords_tensor: The converted coords tensor.\n        tl_data_tensor: The converted traffic light data tensor (if available).\n        avails_tensor: Availabilities tensor identifying real vs zero-padded data in coords_tensor and tl_data_tensor.\n    :raise ValueError: If coordinates and traffic light data size do not match.\n    \"\"\"\n    # trim or zero-pad elements to maintain fixed size\n    coords_tensor = torch.zeros((max_elements, max_points, 2), dtype=torch.float64)\n    avails_tensor = torch.zeros((max_elements, max_points), dtype=torch.bool)\n    tl_data_tensor = (\n        torch.zeros(\n            (len(feature_tl_data_over_time), max_elements, max_points, traffic_light_encoding_dim), dtype=torch.float32\n        )\n        if feature_tl_data_over_time is not None\n        else None\n    )\n    # tl_data_tensor: Tensor<num_frames, max_elements, max_points, traffic_light_encoding_dim>\n\n    for element_idx in range(min(len(feature_coords), max_elements)):\n        element_coords = feature_coords[element_idx]\n\n        # interpolate to maintain fixed size according to specified interpolation method if specified\n        if interpolation is not None:\n            num_points = max_points\n            element_coords = interpolate_points(element_coords, max_points, interpolation=interpolation)\n        # otherwise trim/zero-pad points to maintain fixed size\n        else:\n            num_points = min(len(element_coords), max_points)\n            element_coords = element_coords[:num_points]\n\n        coords_tensor[element_idx, :num_points] = element_coords\n        avails_tensor[element_idx, :num_points] = True  # specify real vs zero-padded data\n\n        if (feature_tl_data_over_time is not None) and (tl_data_tensor is not None):\n            for time_ind in range(len(feature_tl_data_over_time)):\n                if len(feature_coords) != len(feature_tl_data_over_time[time_ind]):\n                    raise ValueError(\n                        f\"num_elements between feature_coords and feature_tl_data_over_time inconsistent: {len(feature_coords)}, {len(feature_tl_data_over_time[time_ind])}\"\n                    )\n                tl_data_tensor[time_ind, element_idx, :num_points] = feature_tl_data_over_time[time_ind][element_idx]\n\n    return coords_tensor, tl_data_tensor, avails_tensor\n", "import_text": ["typing.List", "typing.Optional", "typing.Tuple", "torch"], "prompt": "\"\"\"\nDescription: This function interpolates a set of 2D coordinates to a specified maximum number of points.\n\nArgs:\n    coords (torch.Tensor): A tensor of shape (*, 2) representing the 2D coordinates to be interpolated.\n    max_points (int): The maximum number of points to interpolate to.\n    interpolation (str): The type of interpolation to use. Can be 'linear' or 'nearest'.\n\nReturns:\n    torch.Tensor: A tensor of shape (*, max_points, 2) representing the interpolated 2D coordinates.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Interpolate points within map element to maintain fixed size.\n    :param coords: Sequence of coordinate points representing map element. <torch.Tensor: num_points, 2>\n    :param max_points: Desired size to interpolate to.\n    :param interpolation: Torch interpolation mode. Available options: 'linear' and 'area'.\n    :return: Coordinate points interpolated to max_points size.\n    :raise ValueError: If coordinates dimensions are not valid.\n    \"\"\"", "function_dependencies": ["torch.nn.functional.interpolate", "torch.stack", "torch.stack.squeeze"], "project_create_time": "2021-05-27T08:42:11+00:00", "project_update_time": "2024-04-17T21:58:30+00:00", "file_create_time": "2022-08-04T14:34:11Z", "file_update_time": "2023-04-25T09:47:14Z", "function_update_time": "2022-08-04T14:34:11Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["torch.nn.functional.interpolate"], "test_function": [{"file_path": "/nuplan-devkit-nuplan-devkit-v1.2/nuplan-devkit-nuplan-devkit-v1.2/nuplan/planning/training/preprocessing/utils/test/test_vector_preprocessing.py", "class_name": "TestVectorPreprocessing", "function_name": "test_interpolate_points_functionality", "code": "    def test_interpolate_points_functionality(self) -> None:\n        coords = torch.tensor([[1, 1], [3, 1], [5, 1]], dtype=torch.float64)\n\n        interpolated_coords = interpolate_points(coords, 5, interpolation='linear')\n        self.assertEqual(interpolated_coords.shape, (5, 2))\n        torch.testing.assert_allclose(coords, interpolated_coords[::2])\n        torch.testing.assert_allclose(interpolated_coords[:, 1], torch.ones((5), dtype=torch.float64))\n        self.assertTrue(interpolated_coords[1][0].item() > interpolated_coords[0][0].item())\n        self.assertTrue(interpolated_coords[1][0].item() < interpolated_coords[2][0].item())\n        self.assertTrue(interpolated_coords[3][0].item() > interpolated_coords[2][0].item())\n        self.assertTrue(interpolated_coords[3][0].item() < interpolated_coords[4][0].item())\n\n        interpolated_coords = interpolate_points(coords, 5, interpolation='area')\n        self.assertEqual(interpolated_coords.shape, (5, 2))\n        torch.testing.assert_allclose(coords, interpolated_coords[::2])\n        torch.testing.assert_allclose(interpolated_coords[:, 1], torch.ones((5), dtype=torch.float64))\n        self.assertTrue(interpolated_coords[1][0].item() > interpolated_coords[0][0].item())\n        self.assertTrue(interpolated_coords[1][0].item() < interpolated_coords[2][0].item())\n        self.assertTrue(interpolated_coords[3][0].item() > interpolated_coords[2][0].item())\n        self.assertTrue(interpolated_coords[3][0].item() < interpolated_coords[4][0].item())"}, {"file_path": "/nuplan-devkit-nuplan-devkit-v1.2/nuplan-devkit-nuplan-devkit-v1.2/nuplan/planning/training/preprocessing/utils/test/test_vector_preprocessing.py", "class_name": "TestVectorPreprocessing", "function_name": "test_interpolate_points_scriptability", "code": "    def test_interpolate_points_scriptability(self) -> None:\n\n        class tmp_module(torch.nn.Module):\n            def __init__(self) -> None:\n                super().__init__()\n\n            def forward(self, coords: torch.Tensor, max_points: int, interpolation: str) -> torch.Tensor:\n                result = interpolate_points(coords, max_points, interpolation)\n                return result\n\n        to_script = tmp_module()\n        scripted = torch.jit.script(to_script)\n\n        test_coords = torch.tensor([[1, 1], [3, 1], [5, 1]], dtype=torch.float64)\n\n        py_result = to_script.forward(test_coords, 5, 'linear')\n        script_result = scripted.forward(test_coords, 5, 'linear')\n\n        torch.testing.assert_allclose(py_result, script_result)"}]}, {"git_group": "mne-tools", "git_name": "mne-python", "version": "v1.7.0", "language": "Python", "project_name": "mne-python-v1.7.0.zip", "file_path": "/mne-python-v1.7.0/mne-python-1.7.0/mne/transforms.py", "file_name": "transforms.py", "focal_class": null, "focal_name": "get_ras_to_neuromag_trans", "focal_parameter": ["nasion", "lpa", "rpa"], "solution": "def get_ras_to_neuromag_trans(nasion, lpa, rpa):\n    # check input args\n    nasion = np.asarray(nasion)\n    lpa = np.asarray(lpa)\n    rpa = np.asarray(rpa)\n    for pt in (nasion, lpa, rpa):\n        if pt.ndim != 1 or len(pt) != 3:\n            raise ValueError(\n                \"Points have to be provided as one dimensional \" \"arrays of length 3.\"\n            )\n\n    right = rpa - lpa\n    right_unit = right / np.linalg.norm(right)\n\n    origin = lpa + np.dot(nasion - lpa, right_unit) * right_unit\n\n    anterior = nasion - origin\n    anterior_unit = anterior / np.linalg.norm(anterior)\n\n    superior_unit = np.cross(right_unit, anterior_unit)\n\n    x, y, z = -origin\n    origin_trans = translation(x, y, z)\n\n    trans_l = np.vstack((right_unit, anterior_unit, superior_unit, [0, 0, 0]))\n    trans_r = np.reshape([0, 0, 0, 1], (4, 1))\n    rot_trans = np.hstack((trans_l, trans_r))\n\n    trans = np.dot(rot_trans, origin_trans)\n    return trans", "function_signature": "def get_ras_to_neuromag_trans(nasion, lpa, rpa) :", "left_context": "\"\"\"Helpers for various transformations.\"\"\"\n\n# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Christian Brodbeck <christianbrodbeck@nyu.edu>\n#\n# License: BSD-3-Clause\n# Copyright the MNE-Python contributors.\n\nimport glob\nimport os\nfrom copy import deepcopy\nfrom pathlib import Path\n\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.spatial.distance import cdist\nfrom scipy.special import sph_harm\n\nfrom ._fiff.constants import FIFF\nfrom ._fiff.open import fiff_open\nfrom ._fiff.tag import read_tag\nfrom ._fiff.write import start_and_end_file, write_coord_trans\nfrom .defaults import _handle_default\nfrom .fixes import _get_img_fdata, jit\nfrom .utils import (\n    _check_fname,\n    _check_option,\n    _ensure_int,\n    _import_nibabel,\n    _path_like,\n    _require_version,\n    _validate_type,\n    check_fname,\n    fill_doc,\n    get_subjects_dir,\n    logger,\n    verbose,\n    wrapped_stdout,\n)\n\n# transformation from anterior/left/superior coordinate system to\n# right/anterior/superior:\nals_ras_trans = np.array([[0, -1, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n\n\n_str_to_frame = dict(\n    meg=FIFF.FIFFV_COORD_DEVICE,\n    mri=FIFF.FIFFV_COORD_MRI,\n    mri_voxel=FIFF.FIFFV_MNE_COORD_MRI_VOXEL,\n    head=FIFF.FIFFV_COORD_HEAD,\n    mni_tal=FIFF.FIFFV_MNE_COORD_MNI_TAL,\n    ras=FIFF.FIFFV_MNE_COORD_RAS,\n    fs_tal=FIFF.FIFFV_MNE_COORD_FS_TAL,\n    ctf_head=FIFF.FIFFV_MNE_COORD_CTF_HEAD,\n    ctf_meg=FIFF.FIFFV_MNE_COORD_CTF_DEVICE,\n    unknown=FIFF.FIFFV_COORD_UNKNOWN,\n)\n_frame_to_str = {val: key for key, val in _str_to_frame.items()}\n\n_verbose_frames = {\n    FIFF.FIFFV_COORD_UNKNOWN: \"unknown\",\n    FIFF.FIFFV_COORD_DEVICE: \"MEG device\",\n    FIFF.FIFFV_COORD_ISOTRAK: \"isotrak\",\n    FIFF.FIFFV_COORD_HPI: \"hpi\",\n    FIFF.FIFFV_COORD_HEAD: \"head\",\n    FIFF.FIFFV_COORD_MRI: \"MRI (surface RAS)\",\n    FIFF.FIFFV_MNE_COORD_MRI_VOXEL: \"MRI voxel\",\n    FIFF.FIFFV_COORD_MRI_SLICE: \"MRI slice\",\n    FIFF.FIFFV_COORD_MRI_DISPLAY: \"MRI display\",\n    FIFF.FIFFV_MNE_COORD_CTF_DEVICE: \"CTF MEG device\",\n    FIFF.FIFFV_MNE_COORD_CTF_HEAD: \"CTF/4D/KIT head\",\n    FIFF.FIFFV_MNE_COORD_RAS: \"RAS (non-zero origin)\",\n    FIFF.FIFFV_MNE_COORD_MNI_TAL: \"MNI Talairach\",\n    FIFF.FIFFV_MNE_COORD_FS_TAL_GTZ: \"Talairach (MNI z > 0)\",\n    FIFF.FIFFV_MNE_COORD_FS_TAL_LTZ: \"Talairach (MNI z < 0)\",\n    -1: \"unknown\",\n}\n\n\ndef _to_const(cf):\n    \"\"\"Convert string or int coord frame into int.\"\"\"\n    if isinstance(cf, str):\n        if cf not in _str_to_frame:\n            raise ValueError(\n                f\"Unknown coordinate frame {cf}, \"\n                'expected \"' + '\", \"'.join(_str_to_frame.keys()) + '\"'\n            )\n        cf = _str_to_frame[cf]\n    else:\n        cf = _ensure_int(cf, \"coordinate frame\", \"a str or int\")\n    return int(cf)\n\n\nclass Transform(dict):\n    \"\"\"A transform.\n\n    Parameters\n    ----------\n    fro : str | int\n        The starting coordinate frame. See notes for valid coordinate frames.\n    to : str | int\n        The ending coordinate frame. See notes for valid coordinate frames.\n    trans : array of shape (4, 4) | None\n        The transformation matrix. If None, an identity matrix will be\n        used.\n\n    Notes\n    -----\n    Valid coordinate frames are ``'meg'``, ``'mri'``, ``'mri_voxel'``,\n    ``'head'``, ``'mri_tal'``, ``'ras'``, ``'fs_tal'``, ``'ctf_head'``,\n    ``'ctf_meg'``, ``'unknown'``.\n    \"\"\"\n\n    def __init__(self, fro, to, trans=None):\n        super().__init__()\n        # we could add some better sanity checks here\n        fro = _to_const(fro)\n        to = _to_const(to)\n        trans = np.eye(4) if trans is None else np.asarray(trans, np.float64)\n        if trans.shape != (4, 4):\n            raise ValueError(f\"Transformation must be shape (4, 4) not {trans.shape}\")\n        self[\"from\"] = fro\n        self[\"to\"] = to\n        self[\"trans\"] = trans\n\n    def __repr__(self):  # noqa: D105\n        with np.printoptions(suppress=True):  # suppress scientific notation\n            return \"<Transform | {fro}->{to}>\\n{trans}\".format(\n                fro=_coord_frame_name(self[\"from\"]),\n                to=_coord_frame_name(self[\"to\"]),\n                trans=self[\"trans\"],\n            )\n\n    def __eq__(self, other, rtol=0.0, atol=0.0):\n        \"\"\"Check for equality.\n\n        Parameter\n        ---------\n        other : instance of Transform\n            The other transform.\n        rtol : float\n            Relative tolerance.\n        atol : float\n            Absolute tolerance.\n\n        Returns\n        -------\n        eq : bool\n            True if the transforms are equal.\n        \"\"\"\n        return (\n            isinstance(other, Transform)\n            and self[\"from\"] == other[\"from\"]\n            and self[\"to\"] == other[\"to\"]\n            and np.allclose(self[\"trans\"], other[\"trans\"], rtol=rtol, atol=atol)\n        )\n\n    def __ne__(self, other, rtol=0.0, atol=0.0):\n        \"\"\"Check for inequality.\n\n        Parameter\n        ---------\n        other : instance of Transform\n            The other transform.\n        rtol : float\n            Relative tolerance.\n        atol : float\n            Absolute tolerance.\n\n        Returns\n        -------\n        eq : bool\n            True if the transforms are not equal.\n        \"\"\"\n        return not self == other\n\n    @property\n    def from_str(self):\n        \"\"\"The \"from\" frame as a string.\"\"\"\n        return _coord_frame_name(self[\"from\"])\n\n    @property\n    def to_str(self):\n        \"\"\"The \"to\" frame as a string.\"\"\"\n        return _coord_frame_name(self[\"to\"])\n\n    @fill_doc\n    @verbose\n    def save(self, fname, *, overwrite=False, verbose=None):\n        \"\"\"Save the transform as -trans.fif file.\n\n        Parameters\n        ----------\n        fname : path-like\n            The name of the file, which should end in ``-trans.fif``.\n        %(overwrite)s\n        %(verbose)s\n        \"\"\"\n        write_trans(fname, self, overwrite=overwrite, verbose=verbose)\n\n    def copy(self):\n        \"\"\"Make a copy of the transform.\"\"\"\n        return deepcopy(self)\n\n\ndef _coord_frame_name(cframe):\n    \"\"\"Map integers to human-readable (verbose) names.\"\"\"\n    return _verbose_frames.get(int(cframe), \"unknown\")\n\n\ndef _print_coord_trans(\n    t, prefix=\"Coordinate transformation: \", units=\"m\", level=\"info\"\n):\n    # Units gives the units of the transformation. This always prints in mm.\n    log_func = getattr(logger, level)\n    log_func(\n        prefix\n        + \"{fro} -> {to}\".format(\n            fro=_coord_frame_name(t[\"from\"]), to=_coord_frame_name(t[\"to\"])\n        )\n    )\n    for ti, tt in enumerate(t[\"trans\"]):\n        scale = 1000.0 if (ti != 3 and units != \"mm\") else 1.0\n        text = \" mm\" if ti != 3 else \"\"\n        log_func(\n            f\"    {tt[0]:8.6f} {tt[1]:8.6f} {tt[2]:8.6f}    {scale * tt[3]:7.2f}{text}\"\n        )\n\n\ndef _find_trans(subject, subjects_dir=None):\n    if subject is None:\n        if \"SUBJECT\" in os.environ:\n            subject = os.environ[\"SUBJECT\"]\n        else:\n            raise ValueError(\"SUBJECT environment variable not set\")\n\n    trans_fnames = glob.glob(str(subjects_dir / subject / \"*-trans.fif\"))\n    if len(trans_fnames) < 1:\n        raise RuntimeError(f\"Could not find the transformation for {subject}\")\n    elif len(trans_fnames) > 1:\n        raise RuntimeError(f\"Found multiple transformations for {subject}\")\n    return Path(trans_fnames[0])\n\n\ndef apply_trans(trans, pts, move=True):\n    \"\"\"Apply a transform matrix to an array of points.\n\n    Parameters\n    ----------\n    trans : array, shape = (4, 4) | instance of Transform\n        Transform matrix.\n    pts : array, shape = (3,) | (n, 3)\n        Array with coordinates for one or n points.\n    move : bool\n        If True (default), apply translation.\n\n    Returns\n    -------\n    transformed_pts : shape = (3,) | (n, 3)\n        Transformed point(s).\n    \"\"\"\n    if isinstance(trans, dict):\n        trans = trans[\"trans\"]\n    pts = np.asarray(pts)\n    if pts.size == 0:\n        return pts.copy()\n\n    # apply rotation & scale\n    out_pts = np.dot(pts, trans[:3, :3].T)\n    # apply translation\n    if move:\n        out_pts += trans[:3, 3]\n\n    return out_pts\n\n\ndef rotation(x=0, y=0, z=0):\n    \"\"\"Create an array with a 4 dimensional rotation matrix.\n\n    Parameters\n    ----------\n    x, y, z : scalar\n        Rotation around the origin (in rad).\n\n    Returns\n    -------\n    r : array, shape = (4, 4)\n        The rotation matrix.\n    \"\"\"\n    r = np.eye(4)\n    r[:3, :3] = rotation3d(x=x, y=y, z=z)\n    return r\n\n\ndef rotation3d(x=0, y=0, z=0):\n    \"\"\"Create an array with a 3 dimensional rotation matrix.\n\n    Parameters\n    ----------\n    x, y, z : scalar\n        Rotation around the origin (in rad).\n\n    Returns\n    -------\n    r : array, shape = (3, 3)\n        The rotation matrix.\n    \"\"\"\n    cos_x = np.cos(x)\n    cos_y = np.cos(y)\n    cos_z = np.cos(z)\n    sin_x = np.sin(x)\n    sin_y = np.sin(y)\n    sin_z = np.sin(z)\n    r = np.array(\n        [\n            [\n                cos_y * cos_z,\n                -cos_x * sin_z + sin_x * sin_y * cos_z,\n                sin_x * sin_z + cos_x * sin_y * cos_z,\n            ],\n            [\n                cos_y * sin_z,\n                cos_x * cos_z + sin_x * sin_y * sin_z,\n                -sin_x * cos_z + cos_x * sin_y * sin_z,\n            ],\n            [-sin_y, sin_x * cos_y, cos_x * cos_y],\n        ],\n        dtype=float,\n    )\n    return r\n\n\ndef rotation3d_align_z_axis(target_z_axis):\n    \"\"\"Compute a rotation matrix to align [ 0 0 1] with supplied target z axis.\n\n    Parameters\n    ----------\n    target_z_axis : array, shape (1, 3)\n        z axis. computed matrix (r) will map [0 0 1] to target_z_axis\n\n    Returns\n    -------\n    r : array, shape (3, 3)\n        The rotation matrix.\n    \"\"\"\n    target_z_axis = target_z_axis / np.linalg.norm(target_z_axis)\n    r = np.zeros((3, 3))\n    if (1.0 + target_z_axis[2]) < 1e-12:\n        r[0, 0] = 1.0\n        r[1, 1] = -1.0\n        r[2, 2] = -1.0\n    else:\n        f = 1.0 / (1.0 + target_z_axis[2])\n        r[0, 0] = 1.0 - 1.0 * f * target_z_axis[0] * target_z_axis[0]\n        r[0, 1] = -1.0 * f * target_z_axis[0] * target_z_axis[1]\n        r[0, 2] = target_z_axis[0]\n        r[1, 0] = -1.0 * f * target_z_axis[0] * target_z_axis[1]\n        r[1, 1] = 1.0 - 1.0 * f * target_z_axis[1] * target_z_axis[1]\n        r[1, 2] = target_z_axis[1]\n        r[2, 0] = -target_z_axis[0]\n        r[2, 1] = -target_z_axis[1]\n        r[2, 2] = 1.0 - f * (\n            target_z_axis[0] * target_z_axis[0] + target_z_axis[1] * target_z_axis[1]\n        )\n\n    # assert that r is a rotation matrix r^t * r = I and det(r) = 1\n    assert np.any((r.dot(r.T) - np.identity(3)) < 1e-12)\n    assert (np.linalg.det(r) - 1.0) < 1e-12\n    # assert that r maps [0 0 1] on the device z axis (target_z_axis)\n    assert np.linalg.norm(target_z_axis - r.dot([0, 0, 1])) < 1e-12\n\n    return r\n\n\ndef rotation_angles(m):\n    \"\"\"Find rotation angles from a transformation matrix.\n\n    Parameters\n    ----------\n    m : array, shape >= (3, 3)\n        Rotation matrix. Only the top left 3 x 3 partition is accessed.\n\n    Returns\n    -------\n    x, y, z : float\n        Rotation around x, y and z axes.\n    \"\"\"\n    x = np.arctan2(m[2, 1], m[2, 2])\n    c2 = np.sqrt(m[0, 0] ** 2 + m[1, 0] ** 2)\n    y = np.arctan2(-m[2, 0], c2)\n    s1 = np.sin(x)\n    c1 = np.cos(x)\n    z = np.arctan2(s1 * m[0, 2] - c1 * m[0, 1], c1 * m[1, 1] - s1 * m[1, 2])\n    return x, y, z\n\n\ndef scaling(x=1, y=1, z=1):\n    \"\"\"Create an array with a scaling matrix.\n\n    Parameters\n    ----------\n    x, y, z : scalar\n        Scaling factors.\n\n    Returns\n    -------\n    s : array, shape = (4, 4)\n        The scaling matrix.\n    \"\"\"\n    s = np.array([[x, 0, 0, 0], [0, y, 0, 0], [0, 0, z, 0], [0, 0, 0, 1]], dtype=float)\n    return s\n\n\ndef translation(x=0, y=0, z=0):\n    \"\"\"Create an array with a translation matrix.\n\n    Parameters\n    ----------\n    x, y, z : scalar\n        Translation parameters.\n\n    Returns\n    -------\n    m : array, shape = (4, 4)\n        The translation matrix.\n    \"\"\"\n    m = np.array([[1, 0, 0, x], [0, 1, 0, y], [0, 0, 1, z], [0, 0, 0, 1]], dtype=float)\n    return m\n\n\ndef _ensure_trans(trans, fro=\"mri\", to=\"head\"):\n    \"\"\"Ensure we have the proper transform.\"\"\"\n    if isinstance(fro, str):\n        from_str = fro\n        from_const = _str_to_frame[fro]\n    else:\n        from_str = _frame_to_str[fro]\n        from_const = fro\n    del fro\n    if isinstance(to, str):\n        to_str = to\n        to_const = _str_to_frame[to]\n    else:\n        to_str = _frame_to_str[to]\n        to_const = to\n    del to\n    err_str = \"trans must be a Transform between \" f\"{from_str}<->{to_str}, got\"\n    if not isinstance(trans, (list, tuple)):\n        trans = [trans]\n    # Ensure that we have exactly one match\n    idx = list()\n    misses = list()\n    for ti, this_trans in enumerate(trans):\n        if not isinstance(this_trans, Transform):\n            raise ValueError(f\"{err_str} None\")\n        if {this_trans[\"from\"], this_trans[\"to\"]} == {from_const, to_const}:\n            idx.append(ti)\n        else:\n            misses += [\n                \"{fro}->{to}\".format(\n                    fro=_frame_to_str[this_trans[\"from\"]],\n                    to=_frame_to_str[this_trans[\"to\"]],\n                )\n            ]\n    if len(idx) != 1:\n        raise ValueError(f\"{err_str} \" + \", \".join(misses))\n    trans = trans[idx[0]]\n    if trans[\"from\"] != from_const:\n        trans = invert_transform(trans)\n    return trans\n\n\ndef _get_trans(trans, fro=\"mri\", to=\"head\", allow_none=True):\n    \"\"\"Get mri_head_t (from=mri, to=head) from mri filename.\"\"\"\n    types = (Transform, \"path-like\")\n    if allow_none:\n        types += (None,)\n    _validate_type(trans, types, \"trans\")\n    if _path_like(trans):\n        if trans == \"fsaverage\":\n            trans = Path(__file__).parent / \"data\" / \"fsaverage\" / \"fsaverage-trans.fif\"\n        trans = Path(trans)\n        if not trans.is_file():\n            raise OSError(f'trans file \"{trans}\" not found')\n        if trans.suffix in [\".fif\", \".gz\"]:\n            fro_to_t = read_trans(trans)\n        else:\n            # convert \"-trans.txt\" to \"-trans.fif\" mri-type equivalent\n            # these are usually actually in to_fro form\n            t = np.genfromtxt(trans)\n            if t.ndim != 2 or t.shape != (4, 4):\n                raise RuntimeError(f'File \"{trans}\" did not have 4x4 entries')\n            fro_to_t = Transform(to, fro, t)\n    elif isinstance(trans, Transform):\n        fro_to_t = trans\n        trans = \"instance of Transform\"\n    else:\n        assert trans is None\n        fro_to_t = Transform(fro, to)\n        trans = \"identity\"\n    # it's usually a head->MRI transform, so we probably need to invert it\n    fro_to_t = _ensure_trans(fro_to_t, fro, to)\n    return fro_to_t, trans\n\n\ndef combine_transforms(t_first, t_second, fro, to):\n    \"\"\"Combine two transforms.\n\n    Parameters\n    ----------\n    t_first : dict\n        First transform.\n    t_second : dict\n        Second transform.\n    fro : int\n        From coordinate frame.\n    to : int\n        To coordinate frame.\n\n    Returns\n    -------\n    trans : dict\n        Combined transformation.\n    \"\"\"\n    fro = _to_const(fro)\n    to = _to_const(to)\n    if t_first[\"from\"] != fro:\n        raise RuntimeError(\n            'From mismatch: {fro1} (\"{cf1}\") != {fro2} (\"{cf2}\")'.format(\n                fro1=t_first[\"from\"],\n                cf1=_coord_frame_name(t_first[\"from\"]),\n                fro2=fro,\n                cf2=_coord_frame_name(fro),\n            )\n        )\n    if t_first[\"to\"] != t_second[\"from\"]:\n        raise RuntimeError(\n            'Transform mismatch: t1[\"to\"] = {to1} (\"{cf1}\"), '\n            't2[\"from\"] = {fro2} (\"{cf2}\")'.format(\n                to1=t_first[\"to\"],\n                cf1=_coord_frame_name(t_first[\"to\"]),\n                fro2=t_second[\"from\"],\n                cf2=_coord_frame_name(t_second[\"from\"]),\n            )\n        )\n    if t_second[\"to\"] != to:\n        raise RuntimeError(\n            'To mismatch: {to1} (\"{cf1}\") != {to2} (\"{cf2}\")'.format(\n                to1=t_second[\"to\"],\n                cf1=_coord_frame_name(t_second[\"to\"]),\n                to2=to,\n                cf2=_coord_frame_name(to),\n            )\n        )\n    return Transform(fro, to, np.dot(t_second[\"trans\"], t_first[\"trans\"]))\n\n\n@verbose\ndef read_trans(fname, return_all=False, verbose=None):\n    \"\"\"Read a ``-trans.fif`` file.\n\n    Parameters\n    ----------\n    fname : path-like\n        The name of the file.\n    return_all : bool\n        If True, return all transformations in the file.\n        False (default) will only return the first.\n\n        .. versionadded:: 0.15\n    %(verbose)s\n\n    Returns\n    -------\n    trans : dict | list of dict\n        The transformation dictionary from the fif file.\n\n    See Also\n    --------\n    write_trans\n    mne.transforms.Transform\n    \"\"\"\n    fname = _check_fname(fname, overwrite=\"read\", must_exist=True)\n    fid, tree, directory = fiff_open(fname)\n\n    trans = list()\n    with fid:\n        for t in directory:\n            if t.kind == FIFF.FIFF_COORD_TRANS:\n                trans.append(read_tag(fid, t.pos).data)\n                if not return_all:\n                    break\n    if len(trans) == 0:\n        raise OSError(\"This does not seem to be a -trans.fif file.\")\n    return trans if return_all else trans[0]\n\n\n@verbose\ndef write_trans(fname, trans, *, overwrite=False, verbose=None):\n    \"\"\"Write a transformation FIF file.\n\n    Parameters\n    ----------\n    fname : path-like\n        The name of the file, which should end in ``-trans.fif``.\n    trans : dict\n        Trans file data, as returned by `~mne.read_trans`.\n    %(overwrite)s\n    %(verbose)s\n\n    See Also\n    --------\n    read_trans\n    \"\"\"\n    check_fname(\n        fname, \"trans\", (\"-trans.fif\", \"-trans.fif.gz\", \"_trans.fif\", \"_trans.fif.gz\")\n    )\n    fname = _check_fname(fname=fname, overwrite=overwrite)\n    with start_and_end_file(fname) as fid:\n        write_coord_trans(fid, trans)\n\n\ndef invert_transform(trans):\n    \"\"\"Invert a transformation between coordinate systems.\n\n    Parameters\n    ----------\n    trans : dict\n        Transform to invert.\n\n    Returns\n    -------\n    inv_trans : dict\n        Inverse transform.\n    \"\"\"\n    return Transform(trans[\"to\"], trans[\"from\"], np.linalg.inv(trans[\"trans\"]))\n\n\ndef transform_surface_to(surf, dest, trans, copy=False):\n    \"\"\"Transform surface to the desired coordinate system.\n\n    Parameters\n    ----------\n    surf : dict\n        Surface.\n    dest : 'meg' | 'mri' | 'head' | int\n        Destination coordinate system. Can be an integer for using\n        FIFF types.\n    trans : dict | list of dict\n        Transformation to use (or a list of possible transformations to\n        check).\n    copy : bool\n        If False (default), operate in-place.\n\n    Returns\n    -------\n    res : dict\n        Transformed source space.\n    \"\"\"\n    surf = deepcopy(surf) if copy else surf\n    if isinstance(dest, str):\n        if dest not in _str_to_frame:\n            raise KeyError(\n                f'dest must be one of {list(_str_to_frame.keys())}, not \"{dest}\"'\n            )\n        dest = _str_to_frame[dest]  # convert to integer\n    if surf[\"coord_frame\"] == dest:\n        return surf\n\n    trans = _ensure_trans(trans, int(surf[\"coord_frame\"]), dest)\n    surf[\"coord_frame\"] = dest\n    surf[\"rr\"] = apply_trans(trans, surf[\"rr\"])\n    if \"nn\" in surf:\n        surf[\"nn\"] = apply_trans(trans, surf[\"nn\"], move=False)\n    return surf\n\n", "right_context": "\n\ndef _get_transforms_to_coord_frame(info, trans, coord_frame=\"mri\"):\n    \"\"\"Get the transforms to a coordinate frame from device, head and mri.\"\"\"\n    head_mri_t = _get_trans(trans, \"head\", \"mri\")[0]\n    dev_head_t = _get_trans(info[\"dev_head_t\"], \"meg\", \"head\")[0]\n    mri_dev_t = invert_transform(\n        combine_transforms(dev_head_t, head_mri_t, \"meg\", \"mri\")\n    )\n    to_cf_t = dict(\n        meg=_ensure_trans(\n            [dev_head_t, mri_dev_t, Transform(\"meg\", \"meg\")], fro=\"meg\", to=coord_frame\n        ),\n        head=_ensure_trans(\n            [dev_head_t, head_mri_t, Transform(\"head\", \"head\")],\n            fro=\"head\",\n            to=coord_frame,\n        ),\n        mri=_ensure_trans(\n            [head_mri_t, mri_dev_t, Transform(\"mri\", \"mri\")], fro=\"mri\", to=coord_frame\n        ),\n    )\n    return to_cf_t\n\n\n###############################################################################\n# Spherical coordinates and harmonics\n\n\ndef _cart_to_sph(cart):\n    \"\"\"Convert Cartesian coordinates to spherical coordinates.\n\n    Parameters\n    ----------\n    cart_pts : ndarray, shape (n_points, 3)\n        Array containing points in Cartesian coordinates (x, y, z)\n\n    Returns\n    -------\n    sph_pts : ndarray, shape (n_points, 3)\n        Array containing points in spherical coordinates (rad, azimuth, polar)\n    \"\"\"\n    cart = np.atleast_2d(cart)\n    assert cart.ndim == 2 and cart.shape[1] == 3, cart.shape\n    out = np.empty((len(cart), 3))\n    out[:, 0] = np.sqrt(np.sum(cart * cart, axis=1))\n    norm = np.where(out[:, 0] > 0, out[:, 0], 1)  # protect against / 0\n    out[:, 1] = np.arctan2(cart[:, 1], cart[:, 0])\n    out[:, 2] = np.arccos(cart[:, 2] / norm)\n    out = np.nan_to_num(out)\n    return out\n\n\ndef _sph_to_cart(sph_pts):\n    \"\"\"Convert spherical coordinates to Cartesian coordinates.\n\n    Parameters\n    ----------\n    sph_pts : ndarray, shape (n_points, 3)\n        Array containing points in spherical coordinates (rad, azimuth, polar)\n\n    Returns\n    -------\n    cart_pts : ndarray, shape (n_points, 3)\n        Array containing points in Cartesian coordinates (x, y, z)\n\n    \"\"\"\n    sph_pts = np.atleast_2d(sph_pts)\n    assert sph_pts.ndim == 2 and sph_pts.shape[1] == 3\n    cart_pts = np.empty((len(sph_pts), 3))\n    cart_pts[:, 2] = sph_pts[:, 0] * np.cos(sph_pts[:, 2])\n    xy = sph_pts[:, 0] * np.sin(sph_pts[:, 2])\n    cart_pts[:, 0] = xy * np.cos(sph_pts[:, 1])\n    cart_pts[:, 1] = xy * np.sin(sph_pts[:, 1])\n    return cart_pts\n\n\ndef _get_n_moments(order):\n    \"\"\"Compute the number of multipolar moments (spherical harmonics).\n\n    Equivalent to :footcite:`DarvasEtAl2006` Eq. 32.\n\n    .. note:: This count excludes ``degree=0`` (for ``order=0``).\n\n    Parameters\n    ----------\n    order : array-like\n        Expansion orders, often ``[int_order, ext_order]``.\n\n    Returns\n    -------\n    M : ndarray\n        Number of moments due to each order.\n    \"\"\"\n    order = np.asarray(order, int)\n    return (order + 2) * order\n\n\ndef _sph_to_cart_partials(az, pol, g_rad, g_az, g_pol):\n    \"\"\"Convert spherical partial derivatives to cartesian coords.\n\n    Note: Because we are dealing with partial derivatives, this calculation is\n    not a static transformation. The transformation matrix itself is dependent\n    on azimuth and polar coord.\n\n    See the 'Spherical coordinate sytem' section here:\n    wikipedia.org/wiki/Vector_fields_in_cylindrical_and_spherical_coordinates\n\n    Parameters\n    ----------\n    az : ndarray, shape (n_points,)\n        Array containing spherical coordinates points (azimuth).\n    pol : ndarray, shape (n_points,)\n        Array containing spherical coordinates points (polar).\n    sph_grads : ndarray, shape (n_points, 3)\n        Array containing partial derivatives at each spherical coordinate\n        (radius, azimuth, polar).\n\n    Returns\n    -------\n    cart_grads : ndarray, shape (n_points, 3)\n        Array containing partial derivatives in Cartesian coordinates (x, y, z)\n    \"\"\"\n    sph_grads = np.c_[g_rad, g_az, g_pol]\n    c_as, s_as = np.cos(az), np.sin(az)\n    c_ps, s_ps = np.cos(pol), np.sin(pol)\n    trans = np.array(\n        [\n            [c_as * s_ps, -s_as, c_as * c_ps],\n            [s_as * s_ps, c_as, c_ps * s_as],\n            [c_ps, np.zeros_like(c_as), -s_ps],\n        ]\n    )\n    cart_grads = np.einsum(\"ijk,kj->ki\", trans, sph_grads)\n    return cart_grads\n\n\ndef _deg_ord_idx(deg, order):\n    \"\"\"Get the index into S_in or S_out given a degree and order.\"\"\"\n    # The -1 here is because we typically exclude the degree=0 term\n    return deg * deg + deg + order - 1\n\n\ndef _sh_negate(sh, order):\n    \"\"\"Get the negative spherical harmonic from a positive one.\"\"\"\n    assert order >= 0\n    return sh.conj() * (-1.0 if order % 2 else 1.0)  # == (-1) ** order\n\n\ndef _sh_complex_to_real(sh, order):\n    \"\"\"Convert complex to real basis functions.\n\n    Parameters\n    ----------\n    sh : array-like\n        Spherical harmonics. Must be from order >=0 even if negative orders\n        are used.\n    order : int\n        Order (usually 'm') of multipolar moment.\n\n    Returns\n    -------\n    real_sh : array-like\n        The real version of the spherical harmonics.\n\n    Notes\n    -----\n    This does not include the Condon-Shortely phase.\n    \"\"\"\n    if order == 0:\n        return np.real(sh)\n    else:\n        return np.sqrt(2.0) * (np.real if order > 0 else np.imag)(sh)\n\n\ndef _sh_real_to_complex(shs, order):\n    \"\"\"Convert real spherical harmonic pair to complex.\n\n    Parameters\n    ----------\n    shs : ndarray, shape (2, ...)\n        The real spherical harmonics at ``[order, -order]``.\n    order : int\n        Order (usually 'm') of multipolar moment.\n\n    Returns\n    -------\n    sh : array-like, shape (...)\n        The complex version of the spherical harmonics.\n    \"\"\"\n    if order == 0:\n        return shs[0]\n    else:\n        return (shs[0] + 1j * np.sign(order) * shs[1]) / np.sqrt(2.0)\n\n\ndef _compute_sph_harm(order, az, pol):\n    \"\"\"Compute complex spherical harmonics of spherical coordinates.\"\"\"\n    out = np.empty((len(az), _get_n_moments(order) + 1))\n    # _deg_ord_idx(0, 0) = -1 so we're actually okay to use it here\n    for degree in range(order + 1):\n        for order_ in range(degree + 1):\n            sph = sph_harm(order_, degree, az, pol)\n            out[:, _deg_ord_idx(degree, order_)] = _sh_complex_to_real(sph, order_)\n            if order_ > 0:\n                out[:, _deg_ord_idx(degree, -order_)] = _sh_complex_to_real(\n                    _sh_negate(sph, order_), -order_\n                )\n    return out\n\n\n###############################################################################\n# Thin-plate spline transformations\n\n# Adapted from code from the MATLAB file exchange:\n#    https://www.mathworks.com/matlabcentral/fileexchange/\n#            53867-3d-point-set-warping-by-thin-plate-rbf-function\n#    https://www.mathworks.com/matlabcentral/fileexchange/\n#            53828-rbf-or-thin-plate-splines-image-warping\n# Associated (BSD 2-clause) license:\n#\n# Copyright (c) 2015, Wang Lin\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#     * Redistributions of source code must retain the above copyright\n#       notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above copyright\n#       notice, this list of conditions and the following disclaimer in\n#       the documentation and/or other materials provided with the distribution\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n\nclass _TPSWarp:\n    \"\"\"Transform points using thin-plate spline (TPS) warping.\n\n    Notes\n    -----\n    Based on the method by :footcite:`Bookstein1989` and\n    adapted from code by Wang Lin (wanglin193@hotmail.com>).\n\n    References\n    ----------\n    .. footbibliography::\n    \"\"\"\n\n    def fit(self, source, destination, reg=1e-3):\n        assert source.shape[1] == destination.shape[1] == 3\n        assert source.shape[0] == destination.shape[0]\n        # Forward warping, different from image warping, use |dist|**2\n        dists = _tps(cdist(source, destination, \"sqeuclidean\"))\n        # Y = L * w\n        # L: RBF matrix about source\n        # Y: Points matrix about destination\n        P = np.concatenate((np.ones((source.shape[0], 1)), source), axis=-1)\n        L = np.vstack([np.hstack([dists, P]), np.hstack([P.T, np.zeros((4, 4))])])\n        Y = np.concatenate((destination, np.zeros((4, 3))), axis=0)\n        # Regularize it a bit\n        L += reg * np.eye(L.shape[0])\n        self._destination = destination.copy()\n        self._weights = linalg.lstsq(L, Y)[0]\n        return self\n\n    @verbose\n    def transform(self, pts, verbose=None):\n        \"\"\"Apply the warp.\n\n        Parameters\n        ----------\n        pts : shape (n_transform, 3)\n            Source points to warp to the destination.\n\n        Returns\n        -------\n        dest : shape (n_transform, 3)\n            The transformed points.\n        \"\"\"\n        logger.info(f\"Transforming {len(pts)} points\")\n        assert pts.shape[1] == 3\n        # for memory reasons, we should do this in ~100 MB chunks\n        out = np.zeros_like(pts)\n        n_splits = max(\n            int((pts.shape[0] * self._destination.shape[0]) / (100e6 / 8.0)), 1\n        )\n        for this_out, this_pts in zip(\n            np.array_split(out, n_splits), np.array_split(pts, n_splits)\n        ):\n            dists = _tps(cdist(this_pts, self._destination, \"sqeuclidean\"))\n            L = np.hstack((dists, np.ones((dists.shape[0], 1)), this_pts))\n            this_out[:] = np.dot(L, self._weights)\n        assert not (out == 0).any()\n        return out\n\n\ndef _tps(distsq):\n    \"\"\"Thin-plate function (r ** 2) * np.log(r).\"\"\"\n    # NOTE: For our warping functions, a radial basis like\n    # exp(-distsq / radius ** 2) could also be used\n    out = np.zeros_like(distsq)\n    mask = distsq > 0  # avoid log(0)\n    valid = distsq[mask]\n    out[mask] = valid * np.log(valid)\n    return out\n\n\n###############################################################################\n# Spherical harmonic approximation + TPS warp\n\n\nclass _SphericalSurfaceWarp:\n    \"\"\"Warp surfaces via spherical harmonic smoothing and thin-plate splines.\n\n    Notes\n    -----\n    This class can be used to warp data from a source subject to\n    a destination subject, as described in :footcite:`DarvasEtAl2006`.\n\n    The procedure is:\n\n        1. Perform a spherical harmonic approximation to the source and\n           destination surfaces, which smooths them and allows arbitrary\n           interpolation.\n        2. Choose a set of matched points on the two surfaces.\n        3. Use thin-plate spline warping (common in 2D image manipulation)\n           to generate transformation coefficients.\n        4. Warp points from the source subject (which should be inside the\n           original surface) to the destination subject.\n\n    .. versionadded:: 0.14\n\n    References\n    ----------\n    .. footbibliography::\n    \"\"\"\n\n    def __repr__(self):\n        rep = \"<SphericalSurfaceWarp : \"\n        if not hasattr(self, \"_warp\"):\n            rep += \"no fitting done >\"\n        else:\n            rep += \"fit %d->%d pts using match=%s (%d pts), order=%s, reg=%s>\" % tuple(\n                self._fit_params[key]\n                for key in [\"n_src\", \"n_dest\", \"match\", \"n_match\", \"order\", \"reg\"]\n            )\n        return rep\n\n    @verbose\n    def fit(\n        self,\n        source,\n        destination,\n        order=4,\n        reg=1e-5,\n        center=True,\n        match=\"oct5\",\n        verbose=None,\n    ):\n        \"\"\"Fit the warp from source points to destination points.\n\n        Parameters\n        ----------\n        source : array, shape (n_src, 3)\n            The source points.\n        destination : array, shape (n_dest, 3)\n            The destination points.\n        order : int\n            Order of the spherical harmonic fit.\n        reg : float\n            Regularization of the TPS warp.\n        center : bool\n            If True, center the points by fitting a sphere to points\n            that are in a reasonable region for head digitization.\n        match : str\n            The uniformly-spaced points to match on the two surfaces.\n            Can be \"ico#\" or \"oct#\" where \"#\" is an integer.\n            The default is \"oct5\".\n        %(verbose)s\n\n        Returns\n        -------\n        inst : instance of SphericalSurfaceWarp\n            The warping object (for chaining).\n        \"\"\"\n        from .bem import _fit_sphere\n        from .source_space._source_space import _check_spacing\n\n        match_rr = _check_spacing(match, verbose=False)[2][\"rr\"]\n        logger.info(\"Computing TPS warp\")\n        src_center = dest_center = np.zeros(3)\n        if center:\n            logger.info(\"    Centering data\")\n            hsp = np.array([p for p in source if not (p[2] < -1e-6 and p[1] > 1e-6)])\n            src_center = _fit_sphere(hsp, disp=False)[1]\n            source = source - src_center\n            hsp = np.array([p for p in destination if not (p[2] < 0 and p[1] > 0)])\n            dest_center = _fit_sphere(hsp, disp=False)[1]\n            destination = destination - dest_center\n            logger.info(\n                \"    Using centers {np.array_str(src_center, None, 3)} -> \"\n                \"{np.array_str(dest_center, None, 3)}\"\n            )\n        self._fit_params = dict(\n            n_src=len(source),\n            n_dest=len(destination),\n            match=match,\n            n_match=len(match_rr),\n            order=order,\n            reg=reg,\n        )\n        assert source.shape[1] == destination.shape[1] == 3\n        self._destination = destination.copy()\n        # 1. Compute spherical coordinates of source and destination points\n        logger.info(\"    Converting to spherical coordinates\")\n        src_rad_az_pol = _cart_to_sph(source).T\n        dest_rad_az_pol = _cart_to_sph(destination).T\n        match_rad_az_pol = _cart_to_sph(match_rr).T\n        del match_rr\n        # 2. Compute spherical harmonic coefficients for all points\n        logger.info(\n            \"    Computing spherical harmonic approximation with \" \"order %s\" % order\n        )\n        src_sph = _compute_sph_harm(order, *src_rad_az_pol[1:])\n        dest_sph = _compute_sph_harm(order, *dest_rad_az_pol[1:])\n        match_sph = _compute_sph_harm(order, *match_rad_az_pol[1:])\n        # 3. Fit spherical harmonics to both surfaces to smooth them\n        src_coeffs = linalg.lstsq(src_sph, src_rad_az_pol[0])[0]\n        dest_coeffs = linalg.lstsq(dest_sph, dest_rad_az_pol[0])[0]\n        # 4. Smooth both surfaces using these coefficients, and evaluate at\n        #     the \"shape\" points\n        logger.info(\n            \"    Matching %d points (%s) on smoothed surfaces\" % (len(match_sph), match)\n        )\n        src_rad_az_pol = match_rad_az_pol.copy()\n        src_rad_az_pol[0] = np.abs(np.dot(match_sph, src_coeffs))\n        dest_rad_az_pol = match_rad_az_pol.copy()\n        dest_rad_az_pol[0] = np.abs(np.dot(match_sph, dest_coeffs))\n        # 5. Convert matched points to Cartesian coordinates and put back\n        source = _sph_to_cart(src_rad_az_pol.T)\n        source += src_center\n        destination = _sph_to_cart(dest_rad_az_pol.T)\n        destination += dest_center\n        # 6. Compute TPS warp of matched points from smoothed surfaces\n        self._warp = _TPSWarp().fit(source, destination, reg)\n        self._matched = np.array([source, destination])\n        logger.info(\"[done]\")\n        return self\n\n    @verbose\n    def transform(self, source, verbose=None):\n        \"\"\"Transform arbitrary source points to the destination.\n\n        Parameters\n        ----------\n        source : ndarray, shape (n_pts, 3)\n            Source points to transform. They do not need to be the same\n            points that were used to generate the model, although ideally\n            they will be inside the convex hull formed by the original\n            source points.\n        %(verbose)s\n\n        Returns\n        -------\n        destination : ndarray, shape (n_pts, 3)\n            The points transformed to the destination space.\n        \"\"\"\n        return self._warp.transform(source)\n\n\n###############################################################################\n# Other transforms\n\n\ndef _pol_to_cart(pol):\n    \"\"\"Transform polar coordinates to cartesian.\"\"\"\n    out = np.empty((len(pol), 2))\n    if pol.shape[1] == 2:  # phi, theta\n        out[:, 0] = pol[:, 0] * np.cos(pol[:, 1])\n        out[:, 1] = pol[:, 0] * np.sin(pol[:, 1])\n    else:  # radial distance, theta, phi\n        d = pol[:, 0] * np.sin(pol[:, 2])\n        out[:, 0] = d * np.cos(pol[:, 1])\n        out[:, 1] = d * np.sin(pol[:, 1])\n    return out\n\n\ndef _topo_to_sph(topo):\n    \"\"\"Convert 2D topo coordinates to spherical coordinates.\"\"\"\n    assert topo.ndim == 2 and topo.shape[1] == 2\n    sph = np.ones((len(topo), 3))\n    sph[:, 1] = -np.deg2rad(topo[:, 0])\n    sph[:, 2] = np.pi * topo[:, 1]\n    return sph\n\n\n###############################################################################\n# Quaternions\n\n\n@jit()\ndef quat_to_rot(quat):\n    \"\"\"Convert a set of quaternions to rotations.\n\n    Parameters\n    ----------\n    quat : array, shape (..., 3)\n        The q1, q2, and q3 (x, y, z) parameters of a unit quaternion.\n\n    Returns\n    -------\n    rot : array, shape (..., 3, 3)\n        The corresponding rotation matrices.\n\n    See Also\n    --------\n    rot_to_quat\n    \"\"\"\n    # z = a + bi + cj + dk\n    b, c, d = quat[..., 0], quat[..., 1], quat[..., 2]\n    bb, cc, dd = b * b, c * c, d * d\n    # use max() here to be safe in case roundoff errs put us over\n    aa = np.maximum(1.0 - bb - cc - dd, 0.0)\n    a = np.sqrt(aa)\n    ab_2 = 2 * a * b\n    ac_2 = 2 * a * c\n    ad_2 = 2 * a * d\n    bc_2 = 2 * b * c\n    bd_2 = 2 * b * d\n    cd_2 = 2 * c * d\n    rotation = np.empty(quat.shape[:-1] + (3, 3))\n    rotation[..., 0, 0] = aa + bb - cc - dd\n    rotation[..., 0, 1] = bc_2 - ad_2\n    rotation[..., 0, 2] = bd_2 + ac_2\n    rotation[..., 1, 0] = bc_2 + ad_2\n    rotation[..., 1, 1] = aa + cc - bb - dd\n    rotation[..., 1, 2] = cd_2 - ab_2\n    rotation[..., 2, 0] = bd_2 - ac_2\n    rotation[..., 2, 1] = cd_2 + ab_2\n    rotation[..., 2, 2] = aa + dd - bb - cc\n    return rotation\n\n\n@jit()\ndef _one_rot_to_quat(rot):\n    \"\"\"Convert a rotation matrix to quaternions.\"\"\"\n    # see e.g. http://www.euclideanspace.com/maths/geometry/rotations/\n    #                 conversions/matrixToQuaternion/\n    det = np.linalg.det(np.reshape(rot, (3, 3)))\n    if np.abs(det - 1.0) > 1e-3:\n        raise ValueError(\"Matrix is not a pure rotation, got determinant != 1\")\n    t = 1.0 + rot[0] + rot[4] + rot[8]\n    if t > np.finfo(rot.dtype).eps:\n        s = np.sqrt(t) * 2.0\n        # qw = 0.25 * s\n        qx = (rot[7] - rot[5]) / s\n        qy = (rot[2] - rot[6]) / s\n        qz = (rot[3] - rot[1]) / s\n    elif rot[0] > rot[4] and rot[0] > rot[8]:\n        s = np.sqrt(1.0 + rot[0] - rot[4] - rot[8]) * 2.0\n        # qw = (rot[7] - rot[5]) / s\n        qx = 0.25 * s\n        qy = (rot[1] + rot[3]) / s\n        qz = (rot[2] + rot[6]) / s\n    elif rot[4] > rot[8]:\n        s = np.sqrt(1.0 - rot[0] + rot[4] - rot[8]) * 2\n        # qw = (rot[2] - rot[6]) / s\n        qx = (rot[1] + rot[3]) / s\n        qy = 0.25 * s\n        qz = (rot[5] + rot[7]) / s\n    else:\n        s = np.sqrt(1.0 - rot[0] - rot[4] + rot[8]) * 2.0\n        # qw = (rot[3] - rot[1]) / s\n        qx = (rot[2] + rot[6]) / s\n        qy = (rot[5] + rot[7]) / s\n        qz = 0.25 * s\n    return np.array((qx, qy, qz))\n\n\ndef rot_to_quat(rot):\n    \"\"\"Convert a set of rotations to quaternions.\n\n    Parameters\n    ----------\n    rot : array, shape (..., 3, 3)\n        The rotation matrices to convert.\n\n    Returns\n    -------\n    quat : array, shape (..., 3)\n        The q1, q2, and q3 (x, y, z) parameters of the corresponding\n        unit quaternions.\n\n    See Also\n    --------\n    quat_to_rot\n    \"\"\"\n    rot = rot.reshape(rot.shape[:-2] + (9,))\n    return np.apply_along_axis(_one_rot_to_quat, -1, rot)\n\n\ndef _quat_to_affine(quat):\n    assert quat.shape == (6,)\n    affine = np.eye(4)\n    affine[:3, :3] = quat_to_rot(quat[:3])\n    affine[:3, 3] = quat[3:]\n    return affine\n\n\ndef _affine_to_quat(affine):\n    assert affine.shape[-2:] == (4, 4)\n    return np.concatenate(\n        [rot_to_quat(affine[..., :3, :3]), affine[..., :3, 3]],\n        axis=-1,\n    )\n\n\ndef _angle_dist_between_rigid(a, b=None, *, angle_units=\"rad\", distance_units=\"m\"):\n    a = _affine_to_quat(a)\n    b = np.zeros(6) if b is None else _affine_to_quat(b)\n    ang = _angle_between_quats(a[..., :3], b[..., :3])\n    dist = np.linalg.norm(a[..., 3:] - b[..., 3:], axis=-1)\n    assert isinstance(angle_units, str) and angle_units in (\"rad\", \"deg\")\n    if angle_units == \"deg\":\n        ang = np.rad2deg(ang)\n    assert isinstance(distance_units, str) and distance_units in (\"m\", \"mm\")\n    if distance_units == \"mm\":\n        dist *= 1e3\n    return ang, dist\n\n\ndef _angle_between_quats(x, y=None):\n    \"\"\"Compute the ang between two quaternions w/3-element representations.\"\"\"\n    # z = conj(x) * y\n    # conjugate just negates all but the first element in a 4-element quat,\n    # so it's just a negative for us\n    y = np.zeros(3) if y is None else y\n    z = _quat_mult(-x, y)\n    z0 = _quat_real(z)\n    return 2 * np.arctan2(np.linalg.norm(z, axis=-1), z0)\n\n\ndef _quat_real(quat):\n    \"\"\"Get the real part of our 3-element quat.\"\"\"\n    assert quat.shape[-1] == 3, quat.shape[-1]\n    return np.sqrt(\n        np.maximum(\n            1.0\n            - quat[..., 0] * quat[..., 0]\n            - quat[..., 1] * quat[..., 1]\n            - quat[..., 2] * quat[..., 2],\n            0.0,\n        )\n    )\n\n\ndef _quat_mult(one, two):\n    assert one.shape[-1] == two.shape[-1] == 3\n    w1 = _quat_real(one)\n    w2 = _quat_real(two)\n    out = np.empty(np.broadcast(one, two).shape)\n    # Most mathematical expressions use this sort of notation\n    x1, x2 = one[..., 0], two[..., 0]\n    y1, y2 = one[..., 1], two[..., 1]\n    z1, z2 = one[..., 2], two[..., 2]\n    out[..., 0] = w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2\n    out[..., 1] = w1 * y2 - x1 * z2 + y1 * w2 + z1 * x2\n    out[..., 2] = w1 * z2 + x1 * y2 - y1 * x2 + z1 * w2\n    # only need to compute w because we need signs from it\n    w = w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2\n    signs = np.sign(w)\n    signs = np.where(signs, signs, 1)\n    out *= signs[..., np.newaxis]\n    return out\n\n\ndef _skew_symmetric_cross(a):\n    \"\"\"Compute the skew-symmetric cross product of a vector.\"\"\"\n    return np.array([[0.0, -a[2], a[1]], [a[2], 0.0, -a[0]], [-a[1], a[0], 0.0]])\n\n\ndef _find_vector_rotation(a, b):\n    \"\"\"Find the rotation matrix that maps unit vector a to b.\"\"\"\n    # Rodrigues' rotation formula:\n    #   https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula\n    #   http://math.stackexchange.com/a/476311\n    R = np.eye(3)\n    v = np.cross(a, b)\n    if np.allclose(v, 0.0):  # identical\n        return R\n    s = np.dot(v, v)  # sine of the angle between them\n    c = np.dot(a, b)  # cosine of the angle between them\n    vx = _skew_symmetric_cross(v)\n    R += vx + np.dot(vx, vx) * (1 - c) / s\n    return R\n\n\n@jit()\ndef _fit_matched_points(p, x, weights=None, scale=False):\n    \"\"\"Fit matched points using an analytical formula.\"\"\"\n    # Follow notation of P.J. Besl and N.D. McKay, A Method for\n    # Registration of 3-D Shapes, IEEE Trans. Patt. Anal. Machine Intell., 14,\n    # 239 - 255, 1992.\n    #\n    # The original method is actually by Horn, Closed-form solution of absolute\n    # orientation using unit quaternions, J Opt. Soc. Amer. A vol 4 no 4\n    # pp 629-642, Apr. 1987. This paper describes how weights can be\n    # easily incorporated, and a uniform scale factor can be computed.\n    #\n    # Caution: This can be dangerous if there are 3 points, or 4 points in\n    #          a symmetric layout, as the geometry can be explained\n    #          equivalently under 180 degree rotations.\n    #\n    # Eventually this can be extended to also handle a uniform scale factor,\n    # as well.\n    assert p.shape == x.shape\n    assert p.ndim == 2\n    assert p.shape[1] == 3\n    # (weighted) centroids\n    weights_ = np.full((p.shape[0], 1), 1.0 / max(p.shape[0], 1))\n    if weights is not None:\n        weights_[:] = np.reshape(weights / weights.sum(), (weights.size, 1))\n    mu_p = np.dot(weights_.T, p)[0]\n    mu_x = np.dot(weights_.T, x)[0]\n    dots = np.dot(p.T, weights_ * x)\n    Sigma_px = dots - np.outer(mu_p, mu_x)  # eq 24\n    # x and p should no longer be used\n    A_ij = Sigma_px - Sigma_px.T\n    Delta = np.array([A_ij[1, 2], A_ij[2, 0], A_ij[0, 1]])\n    tr_Sigma_px = np.trace(Sigma_px)\n    # \"N\" in Horn:\n    Q = np.empty((4, 4))\n    Q[0, 0] = tr_Sigma_px\n    Q[0, 1:] = Delta\n    Q[1:, 0] = Delta\n    Q[1:, 1:] = Sigma_px + Sigma_px.T - tr_Sigma_px * np.eye(3)\n    _, v = np.linalg.eigh(Q)  # sorted ascending\n    quat = np.empty(6)\n    quat[:3] = v[1:, -1]\n    if v[0, -1] != 0:\n        quat[:3] *= np.sign(v[0, -1])\n    rot = quat_to_rot(quat[:3])\n    # scale factor is easy once we know the rotation\n    if scale:  # p is \"right\" (from), x is \"left\" (to) in Horn 1987\n        dev_x = x - mu_x\n        dev_p = p - mu_p\n        dev_x *= dev_x\n        dev_p *= dev_p\n        if weights is not None:\n            dev_x *= weights_\n            dev_p *= weights_\n        s = np.sqrt(np.sum(dev_x) / np.sum(dev_p))\n    else:\n        s = 1.0\n    # translation is easy once rotation and scale are known\n    quat[3:] = mu_x - s * np.dot(rot, mu_p)\n    return quat, s\n\n\ndef _average_quats(quats, weights=None):\n    \"\"\"Average unit quaternions properly.\"\"\"\n    assert quats.ndim == 2 and quats.shape[1] in (3, 4)\n    if weights is None:\n        weights = np.ones(quats.shape[0])\n    assert (weights >= 0).all()\n    norm = weights.sum()\n    if weights.sum() == 0:\n        return np.zeros(3)\n    weights = weights / norm\n    # The naive step here would be:\n    #\n    #     avg_quat = np.dot(weights, quats[:, :3])\n    #\n    # But this is not robust to quaternions having sign ambiguity,\n    # i.e., q == -q. Thus we instead use the rank 1 update method:\n    #\n    #     https://arc.aiaa.org/doi/abs/10.2514/1.28949?journalCode=jgcd\n    #     https://github.com/tolgabirdal/averaging_quaternions/blob/master/wavg_quaternion_markley.m  # noqa: E501\n    #\n    # We use unit quats and don't store the last element, so reconstruct it\n    # to get our 4-element quaternions:\n    quats = np.concatenate((_quat_real(quats)[..., np.newaxis], quats), -1)\n    quats *= weights[:, np.newaxis]\n    A = np.einsum(\"ij,ik->jk\", quats, quats)  # sum of outer product of each q\n    avg_quat = linalg.eigh(A)[1][:, -1]  # largest eigenvector is the avg\n    # Same as the largest eigenvector from the concatenation of all as\n    # svd(quats, full_matrices=False)[-1][0], but faster.\n    #\n    # By local convention we take the real term (which we remove from our\n    # representation) as positive. Since it can be zero, let's just ensure\n    # that the first non-zero element is positive. This shouldn't matter once\n    # we go to a rotation matrix, but it's nice for testing to have\n    # consistency.\n    avg_quat *= np.sign(avg_quat[avg_quat != 0][0])\n    avg_quat = avg_quat[1:]\n    return avg_quat\n\n\n@fill_doc\ndef read_ras_mni_t(subject, subjects_dir=None):\n    \"\"\"Read a subject's RAS to MNI transform.\n\n    Parameters\n    ----------\n    subject : str\n        The subject.\n    %(subjects_dir)s\n\n    Returns\n    -------\n    ras_mni_t : instance of Transform\n        The transform from RAS to MNI (in mm).\n    \"\"\"\n    subjects_dir = Path(get_subjects_dir(subjects_dir=subjects_dir, raise_error=True))\n    _validate_type(subject, \"str\", \"subject\")\n    fname = subjects_dir / subject / \"mri\" / \"transforms\" / \"talairach.xfm\"\n    fname = str(\n        _check_fname(\n            fname,\n            \"read\",\n            True,\n            \"FreeSurfer Talairach transformation file\",\n        )\n    )\n    return Transform(\"ras\", \"mni_tal\", _read_fs_xfm(fname)[0])\n\n\ndef _read_fs_xfm(fname):\n    \"\"\"Read a Freesurfer transform from a .xfm file.\"\"\"\n    assert fname.endswith(\".xfm\")\n    with open(fname) as fid:\n        logger.debug(\"Reading FreeSurfer talairach.xfm file:\\n%s\" % fname)\n\n        # read lines until we get the string 'Linear_Transform', which precedes\n        # the data transformation matrix\n        comp = \"Linear_Transform\"\n        for li, line in enumerate(fid):\n            if li == 0:\n                kind = line.strip()\n                logger.debug(f\"Found: {repr(kind)}\")\n            if line[: len(comp)] == comp:\n                # we have the right line, so don't read any more\n                break\n        else:\n            raise ValueError(\n                'Failed to find \"Linear_Transform\" string in ' \"xfm file:\\n%s\" % fname\n            )\n\n        xfm = list()\n        # read the transformation matrix (3x4)\n        for ii, line in enumerate(fid):\n            digs = [float(s) for s in line.strip(\"\\n;\").split()]\n            xfm.append(digs)\n            if ii == 2:\n                break\n        else:\n            raise ValueError(\"Could not find enough linear transform lines\")\n    xfm.append([0.0, 0.0, 0.0, 1.0])\n    xfm = np.array(xfm, dtype=float)\n    return xfm, kind\n\n\ndef _write_fs_xfm(fname, xfm, kind):\n    \"\"\"Write a Freesurfer transform to a .xfm file.\"\"\"\n    with open(fname, \"wb\") as fid:\n        fid.write((kind + \"\\n\\nTtransform_Type = Linear;\\n\").encode(\"ascii\"))\n        fid.write(\"Linear_Transform =\\n\".encode(\"ascii\"))\n        for li, line in enumerate(xfm[:-1]):\n            line = \" \".join([\"%0.6f\" % part for part in line])\n            line += \"\\n\" if li < 2 else \";\\n\"\n            fid.write(line.encode(\"ascii\"))\n\n\ndef _quat_to_euler(quat):\n    euler = np.empty(quat.shape)\n    x, y, z = quat[..., 0], quat[..., 1], quat[..., 2]\n    w = _quat_real(quat)\n    np.arctan2(2 * (w * x + y * z), 1 - 2 * (x * x + y * y), out=euler[..., 0])\n    np.arcsin(2 * (w * y - x * z), out=euler[..., 1])\n    np.arctan2(2 * (w * z + x * y), 1 - 2 * (y * y + z * z), out=euler[..., 2])\n    return euler\n\n\ndef _euler_to_quat(euler):\n    quat = np.empty(euler.shape)\n    phi, theta, psi = euler[..., 0] / 2, euler[..., 1] / 2, euler[..., 2] / 2\n    cphi, sphi = np.cos(phi), np.sin(phi)\n    del phi\n    ctheta, stheta = np.cos(theta), np.sin(theta)\n    del theta\n    cpsi, spsi = np.cos(psi), np.sin(psi)\n    del psi\n    mult = np.sign(cphi * ctheta * cpsi + sphi * stheta * spsi)\n    if np.isscalar(mult):\n        mult = 1.0 if mult == 0 else mult\n    else:\n        mult[mult == 0] = 1.0\n    mult = mult[..., np.newaxis]\n    quat[..., 0] = sphi * ctheta * cpsi - cphi * stheta * spsi\n    quat[..., 1] = cphi * stheta * cpsi + sphi * ctheta * spsi\n    quat[..., 2] = cphi * ctheta * spsi - sphi * stheta * cpsi\n    quat *= mult\n    return quat\n\n\n###############################################################################\n# Affine Registration and SDR\n\n_ORDERED_STEPS = (\"translation\", \"rigid\", \"affine\", \"sdr\")\n\n\ndef _validate_zooms(zooms):\n    _validate_type(zooms, (dict, list, tuple, \"numeric\", None), \"zooms\")\n    zooms = _handle_default(\"transform_zooms\", zooms)\n    for key, val in zooms.items():\n        _check_option(\"zooms key\", key, _ORDERED_STEPS)\n        if val is not None:\n            val = tuple(float(x) for x in np.array(val, dtype=float).ravel())\n            _check_option(f\"len(zooms[{repr(key)})\", len(val), (1, 3))\n            if len(val) == 1:\n                val = val * 3\n            for this_zoom in val:\n                if this_zoom <= 1:\n                    raise ValueError(f\"Zooms must be > 1, got {this_zoom}\")\n            zooms[key] = val\n    return zooms\n\n\ndef _validate_niter(niter):\n    _validate_type(niter, (dict, list, tuple, None), \"niter\")\n    niter = _handle_default(\"transform_niter\", niter)\n    for key, value in niter.items():\n        _check_option(\"niter key\", key, _ORDERED_STEPS)\n        _check_option(f\"len(niter[{repr(key)}])\", len(value), (1, 2, 3))\n    return niter\n\n\ndef _validate_pipeline(pipeline):\n    _validate_type(pipeline, (str, list, tuple), \"pipeline\")\n    pipeline_defaults = dict(\n        all=_ORDERED_STEPS,\n        rigids=_ORDERED_STEPS[: _ORDERED_STEPS.index(\"rigid\") + 1],\n        affines=_ORDERED_STEPS[: _ORDERED_STEPS.index(\"affine\") + 1],\n    )\n    if isinstance(pipeline, str):  # use defaults\n        _check_option(\n            \"pipeline\", pipeline, (\"all\", \"rigids\", \"affines\"), extra=\"when str\"\n        )\n        pipeline = pipeline_defaults[pipeline]\n    for ii, step in enumerate(pipeline):\n        name = f\"pipeline[{ii}]\"\n        _validate_type(step, str, name)\n        _check_option(name, step, _ORDERED_STEPS)\n    ordered_pipeline = tuple(sorted(pipeline, key=lambda x: _ORDERED_STEPS.index(x)))\n    if tuple(pipeline) != ordered_pipeline:\n        raise ValueError(\n            f\"Steps in pipeline are out of order, expected {ordered_pipeline} \"\n            f\"but got {pipeline} instead\"\n        )\n    if len(set(pipeline)) != len(pipeline):\n        raise ValueError(\"Steps in pipeline should not be repeated\")\n    return tuple(pipeline)\n\n\ndef _compute_r2(a, b):\n    return 100 * (a.ravel() @ b.ravel()) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n\ndef _reslice_normalize(img, zooms):\n    from dipy.align.reslice import reslice\n\n    img_zooms = img.header.get_zooms()[:3]\n    img_affine = img.affine\n    img = _get_img_fdata(img)\n    if zooms is not None:\n        img, img_affine = reslice(img, img_affine, img_zooms, zooms)\n    img /= img.max()  # normalize\n    return img, img_affine\n\n\n@verbose\ndef compute_volume_registration(\n    moving,\n    static,\n    pipeline=\"all\",\n    zooms=None,\n    niter=None,\n    *,\n    starting_affine=None,\n    verbose=None,\n):\n    \"\"\"Align two volumes using an affine and, optionally, SDR.\n\n    Parameters\n    ----------\n    %(moving)s\n    %(static)s\n    %(pipeline)s\n    zooms : float | tuple | dict | None\n        The voxel size of volume for each spatial dimension in mm.\n        If None (default), MRIs won't be resliced (slow, but most accurate).\n        Can be a tuple to provide separate zooms for each dimension (X/Y/Z),\n        or a dict with keys ``['translation', 'rigid', 'affine', 'sdr']``\n        (each with values that are float`, tuple, or None) to provide separate\n        reslicing/accuracy for the steps.\n    %(niter)s\n    starting_affine : ndarray\n        The affine to initialize the registration with.\n\n        .. versionadded:: 1.2\n    %(verbose)s\n\n    Returns\n    -------\n    %(reg_affine)s\n    %(sdr_morph)s\n\n    Notes\n    -----\n    This function is heavily inspired by and extends\n    :func:`dipy.align.affine_registration\n    <dipy.align._public.affine_registration>`.\n\n    .. versionadded:: 0.24\n    \"\"\"\n    return _compute_volume_registration(\n        moving, static, pipeline, zooms, niter, starting_affine=starting_affine\n    )[:2]\n\n\ndef _compute_volume_registration(\n    moving, static, pipeline, zooms, niter, *, starting_affine=None\n):\n    nib = _import_nibabel(\"SDR morph\")\n    _require_version(\"dipy\", \"SDR morph\", \"0.10.1\")\n    with np.testing.suppress_warnings():\n        from dipy.align import (\n            affine,\n            affine_registration,\n            center_of_mass,\n            imwarp,\n            metrics,\n            rigid,\n            translation,\n        )\n        from dipy.align.imaffine import AffineMap\n\n    # input validation\n    _validate_type(moving, nib.spatialimages.SpatialImage, \"moving\")\n    _validate_type(static, nib.spatialimages.SpatialImage, \"static\")\n    original_zoom = np.mean(moving.header.get_zooms()[:3])\n    zooms = _validate_zooms(zooms)\n    niter = _validate_niter(niter)\n    pipeline = _validate_pipeline(pipeline)\n\n    logger.info(\"Computing registration...\")\n\n    # affine optimizations\n    reg_affine = starting_affine\n    sdr_morph = None\n    pipeline_options = dict(\n        translation=[center_of_mass, translation], rigid=[rigid], affine=[affine]\n    )\n    sigmas_mm = np.array([3.0, 1.0, 0.0])  # default for affine_registration\n    sigma_diff_mm = 2.0\n    factors = [4, 2, 1]\n    current_zoom = None\n    for i, step in enumerate(pipeline):\n        # reslice image with zooms\n        if i == 0 or zooms[step] != zooms[pipeline[i - 1]]:\n            if zooms[step] is not None:\n                logger.info(f\"Reslicing to zooms={zooms[step]} for {step} ...\")\n                current_zoom = np.mean(zooms[step])\n            else:\n                logger.info(f\"Using original zooms for {step} ...\")\n                current_zoom = original_zoom\n            static_zoomed, static_affine = _reslice_normalize(static, zooms[step])\n            moving_zoomed, moving_affine = _reslice_normalize(moving, zooms[step])\n        logger.info(f\"Optimizing {step}:\")\n        if step == \"sdr\":  # happens last\n            sigma_diff_vox = sigma_diff_mm / current_zoom\n            affine_map = AffineMap(\n                reg_affine,  # apply registration here\n                static_zoomed.shape,\n                static_affine,\n                moving_zoomed.shape,\n                moving_affine,\n            )\n            moving_zoomed = affine_map.transform(moving_zoomed)\n            metric = metrics.CCMetric(\n                dim=3,\n                sigma_diff=sigma_diff_vox,\n                radius=max(int(np.ceil(2 * sigma_diff_vox)), 1),\n            )\n            sdr = imwarp.SymmetricDiffeomorphicRegistration(metric, niter[step])\n            with wrapped_stdout(indent=\"    \", cull_newlines=True):\n                sdr_morph = sdr.optimize(\n                    static_zoomed, moving_zoomed, static_affine, static_affine\n                )\n            moved_zoomed = sdr_morph.transform(moving_zoomed)\n        else:\n            sigmas_vox = list(sigmas_mm / current_zoom)\n            with wrapped_stdout(indent=\"    \", cull_newlines=True):\n                moved_zoomed, reg_affine = affine_registration(\n                    moving_zoomed,\n                    static_zoomed,\n                    moving_affine,\n                    static_affine,\n                    nbins=32,\n                    metric=\"MI\",\n                    pipeline=pipeline_options[step],\n                    level_iters=niter[step],\n                    sigmas=sigmas_vox,\n                    factors=factors,\n                    starting_affine=reg_affine,\n                )\n\n            # report some useful information\n            if step in (\"translation\", \"rigid\"):\n                angle, dist = _angle_dist_between_rigid(reg_affine, angle_units=\"deg\")\n                logger.info(f\"    Translation: {dist:6.1f} mm\")\n                if step == \"rigid\":\n                    logger.info(f\"    Rotation:    {angle:6.1f}\u00b0\")\n        assert moved_zoomed.shape == static_zoomed.shape, step\n        r2 = _compute_r2(static_zoomed, moved_zoomed)\n        logger.info(f\"    R\u00b2:          {r2:6.1f}%\")\n    return (\n        reg_affine,\n        sdr_morph,\n        static_zoomed.shape,\n        static_affine,\n        moving_zoomed.shape,\n        moving_affine,\n    )\n\n\n@verbose\ndef apply_volume_registration(\n    moving,\n    static,\n    reg_affine,\n    sdr_morph=None,\n    interpolation=\"linear\",\n    cval=0.0,\n    verbose=None,\n):\n    \"\"\"Apply volume registration.\n\n    Uses registration parameters computed by\n    :func:`~mne.transforms.compute_volume_registration`.\n\n    Parameters\n    ----------\n    %(moving)s\n    %(static)s\n    %(reg_affine)s\n    %(sdr_morph)s\n    interpolation : str\n        Interpolation to be used during the interpolation.\n        Can be ``\"linear\"`` (default) or ``\"nearest\"``.\n    cval : float | str\n        The constant value to assume exists outside the bounds of the\n        ``moving`` image domain. Can be a string percentage like ``'1%%'``\n        to use the given percentile of image data as the constant value.\n    %(verbose)s\n\n    Returns\n    -------\n    reg_img : instance of SpatialImage\n        The image after affine (and SDR, if provided) registration.\n\n    Notes\n    -----\n    .. versionadded:: 0.24\n    \"\"\"\n    _require_version(\"dipy\", \"SDR morph\", \"0.10.1\")\n    _import_nibabel(\"SDR morph\")\n    from dipy.align.imaffine import AffineMap\n    from dipy.align.imwarp import DiffeomorphicMap\n    from nibabel.spatialimages import SpatialImage\n\n    _validate_type(moving, SpatialImage, \"moving\")\n    _validate_type(static, SpatialImage, \"static\")\n    _validate_type(reg_affine, np.ndarray, \"reg_affine\")\n    _check_option(\"reg_affine.shape\", reg_affine.shape, ((4, 4),))\n    _validate_type(sdr_morph, (DiffeomorphicMap, None), \"sdr_morph\")\n    _validate_type(cval, (\"numeric\", str), \"cval\")\n    perc = None\n    if isinstance(cval, str):\n        if not cval.endswith(\"%\"):\n            raise ValueError(f\"cval must end with % if str, got {cval}\")\n        perc = float(cval[:-1])\n    logger.info(\"Applying affine registration ...\")\n    moving_affine = moving.affine\n    moving = np.asarray(moving.dataobj, dtype=float)\n    if perc is not None:\n        cval = np.percentile(moving, perc)\n        logger.info(f\"Using a lower bound at the {perc} percentile: {cval}\")\n    moving -= cval\n    static, static_affine = np.asarray(static.dataobj), static.affine\n    affine_map = AffineMap(\n        reg_affine, static.shape, static_affine, moving.shape, moving_affine\n    )\n    reg_data = affine_map.transform(moving, interpolation=interpolation)\n    if sdr_morph is not None:\n        logger.info(\"Applying SDR warp ...\")\n        reg_data = sdr_morph.transform(\n            reg_data,\n            interpolation=interpolation,\n            image_world2grid=np.linalg.inv(static_affine),\n            out_shape=static.shape,\n            out_grid2world=static_affine,\n        )\n    reg_data += cval\n    reg_img = SpatialImage(reg_data, static_affine)\n    logger.info(\"[done]\")\n    return reg_img\n\n\n@verbose\ndef apply_volume_registration_points(\n    info, trans, moving, static, reg_affine, sdr_morph=None, verbose=None\n):\n    \"\"\"Apply volume registration.\n\n    Uses registration parameters computed by\n    :func:`~mne.transforms.compute_volume_registration`.\n\n    Parameters\n    ----------\n    %(info_not_none)s\n    %(trans_not_none)s\n    %(moving)s\n    %(static)s\n    %(reg_affine)s\n    %(sdr_morph)s\n    %(verbose)s\n\n    Returns\n    -------\n    %(info_not_none)s\n    trans2 : instance of Transform\n        The head->mri (surface RAS) transform for the static image.\n\n    Notes\n    -----\n    .. versionadded:: 1.4.0\n    \"\"\"\n    from .channels import compute_native_head_t, make_dig_montage\n\n    _require_version(\"nibabel\", \"volume registration\", \"2.1.0\")\n    from dipy.align.imwarp import DiffeomorphicMap\n    from nibabel import MGHImage\n    from nibabel.spatialimages import SpatialImage\n\n    _validate_type(moving, SpatialImage, \"moving\")\n    _validate_type(static, SpatialImage, \"static\")\n    _validate_type(reg_affine, np.ndarray, \"reg_affine\")\n    _check_option(\"reg_affine.shape\", reg_affine.shape, ((4, 4),))\n    _validate_type(sdr_morph, (DiffeomorphicMap, None), \"sdr_morph\")\n\n    moving_mgh = MGHImage(np.array(moving.dataobj).astype(np.float32), moving.affine)\n    static_mgh = MGHImage(np.array(static.dataobj).astype(np.float32), static.affine)\n\n    montage = info.get_montage()\n    montage_kwargs = montage.get_positions()\n    trans = _ensure_trans(trans, \"head\", \"mri\")\n    montage.apply_trans(trans)  # to moving surface RAS\n\n    locs = np.array(list(montage.get_positions()[\"ch_pos\"].values()))\n\n    locs = apply_trans(\n        Transform(  # to moving voxels\n            fro=\"mri\",\n            to=\"mri_voxel\",\n            trans=np.linalg.inv(moving_mgh.header.get_vox2ras_tkr()),\n        ),\n        locs * 1000,\n    )\n    locs = apply_trans(\n        Transform(  # to moving ras\n            fro=\"mri_voxel\", to=\"ras\", trans=moving_mgh.header.get_vox2ras()\n        ),\n        locs,\n    )\n    locs = apply_trans(\n        Transform(  # to static ras\n            fro=\"ras\", to=\"ras\", trans=np.linalg.inv(reg_affine)\n        ),\n        locs,\n    )\n    if sdr_morph is not None:\n        _require_version(\"dipy\", \"SDR morph\", \"1.6.0\")\n        locs = sdr_morph.transform_points(\n            locs, sdr_morph.domain_grid2world, sdr_morph.domain_world2grid\n        )\n    locs = apply_trans(\n        Transform(  # to static voxels\n            fro=\"ras\",\n            to=\"mri_voxel\",\n            trans=np.linalg.inv(static_mgh.header.get_vox2ras()),\n        ),\n        locs,\n    )\n    locs = (\n        apply_trans(\n            Transform(  # to static surface RAS\n                fro=\"mri_voxel\", to=\"mri\", trans=static_mgh.header.get_vox2ras_tkr()\n            ),\n            locs,\n        )\n        / 1000\n    )\n\n    montage_kwargs[\"coord_frame\"] = \"mri\"\n    montage_kwargs[\"ch_pos\"] = {ch: loc for ch, loc in zip(montage.ch_names, locs)}\n    montage2 = make_dig_montage(**montage_kwargs)\n\n    trans2 = compute_native_head_t(montage2)\n    info2 = info.copy()\n    info2.set_montage(montage2)  # converts to head coordinates\n\n    return info2, trans2\n\n\nclass _MatchedDisplacementFieldInterpolator:\n    \"\"\"Interpolate from matched points using a displacement field in ND.\n\n    For a demo, see\n    https://gist.github.com/larsoner/fbe32d57996848395854d5e59dff1e10\n    and related tests.\n    \"\"\"\n\n    def __init__(self, fro, to, *, extrema=None):\n        from scipy.interpolate import LinearNDInterpolator\n\n        fro = np.array(fro, float)\n        to = np.array(to, float)\n        assert fro.shape == to.shape\n        assert fro.ndim == 2\n        # this restriction is only necessary because it's what\n        # _fit_matched_points requires\n        assert fro.shape[1] == 3\n\n        # Prealign using affine + uniform scaling\n        self._quat, self._scale = _fit_matched_points(fro, to, scale=True)\n        trans = _quat_to_affine(self._quat)\n        trans[:3, :3] *= self._scale\n        self._affine = trans\n        fro = apply_trans(trans, fro)\n\n        # Add points at extrema\n        if extrema is None:\n            delta = (to.max(axis=0) - to.min(axis=0)) / 2.0\n            assert (delta > 0).all()\n            extrema = np.array([fro.min(axis=0) - delta, fro.max(axis=0) + delta])\n        assert extrema.shape == (2, 3)  # min, max\n        self._extrema = np.array(np.meshgrid(*extrema.T)).T.reshape(-1, fro.shape[-1])\n        fro_concat = np.concatenate((fro, self._extrema))\n        to_concat = np.concatenate((to, self._extrema))\n\n        # Compute the interpolator (which internally uses Delaunay)\n        self._interp = LinearNDInterpolator(fro_concat, to_concat)\n\n    def __call__(self, x):\n        assert x.ndim in (1, 2) and x.shape[-1] == 3\n        assert np.isfinite(x).all()\n        singleton = x.ndim == 1\n        x = apply_trans(self._affine, x)\n        assert np.isfinite(x).all()\n        out = self._interp(x)\n        assert np.isfinite(out).all()\n        self._last_deltas = np.linalg.norm(x - out, axis=1)\n        out = out[0] if singleton else out\n        return out\n", "import_text": ["glob", "os", "copy.deepcopy", "pathlib.Path", "numpy", "scipy.linalg", "scipy.spatial.distance.cdist", "scipy.special.sph_harm"], "prompt": "\"\"\"\nDescription: This function calculates the transformation matrix from RAS to Neuromag coordinates.\n\nArgs:\n    nasion (array-like): The nasion point in RAS coordinates.\n    lpa (array-like): The left posterior angle point in RAS coordinates.\n    rpa (array-like): The right posterior angle point in RAS coordinates.\n\nReturns:\n    array: The transformation matrix from RAS to Neuromag coordinates.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Construct a transformation matrix to the MNE head coordinate system.\n\n    Construct a transformation matrix from an arbitrary RAS coordinate system\n    to the MNE head coordinate system, in which the x axis passes through the\n    two preauricular points, and the y axis passes through the nasion and is\n    normal to the x axis. (see mne manual, pg. 97)\n\n    Parameters\n    ----------\n    nasion : array_like, shape (3,)\n        Nasion point coordinate.\n    lpa : array_like, shape (3,)\n        Left peri-auricular point coordinate.\n    rpa : array_like, shape (3,)\n        Right peri-auricular point coordinate.\n\n    Returns\n    -------\n    trans : numpy.array, shape = (4, 4)\n        Transformation matrix to MNE head space.\n    \"\"\"", "function_dependencies": ["numpy.asarray", "numpy.linalg.norm", "numpy.dot", "numpy.cross", "numpy.vstack", "numpy.reshape", "numpy.hstack"], "project_create_time": "2011-01-28T03:31:13+00:00", "project_update_time": "2024-04-15T11:41:19+00:00", "file_create_time": "2011-01-28T02:54:26Z", "file_update_time": "2024-03-19T16:30:50Z", "function_update_time": "2011-01-28T02:54:26Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["numpy.reshape"], "test_function": [{"file_path": "/mne-python-v1.7.0/mne-python-1.7.0/mne/tests/test_transforms.py", "class_name": null, "function_name": "test_get_ras_to_neuromag_trans", "code": "def test_get_ras_to_neuromag_trans():\n    # create model points in neuromag-like space\n    rng = np.random.RandomState(0)\n    anterior = [0, 1, 0]\n    left = [-1, 0, 0]\n    right = [0.8, 0, 0]\n    up = [0, 0, 1]\n    rand_pts = rng.uniform(-1, 1, (3, 3))\n    pts = np.vstack((anterior, left, right, up, rand_pts))\n\n    # change coord system\n    rx, ry, rz, tx, ty, tz = rng.uniform(-2 * np.pi, 2 * np.pi, 6)\n    trans = np.dot(translation(tx, ty, tz), rotation(rx, ry, rz))\n    pts_changed = apply_trans(trans, pts)\n\n    # transform back into original space\n    nas, lpa, rpa = pts_changed[:3]\n    hsp_trans = get_ras_to_neuromag_trans(nas, lpa, rpa)\n    pts_restored = apply_trans(hsp_trans, pts_changed)\n\n    err = \"Neuromag transformation failed\"\n    assert_allclose(pts_restored, pts, atol=1e-6, err_msg=err)"}]}, {"git_group": "catboost", "git_name": "catboost", "version": "v1.2.5", "language": "Python", "project_name": "catboost-v1.2.5.zip", "file_path": "/catboost-v1.2.5/catboost-1.2.5/contrib/python/scipy/py2/scipy/signal/wavelets.py", "file_name": "wavelets.py", "focal_class": null, "focal_name": "cwt", "focal_parameter": ["data", "wavelet", "widths"], "solution": "def cwt(data, wavelet, widths):\n    output = np.zeros([len(widths), len(data)])\n    for ind, width in enumerate(widths):\n        wavelet_data = wavelet(min(10 * width, len(data)), width)\n        output[ind, :] = convolve(data, wavelet_data,\n                                  mode='same')\n    return output", "function_signature": "def cwt(data, wavelet, widths) :", "left_context": "from __future__ import division, print_function, absolute_import\n\nimport numpy as np\nfrom numpy.dual import eig\nfrom scipy.special import comb\nfrom scipy import linspace, pi, exp\nfrom scipy.signal import convolve\n\n__all__ = ['daub', 'qmf', 'cascade', 'morlet', 'ricker', 'cwt']\n\n\ndef daub(p):\n    \"\"\"\n    The coefficients for the FIR low-pass filter producing Daubechies wavelets.\n\n    p>=1 gives the order of the zero at f=1/2.\n    There are 2p filter coefficients.\n\n    Parameters\n    ----------\n    p : int\n        Order of the zero at f=1/2, can have values from 1 to 34.\n\n    Returns\n    -------\n    daub : ndarray\n        Return\n\n    \"\"\"\n    sqrt = np.sqrt\n    if p < 1:\n        raise ValueError(\"p must be at least 1.\")\n    if p == 1:\n        c = 1 / sqrt(2)\n        return np.array([c, c])\n    elif p == 2:\n        f = sqrt(2) / 8\n        c = sqrt(3)\n        return f * np.array([1 + c, 3 + c, 3 - c, 1 - c])\n    elif p == 3:\n        tmp = 12 * sqrt(10)\n        z1 = 1.5 + sqrt(15 + tmp) / 6 - 1j * (sqrt(15) + sqrt(tmp - 15)) / 6\n        z1c = np.conj(z1)\n        f = sqrt(2) / 8\n        d0 = np.real((1 - z1) * (1 - z1c))\n        a0 = np.real(z1 * z1c)\n        a1 = 2 * np.real(z1)\n        return f / d0 * np.array([a0, 3 * a0 - a1, 3 * a0 - 3 * a1 + 1,\n                                  a0 - 3 * a1 + 3, 3 - a1, 1])\n    elif p < 35:\n        # construct polynomial and factor it\n        if p < 35:\n            P = [comb(p - 1 + k, k, exact=1) for k in range(p)][::-1]\n            yj = np.roots(P)\n        else:  # try different polynomial --- needs work\n            P = [comb(p - 1 + k, k, exact=1) / 4.0**k\n                 for k in range(p)][::-1]\n            yj = np.roots(P) / 4\n        # for each root, compute two z roots, select the one with |z|>1\n        # Build up final polynomial\n        c = np.poly1d([1, 1])**p\n        q = np.poly1d([1])\n        for k in range(p - 1):\n            yval = yj[k]\n            part = 2 * sqrt(yval * (yval - 1))\n            const = 1 - 2 * yval\n            z1 = const + part\n            if (abs(z1)) < 1:\n                z1 = const - part\n            q = q * [1, -z1]\n\n        q = c * np.real(q)\n        # Normalize result\n        q = q / np.sum(q) * sqrt(2)\n        return q.c[::-1]\n    else:\n        raise ValueError(\"Polynomial factorization does not work \"\n                         \"well for p too large.\")\n\n\ndef qmf(hk):\n    \"\"\"\n    Return high-pass qmf filter from low-pass\n\n    Parameters\n    ----------\n    hk : array_like\n        Coefficients of high-pass filter.\n\n    \"\"\"\n    N = len(hk) - 1\n    asgn = [{0: 1, 1: -1}[k % 2] for k in range(N + 1)]\n    return hk[::-1] * np.array(asgn)\n\n\ndef cascade(hk, J=7):\n    \"\"\"\n    Return (x, phi, psi) at dyadic points ``K/2**J`` from filter coefficients.\n\n    Parameters\n    ----------\n    hk : array_like\n        Coefficients of low-pass filter.\n    J : int, optional\n        Values will be computed at grid points ``K/2**J``. Default is 7.\n\n    Returns\n    -------\n    x : ndarray\n        The dyadic points ``K/2**J`` for ``K=0...N * (2**J)-1`` where\n        ``len(hk) = len(gk) = N+1``.\n    phi : ndarray\n        The scaling function ``phi(x)`` at `x`:\n        ``phi(x) = sum(hk * phi(2x-k))``, where k is from 0 to N.\n    psi : ndarray, optional\n        The wavelet function ``psi(x)`` at `x`:\n        ``phi(x) = sum(gk * phi(2x-k))``, where k is from 0 to N.\n        `psi` is only returned if `gk` is not None.\n\n    Notes\n    -----\n    The algorithm uses the vector cascade algorithm described by Strang and\n    Nguyen in \"Wavelets and Filter Banks\".  It builds a dictionary of values\n    and slices for quick reuse.  Then inserts vectors into final vector at the\n    end.\n\n    \"\"\"\n    N = len(hk) - 1\n\n    if (J > 30 - np.log2(N + 1)):\n        raise ValueError(\"Too many levels.\")\n    if (J < 1):\n        raise ValueError(\"Too few levels.\")\n\n    # construct matrices needed\n    nn, kk = np.ogrid[:N, :N]\n    s2 = np.sqrt(2)\n    # append a zero so that take works\n    thk = np.r_[hk, 0]\n    gk = qmf(hk)\n    tgk = np.r_[gk, 0]\n\n    indx1 = np.clip(2 * nn - kk, -1, N + 1)\n    indx2 = np.clip(2 * nn - kk + 1, -1, N + 1)\n    m = np.zeros((2, 2, N, N), 'd')\n    m[0, 0] = np.take(thk, indx1, 0)\n    m[0, 1] = np.take(thk, indx2, 0)\n    m[1, 0] = np.take(tgk, indx1, 0)\n    m[1, 1] = np.take(tgk, indx2, 0)\n    m *= s2\n\n    # construct the grid of points\n    x = np.arange(0, N * (1 << J), dtype=float) / (1 << J)\n    phi = 0 * x\n\n    psi = 0 * x\n\n    # find phi0, and phi1\n    lam, v = eig(m[0, 0])\n    ind = np.argmin(np.absolute(lam - 1))\n    # a dictionary with a binary representation of the\n    #   evaluation points x < 1 -- i.e. position is 0.xxxx\n    v = np.real(v[:, ind])\n    # need scaling function to integrate to 1 so find\n    #  eigenvector normalized to sum(v,axis=0)=1\n    sm = np.sum(v)\n    if sm < 0:  # need scaling function to integrate to 1\n        v = -v\n        sm = -sm\n    bitdic = {'0': v / sm}\n    bitdic['1'] = np.dot(m[0, 1], bitdic['0'])\n    step = 1 << J\n    phi[::step] = bitdic['0']\n    phi[(1 << (J - 1))::step] = bitdic['1']\n    psi[::step] = np.dot(m[1, 0], bitdic['0'])\n    psi[(1 << (J - 1))::step] = np.dot(m[1, 1], bitdic['0'])\n    # descend down the levels inserting more and more values\n    #  into bitdic -- store the values in the correct location once we\n    #  have computed them -- stored in the dictionary\n    #  for quicker use later.\n    prevkeys = ['1']\n    for level in range(2, J + 1):\n        newkeys = ['%d%s' % (xx, yy) for xx in [0, 1] for yy in prevkeys]\n        fac = 1 << (J - level)\n        for key in newkeys:\n            # convert key to number\n            num = 0\n            for pos in range(level):\n                if key[pos] == '1':\n                    num += (1 << (level - 1 - pos))\n            pastphi = bitdic[key[1:]]\n            ii = int(key[0])\n            temp = np.dot(m[0, ii], pastphi)\n            bitdic[key] = temp\n            phi[num * fac::step] = temp\n            psi[num * fac::step] = np.dot(m[1, ii], pastphi)\n        prevkeys = newkeys\n\n    return x, phi, psi\n\n\ndef morlet(M, w=5.0, s=1.0, complete=True):\n    \"\"\"\n    Complex Morlet wavelet.\n\n    Parameters\n    ----------\n    M : int\n        Length of the wavelet.\n    w : float, optional\n        Omega0. Default is 5\n    s : float, optional\n        Scaling factor, windowed from ``-s*2*pi`` to ``+s*2*pi``. Default is 1.\n    complete : bool, optional\n        Whether to use the complete or the standard version.\n\n    Returns\n    -------\n    morlet : (M,) ndarray\n\n    See Also\n    --------\n    scipy.signal.gausspulse\n\n    Notes\n    -----\n    The standard version::\n\n        pi**-0.25 * exp(1j*w*x) * exp(-0.5*(x**2))\n\n    This commonly used wavelet is often referred to simply as the\n    Morlet wavelet.  Note that this simplified version can cause\n    admissibility problems at low values of `w`.\n\n    The complete version::\n\n        pi**-0.25 * (exp(1j*w*x) - exp(-0.5*(w**2))) * exp(-0.5*(x**2))\n\n    This version has a correction\n    term to improve admissibility. For `w` greater than 5, the\n    correction term is negligible.\n\n    Note that the energy of the return wavelet is not normalised\n    according to `s`.\n\n    The fundamental frequency of this wavelet in Hz is given\n    by ``f = 2*s*w*r / M`` where `r` is the sampling rate.\n    \n    Note: This function was created before `cwt` and is not compatible\n    with it.\n\n    \"\"\"\n    x = linspace(-s * 2 * pi, s * 2 * pi, M)\n    output = exp(1j * w * x)\n\n    if complete:\n        output -= exp(-0.5 * (w**2))\n\n    output *= exp(-0.5 * (x**2)) * pi**(-0.25)\n\n    return output\n\n\ndef ricker(points, a):\n    \"\"\"\n    Return a Ricker wavelet, also known as the \"Mexican hat wavelet\".\n\n    It models the function:\n\n        ``A (1 - x^2/a^2) exp(-x^2/2 a^2)``,\n\n    where ``A = 2/sqrt(3a)pi^1/4``.\n\n    Parameters\n    ----------\n    points : int\n        Number of points in `vector`.\n        Will be centered around 0.\n    a : scalar\n        Width parameter of the wavelet.\n\n    Returns\n    -------\n    vector : (N,) ndarray\n        Array of length `points` in shape of ricker curve.\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n\n    >>> points = 100\n    >>> a = 4.0\n    >>> vec2 = signal.ricker(points, a)\n    >>> print(len(vec2))\n    100\n    >>> plt.plot(vec2)\n    >>> plt.show()\n\n    \"\"\"\n    A = 2 / (np.sqrt(3 * a) * (np.pi**0.25))\n    wsq = a**2\n    vec = np.arange(0, points) - (points - 1.0) / 2\n    xsq = vec**2\n    mod = (1 - xsq / wsq)\n    gauss = np.exp(-xsq / (2 * wsq))\n    total = A * mod * gauss\n    return total\n\n", "right_context": "", "import_text": ["numpy", "numpy.dual.eig", "scipy.special.comb", "scipy.linspace", "scipy.pi", "scipy.exp", "scipy.signal.convolve"], "prompt": "\"\"\"\nDescription: This function performs continuous wavelet transform (CWT) on the given data using the specified wavelet function.\n\nArgs:\n    data (array-like): The input data to be transformed.\n    wavelet (function): The wavelet function to be used for the transformation.\n    widths (array-like): The widths of the wavelets to be used.\n\nReturns:\n    array: The transformed data in the form of a 2D array, where each row corresponds to a different width in the 'widths' list.\n\"\"\"", "comment": "    \"\"\"\n    Continuous wavelet transform.\n\n    Performs a continuous wavelet transform on `data`,\n    using the `wavelet` function. A CWT performs a convolution\n    with `data` using the `wavelet` function, which is characterized\n    by a width parameter and length parameter.\n\n    Parameters\n    ----------\n    data : (N,) ndarray\n        data on which to perform the transform.\n    wavelet : function\n        Wavelet function, which should take 2 arguments.\n        The first argument is the number of points that the returned vector\n        will have (len(wavelet(length,width)) == length).\n        The second is a width parameter, defining the size of the wavelet\n        (e.g. standard deviation of a gaussian). See `ricker`, which\n        satisfies these requirements.\n    widths : (M,) sequence\n        Widths to use for transform.\n\n    Returns\n    -------\n    cwt: (M, N) ndarray\n        Will have shape of (len(widths), len(data)).\n\n    Notes\n    -----\n    ::\n\n        length = min(10 * width[ii], len(data))\n        cwt[ii,:] = signal.convolve(data, wavelet(length,\n                                    width[ii]), mode='same')\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n    >>> t = np.linspace(-1, 1, 200, endpoint=False)\n    >>> sig  = np.cos(2 * np.pi * 7 * t) + signal.gausspulse(t - 0.4, fc=2)\n    >>> widths = np.arange(1, 31)\n    >>> cwtmatr = signal.cwt(sig, signal.ricker, widths)\n    >>> plt.imshow(cwtmatr, extent=[-1, 1, 31, 1], cmap='PRGn', aspect='auto',\n    ...            vmax=abs(cwtmatr).max(), vmin=-abs(cwtmatr).max())\n    >>> plt.show()\n\n    \"\"\"", "prompt_is_gen_from_api": true, "function_dependencies": ["numpy.zeros", "scipy.signal.convolve"], "project_create_time": "2017-07-18T05:29:04+00:00", "project_update_time": "2024-04-17T19:09:27+00:00", "file_create_time": "2021-12-13T16:11:19Z", "file_update_time": "2023-01-16T12:51:16Z", "function_update_time": "2021-12-13T16:11:19Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["scipy.signal.convolve"], "test_function": [{"file_path": "/catboost-v1.2.5/catboost-1.2.5/contrib/python/scipy/py2/scipy/signal/tests/test_wavelets.py", "class_name": "TestWavelets", "function_name": "test_cwt", "code": "\n    def test_cwt(self):\n        widths = [1.0]\n        delta_wavelet = lambda s, t: np.array([1])\n        len_data = 100\n        test_data = np.sin(np.pi * np.arange(0, len_data) / 10.0)\n\n        #Test delta function input gives same data as output\n        cwt_dat = wavelets.cwt(test_data, delta_wavelet, widths)\n        assert_(cwt_dat.shape == (len(widths), len_data))\n        assert_array_almost_equal(test_data, cwt_dat.flatten())\n\n        #Check proper shape on output\n        widths = [1, 3, 4, 5, 10]\n        cwt_dat = wavelets.cwt(test_data, wavelets.ricker, widths)\n        assert_(cwt_dat.shape == (len(widths), len_data))\n\n        widths = [len_data * 10]\n        #Note: this wavelet isn't defined quite right, but is fine for this test\n        flat_wavelet = lambda l, w: np.ones(w) / w\n        cwt_dat = wavelets.cwt(test_data, flat_wavelet, widths)\n        assert_array_almost_equal(cwt_dat, np.mean(test_data))"}]}, {"git_group": "facebook", "git_name": "Ax", "version": "v0.1.20", "language": "Python", "project_name": "Ax-v0.1.20.zip", "file_path": "/Ax-v0.1.20/Ax-0.1.20/ax/models/torch/botorch_modular/model.py", "file_name": "model.py", "focal_class": "BoTorchModel", "focal_name": "gen", "focal_parameter": [], "solution": "\n    def gen(\n        self,\n        n: int,\n        bounds: List[Tuple[float, float]],\n        objective_weights: Tensor,\n        outcome_constraints: Optional[Tuple[Tensor, Tensor]] = None,\n        linear_constraints: Optional[Tuple[Tensor, Tensor]] = None,\n        fixed_features: Optional[Dict[int, float]] = None,\n        pending_observations: Optional[List[Tensor]] = None,\n        model_gen_options: Optional[TConfig] = None,\n        rounding_func: Optional[Callable[[Tensor], Tensor]] = None,\n        target_fidelities: Optional[Dict[int, float]] = None,\n    ) -> Tuple[Tensor, Tensor, TGenMetadata, Optional[List[TCandidateMetadata]]]:\n        acq_options, opt_options = construct_acquisition_and_optimizer_options(\n            acqf_options=self.acquisition_options, model_gen_options=model_gen_options\n        )\n        acqf = self._instantiate_acquisition(\n            bounds=bounds,\n            objective_weights=objective_weights,\n            outcome_constraints=outcome_constraints,\n            linear_constraints=linear_constraints,\n            fixed_features=fixed_features,\n            pending_observations=pending_observations,\n            target_fidelities=target_fidelities,\n            acq_options=acq_options,\n        )\n\n        botorch_rounding_func = get_rounding_func(rounding_func)\n        candidates, expected_acquisition_value = acqf.optimize(\n            bounds=self._bounds_as_tensor(bounds=bounds),\n            n=n,\n            inequality_constraints=_to_inequality_constraints(\n                linear_constraints=linear_constraints\n            ),\n            fixed_features=fixed_features,\n            rounding_func=botorch_rounding_func,\n            optimizer_options=checked_cast(dict, opt_options),\n        )\n        return (\n            candidates.detach().cpu(),\n            torch.ones(n, dtype=self.surrogate.dtype),\n            {Keys.EXPECTED_ACQF_VAL: expected_acquisition_value.tolist()},\n            None,\n        )", "function_signature": "def gen(\n        self,\n        n: int,\n        bounds: List[Tuple[float, float]],\n        objective_weights: Tensor,\n        outcome_constraints: Optional[Tuple[Tensor, Tensor]] = None,\n        linear_constraints: Optional[Tuple[Tensor, Tensor]] = None,\n        fixed_features: Optional[Dict[int, float]] = None,\n        pending_observations: Optional[List[Tensor]] = None,\n        model_gen_options: Optional[TConfig] = None,\n        rounding_func: Optional[Callable[[Tensor], Tensor]] = None,\n        target_fidelities: Optional[Dict[int, float]] = None,\n    ) -> Tuple[Tensor, Tensor, TGenMetadata, Optional[List[TCandidateMetadata]]] :", "left_context": "#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom copy import deepcopy\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n\nimport torch\nfrom ax.core.types import TCandidateMetadata, TConfig, TGenMetadata\nfrom ax.models.torch.botorch import get_rounding_func\nfrom ax.models.torch.botorch_modular.acquisition import Acquisition\nfrom ax.models.torch.botorch_modular.list_surrogate import ListSurrogate\nfrom ax.models.torch.botorch_modular.surrogate import Surrogate\nfrom ax.models.torch.botorch_modular.utils import (\n    choose_botorch_acqf_class,\n    choose_model_class,\n    construct_acquisition_and_optimizer_options,\n    construct_single_training_data,\n    construct_training_data_list,\n    use_model_list,\n    validate_data_format,\n)\nfrom ax.models.torch.utils import _to_inequality_constraints\nfrom ax.models.torch_base import TorchModel\nfrom ax.utils.common.base import Base\nfrom ax.utils.common.constants import Keys\nfrom ax.utils.common.docutils import copy_doc\nfrom ax.utils.common.typeutils import checked_cast, not_none\nfrom botorch.acquisition.acquisition import AcquisitionFunction\nfrom botorch.utils.containers import TrainingData\nfrom torch import Tensor\n\n\nclass BoTorchModel(TorchModel, Base):\n    \"\"\"**All classes in 'botorch_modular' directory are under\n    construction, incomplete, and should be treated as alpha\n    versions only.**\n\n    Modular `Model` class for combining BoTorch subcomponents\n    in Ax. Specified via `Surrogate` and `Acquisition`, which wrap\n    BoTorch `Model` and `AcquisitionFunction`, respectively, for\n    convenient use in Ax.\n\n    Args:\n        acquisition_class: Type of `Acquisition` to be used in\n            this model, auto-selected based on experiment and data\n            if not specified.\n        acquisition_options: Optional dict of kwargs, passed to\n            the constructor of BoTorch `AcquisitionFunction`.\n        botorch_acqf_class: Type of `AcquisitionFunction` to be\n            used in this model, auto-selected based on experiment\n            and data if not specified.\n        surrogate: An instance of `Surrogate` to be used as part of\n            this model; if not specified, type of `Surrogate` and\n            underlying BoTorch `Model` will be auto-selected based\n            on experiment and data, with kwargs in `surrogate_options`\n            applied.\n        surrogate_options: Optional dict of kwargs for `Surrogate`\n            (used if no pre-instantiated Surrogate via is passed via `surrogate`).\n            Can include:\n            - model_options: Dict of options to surrogate's underlying\n            BoTorch `Model`,\n            - submodel_options or submodel_options_per_outcome:\n            Options for submodels in `ListSurrogate`, see documentation\n            for `ListSurrogate`.\n        refit_on_update: Whether to reoptimize model parameters during call\n            to `BoTorchModel.update`. If false, training data for the model\n            (used for inference) is still swapped for new training data, but\n            model parameters are not reoptimized.\n        refit_on_cv: Whether to reoptimize model parameters during call to\n            `BoTorchmodel.cross_validate`.\n        warm_start_refit: Whether to load parameters from either the provided\n            state dict or the state dict of the current BoTorch `Model` during\n            refitting. If False, model parameters will be reoptimized from\n            scratch on refit. NOTE: This setting is ignored during `update` or\n            `cross_validate` if the corresponding `refit_on_...` is False.\n    \"\"\"\n\n    acquisition_class: Type[Acquisition]\n    acquisition_options: Dict[str, Any]\n    surrogate_options: Dict[str, Any]\n    _surrogate: Optional[Surrogate]\n    _botorch_acqf_class: Optional[Type[AcquisitionFunction]]\n\n    def __init__(\n        self,\n        acquisition_class: Optional[Type[Acquisition]] = None,\n        acquisition_options: Optional[Dict[str, Any]] = None,\n        botorch_acqf_class: Optional[Type[AcquisitionFunction]] = None,\n        surrogate: Optional[Surrogate] = None,\n        surrogate_options: Optional[Dict[str, Any]] = None,\n        refit_on_update: bool = True,\n        refit_on_cv: bool = False,\n        warm_start_refit: bool = True,\n    ) -> None:\n        self._surrogate = surrogate\n        if surrogate and surrogate_options:\n            raise ValueError(  # pragma: no cover\n                \"`surrogate_options` are only applied when using the default \"\n                \"surrogate, so only one of `surrogate` and `surrogate_options`\"\n                \" arguments is expected.\"\n            )\n        self.surrogate_options = surrogate_options or {}\n        self.acquisition_class = acquisition_class or Acquisition\n        # `_botorch_acqf_class` can be set to `None` here. If so,\n        # `Model.gen` will set it with `choose_botorch_acqf_class`.\n        self._botorch_acqf_class = (\n            botorch_acqf_class or self.acquisition_class.default_botorch_acqf_class\n        )\n        self.acquisition_options = acquisition_options or {}\n        self.refit_on_update = refit_on_update\n        self.refit_on_cv = refit_on_cv\n        self.warm_start_refit = warm_start_refit\n\n    @property\n    def surrogate(self) -> Surrogate:\n        if not self._surrogate:\n            raise ValueError(\"Surrogate has not yet been set.\")\n        return not_none(self._surrogate)\n\n    @property\n    def botorch_acqf_class(self) -> Type[AcquisitionFunction]:\n        if not self._botorch_acqf_class:\n            raise ValueError(\"BoTorch `AcquisitionFunction` has not yet been set.\")\n        return not_none(self._botorch_acqf_class)\n\n    @copy_doc(TorchModel.fit)\n    def fit(\n        self,\n        Xs: List[Tensor],\n        Ys: List[Tensor],\n        Yvars: List[Tensor],\n        bounds: List[Tuple[float, float]],\n        task_features: List[int],\n        feature_names: List[str],\n        metric_names: List[str],\n        fidelity_features: List[int],\n        target_fidelities: Optional[Dict[int, float]] = None,\n        candidate_metadata: Optional[List[List[TCandidateMetadata]]] = None,\n        state_dict: Optional[Dict[str, Tensor]] = None,\n        refit: bool = True,\n    ) -> None:\n        # Ensure that parts of data all have equal lengths.\n        validate_data_format(Xs=Xs, Ys=Ys, Yvars=Yvars, metric_names=metric_names)\n\n        # Choose `Surrogate` and undelying `Model` based on properties of data.\n        if not self._surrogate:\n            self._autoset_surrogate(\n                Xs=Xs,\n                Ys=Ys,\n                Yvars=Yvars,\n                task_features=task_features,\n                fidelity_features=fidelity_features,\n                metric_names=metric_names,\n            )\n\n        self.surrogate.fit(\n            # pyre-ignore[6]: Base `Surrogate` expects only single `TrainingData`,\n            # but `ListSurrogate` expects a list of them, so `training_data` here is\n            # a union of the two.\n            training_data=self._mk_training_data(Xs=Xs, Ys=Ys, Yvars=Yvars),\n            bounds=bounds,\n            task_features=task_features,\n            feature_names=feature_names,\n            fidelity_features=fidelity_features,\n            target_fidelities=target_fidelities,\n            metric_names=metric_names,\n            candidate_metadata=candidate_metadata,\n            state_dict=state_dict,\n            refit=refit,\n        )\n\n    @copy_doc(TorchModel.update)\n    def update(\n        self,\n        Xs: List[Tensor],\n        Ys: List[Tensor],\n        Yvars: List[Tensor],\n        bounds: List[Tuple[float, float]],\n        task_features: List[int],\n        feature_names: List[str],\n        metric_names: List[str],\n        fidelity_features: List[int],\n        target_fidelities: Optional[Dict[int, float]] = None,\n        candidate_metadata: Optional[List[List[TCandidateMetadata]]] = None,\n    ) -> None:\n        if not self._surrogate:\n            raise ValueError(\"Cannot update model that has not been fitted.\")\n\n        # Sometimes the model fit should be restarted from scratch on update, for models\n        # that are prone to overfitting. In those cases, `self.warm_start_refit` should\n        # be false and `Surrogate.update` will not receive a state dict and will not\n        # pass it to the underlying `Surrogate.fit`.\n        state_dict = (\n            None\n            if self.refit_on_update and not self.warm_start_refit\n            else self.surrogate.model.state_dict()\n        )\n\n        self.surrogate.update(\n            # pyre-ignore[6]: Base `Surrogate` expects only single `TrainingData`,\n            # but `ListSurrogate` expects a list of them, so `training_data` here is\n            # a union of the two.\n            training_data=self._mk_training_data(Xs=Xs, Ys=Ys, Yvars=Yvars),\n            bounds=bounds,\n            task_features=task_features,\n            feature_names=feature_names,\n            metric_names=metric_names,\n            fidelity_features=fidelity_features,\n            target_fidelities=target_fidelities,\n            candidate_metadata=candidate_metadata,\n            state_dict=state_dict,\n            refit=self.refit_on_update,\n        )\n\n    @copy_doc(TorchModel.predict)\n    def predict(self, X: Tensor) -> Tuple[Tensor, Tensor]:\n        return self.surrogate.predict(X=X)\n\n    @copy_doc(TorchModel.gen)", "right_context": "\n    @copy_doc(TorchModel.best_point)\n    def best_point(\n        self,\n        bounds: List[Tuple[float, float]],\n        objective_weights: Tensor,\n        outcome_constraints: Optional[Tuple[Tensor, Tensor]] = None,\n        linear_constraints: Optional[Tuple[Tensor, Tensor]] = None,\n        fixed_features: Optional[Dict[int, float]] = None,\n        model_gen_options: Optional[TConfig] = None,\n        target_fidelities: Optional[Dict[int, float]] = None,\n    ) -> Optional[Tensor]:\n        raise NotImplementedError(\"Coming soon.\")\n\n    @copy_doc(TorchModel.evaluate_acquisition_function)\n    def evaluate_acquisition_function(\n        self,\n        X: Tensor,\n        bounds: List[Tuple[float, float]],\n        objective_weights: Tensor,\n        outcome_constraints: Optional[Tuple[Tensor, Tensor]] = None,\n        linear_constraints: Optional[Tuple[Tensor, Tensor]] = None,\n        fixed_features: Optional[Dict[int, float]] = None,\n        pending_observations: Optional[List[Tensor]] = None,\n        target_fidelities: Optional[Dict[int, float]] = None,\n        acq_options: Optional[Dict[str, Any]] = None,\n    ) -> Tensor:\n        acqf = self._instantiate_acquisition(\n            bounds=bounds,\n            objective_weights=objective_weights,\n            outcome_constraints=outcome_constraints,\n            linear_constraints=linear_constraints,\n            fixed_features=fixed_features,\n            pending_observations=pending_observations,\n            target_fidelities=target_fidelities,\n            acq_options=acq_options,\n        )\n        return acqf.evaluate(X=X)\n\n    def cross_validate(\n        self,\n        Xs_train: List[Tensor],\n        Ys_train: List[Tensor],\n        Yvars_train: List[Tensor],\n        X_test: Tensor,\n        bounds: List[Tuple[float, float]],\n        task_features: List[int],\n        feature_names: List[str],\n        metric_names: List[str],\n        fidelity_features: List[int],\n    ) -> Tuple[Tensor, Tensor]:\n        current_surrogate = self.surrogate\n        # If we should be refitting but not warm-starting the refit, set\n        # `state_dict` to None to avoid loading it.\n        state_dict = (\n            None\n            if self.refit_on_cv and not self.warm_start_refit\n            else deepcopy(current_surrogate.model.state_dict())\n        )\n\n        # Temporarily set `_surrogate` to cloned surrogate to set\n        # the training data on cloned surrogate to train set and\n        # use it to predict the test point.\n        surrogate_clone = self.surrogate.clone_reset()\n        self._surrogate = surrogate_clone\n\n        try:\n            self.fit(\n                Xs=Xs_train,\n                Ys=Ys_train,\n                Yvars=Yvars_train,\n                bounds=bounds,\n                task_features=task_features,\n                feature_names=feature_names,\n                metric_names=metric_names,\n                fidelity_features=fidelity_features,\n                state_dict=state_dict,\n                refit=self.refit_on_cv,\n            )\n            X_test_prediction = self.predict(X=X_test)\n        finally:\n            # Reset the surrogate back to this model's surrogate, make\n            # sure the cloned surrogate doesn't stay around if fit or\n            # predict fail.\n            self._surrogate = current_surrogate\n        return X_test_prediction\n\n    def _autoset_surrogate(\n        self,\n        Xs: List[Tensor],\n        Ys: List[Tensor],\n        Yvars: List[Tensor],\n        task_features: List[int],\n        fidelity_features: List[int],\n        metric_names: List[str],\n    ) -> None:\n        \"\"\"Sets a default surrogate on this model if one was not explicitly\n        provided.\n        \"\"\"\n        # To determine whether to use `ListSurrogate`, we need to check for\n        # the batched multi-output case, so we first see which model would\n        # be chosen given the Yvars and the properties of data.\n        botorch_model_class = choose_model_class(\n            Yvars=Yvars,\n            task_features=task_features,\n            fidelity_features=fidelity_features,\n        )\n        if use_model_list(Xs=Xs, botorch_model_class=botorch_model_class):\n            # If using `ListSurrogate` / `ModelListGP`, pick submodels for each\n            # outcome.\n            botorch_submodel_class_per_outcome = {\n                metric_name: choose_model_class(\n                    Yvars=[Yvar],\n                    task_features=task_features,\n                    fidelity_features=fidelity_features,\n                )\n                for Yvar, metric_name in zip(Yvars, metric_names)\n            }\n            self._surrogate = ListSurrogate(\n                botorch_submodel_class_per_outcome=botorch_submodel_class_per_outcome,\n                **self.surrogate_options,\n            )\n        else:\n            # Using regular `Surrogate`, so botorch model picked at the beginning\n            # of the function is the one we should use.\n            self._surrogate = Surrogate(\n                botorch_model_class=botorch_model_class, **self.surrogate_options\n            )\n\n    def _bounds_as_tensor(self, bounds: List[Tuple[float, float]]) -> Tensor:\n        bounds_ = torch.tensor(\n            bounds, dtype=self.surrogate.dtype, device=self.surrogate.device\n        )\n        return bounds_.transpose(0, 1)\n\n    def _instantiate_acquisition(\n        self,\n        bounds: List[Tuple[float, float]],\n        objective_weights: Tensor,\n        outcome_constraints: Optional[Tuple[Tensor, Tensor]] = None,\n        linear_constraints: Optional[Tuple[Tensor, Tensor]] = None,\n        fixed_features: Optional[Dict[int, float]] = None,\n        pending_observations: Optional[List[Tensor]] = None,\n        target_fidelities: Optional[Dict[int, float]] = None,\n        acq_options: Optional[Dict[str, Any]] = None,\n    ) -> Acquisition:\n        if not self._botorch_acqf_class:\n            self._botorch_acqf_class = choose_botorch_acqf_class()\n\n        return self.acquisition_class(\n            surrogate=self.surrogate,\n            botorch_acqf_class=self.botorch_acqf_class,\n            bounds=bounds,\n            objective_weights=objective_weights,\n            outcome_constraints=outcome_constraints,\n            linear_constraints=linear_constraints,\n            fixed_features=fixed_features,\n            pending_observations=pending_observations,\n            target_fidelities=target_fidelities,\n            options=acq_options,\n        )\n\n    def _mk_training_data(\n        self,\n        Xs: List[Tensor],\n        Ys: List[Tensor],\n        Yvars: List[Tensor],\n    ) -> Union[TrainingData, List[TrainingData]]:\n        if isinstance(self.surrogate, ListSurrogate):\n            return construct_training_data_list(Xs=Xs, Ys=Ys, Yvars=Yvars)\n        return construct_single_training_data(Xs=Xs, Ys=Ys, Yvars=Yvars)\n", "import_text": ["copy.deepcopy", "typing.Any", "typing.Callable", "typing.Dict", "typing.List", "typing.Optional", "typing.Tuple", "typing.Type", "typing.Union", "torch", "ax.core.types.TCandidateMetadata", "ax.core.types.TConfig", "ax.core.types.TGenMetadata", "ax.models.torch.botorch.get_rounding_func", "ax.models.torch.botorch_modular.acquisition.Acquisition", "ax.models.torch.botorch_modular.list_surrogate.ListSurrogate", "ax.models.torch.botorch_modular.surrogate.Surrogate", "ax.models.torch.botorch_modular.utils.choose_botorch_acqf_class", "ax.models.torch.botorch_modular.utils.choose_model_class", "ax.models.torch.botorch_modular.utils.construct_acquisition_and_optimizer_options", "ax.models.torch.botorch_modular.utils.construct_single_training_data", "ax.models.torch.botorch_modular.utils.construct_training_data_list", "ax.models.torch.botorch_modular.utils.use_model_list", "ax.models.torch.botorch_modular.utils.validate_data_format", "ax.models.torch.utils._to_inequality_constraints", "ax.models.torch_base.TorchModel", "ax.utils.common.base.Base", "ax.utils.common.constants.Keys", "ax.utils.common.docutils.copy_doc", "ax.utils.common.typeutils.checked_cast", "ax.utils.common.typeutils.not_none", "botorch.acquisition.acquisition.AcquisitionFunction", "botorch.utils.containers.TrainingData", "torch.Tensor"], "prompt": "\"\"\"\nDescription: This function generates candidates and expected acquisition values using Bayesian Optimization.\n\nArgs:\n    n (int): The number of candidates to generate.\n    bounds (List[Tuple[float, float]]): The bounds for each feature in the search space.\n    objective_weights (Tensor): The weights for each objective.\n    outcome_constraints (Optional[Tuple[Tensor, Tensor]]): The outcome constraints for the optimization.\n    linear_constraints (Optional[Tuple[Tensor, Tensor]]): The linear constraints for the optimization.\n    fixed_features (Optional[Dict[int, float]]): The fixed features for the optimization.\n    pending_observations (Optional[List[Tensor]]): The pending observations for the optimization.\n    model_gen_options (Optional[TConfig]): The model generation options for the optimization.\n    rounding_func (Optional[Callable[[Tensor], Tensor]]): The rounding function for the optimization.\n    target_fidelities (Optional[Dict[int, float]]): The target fidelities for the optimization.\n\nReturns:\n    Tuple[Tensor, Tensor, TGenMetadata, Optional[List[TCandidateMetadata]]]: The generated candidates, a tensor of ones, the expected acquisition value, and None.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["ax.models.torch.botorch_modular.utils.construct_acquisition_and_optimizer_options", "ax.models.torch.botorch.get_rounding_func", "ax.models.torch.utils._to_inequality_constraints", "ax.utils.common.typeutils.checked_cast", "torch.ones"], "project_create_time": "2019-02-09T15:23:44+00:00", "project_update_time": "2024-04-15T08:03:49+00:00", "file_create_time": "2020-08-28T23:43:44Z", "file_update_time": "2021-01-07T21:22:58Z", "function_update_time": "2020-08-28T23:43:44Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["torch.ones"], "test_function": [{"file_path": "/Ax-v0.1.20/Ax-0.1.20/ax/models/torch/tests/test_model.py", "class_name": "BoTorchModelTest", "function_name": "test_gen", "code": "\n    def test_gen(\n        self,\n        mock_choose_botorch_acqf_class,\n        mock_inequality_constraints,\n        mock_rounding,\n        mock_kg,\n        mock_construct_options,\n    ):\n        mock_kg.return_value.optimize.return_value = (\n            torch.tensor([1.0]),\n            torch.tensor([2.0]),\n        )\n        model = BoTorchModel(\n            surrogate=self.surrogate,\n            acquisition_class=KnowledgeGradient,\n            acquisition_options=self.acquisition_options,\n        )\n        model.surrogate.construct(\n            training_data=self.training_data, fidelity_features=self.fidelity_features\n        )\n        model._botorch_acqf_class = None\n        model.gen(\n            n=1,\n            bounds=self.bounds,\n            objective_weights=self.objective_weights,\n            outcome_constraints=self.outcome_constraints,\n            linear_constraints=self.linear_constraints,\n            fixed_features=self.fixed_features,\n            pending_observations=self.pending_observations,\n            model_gen_options=self.model_gen_options,\n            rounding_func=self.rounding_func,\n            target_fidelities=self.target_fidelities,\n        )\n        # Assert `construct_acquisition_and_optimizer_options` called with kwargs\n        mock_construct_options.assert_called_with(\n            acqf_options=self.acquisition_options,\n            model_gen_options=self.model_gen_options,\n        )\n        # Assert `choose_botorch_acqf_class` is called\n        mock_choose_botorch_acqf_class.assert_called_once()\n        self.assertEqual(model._botorch_acqf_class, qKnowledgeGradient)\n        # Assert `acquisition_class` called with kwargs\n        mock_kg.assert_called_with(\n            surrogate=self.surrogate,\n            botorch_acqf_class=model.botorch_acqf_class,\n            bounds=self.bounds,\n            objective_weights=self.objective_weights,\n            outcome_constraints=self.outcome_constraints,\n            linear_constraints=self.linear_constraints,\n            fixed_features=self.fixed_features,\n            pending_observations=self.pending_observations,\n            target_fidelities=self.target_fidelities,\n            options=self.acquisition_options,\n        )\n        # Assert `optimize` called with kwargs\n        mock_kg.return_value.optimize.assert_called_with(\n            bounds=ANY,\n            n=1,\n            inequality_constraints=[],\n            fixed_features=self.fixed_features,\n            rounding_func=\"func\",\n            optimizer_options=self.optimizer_options,\n        )"}, {"file_path": "/Ax-v0.1.20/Ax-0.1.20/ax/models/torch/tests/test_model.py", "class_name": "BoTorchModelTest", "function_name": "test_list_surrogate_choice", "code": "\n    def test_list_surrogate_choice(self, _, mock_extract_training_data):\n        model = BoTorchModel()\n        model.fit(\n            Xs=self.Xs,\n            Ys=self.Ys,\n            Yvars=self.Yvars,\n            bounds=self.bounds,\n            task_features=self.task_features,\n            feature_names=self.feature_names,\n            metric_names=self.metric_names_for_list_surrogate,\n            fidelity_features=self.fidelity_features,\n            target_fidelities=self.target_fidelities,\n            candidate_metadata=self.candidate_metadata,\n        )\n        # A list surrogate should be chosen, since Xs are not all the same.\n        self.assertIsInstance(model.surrogate.model, ModelListGP)\n        for submodel in model.surrogate.model.models:\n            # There are fidelity features and nonempty Yvars, so\n            # fixed noise MFGP should be chosen.\n            self.assertIsInstance(submodel, FixedNoiseMultiFidelityGP)\n        model.gen(\n            n=1,\n            bounds=self.bounds,\n            objective_weights=self.objective_weights,\n            outcome_constraints=self.outcome_constraints,\n            linear_constraints=self.linear_constraints,\n            fixed_features=self.fixed_features,\n            pending_observations=self.pending_observations,\n            model_gen_options=self.model_gen_options,\n            rounding_func=self.rounding_func,\n            target_fidelities=self.target_fidelities,\n        )\n        mock_extract_training_data.assert_called_once()\n        self.assertIsInstance(\n            mock_extract_training_data.call_args[1][\"surrogate\"], ListSurrogate\n        )"}]}, {"git_group": "sympy", "git_name": "sympy", "version": "sympy-1.12.1rc1", "language": "Python", "project_name": "sympy-sympy-1.12.1rc1.zip", "file_path": "/sympy-sympy-1.12.1rc1/sympy-sympy-1.12.1rc1/sympy/physics/vector/functions.py", "file_name": "functions.py", "focal_class": null, "focal_name": "express", "focal_parameter": ["expr", "frame"], "solution": "def express(expr, frame, frame2=None, variables=False):\n\n    _check_frame(frame)\n\n    if expr == 0:\n        return expr\n\n    if isinstance(expr, Vector):\n        # Given expr is a Vector\n        if variables:\n            # If variables attribute is True, substitute the coordinate\n            # variables in the Vector\n            frame_list = [x[-1] for x in expr.args]\n            subs_dict = {}\n            for f in frame_list:\n                subs_dict.update(f.variable_map(frame))\n            expr = expr.subs(subs_dict)\n        # Re-express in this frame\n        outvec = Vector([])\n        for i, v in enumerate(expr.args):\n            if v[1] != frame:\n                temp = frame.dcm(v[1]) * v[0]\n                if Vector.simp:\n                    temp = temp.applyfunc(lambda x:\n                                          trigsimp(x, method='fu'))\n                outvec += Vector([(temp, frame)])\n            else:\n                outvec += Vector([v])\n        return outvec\n\n    if isinstance(expr, Dyadic):\n        if frame2 is None:\n            frame2 = frame\n        _check_frame(frame2)\n        ol = Dyadic(0)\n        for i, v in enumerate(expr.args):\n            ol += express(v[0], frame, variables=variables) * \\\n                  (express(v[1], frame, variables=variables) |\n                   express(v[2], frame2, variables=variables))\n        return ol\n\n    else:\n        if variables:\n            # Given expr is a scalar field\n            frame_set = set()\n            expr = sympify(expr)\n            # Substitute all the coordinate variables\n            for x in expr.free_symbols:\n                if isinstance(x, CoordinateSym) and x.frame != frame:\n                    frame_set.add(x.frame)\n            subs_dict = {}\n            for f in frame_set:\n                subs_dict.update(f.variable_map(frame))\n            return expr.subs(subs_dict)\n        return expr", "function_signature": "def express(expr, frame, frame2=None, variables=False) :", "left_context": "from functools import reduce\n\nfrom sympy.core.backend import (sympify, diff, sin, cos, Matrix, symbols,\n                                Function, S, Symbol)\nfrom sympy.integrals.integrals import integrate\nfrom sympy.simplify.trigsimp import trigsimp\nfrom .vector import Vector, _check_vector\nfrom .frame import CoordinateSym, _check_frame\nfrom .dyadic import Dyadic\nfrom .printing import vprint, vsprint, vpprint, vlatex, init_vprinting\nfrom sympy.utilities.iterables import iterable\nfrom sympy.utilities.misc import translate\n\n__all__ = ['cross', 'dot', 'express', 'time_derivative', 'outer',\n           'kinematic_equations', 'get_motion_params', 'partial_velocity',\n           'dynamicsymbols', 'vprint', 'vsprint', 'vpprint', 'vlatex',\n           'init_vprinting']\n\n\ndef cross(vec1, vec2):\n    \"\"\"Cross product convenience wrapper for Vector.cross(): \\n\"\"\"\n    if not isinstance(vec1, (Vector, Dyadic)):\n        raise TypeError('Cross product is between two vectors')\n    return vec1 ^ vec2\n\n\ncross.__doc__ += Vector.cross.__doc__  # type: ignore\n\n\ndef dot(vec1, vec2):\n    \"\"\"Dot product convenience wrapper for Vector.dot(): \\n\"\"\"\n    if not isinstance(vec1, (Vector, Dyadic)):\n        raise TypeError('Dot product is between two vectors')\n    return vec1 & vec2\n\n\ndot.__doc__ += Vector.dot.__doc__  # type: ignore\n\n", "right_context": "\n\ndef time_derivative(expr, frame, order=1):\n    \"\"\"\n    Calculate the time derivative of a vector/scalar field function\n    or dyadic expression in given frame.\n\n    References\n    ==========\n\n    https://en.wikipedia.org/wiki/Rotating_reference_frame#Time_derivatives_in_the_two_frames\n\n    Parameters\n    ==========\n\n    expr : Vector/Dyadic/sympifyable\n        The expression whose time derivative is to be calculated\n\n    frame : ReferenceFrame\n        The reference frame to calculate the time derivative in\n\n    order : integer\n        The order of the derivative to be calculated\n\n    Examples\n    ========\n\n    >>> from sympy.physics.vector import ReferenceFrame, dynamicsymbols\n    >>> from sympy.physics.vector import init_vprinting\n    >>> init_vprinting(pretty_print=False)\n    >>> from sympy import Symbol\n    >>> q1 = Symbol('q1')\n    >>> u1 = dynamicsymbols('u1')\n    >>> N = ReferenceFrame('N')\n    >>> A = N.orientnew('A', 'Axis', [q1, N.x])\n    >>> v = u1 * N.x\n    >>> A.set_ang_vel(N, 10*A.x)\n    >>> from sympy.physics.vector import time_derivative\n    >>> time_derivative(v, N)\n    u1'*N.x\n    >>> time_derivative(u1*A[0], N)\n    N_x*u1'\n    >>> B = N.orientnew('B', 'Axis', [u1, N.z])\n    >>> from sympy.physics.vector import outer\n    >>> d = outer(N.x, N.x)\n    >>> time_derivative(d, B)\n    - u1'*(N.y|N.x) - u1'*(N.x|N.y)\n\n    \"\"\"\n\n    t = dynamicsymbols._t\n    _check_frame(frame)\n\n    if order == 0:\n        return expr\n    if order % 1 != 0 or order < 0:\n        raise ValueError(\"Unsupported value of order entered\")\n\n    if isinstance(expr, Vector):\n        outlist = []\n        for i, v in enumerate(expr.args):\n            if v[1] == frame:\n                outlist += [(express(v[0], frame, variables=True).diff(t),\n                             frame)]\n            else:\n                outlist += (time_derivative(Vector([v]), v[1]) +\n                            (v[1].ang_vel_in(frame) ^ Vector([v]))).args\n        outvec = Vector(outlist)\n        return time_derivative(outvec, frame, order - 1)\n\n    if isinstance(expr, Dyadic):\n        ol = Dyadic(0)\n        for i, v in enumerate(expr.args):\n            ol += (v[0].diff(t) * (v[1] | v[2]))\n            ol += (v[0] * (time_derivative(v[1], frame) | v[2]))\n            ol += (v[0] * (v[1] | time_derivative(v[2], frame)))\n        return time_derivative(ol, frame, order - 1)\n\n    else:\n        return diff(express(expr, frame, variables=True), t, order)\n\n\ndef outer(vec1, vec2):\n    \"\"\"Outer product convenience wrapper for Vector.outer():\\n\"\"\"\n    if not isinstance(vec1, Vector):\n        raise TypeError('Outer product is between two Vectors')\n    return vec1 | vec2\n\n\nouter.__doc__ += Vector.outer.__doc__  # type: ignore\n\n\ndef kinematic_equations(speeds, coords, rot_type, rot_order=''):\n    \"\"\"Gives equations relating the qdot's to u's for a rotation type.\n\n    Supply rotation type and order as in orient. Speeds are assumed to be\n    body-fixed; if we are defining the orientation of B in A using by rot_type,\n    the angular velocity of B in A is assumed to be in the form: speed[0]*B.x +\n    speed[1]*B.y + speed[2]*B.z\n\n    Parameters\n    ==========\n\n    speeds : list of length 3\n        The body fixed angular velocity measure numbers.\n    coords : list of length 3 or 4\n        The coordinates used to define the orientation of the two frames.\n    rot_type : str\n        The type of rotation used to create the equations. Body, Space, or\n        Quaternion only\n    rot_order : str or int\n        If applicable, the order of a series of rotations.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.vector import dynamicsymbols\n    >>> from sympy.physics.vector import kinematic_equations, vprint\n    >>> u1, u2, u3 = dynamicsymbols('u1 u2 u3')\n    >>> q1, q2, q3 = dynamicsymbols('q1 q2 q3')\n    >>> vprint(kinematic_equations([u1,u2,u3], [q1,q2,q3], 'body', '313'),\n    ...     order=None)\n    [-(u1*sin(q3) + u2*cos(q3))/sin(q2) + q1', -u1*cos(q3) + u2*sin(q3) + q2', (u1*sin(q3) + u2*cos(q3))*cos(q2)/sin(q2) - u3 + q3']\n\n    \"\"\"\n\n    # Code below is checking and sanitizing input\n    approved_orders = ('123', '231', '312', '132', '213', '321', '121', '131',\n                       '212', '232', '313', '323', '1', '2', '3', '')\n    # make sure XYZ => 123 and rot_type is in lower case\n    rot_order = translate(str(rot_order), 'XYZxyz', '123123')\n    rot_type = rot_type.lower()\n\n    if not isinstance(speeds, (list, tuple)):\n        raise TypeError('Need to supply speeds in a list')\n    if len(speeds) != 3:\n        raise TypeError('Need to supply 3 body-fixed speeds')\n    if not isinstance(coords, (list, tuple)):\n        raise TypeError('Need to supply coordinates in a list')\n    if rot_type in ['body', 'space']:\n        if rot_order not in approved_orders:\n            raise ValueError('Not an acceptable rotation order')\n        if len(coords) != 3:\n            raise ValueError('Need 3 coordinates for body or space')\n        # Actual hard-coded kinematic differential equations\n        w1, w2, w3 = speeds\n        if w1 == w2 == w3 == 0:\n            return [S.Zero]*3\n        q1, q2, q3 = coords\n        q1d, q2d, q3d = [diff(i, dynamicsymbols._t) for i in coords]\n        s1, s2, s3 = [sin(q1), sin(q2), sin(q3)]\n        c1, c2, c3 = [cos(q1), cos(q2), cos(q3)]\n        if rot_type == 'body':\n            if rot_order == '123':\n                return [q1d - (w1 * c3 - w2 * s3) / c2, q2d - w1 * s3 - w2 *\n                        c3, q3d - (-w1 * c3 + w2 * s3) * s2 / c2 - w3]\n            if rot_order == '231':\n                return [q1d - (w2 * c3 - w3 * s3) / c2, q2d - w2 * s3 - w3 *\n                        c3, q3d - w1 - (- w2 * c3 + w3 * s3) * s2 / c2]\n            if rot_order == '312':\n                return [q1d - (-w1 * s3 + w3 * c3) / c2, q2d - w1 * c3 - w3 *\n                        s3, q3d - (w1 * s3 - w3 * c3) * s2 / c2 - w2]\n            if rot_order == '132':\n                return [q1d - (w1 * c3 + w3 * s3) / c2, q2d + w1 * s3 - w3 *\n                        c3, q3d - (w1 * c3 + w3 * s3) * s2 / c2 - w2]\n            if rot_order == '213':\n                return [q1d - (w1 * s3 + w2 * c3) / c2, q2d - w1 * c3 + w2 *\n                        s3, q3d - (w1 * s3 + w2 * c3) * s2 / c2 - w3]\n            if rot_order == '321':\n                return [q1d - (w2 * s3 + w3 * c3) / c2, q2d - w2 * c3 + w3 *\n                        s3, q3d - w1 - (w2 * s3 + w3 * c3) * s2 / c2]\n            if rot_order == '121':\n                return [q1d - (w2 * s3 + w3 * c3) / s2, q2d - w2 * c3 + w3 *\n                        s3, q3d - w1 + (w2 * s3 + w3 * c3) * c2 / s2]\n            if rot_order == '131':\n                return [q1d - (-w2 * c3 + w3 * s3) / s2, q2d - w2 * s3 - w3 *\n                        c3, q3d - w1 - (w2 * c3 - w3 * s3) * c2 / s2]\n            if rot_order == '212':\n                return [q1d - (w1 * s3 - w3 * c3) / s2, q2d - w1 * c3 - w3 *\n                        s3, q3d - (-w1 * s3 + w3 * c3) * c2 / s2 - w2]\n            if rot_order == '232':\n                return [q1d - (w1 * c3 + w3 * s3) / s2, q2d + w1 * s3 - w3 *\n                        c3, q3d + (w1 * c3 + w3 * s3) * c2 / s2 - w2]\n            if rot_order == '313':\n                return [q1d - (w1 * s3 + w2 * c3) / s2, q2d - w1 * c3 + w2 *\n                        s3, q3d + (w1 * s3 + w2 * c3) * c2 / s2 - w3]\n            if rot_order == '323':\n                return [q1d - (-w1 * c3 + w2 * s3) / s2, q2d - w1 * s3 - w2 *\n                        c3, q3d - (w1 * c3 - w2 * s3) * c2 / s2 - w3]\n        if rot_type == 'space':\n            if rot_order == '123':\n                return [q1d - w1 - (w2 * s1 + w3 * c1) * s2 / c2, q2d - w2 *\n                        c1 + w3 * s1, q3d - (w2 * s1 + w3 * c1) / c2]\n            if rot_order == '231':\n                return [q1d - (w1 * c1 + w3 * s1) * s2 / c2 - w2, q2d + w1 *\n                        s1 - w3 * c1, q3d - (w1 * c1 + w3 * s1) / c2]\n            if rot_order == '312':\n                return [q1d - (w1 * s1 + w2 * c1) * s2 / c2 - w3, q2d - w1 *\n                        c1 + w2 * s1, q3d - (w1 * s1 + w2 * c1) / c2]\n            if rot_order == '132':\n                return [q1d - w1 - (-w2 * c1 + w3 * s1) * s2 / c2, q2d - w2 *\n                        s1 - w3 * c1, q3d - (w2 * c1 - w3 * s1) / c2]\n            if rot_order == '213':\n                return [q1d - (w1 * s1 - w3 * c1) * s2 / c2 - w2, q2d - w1 *\n                        c1 - w3 * s1, q3d - (-w1 * s1 + w3 * c1) / c2]\n            if rot_order == '321':\n                return [q1d - (-w1 * c1 + w2 * s1) * s2 / c2 - w3, q2d - w1 *\n                        s1 - w2 * c1, q3d - (w1 * c1 - w2 * s1) / c2]\n            if rot_order == '121':\n                return [q1d - w1 + (w2 * s1 + w3 * c1) * c2 / s2, q2d - w2 *\n                        c1 + w3 * s1, q3d - (w2 * s1 + w3 * c1) / s2]\n            if rot_order == '131':\n                return [q1d - w1 - (w2 * c1 - w3 * s1) * c2 / s2, q2d - w2 *\n                        s1 - w3 * c1, q3d - (-w2 * c1 + w3 * s1) / s2]\n            if rot_order == '212':\n                return [q1d - (-w1 * s1 + w3 * c1) * c2 / s2 - w2, q2d - w1 *\n                        c1 - w3 * s1, q3d - (w1 * s1 - w3 * c1) / s2]\n            if rot_order == '232':\n                return [q1d + (w1 * c1 + w3 * s1) * c2 / s2 - w2, q2d + w1 *\n                        s1 - w3 * c1, q3d - (w1 * c1 + w3 * s1) / s2]\n            if rot_order == '313':\n                return [q1d + (w1 * s1 + w2 * c1) * c2 / s2 - w3, q2d - w1 *\n                        c1 + w2 * s1, q3d - (w1 * s1 + w2 * c1) / s2]\n            if rot_order == '323':\n                return [q1d - (w1 * c1 - w2 * s1) * c2 / s2 - w3, q2d - w1 *\n                        s1 - w2 * c1, q3d - (-w1 * c1 + w2 * s1) / s2]\n    elif rot_type == 'quaternion':\n        if rot_order != '':\n            raise ValueError('Cannot have rotation order for quaternion')\n        if len(coords) != 4:\n            raise ValueError('Need 4 coordinates for quaternion')\n        # Actual hard-coded kinematic differential equations\n        e0, e1, e2, e3 = coords\n        w = Matrix(speeds + [0])\n        E = Matrix([[e0, -e3, e2, e1],\n                    [e3, e0, -e1, e2],\n                    [-e2, e1, e0, e3],\n                    [-e1, -e2, -e3, e0]])\n        edots = Matrix([diff(i, dynamicsymbols._t) for i in [e1, e2, e3, e0]])\n        return list(edots.T - 0.5 * w.T * E.T)\n    else:\n        raise ValueError('Not an approved rotation type for this function')\n\n\ndef get_motion_params(frame, **kwargs):\n    \"\"\"\n    Returns the three motion parameters - (acceleration, velocity, and\n    position) as vectorial functions of time in the given frame.\n\n    If a higher order differential function is provided, the lower order\n    functions are used as boundary conditions. For example, given the\n    acceleration, the velocity and position parameters are taken as\n    boundary conditions.\n\n    The values of time at which the boundary conditions are specified\n    are taken from timevalue1(for position boundary condition) and\n    timevalue2(for velocity boundary condition).\n\n    If any of the boundary conditions are not provided, they are taken\n    to be zero by default (zero vectors, in case of vectorial inputs). If\n    the boundary conditions are also functions of time, they are converted\n    to constants by substituting the time values in the dynamicsymbols._t\n    time Symbol.\n\n    This function can also be used for calculating rotational motion\n    parameters. Have a look at the Parameters and Examples for more clarity.\n\n    Parameters\n    ==========\n\n    frame : ReferenceFrame\n        The frame to express the motion parameters in\n\n    acceleration : Vector\n        Acceleration of the object/frame as a function of time\n\n    velocity : Vector\n        Velocity as function of time or as boundary condition\n        of velocity at time = timevalue1\n\n    position : Vector\n        Velocity as function of time or as boundary condition\n        of velocity at time = timevalue1\n\n    timevalue1 : sympyfiable\n        Value of time for position boundary condition\n\n    timevalue2 : sympyfiable\n        Value of time for velocity boundary condition\n\n    Examples\n    ========\n\n    >>> from sympy.physics.vector import ReferenceFrame, get_motion_params, dynamicsymbols\n    >>> from sympy.physics.vector import init_vprinting\n    >>> init_vprinting(pretty_print=False)\n    >>> from sympy import symbols\n    >>> R = ReferenceFrame('R')\n    >>> v1, v2, v3 = dynamicsymbols('v1 v2 v3')\n    >>> v = v1*R.x + v2*R.y + v3*R.z\n    >>> get_motion_params(R, position = v)\n    (v1''*R.x + v2''*R.y + v3''*R.z, v1'*R.x + v2'*R.y + v3'*R.z, v1*R.x + v2*R.y + v3*R.z)\n    >>> a, b, c = symbols('a b c')\n    >>> v = a*R.x + b*R.y + c*R.z\n    >>> get_motion_params(R, velocity = v)\n    (0, a*R.x + b*R.y + c*R.z, a*t*R.x + b*t*R.y + c*t*R.z)\n    >>> parameters = get_motion_params(R, acceleration = v)\n    >>> parameters[1]\n    a*t*R.x + b*t*R.y + c*t*R.z\n    >>> parameters[2]\n    a*t**2/2*R.x + b*t**2/2*R.y + c*t**2/2*R.z\n\n    \"\"\"\n\n    def _process_vector_differential(vectdiff, condition, variable, ordinate,\n                                     frame):\n        \"\"\"\n        Helper function for get_motion methods. Finds derivative of vectdiff\n        wrt variable, and its integral using the specified boundary condition\n        at value of variable = ordinate.\n        Returns a tuple of - (derivative, function and integral) wrt vectdiff\n\n        \"\"\"\n\n        # Make sure boundary condition is independent of 'variable'\n        if condition != 0:\n            condition = express(condition, frame, variables=True)\n        # Special case of vectdiff == 0\n        if vectdiff == Vector(0):\n            return (0, 0, condition)\n        # Express vectdiff completely in condition's frame to give vectdiff1\n        vectdiff1 = express(vectdiff, frame)\n        # Find derivative of vectdiff\n        vectdiff2 = time_derivative(vectdiff, frame)\n        # Integrate and use boundary condition\n        vectdiff0 = Vector(0)\n        lims = (variable, ordinate, variable)\n        for dim in frame:\n            function1 = vectdiff1.dot(dim)\n            abscissa = dim.dot(condition).subs({variable: ordinate})\n            # Indefinite integral of 'function1' wrt 'variable', using\n            # the given initial condition (ordinate, abscissa).\n            vectdiff0 += (integrate(function1, lims) + abscissa) * dim\n        # Return tuple\n        return (vectdiff2, vectdiff, vectdiff0)\n\n    _check_frame(frame)\n    # Decide mode of operation based on user's input\n    if 'acceleration' in kwargs:\n        mode = 2\n    elif 'velocity' in kwargs:\n        mode = 1\n    else:\n        mode = 0\n    # All the possible parameters in kwargs\n    # Not all are required for every case\n    # If not specified, set to default values(may or may not be used in\n    # calculations)\n    conditions = ['acceleration', 'velocity', 'position',\n                  'timevalue', 'timevalue1', 'timevalue2']\n    for i, x in enumerate(conditions):\n        if x not in kwargs:\n            if i < 3:\n                kwargs[x] = Vector(0)\n            else:\n                kwargs[x] = S.Zero\n        elif i < 3:\n            _check_vector(kwargs[x])\n        else:\n            kwargs[x] = sympify(kwargs[x])\n    if mode == 2:\n        vel = _process_vector_differential(kwargs['acceleration'],\n                                           kwargs['velocity'],\n                                           dynamicsymbols._t,\n                                           kwargs['timevalue2'], frame)[2]\n        pos = _process_vector_differential(vel, kwargs['position'],\n                                           dynamicsymbols._t,\n                                           kwargs['timevalue1'], frame)[2]\n        return (kwargs['acceleration'], vel, pos)\n    elif mode == 1:\n        return _process_vector_differential(kwargs['velocity'],\n                                            kwargs['position'],\n                                            dynamicsymbols._t,\n                                            kwargs['timevalue1'], frame)\n    else:\n        vel = time_derivative(kwargs['position'], frame)\n        acc = time_derivative(vel, frame)\n        return (acc, vel, kwargs['position'])\n\n\ndef partial_velocity(vel_vecs, gen_speeds, frame):\n    \"\"\"Returns a list of partial velocities with respect to the provided\n    generalized speeds in the given reference frame for each of the supplied\n    velocity vectors.\n\n    The output is a list of lists. The outer list has a number of elements\n    equal to the number of supplied velocity vectors. The inner lists are, for\n    each velocity vector, the partial derivatives of that velocity vector with\n    respect to the generalized speeds supplied.\n\n    Parameters\n    ==========\n\n    vel_vecs : iterable\n        An iterable of velocity vectors (angular or linear).\n    gen_speeds : iterable\n        An iterable of generalized speeds.\n    frame : ReferenceFrame\n        The reference frame that the partial derivatives are going to be taken\n        in.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.vector import Point, ReferenceFrame\n    >>> from sympy.physics.vector import dynamicsymbols\n    >>> from sympy.physics.vector import partial_velocity\n    >>> u = dynamicsymbols('u')\n    >>> N = ReferenceFrame('N')\n    >>> P = Point('P')\n    >>> P.set_vel(N, u * N.x)\n    >>> vel_vecs = [P.vel(N)]\n    >>> gen_speeds = [u]\n    >>> partial_velocity(vel_vecs, gen_speeds, N)\n    [[N.x]]\n\n    \"\"\"\n\n    if not iterable(vel_vecs):\n        raise TypeError('Velocity vectors must be contained in an iterable.')\n\n    if not iterable(gen_speeds):\n        raise TypeError('Generalized speeds must be contained in an iterable')\n\n    vec_partials = []\n    for vec in vel_vecs:\n        partials = []\n        for speed in gen_speeds:\n            partials.append(vec.diff(speed, frame, var_in_dcm=False))\n        vec_partials.append(partials)\n\n    return vec_partials\n\n\ndef dynamicsymbols(names, level=0, **assumptions):\n    \"\"\"Uses symbols and Function for functions of time.\n\n    Creates a SymPy UndefinedFunction, which is then initialized as a function\n    of a variable, the default being Symbol('t').\n\n    Parameters\n    ==========\n\n    names : str\n        Names of the dynamic symbols you want to create; works the same way as\n        inputs to symbols\n    level : int\n        Level of differentiation of the returned function; d/dt once of t,\n        twice of t, etc.\n    assumptions :\n        - real(bool) : This is used to set the dynamicsymbol as real,\n                    by default is False.\n        - positive(bool) : This is used to set the dynamicsymbol as positive,\n                    by default is False.\n        - commutative(bool) : This is used to set the commutative property of\n                    a dynamicsymbol, by default is True.\n        - integer(bool) : This is used to set the dynamicsymbol as integer,\n                    by default is False.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.vector import dynamicsymbols\n    >>> from sympy import diff, Symbol\n    >>> q1 = dynamicsymbols('q1')\n    >>> q1\n    q1(t)\n    >>> q2 = dynamicsymbols('q2', real=True)\n    >>> q2.is_real\n    True\n    >>> q3 = dynamicsymbols('q3', positive=True)\n    >>> q3.is_positive\n    True\n    >>> q4, q5 = dynamicsymbols('q4,q5', commutative=False)\n    >>> bool(q4*q5 != q5*q4)\n    True\n    >>> q6 = dynamicsymbols('q6', integer=True)\n    >>> q6.is_integer\n    True\n    >>> diff(q1, Symbol('t'))\n    Derivative(q1(t), t)\n\n    \"\"\"\n    esses = symbols(names, cls=Function, **assumptions)\n    t = dynamicsymbols._t\n    if iterable(esses):\n        esses = [reduce(diff, [t] * level, e(t)) for e in esses]\n        return esses\n    else:\n        return reduce(diff, [t] * level, esses(t))\n\n\ndynamicsymbols._t = Symbol('t')  # type: ignore\ndynamicsymbols._str = '\\''  # type: ignore\n", "import_text": ["functools.reduce", "sympy.core.backend.sympify", "sympy.core.backend.diff", "sympy.core.backend.sin", "sympy.core.backend.cos", "sympy.core.backend.Matrix", "sympy.core.backend.symbols", "sympy.core.backend.Function", "sympy.core.backend.S", "sympy.core.backend.Symbol", "sympy.integrals.integrals.integrate", "sympy.simplify.trigsimp.trigsimp", "sympy.utilities.iterables.iterable", "sympy.utilities.misc.translate"], "prompt": "\"\"\"\nDescription: This function is used to express an expression in a given frame.\n\nArgs:\n    expr (type): The expression to be expressed.\n    frame (type): The frame in which the expression is to be expressed.\n    frame2 (type, optional): The second frame. Defaults to None.\n    variables (type, optional): A boolean indicating whether to substitute variables. Defaults to False.\n\nReturns:\n    type: The expressed expression.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Global function for 'express' functionality.\n\n    Re-expresses a Vector, scalar(sympyfiable) or Dyadic in given frame.\n\n    Refer to the local methods of Vector and Dyadic for details.\n    If 'variables' is True, then the coordinate variables (CoordinateSym\n    instances) of other frames present in the vector/scalar field or\n    dyadic expression are also substituted in terms of the base scalars of\n    this frame.\n\n    Parameters\n    ==========\n\n    expr : Vector/Dyadic/scalar(sympyfiable)\n        The expression to re-express in ReferenceFrame 'frame'\n\n    frame: ReferenceFrame\n        The reference frame to express expr in\n\n    frame2 : ReferenceFrame\n        The other frame required for re-expression(only for Dyadic expr)\n\n    variables : boolean\n        Specifies whether to substitute the coordinate variables present\n        in expr, in terms of those of frame\n\n    Examples\n    ========\n\n    >>> from sympy.physics.vector import ReferenceFrame, outer, dynamicsymbols\n    >>> from sympy.physics.vector import init_vprinting\n    >>> init_vprinting(pretty_print=False)\n    >>> N = ReferenceFrame('N')\n    >>> q = dynamicsymbols('q')\n    >>> B = N.orientnew('B', 'Axis', [q, N.z])\n    >>> d = outer(N.x, N.x)\n    >>> from sympy.physics.vector import express\n    >>> express(d, B, N)\n    cos(q)*(B.x|N.x) - sin(q)*(B.y|N.x)\n    >>> express(B.x, N)\n    cos(q)*N.x + sin(q)*N.y\n    >>> express(N[0], B, variables=True)\n    B_x*cos(q) - B_y*sin(q)\n\n    \"\"\"", "function_dependencies": ["sympy.simplify.trigsimp.trigsimp", "sympy.core.backend.sympify", "sympy.core.backend.sympify.subs"], "project_create_time": "2010-04-30T20:37:14+00:00", "project_update_time": "2024-04-17T16:37:42+00:00", "file_create_time": "2014-01-08T07:41:04Z", "file_update_time": "2022-02-27T13:59:05Z", "function_update_time": "2014-01-08T07:41:04Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["sympy.simplify.trigsimp.trigsimp"], "test_function": [{"file_path": "/sympy-sympy-1.12.1rc1/sympy-sympy-1.12.1rc1/sympy/physics/vector/tests/test_functions.py", "class_name": null, "function_name": "test_express", "code": "\ndef test_express():\n    assert express(Vector(0), N) == Vector(0)\n    assert express(S.Zero, N) is S.Zero\n    assert express(A.x, C) == cos(q3)*C.x + sin(q3)*C.z\n    assert express(A.y, C) == sin(q2)*sin(q3)*C.x + cos(q2)*C.y - \\\n        sin(q2)*cos(q3)*C.z\n    assert express(A.z, C) == -sin(q3)*cos(q2)*C.x + sin(q2)*C.y + \\\n        cos(q2)*cos(q3)*C.z\n    assert express(A.x, N) == cos(q1)*N.x + sin(q1)*N.y\n    assert express(A.y, N) == -sin(q1)*N.x + cos(q1)*N.y\n    assert express(A.z, N) == N.z\n    assert express(A.x, A) == A.x\n    assert express(A.y, A) == A.y\n    assert express(A.z, A) == A.z\n    assert express(A.x, B) == B.x\n    assert express(A.y, B) == cos(q2)*B.y - sin(q2)*B.z\n    assert express(A.z, B) == sin(q2)*B.y + cos(q2)*B.z\n    assert express(A.x, C) == cos(q3)*C.x + sin(q3)*C.z\n    assert express(A.y, C) == sin(q2)*sin(q3)*C.x + cos(q2)*C.y - \\\n        sin(q2)*cos(q3)*C.z\n    assert express(A.z, C) == -sin(q3)*cos(q2)*C.x + sin(q2)*C.y + \\\n        cos(q2)*cos(q3)*C.z\n    # Check to make sure UnitVectors get converted properly\n    assert express(N.x, N) == N.x\n    assert express(N.y, N) == N.y\n    assert express(N.z, N) == N.z\n    assert express(N.x, A) == (cos(q1)*A.x - sin(q1)*A.y)\n    assert express(N.y, A) == (sin(q1)*A.x + cos(q1)*A.y)\n    assert express(N.z, A) == A.z\n    assert express(N.x, B) == (cos(q1)*B.x - sin(q1)*cos(q2)*B.y +\n            sin(q1)*sin(q2)*B.z)\n    assert express(N.y, B) == (sin(q1)*B.x + cos(q1)*cos(q2)*B.y -\n            sin(q2)*cos(q1)*B.z)\n    assert express(N.z, B) == (sin(q2)*B.y + cos(q2)*B.z)\n    assert express(N.x, C) == (\n        (cos(q1)*cos(q3) - sin(q1)*sin(q2)*sin(q3))*C.x -\n        sin(q1)*cos(q2)*C.y +\n        (sin(q3)*cos(q1) + sin(q1)*sin(q2)*cos(q3))*C.z)\n    assert express(N.y, C) == (\n        (sin(q1)*cos(q3) + sin(q2)*sin(q3)*cos(q1))*C.x +\n        cos(q1)*cos(q2)*C.y +\n        (sin(q1)*sin(q3) - sin(q2)*cos(q1)*cos(q3))*C.z)\n    assert express(N.z, C) == (-sin(q3)*cos(q2)*C.x + sin(q2)*C.y +\n            cos(q2)*cos(q3)*C.z)\n\n    assert express(A.x, N) == (cos(q1)*N.x + sin(q1)*N.y)\n    assert express(A.y, N) == (-sin(q1)*N.x + cos(q1)*N.y)\n    assert express(A.z, N) == N.z\n    assert express(A.x, A) == A.x\n    assert express(A.y, A) == A.y\n    assert express(A.z, A) == A.z\n    assert express(A.x, B) == B.x\n    assert express(A.y, B) == (cos(q2)*B.y - sin(q2)*B.z)\n    assert express(A.z, B) == (sin(q2)*B.y + cos(q2)*B.z)\n    assert express(A.x, C) == (cos(q3)*C.x + sin(q3)*C.z)\n    assert express(A.y, C) == (sin(q2)*sin(q3)*C.x + cos(q2)*C.y -\n            sin(q2)*cos(q3)*C.z)\n    assert express(A.z, C) == (-sin(q3)*cos(q2)*C.x + sin(q2)*C.y +\n            cos(q2)*cos(q3)*C.z)\n\n    assert express(B.x, N) == (cos(q1)*N.x + sin(q1)*N.y)\n    assert express(B.y, N) == (-sin(q1)*cos(q2)*N.x +\n            cos(q1)*cos(q2)*N.y + sin(q2)*N.z)\n    assert express(B.z, N) == (sin(q1)*sin(q2)*N.x -\n            sin(q2)*cos(q1)*N.y + cos(q2)*N.z)\n    assert express(B.x, A) == A.x\n    assert express(B.y, A) == (cos(q2)*A.y + sin(q2)*A.z)\n    assert express(B.z, A) == (-sin(q2)*A.y + cos(q2)*A.z)\n    assert express(B.x, B) == B.x\n    assert express(B.y, B) == B.y\n    assert express(B.z, B) == B.z\n    assert express(B.x, C) == (cos(q3)*C.x + sin(q3)*C.z)\n    assert express(B.y, C) == C.y\n    assert express(B.z, C) == (-sin(q3)*C.x + cos(q3)*C.z)\n\n    assert express(C.x, N) == (\n        (cos(q1)*cos(q3) - sin(q1)*sin(q2)*sin(q3))*N.x +\n        (sin(q1)*cos(q3) + sin(q2)*sin(q3)*cos(q1))*N.y -\n        sin(q3)*cos(q2)*N.z)\n    assert express(C.y, N) == (\n        -sin(q1)*cos(q2)*N.x + cos(q1)*cos(q2)*N.y + sin(q2)*N.z)\n    assert express(C.z, N) == (\n        (sin(q3)*cos(q1) + sin(q1)*sin(q2)*cos(q3))*N.x +\n        (sin(q1)*sin(q3) - sin(q2)*cos(q1)*cos(q3))*N.y +\n        cos(q2)*cos(q3)*N.z)\n    assert express(C.x, A) == (cos(q3)*A.x + sin(q2)*sin(q3)*A.y -\n            sin(q3)*cos(q2)*A.z)\n    assert express(C.y, A) == (cos(q2)*A.y + sin(q2)*A.z)\n    assert express(C.z, A) == (sin(q3)*A.x - sin(q2)*cos(q3)*A.y +\n            cos(q2)*cos(q3)*A.z)\n    assert express(C.x, B) == (cos(q3)*B.x - sin(q3)*B.z)\n    assert express(C.y, B) == B.y\n    assert express(C.z, B) == (sin(q3)*B.x + cos(q3)*B.z)\n    assert express(C.x, C) == C.x\n    assert express(C.y, C) == C.y\n    assert express(C.z, C) == C.z == (C.z)\n\n    #  Check to make sure Vectors get converted back to UnitVectors\n    assert N.x == express((cos(q1)*A.x - sin(q1)*A.y), N)\n    assert N.y == express((sin(q1)*A.x + cos(q1)*A.y), N)\n    assert N.x == express((cos(q1)*B.x - sin(q1)*cos(q2)*B.y +\n            sin(q1)*sin(q2)*B.z), N)\n    assert N.y == express((sin(q1)*B.x + cos(q1)*cos(q2)*B.y -\n        sin(q2)*cos(q1)*B.z), N)\n    assert N.z == express((sin(q2)*B.y + cos(q2)*B.z), N)\n\n    \"\"\"\n    These don't really test our code, they instead test the auto simplification\n    (or lack thereof) of SymPy.\n    assert N.x == express((\n            (cos(q1)*cos(q3)-sin(q1)*sin(q2)*sin(q3))*C.x -\n            sin(q1)*cos(q2)*C.y +\n            (sin(q3)*cos(q1)+sin(q1)*sin(q2)*cos(q3))*C.z), N)\n    assert N.y == express((\n            (sin(q1)*cos(q3) + sin(q2)*sin(q3)*cos(q1))*C.x +\n            cos(q1)*cos(q2)*C.y +\n            (sin(q1)*sin(q3) - sin(q2)*cos(q1)*cos(q3))*C.z), N)\n    assert N.z == express((-sin(q3)*cos(q2)*C.x + sin(q2)*C.y +\n            cos(q2)*cos(q3)*C.z), N)\n    \"\"\"\n\n    assert A.x == express((cos(q1)*N.x + sin(q1)*N.y), A)\n    assert A.y == express((-sin(q1)*N.x + cos(q1)*N.y), A)\n\n    assert A.y == express((cos(q2)*B.y - sin(q2)*B.z), A)\n    assert A.z == express((sin(q2)*B.y + cos(q2)*B.z), A)\n\n    assert A.x == express((cos(q3)*C.x + sin(q3)*C.z), A)\n\n    # Tripsimp messes up here too.\n    #print express((sin(q2)*sin(q3)*C.x + cos(q2)*C.y -\n    #        sin(q2)*cos(q3)*C.z), A)\n    assert A.y == express((sin(q2)*sin(q3)*C.x + cos(q2)*C.y -\n            sin(q2)*cos(q3)*C.z), A)\n\n    assert A.z == express((-sin(q3)*cos(q2)*C.x + sin(q2)*C.y +\n            cos(q2)*cos(q3)*C.z), A)\n    assert B.x == express((cos(q1)*N.x + sin(q1)*N.y), B)\n    assert B.y == express((-sin(q1)*cos(q2)*N.x +\n            cos(q1)*cos(q2)*N.y + sin(q2)*N.z), B)\n\n    assert B.z == express((sin(q1)*sin(q2)*N.x -\n            sin(q2)*cos(q1)*N.y + cos(q2)*N.z), B)\n\n    assert B.y == express((cos(q2)*A.y + sin(q2)*A.z), B)\n    assert B.z == express((-sin(q2)*A.y + cos(q2)*A.z), B)\n    assert B.x == express((cos(q3)*C.x + sin(q3)*C.z), B)\n    assert B.z == express((-sin(q3)*C.x + cos(q3)*C.z), B)\n\n    \"\"\"\n    assert C.x == express((\n            (cos(q1)*cos(q3)-sin(q1)*sin(q2)*sin(q3))*N.x +\n            (sin(q1)*cos(q3)+sin(q2)*sin(q3)*cos(q1))*N.y -\n                sin(q3)*cos(q2)*N.z), C)\n    assert C.y == express((\n            -sin(q1)*cos(q2)*N.x + cos(q1)*cos(q2)*N.y + sin(q2)*N.z), C)\n    assert C.z == express((\n            (sin(q3)*cos(q1)+sin(q1)*sin(q2)*cos(q3))*N.x +\n            (sin(q1)*sin(q3)-sin(q2)*cos(q1)*cos(q3))*N.y +\n            cos(q2)*cos(q3)*N.z), C)\n    \"\"\"\n    assert C.x == express((cos(q3)*A.x + sin(q2)*sin(q3)*A.y -\n            sin(q3)*cos(q2)*A.z), C)\n    assert C.y == express((cos(q2)*A.y + sin(q2)*A.z), C)\n    assert C.z == express((sin(q3)*A.x - sin(q2)*cos(q3)*A.y +\n            cos(q2)*cos(q3)*A.z), C)\n    assert C.x == express((cos(q3)*B.x - sin(q3)*B.z), C)\n    assert C.z == express((sin(q3)*B.x + cos(q3)*B.z), C)"}]}, {"git_group": "openeemeter", "git_name": "eemeter", "version": "v4.0.1", "language": "Python", "project_name": "eemeter-v4.0.1.zip", "file_path": "/eemeter-v4.0.1/eemeter-4.0.1/eemeter/eemeter/models/hourly/segmentation.py", "file_name": "segmentation.py", "focal_class": null, "focal_name": "iterate_segmented_dataset", "focal_parameter": ["data"], "solution": "def iterate_segmented_dataset(\n    data,\n    segmentation=None,\n    feature_processor=None,\n    feature_processor_kwargs=None,\n    feature_processor_segment_name_mapping=None,\n):\n    if feature_processor is None:\n        feature_processor = filter_zero_weights_feature_processor\n\n    if feature_processor_kwargs is None:\n        feature_processor_kwargs = {}\n\n    if feature_processor_segment_name_mapping is None:\n        feature_processor_segment_name_mapping = {}\n\n    def _apply_feature_processor(segment_name, segment_data):\n        feature_processor_segment_name = feature_processor_segment_name_mapping.get(\n            segment_name, segment_name\n        )\n\n        if feature_processor is not None:\n            segment_data = feature_processor(\n                feature_processor_segment_name, segment_data, **feature_processor_kwargs\n            )\n        return segment_data\n\n    def _add_weights(data, weights):\n        return pd.merge(data, weights, left_index=True, right_index=True)\n\n    if segmentation is None:\n        # spoof segment name and weights column\n        segment_name = None\n        weights = pd.DataFrame({\"weight\": 1}, index=data.index)\n        segment_data = _add_weights(data, weights)\n\n        segment_data = _apply_feature_processor(segment_name, segment_data)\n        yield segment_name, segment_data\n    else:\n        for segment_name, segment_weights in segmentation.items():\n            weights = segment_weights.to_frame(\"weight\")\n            segment_data = _add_weights(data, weights)\n            segment_data = _apply_feature_processor(segment_name, segment_data)\n            yield segment_name, segment_data", "function_signature": "def iterate_segmented_dataset(\n    data,\n    segmentation=None,\n    feature_processor=None,\n    feature_processor_kwargs=None,\n    feature_processor_segment_name_mapping=None,\n) :", "left_context": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\n\n   Copyright 2014-2024 OpenEEmeter contributors\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n\"\"\"\nfrom collections import namedtuple\n\nimport pandas as pd\nfrom patsy import dmatrix\n\n__all__ = (\n    \"iterate_segmented_dataset\",\n    \"segment_time_series\",\n    \"CalTRACKSegmentModel\",\n    \"SegmentedModel\",\n    \"HourlyModelPrediction\",\n)\n\n\nHourlyModelPrediction = namedtuple(\"HourlyModelPrediction\", [\"result\"])\n\n\nclass CalTRACKSegmentModel(object):\n    \"\"\"An object that captures the model fit for one segment.\n\n    Attributes\n    ----------\n    segment_name : :any:`str`\n        The name of the segment of data this model was fit to.\n    model : :any:`object`\n        The fitted model object.\n    formula : :any:`str`\n        The formula of the model regression.\n    model_param : :any:`dict`\n        A dictionary of parameters\n    warnings : :any:`list`\n        A list of eemeter warnings.\n    \"\"\"\n\n    def __init__(self, segment_name, model, formula, model_params, warnings=None):\n        self.segment_name = segment_name\n        self.model = model\n        self.formula = formula\n        self.model_params = model_params\n\n        if warnings is None:\n            warnings = []\n        self.warnings = warnings\n\n    def predict(self, data):\n        \"\"\"A function which takes input data and predicts for this segment model.\"\"\"\n        if self.formula is None:\n            var_str = \"\"\n        else:\n            var_str = self.formula.split(\"~\", 1)[1]\n\n        design_matrix_granular = dmatrix(var_str, data, return_type=\"dataframe\")\n        parameters = pd.Series(self.model_params)\n\n        # Step 1, slice\n        col_type = \"C(hour_of_week)\"\n        hour_of_week_cols = [\n            c\n            for c in design_matrix_granular.columns\n            if col_type in c and c in parameters.keys()\n        ]\n\n        # Step 2, cut out all 0s\n        design_matrix_granular = design_matrix_granular[\n            (design_matrix_granular[hour_of_week_cols] != 0).any(axis=1)\n        ]\n\n        cols_to_predict = list(\n            set(parameters.keys()).intersection(set(design_matrix_granular.keys()))\n        )\n        design_matrix_granular = design_matrix_granular[cols_to_predict]\n        parameters = parameters[cols_to_predict]\n\n        # Step 3, predict\n        prediction = design_matrix_granular.dot(parameters).rename(\"predicted_usage\")\n\n        # Step 4, put nans back in\n        prediction = prediction.reindex(data.index)\n\n        return prediction\n\n    def json(self):\n        \"\"\"Return a JSON-serializable representation of this result.\n\n        The output of this function can be converted to a serialized string\n        with :any:`json.dumps`.\n        \"\"\"\n\n        data = {\n            \"segment_name\": self.segment_name,\n            \"formula\": self.formula,\n            \"warnings\": [w.json() for w in self.warnings],\n            \"model_params\": self.model_params,\n        }\n        return data\n\n    @classmethod\n    def from_json(cls, data):\n        \"\"\"Loads a JSON-serializable representation into the model state.\n\n        The input of this function is a dict which can be the result\n        of :any:`json.loads`.\n        \"\"\"\n\n        c = cls(\n            data.get(\"segment_name\"),\n            None,\n            data.get(\"formula\"),\n            data.get(\"model_params\"),\n            warnings=data.get(\"warnings\"),\n        )\n\n        return c\n\n\nclass SegmentedModel(object):\n    \"\"\"Represent a model which has been broken into multiple model segments (for\n    CalTRACK Hourly, these are month-by-month segments, each of which is associated\n    with a different model.\n\n    Parameters\n    ----------\n    segment_models : :any:`dict` of :any:`eemeter.CalTRACKSegmentModel`\n        Dictionary of segment models, keyed by segment name.\n    prediction_segment_type : :any:`str`\n        Any segment_type that can be passed to :any:`eemeter.segment_time_series`,\n        currently \"single\", \"one_month\", \"three_month\", or \"three_month_weighted\".\n    prediction_segment_name_mapping : :any:`dict` of :any:`str`\n        A dictionary mapping the segment names for the segment type used for predicting to the\n        segment names for the segment type used for fitting,\n        e.g., `{\"<predict_segment_name>\": \"<fit_segment_name>\"}`.\n    prediction_feature_processor : :any:`function`\n        A function that transforms raw inputs (temperatures) into features for each\n        segment.\n    prediction_feature_processor_kwargs : :any:`dict`\n        A dict of keyword arguments to be passed as `**kwargs` to the\n        `prediction_feature_processor` function.\n    \"\"\"\n\n    def __init__(\n        self,\n        segment_models,\n        prediction_segment_type,\n        prediction_segment_name_mapping=None,\n        prediction_feature_processor=None,\n        prediction_feature_processor_kwargs=None,\n    ):\n        self.segment_models = segment_models\n\n        fitted_model_lookup = {\n            segment_model.segment_name: segment_model\n            for segment_model in segment_models\n        }\n        if prediction_segment_name_mapping is None:\n            self.model_lookup = fitted_model_lookup\n        else:\n            self.model_lookup = {\n                pred_name: fitted_model_lookup.get(fit_name)\n                for pred_name, fit_name in prediction_segment_name_mapping.items()\n            }\n        self.prediction_segment_type = prediction_segment_type\n        self.prediction_segment_name_mapping = prediction_segment_name_mapping\n        self.prediction_feature_processor = prediction_feature_processor\n        self.prediction_feature_processor_kwargs = prediction_feature_processor_kwargs\n\n    def predict(\n        self, prediction_index, temperature, **kwargs\n    ):  # ignore extra args with kwargs\n        \"\"\"Predict over a prediction index by combining results from all models.\n\n        Parameters\n        ----------\n        prediction_index : :any:`pandas.DatetimeIndex`\n            The index over which to predict.\n        temperature : :any:`pandas.Series`\n            Hourly temperatures.\n        **kwargs\n            Extra argmuents will be ignored\n        \"\"\"\n        prediction_segmentation = segment_time_series(\n            temperature.index,\n            self.prediction_segment_type,\n            drop_zero_weight_segments=True,\n        )\n\n        iterator = iterate_segmented_dataset(\n            temperature.to_frame(\"temperature_mean\"),\n            segmentation=prediction_segmentation,\n            feature_processor=self.prediction_feature_processor,\n            feature_processor_kwargs=self.prediction_feature_processor_kwargs,\n            feature_processor_segment_name_mapping=self.prediction_segment_name_mapping,\n        )\n\n        predictions = {}\n        for segment_name, segmented_data in iterator:\n            segment_model = self.model_lookup.get(segment_name)\n            if segment_model is None:\n                continue\n            prediction = segment_model.predict(segmented_data) * segmented_data.weight\n            # NaN the zero weights and reindex\n            prediction = prediction[segmented_data.weight > 0].reindex(prediction_index)\n            predictions[segment_name] = prediction\n\n        predictions = pd.DataFrame(predictions)\n        result = pd.DataFrame({\"predicted_usage\": predictions.sum(axis=1, min_count=1)})\n        return HourlyModelPrediction(result=result)\n\n    def json(self):\n        \"\"\"Return a JSON-serializable representation of this result.\n\n        The output of this function can be converted to a serialized string\n        with :any:`json.dumps`.\n        \"\"\"\n\n        def _json_or_none(obj):\n            return None if obj is None else obj.json()\n\n        data = {\n            \"segment_models\": [_json_or_none(m) for m in self.segment_models],\n            \"model_lookup\": {\n                key: _json_or_none(val) for key, val in self.model_lookup.items()\n            },\n            \"prediction_segment_type\": self.prediction_segment_type,\n            \"prediction_segment_name_mapping\": self.prediction_segment_name_mapping,\n            \"prediction_feature_processor\": self.prediction_feature_processor.__name__,\n        }\n        return data\n\n\ndef filter_zero_weights_feature_processor(segment_name, segment_data):\n    \"\"\"A default segment processor to use if none is provided.\"\"\"\n    return segment_data[segment_data.weight > 0]\n\n", "right_context": "\n\ndef _get_calendar_year_coverage_warning(index):\n    pass\n\n\ndef _get_hourly_coverage_warning(index, min_fraction_daily_coverage=0.9):\n    pass\n\n\ndef _segment_weights_single(index):\n    return pd.DataFrame({\"all\": 1.0}, index=index)\n\n\ndef _segment_weights_one_month(index):\n    return pd.DataFrame(\n        {\n            month_name: (index.month == month_number).astype(float)\n            for month_name, month_number in [\n                (\"jan\", 1),\n                (\"feb\", 2),\n                (\"mar\", 3),\n                (\"apr\", 4),\n                (\"may\", 5),\n                (\"jun\", 6),\n                (\"jul\", 7),\n                (\"aug\", 8),\n                (\"sep\", 9),\n                (\"oct\", 10),\n                (\"nov\", 11),\n                (\"dec\", 12),\n            ]\n        },\n        index=index,\n        columns=[\n            \"jan\",\n            \"feb\",\n            \"mar\",\n            \"apr\",\n            \"may\",\n            \"jun\",\n            \"jul\",\n            \"aug\",\n            \"sep\",\n            \"oct\",\n            \"nov\",\n            \"dec\",\n        ],  # guarantee order\n    )\n\n\ndef _segment_weights_three_month(index):\n    return pd.DataFrame(\n        {\n            month_names: (index.month.map(lambda i: i in month_numbers)).astype(float)\n            for month_names, month_numbers in [\n                (\"dec-jan-feb\", (12, 1, 2)),\n                (\"jan-feb-mar\", (1, 2, 3)),\n                (\"feb-mar-apr\", (2, 3, 4)),\n                (\"mar-apr-may\", (3, 4, 5)),\n                (\"apr-may-jun\", (4, 5, 6)),\n                (\"may-jun-jul\", (5, 6, 7)),\n                (\"jun-jul-aug\", (6, 7, 8)),\n                (\"jul-aug-sep\", (7, 8, 9)),\n                (\"aug-sep-oct\", (8, 9, 10)),\n                (\"sep-oct-nov\", (9, 10, 11)),\n                (\"oct-nov-dec\", (10, 11, 12)),\n                (\"nov-dec-jan\", (11, 12, 1)),\n            ]\n        },\n        index=index,\n        columns=[\n            \"dec-jan-feb\",\n            \"jan-feb-mar\",\n            \"feb-mar-apr\",\n            \"mar-apr-may\",\n            \"apr-may-jun\",\n            \"may-jun-jul\",\n            \"jun-jul-aug\",\n            \"jul-aug-sep\",\n            \"aug-sep-oct\",\n            \"sep-oct-nov\",\n            \"oct-nov-dec\",\n            \"nov-dec-jan\",\n        ],  # guarantee order\n    )\n\n\ndef _segment_weights_three_month_weighted(index):\n    return pd.DataFrame(\n        {\n            month_names: index.month.map(\n                lambda i: month_weights.get(str(i), 0.0)\n            ).astype(float)\n            for month_names, month_weights in [\n                (\"dec-jan-feb-weighted\", {\"12\": 0.5, \"1\": 1, \"2\": 0.5}),\n                (\"jan-feb-mar-weighted\", {\"1\": 0.5, \"2\": 1, \"3\": 0.5}),\n                (\"feb-mar-apr-weighted\", {\"2\": 0.5, \"3\": 1, \"4\": 0.5}),\n                (\"mar-apr-may-weighted\", {\"3\": 0.5, \"4\": 1, \"5\": 0.5}),\n                (\"apr-may-jun-weighted\", {\"4\": 0.5, \"5\": 1, \"6\": 0.5}),\n                (\"may-jun-jul-weighted\", {\"5\": 0.5, \"6\": 1, \"7\": 0.5}),\n                (\"jun-jul-aug-weighted\", {\"6\": 0.5, \"7\": 1, \"8\": 0.5}),\n                (\"jul-aug-sep-weighted\", {\"7\": 0.5, \"8\": 1, \"9\": 0.5}),\n                (\"aug-sep-oct-weighted\", {\"8\": 0.5, \"9\": 1, \"10\": 0.5}),\n                (\"sep-oct-nov-weighted\", {\"9\": 0.5, \"10\": 1, \"11\": 0.5}),\n                (\"oct-nov-dec-weighted\", {\"10\": 0.5, \"11\": 1, \"12\": 0.5}),\n                (\"nov-dec-jan-weighted\", {\"11\": 0.5, \"12\": 1, \"1\": 0.5}),\n            ]\n        },\n        index=index,\n        columns=[\n            \"dec-jan-feb-weighted\",\n            \"jan-feb-mar-weighted\",\n            \"feb-mar-apr-weighted\",\n            \"mar-apr-may-weighted\",\n            \"apr-may-jun-weighted\",\n            \"may-jun-jul-weighted\",\n            \"jun-jul-aug-weighted\",\n            \"jul-aug-sep-weighted\",\n            \"aug-sep-oct-weighted\",\n            \"sep-oct-nov-weighted\",\n            \"oct-nov-dec-weighted\",\n            \"nov-dec-jan-weighted\",\n        ],  # guarantee order\n    )\n\n\ndef segment_time_series(index, segment_type=\"single\", drop_zero_weight_segments=False):\n    \"\"\"Split a time series index into segments by applying weights.\n\n    Parameters\n    ----------\n    index : :any:`pandas.DatetimeIndex`\n        A time series index which gets split into segments.\n    segment_type : :any:`str`\n        The method to use when creating segments.\n\n         - \"single\": creates one big segment with the name \"all\".\n         - \"one_month\": creates up to twelve segments, each of which contains a single\n           month. Segment names are \"jan\", \"feb\", ... \"dec\".\n         - \"three_month\": creates up to twelve overlapping segments, each of which\n           contains three calendar months of data. Segment names are \"dec-jan-feb\",\n           \"jan-feb-mar\", ... \"nov-dec-jan\"\n         - \"three_month_weighted\": creates up to twelve overlapping segments, each of\n           contains three calendar months of data with first and third month in each\n           segment having weights of one half. Segment names are\n           \"dec-jan-feb-weighted\", \"jan-feb-mar-weighted\", ... \"nov-dec-jan-weighted\".\n\n    Returns\n    -------\n    segmentation : `pandas.DataFrame`\n        A segmentation of the input index expressed as a dataframe which shares\n        the input index and has named columns of weights.\n    \"\"\"\n    segment_weight_func = {\n        \"single\": _segment_weights_single,\n        \"one_month\": _segment_weights_one_month,\n        \"three_month\": _segment_weights_three_month,\n        \"three_month_weighted\": _segment_weights_three_month_weighted,\n    }.get(segment_type, None)\n\n    if segment_weight_func is None:\n        raise ValueError(\"Invalid segment type: %s\" % (segment_type))\n\n    segment_weights = segment_weight_func(index)\n\n    if drop_zero_weight_segments:\n        # keep only columns with non-zero weights\n        total_weights = segment_weights.sum()\n        columns_to_keep = total_weights[total_weights > 0].index.tolist()\n        segment_weights = segment_weights[columns_to_keep]\n\n    # TODO: Do something with these\n    _get_hourly_coverage_warning(segment_weights)  # each model\n    _get_calendar_year_coverage_warning(index)  # whole index\n\n    return segment_weights\n\n\ndef fit_model_segments(segmented_dataset_dict, fit_segment):\n    \"\"\"A function which fits a model to each item in a dataset.\n\n    Parameters\n    ----------\n    segmented_dataset_dict : :any:`dict` of :any:`pandas.DataFrame`\n        A dict with keys as segment names and values as dataframes of model input.\n    fit_segment : :any:`function`\n        A function which fits a model to a dataset in the `segmented_dataset_dict`.\n\n    Returns\n    -------\n    segment_models : :any:`list` of :any:`object`\n        List of fitted model objects - the return values of the fit_segment function.\n    \"\"\"\n    segment_models = [\n        fit_segment(segment_name, segment_data)\n        for segment_name, segment_data in segmented_dataset_dict.items()\n    ]\n    return segment_models\n", "import_text": ["collections.namedtuple", "pandas", "patsy.dmatrix"], "prompt": "\"\"\"\nDescription: This function iterates over a segmented dataset and applies a feature processor to each segment.\n\nArgs:\n    data (pandas.DataFrame): The dataset to be segmented and processed.\n    segmentation (dict): A dictionary where keys are segment names and values are pandas.Series of weights.\n    feature_processor (callable): A function that processes the features of the dataset.\n    feature_processor_kwargs (dict): Keyword arguments to be passed to the feature processor.\n    feature_processor_segment_name_mapping (dict): A mapping from segment names to feature processor segment names.\n\nReturns:\n    generator: A generator that yields tuples of segment name and processed segment data.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"A utility for iterating over segments which allows providing a function for\n    processing outputs into features.\n\n    Parameters\n    ----------\n    data : :any:`pandas.DataFrame`, required\n        Data to segment,\n    segmentation : :any:`pandas.DataFrame`, default None\n        A segmentation of the input dataframe expressed as a dataframe which shares\n        the timeseries index of the data and has named columns of weights, which\n        are iterated over to create the outputs (or inputs to the feature processor,\n        which then creates the actual outputs).\n    feature_processor : :any:`function`, default None\n        A function that transforms raw inputs (temperatures) into features for each\n        segment.\n    feature_processor_kwargs : :any:`dict`, default None\n        A dict of keyword arguments to be passed as `**kwargs` to the\n        `feature_processor` function.\n    feature_processor_segment_name_mapping : :any:`dict`, default None\n        A mapping from the default segmentation segment names to alternate names. This\n        is useful when prediction uses a different segment type than fitting.\n    \"\"\"", "function_dependencies": ["pandas.merge", "pandas.DataFrame"], "project_create_time": "2016-08-19T23:54:36+00:00", "project_update_time": "2024-04-05T16:07:10+00:00", "file_create_time": "2024-02-22T05:38:42Z", "file_update_time": "2024-02-26T21:31:39Z", "function_update_time": "2024-02-22T05:38:42Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["pandas.merge"], "test_function": [{"file_path": "/eemeter-v4.0.1/eemeter-4.0.1/tests/test_segmentation.py", "class_name": null, "function_name": "test_iterate_segmented_dataset_with_segmentation", "code": "\ndef test_iterate_segmented_dataset_with_segmentation(dataset, segmentation):\n    iterator = iterate_segmented_dataset(dataset, segmentation=segmentation)\n    segment_name, data = next(iterator)\n    assert segment_name == \"jan\"\n    assert list(data.columns) == [\"a\", \"b\", \"weight\"]\n    assert data.shape == (744, 3)\n    assert data.sum().sum() == 2976.0\n\n    segment_name, data = next(iterator)\n    assert segment_name == \"feb\"\n    assert list(data.columns) == [\"a\", \"b\", \"weight\"]\n    assert data.shape == (256, 3)\n    assert data.sum().sum() == 1024.0\n\n    segment_name, data = next(iterator)\n    assert segment_name == \"mar\"\n    assert list(data.columns) == [\"a\", \"b\", \"weight\"]\n    assert data.shape == (0, 3)\n    assert data.sum().sum() == 0.0"}, {"file_path": "/eemeter-v4.0.1/eemeter-4.0.1/tests/test_segmentation.py", "class_name": null, "function_name": "test_iterate_segmented_dataset_with_processor", "code": "\ndef test_iterate_segmented_dataset_with_processor(dataset, segmentation):\n    feature_processor_segment_names = []\n\n    def feature_processor(\n        segment_name, dataset, column_mapping=None\n    ):  # rename some columns\n        feature_processor_segment_names.append(segment_name)\n        return dataset.rename(columns=column_mapping).assign(weight=1)\n\n    iterator = iterate_segmented_dataset(\n        dataset,\n        segmentation=segmentation,\n        feature_processor=feature_processor,\n        feature_processor_kwargs={\"column_mapping\": {\"a\": \"c\", \"b\": \"d\"}},\n        feature_processor_segment_name_mapping={\"jan\": \"jan2\", \"feb\": \"feb2\"},\n    )\n    segment_name, data = next(iterator)\n    assert feature_processor_segment_names == [\"jan2\"]\n    assert segment_name == \"jan\"\n    assert list(data.columns) == [\"c\", \"d\", \"weight\"]\n    assert data.shape == (1000, 3)\n    assert data.sum().sum() == 4000.0\n\n    segment_name, data = next(iterator)\n    assert feature_processor_segment_names == [\"jan2\", \"feb2\"]\n    assert segment_name == \"feb\"\n    assert list(data.columns) == [\"c\", \"d\", \"weight\"]\n    assert data.shape == (1000, 3)\n    assert data.sum().sum() == 4000.0"}]}, {"git_group": "tournesol-app", "git_name": "tournesol", "version": "browser-extension-v3.5.2", "language": "Python", "project_name": "tournesol-browser-extension-v3.5.2.zip", "file_path": "/tournesol-browser-extension-v3.5.2/tournesol-browser-extension-v3.5.2/backend/core/models/user.py", "file_name": "user.py", "focal_class": "User", "focal_name": "validate_email_unique_with_plus", "focal_parameter": ["cls"], "solution": "    def validate_email_unique_with_plus(cls, email: str, username=\"\") -> str:\n        email_split = email.rsplit(\"@\", 1)\n\n        # if there is no `@`, do nothing\n        if len(email_split) == 1:\n            return email\n\n        local_part = email_split[0]\n        local_part_split = local_part.split(\"+\")\n\n        users = User.objects.filter(\n            Q(email__iexact=f\"{local_part_split[0]}@{email_split[-1]}\")\n            | (\n                Q(email__istartswith=f\"{local_part_split[0]}+\")\n                & Q(email__iendswith=f\"@{email_split[-1]}\")\n            ),\n        )\n\n        if username:\n            users = users.exclude(username=username)\n\n        if users.exists():\n            # f-strings are incompatible with Django translations (\"_\")\n            # pylint: disable=consider-using-f-string\n            raise ValidationError(\n                _(\n                    \"A user with an email starting with '%(email)s' already exists\"\n                    \" in this domain.\" % {\"email\": f\"{local_part_split[0]}+\"}\n                )\n            )\n\n        return email", "function_signature": "def validate_email_unique_with_plus(cls, email: str, username=\"\") -> str :", "left_context": "\"\"\"\nUser models and user preferences.\n\"\"\"\n\nimport logging\n\nfrom django.contrib.auth.models import AbstractUser\nfrom django.core.exceptions import ValidationError\nfrom django.db import models\nfrom django.db.models import CheckConstraint, F, Func, Q, Value\nfrom django.db.models.expressions import Exists, OuterRef\nfrom django.db.models.functions import Lower\nfrom django.db.models.query import QuerySet\nfrom django.utils.translation import gettext_lazy as _\n\nlogger = logging.getLogger(__name__)\n\n\nclass User(AbstractUser):\n    \"\"\"\n    Administrative, social and profile information about users.\n\n    This model also contains application settings for each user,\n    and methods related to email validation.\n    \"\"\"\n\n    # Fields used by django-rest-registation to find a user.\n    # Used by reset password mechanism.\n    LOGIN_FIELDS = (\"username\", \"email\")\n\n    email = models.EmailField(_(\"email address\"), unique=True)\n    first_name = models.CharField(\n        max_length=100, blank=True, null=True, help_text=\"First name\"\n    )\n    last_name = models.CharField(\n        max_length=100, blank=True, null=True, help_text=\"Last name\"\n    )\n\n    trust_score = models.FloatField(\n        null=True,\n        blank=True,\n        default=None,\n        help_text=\"The trust score assigned to the user based on trusted\"\n                  \" domains and the vouching mechanism.\",\n    )\n\n    settings = models.JSONField(\n        null=False,\n        blank=True,\n        default=dict,\n        help_text=_(\"The user' preferences.\")\n    )\n\n    @classmethod\n    def with_trusted_email(cls) -> QuerySet[\"User\"]:\n        accepted_domain = EmailDomain.objects.filter(\n            domain=OuterRef(\"user_email_domain\"), status=EmailDomain.STATUS_ACCEPTED\n        )\n        return (\n            cls.objects.alias(\n                # user_email_domain is extracted from user.email, with leading '@'\n                user_email_domain=Lower(\n                    Func(\n                        F(\"email\"),\n                        Value(r\"(.*)(@.*$)\"),\n                        Value(r\"\\2\"),\n                        function=\"regexp_replace\",\n                    )\n                )\n            )\n            .alias(is_trusted=Exists(accepted_domain))\n            .filter(is_trusted=True)\n        )\n\n    @classmethod", "right_context": "\n    @property\n    def has_trusted_email(self):\n        return User.with_trusted_email().filter(pk=self.pk).exists()\n\n    @property\n    def is_trusted(self):\n        # TODO: remove this property. Use 'has_trusted_email' instead,\n        # and rename the associated field in the /accounts/profile/ API.\n        return self.has_trusted_email\n\n    def ensure_email_domain_exists(self):\n        if not self.email:\n            return\n        if \"@\" not in self.email:\n            # Should never happen, as the address format is validated by the field.\n            logger.warning(\n                'Cannot find email domain for user \"%s\" with email \"%s\".',\n                self.username,\n                self.email,\n            )\n            return\n        _, domain_part = self.email.rsplit(\"@\", 1)\n        domain = f\"@{domain_part}\".lower()\n        EmailDomain.objects.get_or_create(domain=domain)\n\n    def clean(self):\n        value = self.email\n\n        similar_email = User.objects.filter(email__iexact=value).exclude(pk=self.pk)\n\n        if similar_email.exists():\n            raise ValidationError(\n                {\"email\": _(\"A user with this email address already exists.\")}\n            )\n\n        try:\n            User.validate_email_unique_with_plus(value, self.username)\n        except ValidationError as error:\n            # Catching the exception here allows us to add the email key, and\n            # makes the message display near the email field in the admin\n            # interface.\n            raise ValidationError({\"email\": error.message}) from error\n\n    def save(self, *args, **kwargs):\n        update_fields = kwargs.get(\"update_fields\")\n        # No need to create the EmailDomain, if email is unchanged\n        if update_fields is None or \"email\" in update_fields:\n            self.ensure_email_domain_exists()\n        return super().save(*args, **kwargs)\n\n    def set_password(self, raw_password):\n        super().set_password(raw_password)\n        # Temporary workaround to force user activation\n        # when a user asks for password reset\n        self.is_active = True\n\n\nclass VerifiableEmail(models.Model):\n    \"\"\"One verified e-mail for a user.\"\"\"\n\n    email = models.EmailField(\n        null=False, blank=False, help_text=\"E-mail address\", max_length=100\n    )\n    is_verified = models.BooleanField(\n        default=False, blank=False, null=False, help_text=\"Verified\"\n    )\n    last_verification_email_ts = models.DateTimeField(\n        help_text=\"Timestamp when the last verification e-mail was sent to this address\",\n        null=True,\n        default=None,\n        blank=True,\n    )\n    token = models.CharField(\n        max_length=1000,\n        blank=True,\n        null=True,\n        help_text=\"The token that needs to be supplied to verify this e-mail address\",\n    )\n    user = models.ForeignKey(\n        to=User,\n        on_delete=models.CASCADE,\n        help_text=\"User that this e-mail belongs to\",\n        null=True,\n        related_name=\"verifiableemails\",\n    )\n    rank = models.IntegerField(default=0, help_text=\"Ordering field\")\n    domain_fk = models.ForeignKey(\n        to=\"EmailDomain\",\n        on_delete=models.SET_NULL,\n        help_text=\"Foreign key to e-mail domain\",\n        null=True,\n        related_name=\"verifiable_emails_domain\",\n        blank=True,\n    )\n\n    def save(self, *args, **kwargs):\n        \"\"\"Creating an e-mail domain entry if it does not exist.\"\"\"\n        domain = (\n            f\"@{str(self.email).split('@')[1]}\" if str(self.email).split(\"@\")[1] else \"\"\n        )\n        domain_object, _ = EmailDomain.objects.get_or_create(domain=domain)\n        self.domain_fk = domain_object\n        if self.pk is None:\n            kwargs[\"force_insert\"] = True\n        return super().save(*args, **kwargs)\n\n    class Meta:\n        ordering = [\"rank\"]\n        unique_together = [\"email\", \"user\"]\n\n    def __str__(self):\n        return self.email\n\n\nclass EmailDomain(models.Model):\n    \"\"\"Domain which can be either accepted or rejected.\"\"\"\n\n    STATUS_REJECTED = \"RJ\"\n    STATUS_ACCEPTED = \"ACK\"\n    STATUS_PENDING = \"PD\"\n\n    domain = models.CharField(\n        max_length=100,\n        null=False,\n        blank=False,\n        help_text=\"E-mail domain with leading @\",\n        unique=True,\n    )\n    status = models.CharField(\n        max_length=10,\n        null=False,\n        blank=False,\n        default=STATUS_PENDING,\n        help_text=\"Status of the domain.\",\n        choices=[\n            (STATUS_REJECTED, \"Rejected\"),\n            (STATUS_ACCEPTED, \"Accepted\"),\n            (STATUS_PENDING, \"Pending\"),\n        ],\n    )\n    datetime_add = models.DateTimeField(\n        auto_now_add=True, help_text=\"Time the domain was added\", null=True, blank=True\n    )\n\n    class Meta:\n        ordering = [\"-datetime_add\", \"domain\"]\n        constraints = [\n            CheckConstraint(\n                check=Q(domain__istartswith=\"@\"),\n                name=\"domain_starts_with_at\",\n            ),\n        ]\n\n    def __str__(self):\n        \"\"\"Get string representation.\"\"\"\n        return f\"{self.domain} [{self.status}]\"\n\n\nclass Degree(models.Model):\n    \"\"\"Educational degree.\"\"\"\n\n    level = models.CharField(\n        null=False, blank=False, help_text=\"Degree level\", max_length=100\n    )\n    domain = models.CharField(\n        null=False, blank=False, help_text=\"Degree domain\", max_length=100\n    )\n    institution = models.CharField(\n        null=False, blank=False, help_text=\"Degree institution\", max_length=100\n    )\n    user = models.ForeignKey(\n        null=True,\n        to=User,\n        help_text=\"User that the degree belongs to.\",\n        on_delete=models.CASCADE,\n        related_name=\"degrees\",\n    )\n    rank = models.IntegerField(default=0, help_text=\"Ordering field\")\n\n    class Meta:\n        ordering = [\"rank\", \"level\", \"domain\", \"institution\"]\n        unique_together = [\"level\", \"domain\", \"institution\", \"user\"]\n\n    def __str__(self):\n        return f\"{self.level}/{self.domain}/{self.institution}\"\n\n\nclass Expertise(models.Model):\n    \"\"\"Expertise for a user.\"\"\"\n\n    name = models.CharField(\n        null=False, blank=False, help_text=\"Expertise description\", max_length=100\n    )\n    user = models.ForeignKey(\n        to=User,\n        on_delete=models.CASCADE,\n        help_text=\"User for the expertise\",\n        related_name=\"expertises\",\n        null=True,\n    )\n    rank = models.IntegerField(default=0, help_text=\"Ordering field\")\n\n    class Meta:\n        ordering = [\"rank\", \"name\"]\n        unique_together = [\"name\", \"user\"]\n\n    def __str__(self):\n        return self.name\n\n\nclass ExpertiseKeyword(models.Model):\n    \"\"\"Expertise keyword for a user.\"\"\"\n\n    name = models.CharField(\n        null=False,\n        blank=False,\n        help_text=\"Expertise keyword description\",\n        max_length=100,\n    )\n    user = models.ForeignKey(\n        to=User,\n        on_delete=models.CASCADE,\n        help_text=\"User for the expertise keywords\",\n        related_name=\"expertise_keywords\",\n        null=True,\n    )\n    rank = models.IntegerField(default=0, help_text=\"Ordering field\")\n\n    class Meta:\n        ordering = [\"rank\", \"name\"]\n        unique_together = [\"name\", \"user\"]\n\n    def __str__(self):\n        return self.name\n", "import_text": ["logging", "django.contrib.auth.models.AbstractUser", "django.core.exceptions.ValidationError", "django.db.models", "django.db.models.CheckConstraint", "django.db.models.F", "django.db.models.Func", "django.db.models.Q", "django.db.models.Value", "django.db.models.expressions.Exists", "django.db.models.expressions.OuterRef", "django.db.models.functions.Lower", "django.db.models.query.QuerySet", "django.utils.translation.gettext_lazy"], "prompt": "\"\"\"\nDescription: This function validates the uniqueness of an email address with a '+' symbol.\n\nArgs:\n    cls: The class instance.\n    email (str): The email address to validate.\n    username (str, optional): The username to exclude from the validation. Defaults to \"\".\n\nReturns:\n    str: The validated email address.\n\nRaises:\n    ValidationError: If a user with an email starting with the same local part and domain already exists.\n\nNotes:\n    - The function splits the email address into a local part and a domain.\n    - It checks if there is a '+' symbol in the local part.\n    - It queries the User model for users with the same email or an email starting with the same local part and domain.\n    - If a username is provided, it excludes the user with that username from the validation.\n    - If a user with the same email or an email starting with the same local part and domain exists, it raises a ValidationError.\n    - The function returns the validated email address.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"Raise ValidationError when similar emails are found in the database.\n\n        Keyword arguments:\n        email -- An email that is going to be written in the database.\n        username -- The logged user's username, used to exclude him/herself from\n                    the validation when updating its own email. Empty for\n                    anonymous users.\n\n        Emails considered similar when:\n            - they share the same non-case-sensitive domain ;\n            - they share, in the local part, the same non case-sensitive string\n              before the `+` symbol.\n\n        Examples of emails considered similar:\n            - bob@example.org\n            - bob+@example.org\n            - bob+tournesol@example.org\n            - BOB+tournesol@example.org\n            - bob+hello@example.org\n            - BOB+HELLO@example.org\n            - etc.\n        \"\"\"", "function_dependencies": ["django.db.models.Q", "django.core.exceptions.ValidationError", "django.utils.translation.gettext_lazy"], "project_create_time": "2021-03-23T18:21:41+00:00", "project_update_time": "2024-04-10T07:50:15+00:00", "file_create_time": "2021-10-05T19:03:11Z", "file_update_time": "2023-11-10T10:09:48Z", "function_update_time": "2021-10-05T19:03:11Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["django.utils.translation.gettext_lazy", "django.core.exceptions.ValidationError"], "test_function": [{"file_path": "/tournesol-browser-extension-v3.5.2/tournesol-browser-extension-v3.5.2/backend/core/tests/test_user.py", "class_name": "UserModelTestCase", "function_name": "test_validate_email_unique_with_plus_rsplit", "code": "    def test_validate_email_unique_with_plus_rsplit(self):\n        already_used = \"already@USED+tournesol@example.org\"\n        UserFactory(email=already_used)\n\n        for email in [\n            \"already@used+@example.org\",\n            \"already@used+@EXAMPLE.org\",\n            \"ALREADY@USED+tournesol@example.org\",\n            \"already@used+TOURNESOL@example.org\",\n            \"already@used+different@example.org\",\n            \"already@used+foo+bar@example.org\",\n        ]:\n            with self.assertRaises(ValidationError):\n                User.validate_email_unique_with_plus(email)"}]}, {"git_group": "goauthentik", "git_name": "authentik", "version": "version/2024.4.0-rc1", "language": "Python", "project_name": "authentik-version-2024.4.0-rc1.zip", "file_path": "/authentik-version-2024.4.0-rc1/authentik-version-2024.4.0-rc1/authentik/blueprints/v1/tasks.py", "file_name": "tasks.py", "focal_class": null, "focal_name": "apply_blueprint", "focal_parameter": [], "solution": "def apply_blueprint(self: SystemTask, instance_pk: str):\n    self.save_on_success = False\n    instance: BlueprintInstance | None = None\n    try:\n        instance: BlueprintInstance = BlueprintInstance.objects.filter(pk=instance_pk).first()\n        if not instance or not instance.enabled:\n            return\n        self.set_uid(slugify(instance.name))\n        blueprint_content = instance.retrieve()\n        file_hash = sha512(blueprint_content.encode()).hexdigest()\n        importer = Importer.from_string(blueprint_content, instance.context)\n        if importer.blueprint.metadata:\n            instance.metadata = asdict(importer.blueprint.metadata)\n        valid, logs = importer.validate()\n        if not valid:\n            instance.status = BlueprintInstanceStatus.ERROR\n            instance.save()\n            self.set_status(TaskStatus.ERROR, *logs)\n            return\n        with capture_logs() as logs:\n            applied = importer.apply()\n            if not applied:\n                instance.status = BlueprintInstanceStatus.ERROR\n                instance.save()\n                self.set_status(TaskStatus.ERROR, *logs)\n                return\n        instance.status = BlueprintInstanceStatus.SUCCESSFUL\n        instance.last_applied_hash = file_hash\n        instance.last_applied = now()\n        self.set_status(TaskStatus.SUCCESSFUL)\n    except (\n        OSError,\n        DatabaseError,\n        ProgrammingError,\n        InternalError,\n        BlueprintRetrievalFailed,\n        EntryInvalidError,\n    ) as exc:\n        if instance:\n            instance.status = BlueprintInstanceStatus.ERROR\n        self.set_error(exc)\n    finally:\n        if instance:\n            instance.save()", "function_signature": "def apply_blueprint(self: SystemTask, instance_pk: str) :", "left_context": "\"\"\"v1 blueprints tasks\"\"\"\n\nfrom dataclasses import asdict, dataclass, field\nfrom hashlib import sha512\nfrom pathlib import Path\nfrom sys import platform\n\nfrom dacite.core import from_dict\nfrom django.db import DatabaseError, InternalError, ProgrammingError\nfrom django.utils.text import slugify\nfrom django.utils.timezone import now\nfrom django.utils.translation import gettext_lazy as _\nfrom structlog.stdlib import get_logger\nfrom watchdog.events import (\n    FileCreatedEvent,\n    FileModifiedEvent,\n    FileSystemEvent,\n    FileSystemEventHandler,\n)\nfrom watchdog.observers import Observer\nfrom yaml import load\nfrom yaml.error import YAMLError\n\nfrom authentik.blueprints.models import (\n    BlueprintInstance,\n    BlueprintInstanceStatus,\n    BlueprintRetrievalFailed,\n)\nfrom authentik.blueprints.v1.common import BlueprintLoader, BlueprintMetadata, EntryInvalidError\nfrom authentik.blueprints.v1.importer import Importer\nfrom authentik.blueprints.v1.labels import LABEL_AUTHENTIK_INSTANTIATE\nfrom authentik.blueprints.v1.oci import OCI_PREFIX\nfrom authentik.events.logs import capture_logs\nfrom authentik.events.models import TaskStatus\nfrom authentik.events.system_tasks import SystemTask, prefill_task\nfrom authentik.events.utils import sanitize_dict\nfrom authentik.lib.config import CONFIG\nfrom authentik.root.celery import CELERY_APP\nfrom authentik.tenants.models import Tenant\n\nLOGGER = get_logger()\n_file_watcher_started = False\n\n\n@dataclass\nclass BlueprintFile:\n    \"\"\"Basic info about a blueprint file\"\"\"\n\n    path: str\n    version: int\n    hash: str\n    last_m: int\n    meta: BlueprintMetadata | None = field(default=None)\n\n\ndef start_blueprint_watcher():\n    \"\"\"Start blueprint watcher, if it's not running already.\"\"\"\n    # This function might be called twice since it's called on celery startup\n\n    global _file_watcher_started  # noqa: PLW0603\n    if _file_watcher_started:\n        return\n    observer = Observer()\n    kwargs = {}\n    if platform.startswith(\"linux\"):\n        kwargs[\"event_filter\"] = (FileCreatedEvent, FileModifiedEvent)\n    observer.schedule(\n        BlueprintEventHandler(), CONFIG.get(\"blueprints_dir\"), recursive=True, **kwargs\n    )\n    observer.start()\n    _file_watcher_started = True\n\n\nclass BlueprintEventHandler(FileSystemEventHandler):\n    \"\"\"Event handler for blueprint events\"\"\"\n\n    # We only ever get creation and modification events.\n    # See the creation of the Observer instance above for the event filtering.\n\n    # Even though we filter to only get file events, we might still get\n    # directory events as some implementations such as inotify do not support\n    # filtering on file/directory.\n\n    def dispatch(self, event: FileSystemEvent) -> None:\n        \"\"\"Call specific event handler method. Ignores directory changes.\"\"\"\n        if event.is_directory:\n            return None\n        return super().dispatch(event)\n\n    def on_created(self, event: FileSystemEvent):\n        \"\"\"Process file creation\"\"\"\n        LOGGER.debug(\"new blueprint file created, starting discovery\")\n        for tenant in Tenant.objects.filter(ready=True):\n            with tenant:\n                blueprints_discovery.delay()\n\n    def on_modified(self, event: FileSystemEvent):\n        \"\"\"Process file modification\"\"\"\n        path = Path(event.src_path)\n        root = Path(CONFIG.get(\"blueprints_dir\")).absolute()\n        rel_path = str(path.relative_to(root))\n        for tenant in Tenant.objects.filter(ready=True):\n            with tenant:\n                for instance in BlueprintInstance.objects.filter(path=rel_path, enabled=True):\n                    LOGGER.debug(\"modified blueprint file, starting apply\", instance=instance)\n                    apply_blueprint.delay(instance.pk.hex)\n\n\n@CELERY_APP.task(\n    throws=(DatabaseError, ProgrammingError, InternalError),\n)\ndef blueprints_find_dict():\n    \"\"\"Find blueprints as `blueprints_find` does, but return a safe dict\"\"\"\n    blueprints = []\n    for blueprint in blueprints_find():\n        blueprints.append(sanitize_dict(asdict(blueprint)))\n    return blueprints\n\n\ndef blueprints_find() -> list[BlueprintFile]:\n    \"\"\"Find blueprints and return valid ones\"\"\"\n    blueprints = []\n    root = Path(CONFIG.get(\"blueprints_dir\"))\n    for path in root.rglob(\"**/*.yaml\"):\n        rel_path = path.relative_to(root)\n        # Check if any part in the path starts with a dot and assume a hidden file\n        if any(part for part in path.parts if part.startswith(\".\")):\n            continue\n        with open(path, encoding=\"utf-8\") as blueprint_file:\n            try:\n                raw_blueprint = load(blueprint_file.read(), BlueprintLoader)\n            except YAMLError as exc:\n                raw_blueprint = None\n                LOGGER.warning(\"failed to parse blueprint\", exc=exc, path=str(rel_path))\n            if not raw_blueprint:\n                continue\n            metadata = raw_blueprint.get(\"metadata\", None)\n            version = raw_blueprint.get(\"version\", 1)\n            if version != 1:\n                LOGGER.warning(\"invalid blueprint version\", version=version, path=str(rel_path))\n                continue\n        file_hash = sha512(path.read_bytes()).hexdigest()\n        blueprint = BlueprintFile(str(rel_path), version, file_hash, int(path.stat().st_mtime))\n        blueprint.meta = from_dict(BlueprintMetadata, metadata) if metadata else None\n        blueprints.append(blueprint)\n    return blueprints\n\n\n@CELERY_APP.task(\n    throws=(DatabaseError, ProgrammingError, InternalError), base=SystemTask, bind=True\n)\n@prefill_task\ndef blueprints_discovery(self: SystemTask, path: str | None = None):\n    \"\"\"Find blueprints and check if they need to be created in the database\"\"\"\n    count = 0\n    for blueprint in blueprints_find():\n        if path and blueprint.path != path:\n            continue\n        check_blueprint_v1_file(blueprint)\n        count += 1\n    self.set_status(\n        TaskStatus.SUCCESSFUL, _(\"Successfully imported %(count)d files.\" % {\"count\": count})\n    )\n\n\ndef check_blueprint_v1_file(blueprint: BlueprintFile):\n    \"\"\"Check if blueprint should be imported\"\"\"\n    instance: BlueprintInstance = BlueprintInstance.objects.filter(path=blueprint.path).first()\n    if (\n        blueprint.meta\n        and blueprint.meta.labels.get(LABEL_AUTHENTIK_INSTANTIATE, \"\").lower() == \"false\"\n    ):\n        return\n    if not instance:\n        instance = BlueprintInstance(\n            name=blueprint.meta.name if blueprint.meta else str(blueprint.path),\n            path=blueprint.path,\n            context={},\n            status=BlueprintInstanceStatus.UNKNOWN,\n            enabled=True,\n            managed_models=[],\n            metadata={},\n        )\n        instance.save()\n        LOGGER.info(\n            \"Creating new blueprint instance from file\", instance=instance, path=instance.path\n        )\n    if instance.last_applied_hash != blueprint.hash:\n        LOGGER.info(\"Applying blueprint due to changed file\", instance=instance, path=instance.path)\n        apply_blueprint.delay(str(instance.pk))\n\n\n@CELERY_APP.task(\n    bind=True,\n    base=SystemTask,\n)", "right_context": "\n\n@CELERY_APP.task()\ndef clear_failed_blueprints():\n    \"\"\"Remove blueprints which couldn't be fetched\"\"\"\n    # Exclude OCI blueprints as those might be temporarily unavailable\n    for blueprint in BlueprintInstance.objects.exclude(path__startswith=OCI_PREFIX):\n        try:\n            blueprint.retrieve()\n        except BlueprintRetrievalFailed:\n            blueprint.delete()\n", "import_text": ["dataclasses.asdict", "dataclasses.dataclass", "dataclasses.field", "hashlib.sha512", "pathlib.Path", "sys.platform", "dacite.core.from_dict", "django.db.DatabaseError", "django.db.InternalError", "django.db.ProgrammingError", "django.utils.text.slugify", "django.utils.timezone.now", "django.utils.translation.gettext_lazy", "structlog.stdlib.get_logger", "watchdog.events.FileCreatedEvent", "watchdog.events.FileModifiedEvent", "watchdog.events.FileSystemEvent", "watchdog.events.FileSystemEventHandler", "watchdog.observers.Observer", "yaml.load", "yaml.error.YAMLError", "authentik.blueprints.models.BlueprintInstance", "authentik.blueprints.models.BlueprintInstanceStatus", "authentik.blueprints.models.BlueprintRetrievalFailed", "authentik.blueprints.v1.common.BlueprintLoader", "authentik.blueprints.v1.common.BlueprintMetadata", "authentik.blueprints.v1.common.EntryInvalidError", "authentik.blueprints.v1.importer.Importer", "authentik.blueprints.v1.labels.LABEL_AUTHENTIK_INSTANTIATE", "authentik.blueprints.v1.oci.OCI_PREFIX", "authentik.events.logs.capture_logs", "authentik.events.models.TaskStatus", "authentik.events.system_tasks.SystemTask", "authentik.events.system_tasks.prefill_task", "authentik.events.utils.sanitize_dict", "authentik.lib.config.CONFIG", "authentik.root.celery.CELERY_APP", "authentik.tenants.models.Tenant"], "prompt": "\"\"\"\nDescription: This function applies a blueprint to an instance.\n\nArgs:\n    self (SystemTask): The instance of the SystemTask class.\n    instance_pk (str): The primary key of the instance.\n\nReturns:\n    None: This function does not return anything.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Apply single blueprint\"\"\"", "function_dependencies": ["authentik.blueprints.models.BlueprintInstance.objects.filter", "authentik.blueprints.models.BlueprintInstance.objects.filter.first", "django.utils.text.slugify", "authentik.blueprints.models.BlueprintInstance.objects.filter.first.retrieve", "hashlib.sha512", "hashlib.sha512.hexdigest", "authentik.blueprints.models.BlueprintInstance.objects.filter.first.retrieve.encode", "authentik.blueprints.v1.importer.Importer.from_string", "dataclasses.asdict", "authentik.blueprints.v1.importer.Importer.from_string.validate", "authentik.blueprints.models.BlueprintInstance.objects.filter.first.save", "authentik.events.logs.capture_logs", "authentik.blueprints.v1.importer.Importer.from_string.apply", "django.utils.timezone.now"], "project_create_time": "2019-12-30T09:19:48+00:00", "project_update_time": "2024-04-18T03:12:01+00:00", "file_create_time": "2022-08-01T21:05:58Z", "file_update_time": "2024-03-28T16:34:34Z", "function_update_time": "2024-01-24T16:23:03Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["django.utils.timezone.now", "django.utils.text.slugify"], "test_function": [{"file_path": "/authentik-version-2024.4.0-rc1/authentik-version-2024.4.0-rc1/authentik/blueprints/tests/test_v1_tasks.py", "class_name": "TestBlueprintsV1Tasks", "function_name": "test_valid_disabled", "code": "    def test_valid_disabled(self):\n        with NamedTemporaryFile(mode=\"w+\", suffix=\".yaml\", dir=TMP) as file:\n            file.write(\n                dump(\n                    {\n                        \"version\": 1,\n                        \"entries\": [],\n                    }\n                )\n            )\n            file.flush()\n            instance: BlueprintInstance = BlueprintInstance.objects.create(\n                name=generate_id(),\n                path=file.name,\n                enabled=False,\n                status=BlueprintInstanceStatus.UNKNOWN,\n            )\n            instance.refresh_from_db()\n            self.assertEqual(instance.last_applied_hash, \"\")\n            self.assertEqual(\n                instance.status,\n                BlueprintInstanceStatus.UNKNOWN,\n            )\n            apply_blueprint(instance.pk)\n            instance.refresh_from_db()\n            self.assertEqual(instance.last_applied_hash, \"\")\n            self.assertEqual(\n                instance.status,\n                BlueprintInstanceStatus.UNKNOWN,\n            )"}]}, {"git_group": "NervanaSystems", "git_name": "neon", "version": "v2.6.0", "language": "Python", "project_name": "neon-v2.6.0.zip", "file_path": "/neon-v2.6.0/neon-2.6.0/examples/faster-rcnn/proposal_layer.py", "file_name": "proposal_layer.py", "focal_class": "ProposalLayer", "focal_name": "fprop", "focal_parameter": ["inputs"], "solution": "    def fprop(self, inputs, inference=False):\n        assert self.inference == inference, \\\n            \"Model was configured for inference={}\".format(self.inference)\n\n        # get needed metadata from the dataloader\n        (self.im_shape, self.im_scale, self.gt_boxes,\n            self.gt_classes, self.num_gt_boxes, _) = self.dataloader.get_metadata_buffers()\n\n        # real H and W need to get in fprop, as every image is different\n        real_H = int(np.round(self.im_shape.get()[1] * self._scale))\n        real_W = int(np.round(self.im_shape.get()[0] * self._scale))\n\n        if self.rpn_scores_v is None:\n            # get output from the RPN network\n            # transform the scores and slice the score for the label=1 class\n            # shape: (KWH, 1)\n            self.rpn_scores_v = self.rpn_obj[0].outputs.reshape((2, -1))[1].T\n\n        if self.bbox_deltas_v is None:\n            # transform the bbox deltas, reshape to (4, KHW) then transpose to\n            # (KHW, 4) to match the shape of anchors.\n            self.bbox_deltas_v = self.rpn_bbox[0].outputs.reshape((4, -1)).T\n\n        # 1. Convert anchors into proposals via bbox transformations\n        # store output in proposals buffer\n        self._bbox_transform_inv(self._dev_anchors, self.bbox_deltas_v, output=self._proposals)\n\n        # 2. clip predicted boxes to image\n        self._clip_boxes(self._proposals, self.im_shape)\n\n        # 3. remove predicted boxes with either height or width < threshold\n        # (NOTE: convert min_size to input image scale stored in im_scale)\n        keep = self._filter_boxes(self._proposals,\n                                  self.min_bbox_size * float(self.im_scale.get()))\n\n        # we set the score of those we discard to -1\n        self._scores[:] = (self.rpn_scores_v * keep) - (1 - keep)\n\n        # 4. set the scores to be -1 for the padded area\n        # set the scores of all the proposals from the padded area to be -1\n        # in order to ignore them after sorting\n        scores_np = self._scores.get()\n        scores_np.reshape(-1, self._conv_height, self._conv_width)[:, real_H:] = -1\n        scores_np.reshape(-1, self._conv_height, self._conv_width)[:, :, real_W:] = -1\n        self._scores[:] = self.be.array(scores_np)\n\n        # 5. sort the scores from highest to lowest and take top pre_nms_topN\n        top_N_ind = self.get_top_N_index(self._scores, self.pre_nms_N)\n\n        # take top pre_nms_topN (e.g. 12000)\n        # (make scores & proposals attributes of layer for unit testing)\n        self.dets.fill(0)\n        self.dets[:len(top_N_ind), :4] = self._proposals[top_N_ind]\n        self.dets[:len(top_N_ind), 4] = self._scores[top_N_ind]\n\n        # 6. apply nms (e.g. threshold = 0.7)\n        keep = self.be.nms(self.dets, self.nms_thresh)\n\n        # 7. take post_nms_N (e.g. 2000)\n        keep = keep[:self.post_nms_N]\n        self.num_proposals = len(keep)\n\n        # for training or debugging, we need to copy these detections to host.\n        if self.debug or not inference:\n            # make scores & proposals attributes of layer for unit testing\n            self.proposals = self.dets[keep, :4].get()\n            self.scores = self.dets[keep, -1].get()\n\n        # 8. provide ROIs in the format of [0, x1, y1, x2, y2]\n        self.dev_proposals.fill(0)\n        self.dev_proposals[:self.num_proposals, 1:] = self.dets[keep, :4]\n\n        # If training, sample the proposals and only propagate those forward\n        if not inference:\n            # Next, we need to set the target buffers with the class labels,\n            # and bbox targets for each roi.\n            ((frcn_labels, frcn_labels_mask),\n             (frcn_bbtargets, frcn_bbmask)) = self.dataloader.get_target_buffers()\n\n            non_zero_gt_boxes = self.gt_boxes.get()\n            num_gt_boxes = self.num_gt_boxes.get()[0][0]\n            non_zero_gt_boxes = non_zero_gt_boxes[:num_gt_boxes]\n\n            # Include ground-truth boxes in the set of candidate rois\n            all_rois = np.vstack((self.proposals, non_zero_gt_boxes))\n\n            # 1. Compute the overlap of each proposal roi with each ground truth roi\n            overlaps = calculate_bb_overlap(all_rois, non_zero_gt_boxes)\n\n            # 2. Use overlaps to compute the gt box each proposal is 'closest' to\n            gt_assignment = overlaps.argmax(axis=1)\n            max_overlaps = overlaps.max(axis=1)\n            labels = self.gt_classes.get()[:num_gt_boxes]\n            labels = labels[gt_assignment]\n\n            # Sample num_rois fg and bg indicies based on overlaps with gt bocxes\n            keep_inds, fg_rois_this_img = self._sample_fg_bg(max_overlaps)\n            # Select sampled values from various arrays:\n            labels = labels[keep_inds]\n            # Clamp labels for the background RoIs to 0\n            labels[fg_rois_this_img:] = 0\n\n            rois = all_rois[keep_inds]\n            targets = compute_targets(non_zero_gt_boxes[gt_assignment[keep_inds]], rois)\n\n            targets = (targets - np.array(BBOX_NORMALIZE_MEANS)) / np.array(BBOX_NORMALIZE_STDS)\n\n            num_proposals = rois.shape[0]\n            bbox_targets, bbox_inside_weights = \\\n                self._get_bbox_regression_labels(targets, labels)\n\n            # Load fast-rcnn training labels and bbox targets back to global buffers\n            labels_full = self._onehot_labels(labels.ravel())\n            frcn_labels[:] = labels_full\n            labels_mask = np.zeros((self.num_rois, self.num_classes))\n            labels_mask[:num_proposals, :] = 1.0\n            frcn_labels_mask[:] = np.ascontiguousarray(labels_mask.T)\n\n            # fcrn_*.shape = (num_classes*4 , 256), so transpose first\n            frcn_bbtargets[:] = np.ascontiguousarray(bbox_targets.T)\n            frcn_bbmask[:] = np.ascontiguousarray(bbox_inside_weights.T)\n\n            # load the sampled proposals back to device\n            rois = np.hstack([np.zeros((num_proposals, 1)), rois])\n            rois = np.ascontiguousarray(rois, dtype=np.float32)\n\n            self.dev_proposals_filtered.fill(0)\n            self.dev_proposals_filtered[:num_proposals, :] = rois\n            self.num_proposals = num_proposals\n\n            # During training, only propagate sampled proposals\n            return (inputs, self.dev_proposals_filtered.T)\n\n        # If inference=True, we're testing, so propagate all proposals\n        else:\n            return (inputs, self.dev_proposals.T)", "function_signature": "def fprop(self, inputs, inference=False) :", "left_context": "# ----------------------------------------------------------------------------\n# Copyright 2016 Nervana Systems Inc.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ----------------------------------------------------------------------------\n\"\"\"\nDefine a layer that takes Region Proposal Network's output and generate\nregion proposals in the format of bounding boxes, compares with ground truth boxes\nand generates bounding box target labels and regression targets\n\"\"\"\nfrom __future__ import division\nimport numpy as np\nfrom neon.layers.layer import Layer\nfrom generate_anchors import generate_all_anchors\nfrom util import compute_targets, calculate_bb_overlap\n\n# Thresholds for the IoU overlap to consider a proposed ROI as\n# foreground or background. Less than BG_THRESH_LO is ignored.\nFG_THRESH = 0.5\nBG_THRESH_HI = 0.5\nBG_THRESH_LO = 0.0\n\n# The percentage of ROIs are from foreground within a minibatch\nFG_FRACTION = 0.25\n\nBBOX_NORMALIZE_MEANS = [0.0, 0.0, 0.0, 0.0]\nBBOX_NORMALIZE_STDS = [0.1, 0.1, 0.2, 0.2]\n\n# precomputed empirically...\n# means: [-0.00624775 -0.01916088 -0.11854464  0.16050844]\n# std: [ 0.27227189  0.42585105  0.48241054  0.49931084]\n\n\nclass ProposalLayer(Layer):\n    \"\"\"\n    Proposal layer takes as input:\n    (1) output of RPN cls_score\n    (2) output of RPN bbox regression\n\n    Converts that to an output ROIs\n    (5, num_ROIs) <- [image_idx, x_min, y_min, x_max, y_max]\n\n    Steps:\n    1. For each anchor, generate anchor boxes and apply bbox deltas to get\n       bbox proposals\n    2. Clip bboxes to image\n    3. remove bbox with H < threshold or W < threshold\n    4. set the scores to be -1 for the padded area\n    5. Take top pre_nms_N scores\n    6. apply NMS with a threshold\n    7. return the top num_ROIs proposals\n    8. provide ROIs\n    9. compute bbox targets and store in target_buffers\n\n\n    \"\"\"\n\n    def __init__(self, rpn_layers, dataloader, inference=False, num_rois=128, pre_nms_N=12000,\n                 post_nms_N=2000, nms_thresh=0.7, min_bbox_size=16,\n                 fg_fraction=None, fg_thresh=None, bg_thresh_hi=None, bg_thresh_lo=None,\n                 deterministic=False, name=None, debug=False):\n        \"\"\"\n        Arguments:\n            rpn_layers (list): References to the RPN layers: [RPN_1x1_obj, RPN_1x1_bbox]\n            target_buffers (tuple): Target buffers for training fast-rcnn: (class, bbox regression)\n            num_rois (int, optional): Number of ROIs to sample from proposed (default: 128)\n            pre_nms_N (int, optional): Number of ROIs to retain before using NMS (default: 12000)\n            post_nms_N (int, optional): Number of ROIs to retain after using NMS (default: 2000)\n            nms_thresh (float, optional): Threshold for non-maximum supression (default: 0.7)\n            min_bbox_size (integer, optional): Minimize bboxes side length (default: 16)\n            name (string, optional): Name of layer (default: None)\n        \"\"\"\n        super(ProposalLayer, self).__init__(name)\n\n        self.rpn_obj, self.rpn_bbox = rpn_layers\n        self.num_rois = num_rois\n        self.pre_nms_N = pre_nms_N\n        self.post_nms_N = post_nms_N\n        self.nms_thresh = nms_thresh\n        self.min_bbox_size = min_bbox_size\n        self.num_classes = dataloader.num_classes\n        self.fg_fraction = fg_fraction if fg_fraction else FG_FRACTION\n        self.fg_thresh = fg_thresh if fg_thresh else FG_THRESH\n        self.bg_thresh_hi = bg_thresh_hi if bg_thresh_hi else BG_THRESH_HI\n        self.bg_thresh_lo = bg_thresh_lo if bg_thresh_lo else BG_THRESH_LO\n        self.deterministic = deterministic\n        self.debug = debug\n\n        # the output shape of this layer depends on whether the network\n        # will be used for inference. In inference mode, the output shape is\n        # (5, post_nms_N). For training, a smaller set of rois are sampled, yielding\n        # an output shape of (5, num_rois)\n        self.inference = inference\n\n        # set references to dataset object buffers\n        self.dataloader = dataloader\n        self._conv_height = dataloader.conv_height\n        self._conv_width = dataloader.conv_width\n        self._scale = dataloader.conv_scale\n\n        # generate anchors and load onto device\n        # self._anchors has shape (KHW, 4)\n        self._anchors = generate_all_anchors(self._conv_height, self._conv_width, self._scale)\n        self._dev_anchors = self.be.array(self._anchors)\n        self._num_anchors = self._anchors.shape[0]\n\n    def configure(self, in_obj):\n        super(ProposalLayer, self).configure(in_obj)\n        # set out_shape as the ROI shape\n        if(self.inference):\n            self.out_shape = ((5, self.post_nms_N))\n        else:\n            self.out_shape = ((5, self.num_rois))\n\n        self.in_shape = in_obj.out_shape\n\n        return (in_obj, self)\n\n    def get_description(self, **kwargs):\n        skip = ['rpn_layers', 'global_buffers', 'dataloader']\n        if 'skip' in kwargs:\n            kwargs['skip'].append(skip)\n        else:\n            kwargs['skip'] = skip\n        return super(ProposalLayer, self).get_description(**kwargs)\n\n    def allocate(self):\n        super(ProposalLayer, self).allocate()\n\n        # internal buffer to store transformed proposals\n        self._proposals = self.be.zeros((self._num_anchors, 4))\n\n        # to store the detections (scores + proposals)\n        self.dets = self.be.zeros((self.pre_nms_N, 5))\n\n        # buffer to store proposals after they have sorted\n        # and filtered with NMS.\n        # Note: The buffer has shape (num_ROIs, 5), where each column\n        # is (0, x_min, y_min, x_max, y_max)\n        # This format is designed to be compatible with the\n        # roi-pooling layer fprop code from Fast-RCNN.\n        self.dev_proposals = self.be.zeros((self.post_nms_N, 5))\n\n        # buffer to store proposals after they have been sampled.\n        # this is passed forward during training.\n        self.dev_proposals_filtered = self.be.zeros((self.num_rois, 5))\n\n        # class member as a view to get scores from RPN outputs\n        self.rpn_scores_v = None\n        self.bbox_deltas_v = None\n\n        # create a local buffer for the rpn scores, otherwise, any in-place\n        # memory change will affect the final cost and training\n        self._scores = self.be.zeros((self._num_anchors, 1))\n", "right_context": "\n    def get_proposals(self):\n        return (self.dev_proposals, self.num_proposals)\n\n    def get_top_N_index(self, scores, N):\n        # this function handles scores still being device tensors\n        count = len(np.where(scores.get() > -1)[0])\n        order = scores.get().ravel().argsort()[::-1].tolist()\n        order = order[:count]\n        if N > 0:\n            order = order[:N]\n\n        return order\n\n    def bprop(self, errors, alpha=1.0, beta=0.0):\n        \"\"\"This layer propagate gradients from ROIs back to lower VGG layers\"\"\"\n        self.deltas = errors\n        self.prev_layer.deltas[:] = errors\n        return errors\n\n    def _clip_boxes(self, boxes, im_shape):\n        boxes[:, 0] = self.be.clip(boxes[:, 0], 0, im_shape[0] - 1)\n        boxes[:, 1] = self.be.clip(boxes[:, 1], 0, im_shape[1] - 1)\n        boxes[:, 2] = self.be.clip(boxes[:, 2], 0, im_shape[0] - 1)\n        boxes[:, 3] = self.be.clip(boxes[:, 3], 0, im_shape[1] - 1)\n\n        return boxes\n\n    def _bbox_transform_inv(self, boxes, deltas, output):\n        widths = boxes[:, 2] - boxes[:, 0] + 1.0\n        heights = boxes[:, 3] - boxes[:, 1] + 1.0\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        dx = deltas[:, 0]\n        dy = deltas[:, 1]\n        dw = deltas[:, 2]\n        dh = deltas[:, 3]\n\n        pred_ctr_x = dx * widths + ctr_x\n        pred_ctr_y = dy * heights + ctr_y\n        pred_w = self.be.exp(dw) * widths\n        pred_h = self.be.exp(dh) * heights\n\n        # pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n        # x1\n        output[:, 0] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        output[:, 1] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        output[:, 2] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        output[:, 3] = pred_ctr_y + 0.5 * pred_h\n\n        return output\n\n    def _filter_boxes(self, boxes, min_size):\n        ws = boxes[:, 2] - boxes[:, 0] + 1\n        hs = boxes[:, 3] - boxes[:, 1] + 1\n        keep = (ws >= min_size) * (hs >= min_size)\n\n        return keep\n\n    def _onehot_labels(self, labels):\n        \"\"\"Converts the roi labels from compressed (1, num_rois) shape\n        to the one-hot format required for the global buffers of shape\n        (num_classes, num_rois)\"\"\"\n        labels_full = np.zeros((self.num_classes, self.num_rois))\n        for idx, l in enumerate(labels):\n            labels_full[int(l), idx] = 1\n        return labels_full\n\n    def _get_bbox_regression_labels(self, bbox_target_data, labels):\n        \"\"\"Bounding-box regression targets (bbox_target_data) are stored in a\n        compact form N x (tx, ty, tw, th)\n        This function expands those targets into the 4-of-4*K representation used\n        by the network (i.e. only one class has non-zero targets).\n        Returns:\n            bbox_targets (ndarray): N x 4K blob of regression targets\n            bbox_inside_weights (ndarray): N x 4K blob of loss weights\n        \"\"\"\n        bbox_targets = np.zeros((self.num_rois, 4 * self.num_classes), dtype=np.float32)\n        bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n        inds = np.where(labels > 0)[0]\n        for ind in inds:\n            l = labels[ind]\n            start = int(4 * l)\n            end = int(start + 4)\n            bbox_targets[ind, start:end] = bbox_target_data[ind]\n            bbox_inside_weights[ind, start:end] = 1.0\n        return bbox_targets, bbox_inside_weights\n\n    def _sample_fg_bg(self, max_overlaps):\n        \"\"\"Return sample of at most fg_fraction * num_rois foreground indicies, padding\n        the remaining num_rois with background indicies. Foreground and background labels\n        are determined based on max_overlaps and the thresholds fg_thresh, bg_thresh_hi,\n        bg_thresh_lo.\n        Returns:\n            keep_inds (array): (num_rois,) sampled indicies of bboxes.\n            fg_rois_per_this_image (int): number of fg rois sampled from the image.\n        \"\"\"\n        # Split proposals into foreground and background based on overlap\n        fg_inds = np.where(max_overlaps >= self.fg_thresh)[0]\n        fg_rois_per_image = np.round(self.fg_fraction * self.num_rois)\n\n        # Guard against the case when an image has fewer than fg_rois_per_image foreground RoIs\n        fg_rois_per_this_image = min(fg_rois_per_image, fg_inds.size)\n\n        # Sample foreground regions without replacement\n        if fg_inds.size > 0 and not self.deterministic:\n            fg_inds = self.be.rng.choice(fg_inds, size=int(fg_rois_per_this_image), replace=False)\n        elif fg_inds.size > 0:\n            fg_inds = fg_inds[:fg_rois_per_this_image]\n\n        # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n        bg_inds = np.where((max_overlaps < self.bg_thresh_hi) &\n                           (max_overlaps >= self.bg_thresh_lo))[0]\n\n        # Compute number of background RoIs to take from this image (guarding\n        # against there being fewer than desired)\n        bg_rois_per_this_image = self.num_rois - fg_rois_per_this_image\n        bg_rois_per_this_image = min(bg_rois_per_this_image, bg_inds.size)\n\n        # Sample background regions without replacement\n        if bg_inds.size > 0 and not self.deterministic:\n            bg_inds = self.be.rng.choice(bg_inds, size=int(bg_rois_per_this_image), replace=False)\n        elif bg_inds.size > 0:\n            bg_inds = bg_inds[:bg_rois_per_this_image]\n\n        # The indices that we're selecting (both fg and bg)\n        keep_inds = np.append(fg_inds, bg_inds)\n        return keep_inds, int(fg_rois_per_this_image)\n", "import_text": ["numpy", "neon.layers.layer.Layer", "generate_anchors.generate_all_anchors", "util.compute_targets", "util.calculate_bb_overlap"], "prompt": "\"\"\"\nDescription: This function is for performing forward propagation in a neural network.\n\nArgs:\n    inputs (array-like): The input data to the network.\n    inference (bool): A flag indicating whether the network is in inference mode.\n\nReturns:\n    tuple: A tuple containing the input data and the proposals for the network.\n\nRaises:\n    AssertionError: If the network is configured for inference and the input flag is not set to True.\n\nNotes:\n    This function uses numpy.vstack to stack arrays in sequence vertically (row wise).\n    It uses numpy.round to round the input to the nearest integer.\n    It uses numpy.hstack to stack arrays in sequence horizontally (column wise).\n    It uses numpy.ascontiguousarray to return a contiguous array (with the same data type) in memory.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"\n        fprop function that does no proposal filtering\n        \"\"\"", "function_dependencies": ["numpy.round", "numpy.vstack", "util.calculate_bb_overlap", "util.calculate_bb_overlap.argmax", "util.calculate_bb_overlap.max", "util.compute_targets", "numpy.array", "numpy.zeros", "numpy.ascontiguousarray", "numpy.hstack"], "project_create_time": "2014-10-16T01:00:17+00:00", "project_update_time": "2024-04-14T06:10:37+00:00", "file_create_time": "2016-05-26T20:08:06Z", "file_update_time": "2016-09-20T00:20:03Z", "function_update_time": "2016-05-26T20:08:06Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["numpy.vstack", "numpy.round", "numpy.hstack", "numpy.ascontiguousarray"], "test_function": [{"file_path": "/neon-v2.6.0/neon-2.6.0/examples/faster-rcnn/tests/test_proposal_layer.py", "class_name": null, "function_name": "test_proposal_layer", "code": "\ndef test_proposal_layer(backend_default, fargs):\n\n    np.random.seed(seed=0)\n\n    # Get a backend for tensor allocation\n    be = backend_default\n    be.bsz = 1\n\n    _conv_size, im_shape_arr, SCALE, pre_nms_topN, post_nms_topN, nms_thresh, min_size = fargs\n\n    im_shape = be.zeros((2, 1), dtype=np.float32)\n    im_shape[:] = np.array(im_shape_arr)\n    im_scale = be.ones((1, 1), dtype=np.float32).fill(1.0 / 16.0)\n    SCALE = be.ones((1, 1), dtype=np.float32).fill(SCALE)\n\n    real_H = np.round(im_shape.get()[1] * im_scale.get()).astype(int).reshape((1,))[0]\n    real_W = np.round(im_shape.get()[0] * im_scale.get()).astype(int).reshape((1,))[0]\n\n    frcn_labels = be.zeros((21, 128), dtype=np.int32)\n    frcn_labels_mask = be.zeros(frcn_labels.shape, dtype=np.int32)\n    frcn_bbtargets = be.zeros((21 * 4, 128), dtype=np.float32)\n    frcn_bbmask = be.zeros(frcn_bbtargets.shape, dtype=np.float32)\n\n    gt_boxes = be.zeros((64, 4), dtype=np.float32)\n    gt_boxes[:3, :] = np.array([[262, 210, 323, 338],\n                               [164, 263, 252, 371],\n                               [240, 193, 294, 298]])\n\n    gt_classes = be.zeros((64, 1), dtype=np.int32)\n    gt_classes[:3, :] = np.array([[9], [9], [9]])\n    num_gt_boxes = be.zeros((1, 1), dtype=np.int32).fill(3)\n\n    num_scores = 2 * 9 * _conv_size * _conv_size\n    rpn_obj_scores_dev = be.array(np.random.choice(num_scores * 2, size=num_scores,\n                                  replace=False) / float(num_scores * 2.0))\n    rpn_bbox_deltas_dev = be.array(np.random.random((4 * 9 * _conv_size * _conv_size, 1)))\n\n    RPN_1x1_obj = mock_layer(rpn_obj_scores_dev)\n    RPN_1x1_bbox = mock_layer(rpn_bbox_deltas_dev)\n\n    # Mock loader\n    # mock RPN_1x1_obj and RPN_1x1_bbox\n    # set inference to true to skip proposal target layer\n    mock_loader = mock_dataloader(_conv_size, im_scale, im_shape, SCALE,\n                                  gt_boxes, gt_classes, num_gt_boxes,\n                                  frcn_labels, frcn_labels_mask, frcn_bbtargets, frcn_bbmask)\n\n    prop_layer = ProposalLayer([[RPN_1x1_obj], [RPN_1x1_bbox]], mock_loader,\n                               pre_nms_N=pre_nms_topN, post_nms_N=post_nms_topN,\n                               nms_thresh=nms_thresh, min_bbox_size=min_size, num_rois=128,\n                               deterministic=True, inference=False, debug=True)\n\n    prop_layer.configure(mock_layer([]))\n    prop_layer.allocate()\n\n    # mock input (is not used)\n    inputs = []\n    inputs, dev_proposals = prop_layer.fprop(inputs, inference=False)\n\n    # extract final proposals and scores from the layer without buffered memory like dev_proposals\n    target_proposals = prop_layer.proposals\n    target_scores = prop_layer.scores\n\n    # Prepare PyCaffe Reference Layer\n    prop_layer_ref = PyCaffeProposalLayer()\n\n    # Re-initalize inputs to same as above\n    rpn_obj_scores = rpn_obj_scores_dev.get()\n    rpn_bbox_deltas = rpn_bbox_deltas_dev.get()\n\n    # reshape from (4KHW, 1) -> (1, K4, H, W) format for pycaffe\n    # NB: pycaffe uses A where we use K\n    # rpn_bbox_deltas = rpn_bbox_deltas.reshape((4, -1, _conv_size, _conv_size))\n    # rpn_bbox_deltas = rpn_bbbox_deltas[:, :, :real_H, :real_W].transpose((1, 0, 2, 3))\n    # rpn_bbox_deltas = rpn_bbox_deltas.reshape((1, -1, real_H, real_W))\n\n    # Skip unnecessecary reshaping (previously to match caffe)\n    rpn_bbox_deltas = rpn_bbox_deltas.reshape((4, -1, _conv_size, _conv_size))\n    rpn_bbox_deltas = rpn_bbox_deltas[:, :, :real_H, :real_W].reshape((4, -1)).T\n\n    # reshape from (2KHW, 1) -> (1, K2, H, W)\n    rpn_obj_scores = rpn_obj_scores.reshape((2, -1, _conv_size, _conv_size))\n    rpn_obj_scores = rpn_obj_scores[:, :, :real_H, :real_W].transpose((0, 1, 2, 3))\n    rpn_obj_scores = rpn_obj_scores.reshape((1, -1, real_H, real_W))\n\n    bottom = [None, None, None]\n    bottom[0] = rpn_obj_scores\n    bottom[1] = rpn_bbox_deltas\n    bottom[2] = [im_shape[1], im_shape[0], SCALE]\n\n    top = [None, None]\n\n    prop_layer_ref.setup(bottom, top, pre_nms_topN=pre_nms_topN, post_nms_topN=post_nms_topN,\n                         nms_thresh=nms_thresh, min_size=min_size)\n    prop_layer_ref.forward(bottom, top)\n\n    # Compare proposals and scores from proposal layer\n    assert np.allclose(top[0][:, 1:], target_proposals, atol=1e-5, rtol=1e-4)\n    assert np.allclose(top[1], target_scores, atol=1e-5, rtol=1e-4)\n\n    # Now testing proposal target layer\n    t_bottom = [0, 1]\n    # use target proposals from neon RPN\n    zeros = np.zeros((target_proposals.shape[0], 1), dtype=target_proposals.dtype)\n    t_bottom[0] = np.hstack((zeros, target_proposals))\n    # convert format of gt_boxes from (num_classes, 4) to (num_gt_boxes, 5)\n    # concat the boxes and the classes and clip to num_gt_boxes and pass it in\n    t_bottom[1] = np.hstack((prop_layer.gt_boxes.get(),\n                            prop_layer.gt_classes.get()))[:prop_layer.num_gt_boxes.get()[0][0]]\n\n    t_top = [None, None, None, None, None]\n\n    prop_target_layer_ref = PyCaffeProposalTargetLayer()\n    prop_target_layer_ref.setup(t_bottom, t_top, deterministic=True)\n    prop_target_layer_ref.forward(t_bottom, t_top)\n\n    frcn_bbtargets_reference = np.zeros(frcn_bbtargets.shape, dtype=np.float32)\n    frcn_bbmask_reference = np.zeros(frcn_bbmask.shape, dtype=np.float32)\n\n    frcn_bbtargets_reference[:t_top[2].shape[0]] = t_top[2].T\n    frcn_bbmask_reference[:t_top[3].shape[0]] = t_top[3].T\n\n    neon_labels = np.zeros((frcn_labels.shape[1],))\n    label_mat = (frcn_labels.get() * frcn_labels_mask.get())\n\n    # Convert neon labels into\n    for cls in range(frcn_labels.shape[0]):\n        for idx, elem in enumerate(label_mat[cls]):\n            if elem != 0:\n                neon_labels[idx] = cls\n\n    # Test proposal layer targets against pycaffe layer\n    assert (np.alltrue(t_top[1] == neon_labels))  # target labels\n    assert (np.allclose(frcn_bbtargets_reference, frcn_bbtargets.get(), atol=1e-4))  # target bbox\n    assert (np.alltrue(frcn_bbmask_reference == frcn_bbmask.get()))   # target bbox mask"}]}, {"git_group": "quantumlib", "git_name": "OpenFermion", "version": "v1.6.1", "language": "Python", "project_name": "OpenFermion-v1.6.1.zip", "file_path": "/OpenFermion-v1.6.1/OpenFermion-1.6.1/src/openfermion/linalg/sparse_tools.py", "file_name": "sparse_tools.py", "focal_class": null, "focal_name": "jordan_wigner_sparse", "focal_parameter": ["fermion_operator"], "solution": "def jordan_wigner_sparse(fermion_operator, n_qubits=None):\n    if n_qubits is None:\n        n_qubits = count_qubits(fermion_operator)\n\n    # Create a list of raising and lowering operators for each orbital.\n    jw_operators = []\n    for tensor_factor in range(n_qubits):\n        jw_operators += [\n            (\n                jordan_wigner_ladder_sparse(n_qubits, tensor_factor, 0),\n                jordan_wigner_ladder_sparse(n_qubits, tensor_factor, 1),\n            )\n        ]\n\n    # Construct the Scipy sparse matrix.\n    n_hilbert = 2**n_qubits\n    values_list = [[]]\n    row_list = [[]]\n    column_list = [[]]\n    for term in fermion_operator.terms:\n        coefficient = fermion_operator.terms[term]\n        sparse_matrix = coefficient * scipy.sparse.identity(\n            2**n_qubits, dtype=complex, format='csc'\n        )\n        for ladder_operator in term:\n            sparse_matrix = sparse_matrix * jw_operators[ladder_operator[0]][ladder_operator[1]]\n\n        if coefficient:\n            # Extract triplets from sparse_term.\n            sparse_matrix = sparse_matrix.tocoo(copy=False)\n            values_list.append(sparse_matrix.data)\n            (row, column) = sparse_matrix.nonzero()\n            row_list.append(row)\n            column_list.append(column)\n\n    values_list = numpy.concatenate(values_list)\n    row_list = numpy.concatenate(row_list)\n    column_list = numpy.concatenate(column_list)\n    sparse_operator = scipy.sparse.coo_matrix(\n        (values_list, (row_list, column_list)), shape=(n_hilbert, n_hilbert)\n    ).tocsc(copy=False)\n    sparse_operator.eliminate_zeros()\n    return sparse_operator", "function_signature": "def jordan_wigner_sparse(fermion_operator, n_qubits=None) :", "left_context": "#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n\"\"\"This module provides functions to interface with scipy.sparse.\"\"\"\nimport itertools\nfrom functools import reduce\nimport numpy.linalg\nimport numpy\n\nimport scipy\nimport scipy.sparse\nimport scipy.sparse.linalg\n\nfrom openfermion.ops.operators import FermionOperator, QubitOperator, BosonOperator, QuadOperator\nfrom openfermion.ops.representations import DiagonalCoulombHamiltonian, PolynomialTensor\nfrom openfermion.transforms.opconversions import normal_ordered\nfrom openfermion.utils.indexing import up_index, down_index\nfrom openfermion.utils.operator_utils import count_qubits, is_hermitian\n\n# Make global definitions.\nidentity_csc = scipy.sparse.identity(2, format='csc', dtype=complex)\npauli_x_csc = scipy.sparse.csc_matrix([[0.0, 1.0], [1.0, 0.0]], dtype=complex)\npauli_y_csc = scipy.sparse.csc_matrix([[0.0, -1.0j], [1.0j, 0.0]], dtype=complex)\npauli_z_csc = scipy.sparse.csc_matrix([[1.0, 0.0], [0.0, -1.0]], dtype=complex)\nq_raise_csc = (pauli_x_csc - 1.0j * pauli_y_csc) / 2.0\nq_lower_csc = (pauli_x_csc + 1.0j * pauli_y_csc) / 2.0\npauli_matrix_map = {'I': identity_csc, 'X': pauli_x_csc, 'Y': pauli_y_csc, 'Z': pauli_z_csc}\n\n\ndef wrapped_kronecker(operator_1, operator_2):\n    \"\"\"Return the Kronecker product of two sparse.csc_matrix operators.\"\"\"\n    return scipy.sparse.kron(operator_1, operator_2, 'csc')\n\n\ndef kronecker_operators(*args):\n    \"\"\"Return the Kronecker product of multiple sparse.csc_matrix operators.\"\"\"\n    return reduce(wrapped_kronecker, *args)\n\n\ndef jordan_wigner_ladder_sparse(n_qubits, tensor_factor, ladder_type):\n    r\"\"\"Make a matrix representation of a fermion ladder operator.\n\n    Operators are mapped as follows:\n    a_j^\\dagger -> Z_0 .. Z_{j-1} (X_j - iY_j) / 2\n    a_j -> Z_0 .. Z_{j-1} (X_j + iY_j) / 2\n\n    Args:\n        index: This is a nonzero integer. The integer indicates the tensor\n            factor and the sign indicates raising or lowering.\n        n_qubits(int): Number qubits in the system Hilbert space.\n\n    Returns:\n        The corresponding Scipy sparse matrix.\n    \"\"\"\n    parities = tensor_factor * [pauli_z_csc]\n    identities = [\n        scipy.sparse.identity(2 ** (n_qubits - tensor_factor - 1), dtype=complex, format='csc')\n    ]\n    if ladder_type:\n        operator = kronecker_operators(parities + [q_raise_csc] + identities)\n    else:\n        operator = kronecker_operators(parities + [q_lower_csc] + identities)\n    return operator\n\n", "right_context": "\n\ndef qubit_operator_sparse(qubit_operator, n_qubits=None):\n    \"\"\"Initialize a Scipy sparse matrix from a QubitOperator.\n\n    Args:\n        qubit_operator(QubitOperator): instance of the QubitOperator class.\n        n_qubits (int): Number of qubits.\n\n    Returns:\n        The corresponding Scipy sparse matrix.\n    \"\"\"\n    if n_qubits is None:\n        n_qubits = count_qubits(qubit_operator)\n    if n_qubits < count_qubits(qubit_operator):\n        raise ValueError('Invalid number of qubits specified.')\n\n    # Construct the Scipy sparse matrix.\n    n_hilbert = 2**n_qubits\n    values_list = [[]]\n    row_list = [[]]\n    column_list = [[]]\n\n    # Loop through the terms.\n    for qubit_term in qubit_operator.terms:\n        tensor_factor = 0\n        coefficient = qubit_operator.terms[qubit_term]\n        sparse_operators = [coefficient]\n        for pauli_operator in qubit_term:\n            # Grow space for missing identity operators.\n            if pauli_operator[0] > tensor_factor:\n                identity_qubits = pauli_operator[0] - tensor_factor\n                identity = scipy.sparse.identity(2**identity_qubits, dtype=complex, format='csc')\n                sparse_operators += [identity]\n\n            # Add actual operator to the list.\n            sparse_operators += [pauli_matrix_map[pauli_operator[1]]]\n            tensor_factor = pauli_operator[0] + 1\n\n        # Grow space at end of string unless operator acted on final qubit.\n        if tensor_factor < n_qubits or not qubit_term:\n            identity_qubits = n_qubits - tensor_factor\n            identity = scipy.sparse.identity(2**identity_qubits, dtype=complex, format='csc')\n            sparse_operators += [identity]\n\n        # Extract triplets from sparse_term.\n        sparse_matrix = kronecker_operators(sparse_operators)\n        values_list.append(sparse_matrix.tocoo(copy=False).data)\n        (column, row) = sparse_matrix.nonzero()\n        column_list.append(column)\n        row_list.append(row)\n\n    # Create sparse operator.\n    values_list = numpy.concatenate(values_list)\n    row_list = numpy.concatenate(row_list)\n    column_list = numpy.concatenate(column_list)\n    sparse_operator = scipy.sparse.coo_matrix(\n        (values_list, (row_list, column_list)), shape=(n_hilbert, n_hilbert)\n    ).tocsc(copy=False)\n    sparse_operator.eliminate_zeros()\n    return sparse_operator\n\n\ndef get_linear_qubit_operator_diagonal(qubit_operator, n_qubits=None):\n    \"\"\"Return a linear operator's diagonal elements.\n\n    The main motivation is to use it for Davidson's algorithm, to find out the\n    lowest n eigenvalues and associated eigenvectors.\n\n    Qubit terms with X or Y operators will contribute nothing to the diagonal\n    elements, while I or Z will contribute a factor of 1 or -1 together with\n    the coefficient.\n\n    Args:\n        qubit_operator(QubitOperator): A qubit operator.\n\n    Returns:\n        linear_operator_diagonal(numpy.ndarray): The diagonal elements for\n            LinearQubitOperator(qubit_operator).\n    \"\"\"\n    if n_qubits is None:\n        n_qubits = count_qubits(qubit_operator)\n    if n_qubits < count_qubits(qubit_operator):\n        raise ValueError('Invalid number of qubits specified.')\n\n    n_hilbert = 2**n_qubits\n    zeros_diagonal = numpy.zeros(n_hilbert)\n    ones_diagonal = numpy.ones(n_hilbert)\n    linear_operator_diagonal = zeros_diagonal\n    # Loop through the terms.\n    for qubit_term in qubit_operator.terms:\n        is_zero = False\n        tensor_factor = 0\n        vecs = [ones_diagonal]\n        for pauli_operator in qubit_term:\n            op = pauli_operator[1]\n            if op in ['X', 'Y']:\n                is_zero = True\n                break\n\n            # Split vector by half and half for each bit.\n            if pauli_operator[0] > tensor_factor:\n                vecs = [\n                    v\n                    for iter_v in vecs\n                    for v in numpy.split(iter_v, 2 ** (pauli_operator[0] - tensor_factor))\n                ]\n\n            vec_pairs = [numpy.split(v, 2) for v in vecs]\n            vecs = [v for vp in vec_pairs for v in (vp[0], -vp[1])]\n            tensor_factor = pauli_operator[0] + 1\n        if not is_zero:\n            linear_operator_diagonal += qubit_operator.terms[qubit_term] * numpy.concatenate(vecs)\n    return linear_operator_diagonal\n\n\ndef jw_configuration_state(occupied_orbitals, n_qubits):\n    \"\"\"Function to produce a basis state in the occupation number basis.\n\n    Args:\n        occupied_orbitals(list): A list of integers representing the indices\n            of the occupied orbitals in the desired basis state\n        n_qubits(int): The total number of qubits\n\n    Returns:\n        basis_vector(sparse): The basis state as a sparse matrix\n    \"\"\"\n    one_index = sum(2 ** (n_qubits - 1 - i) for i in occupied_orbitals)\n    basis_vector = numpy.zeros(2**n_qubits, dtype=float)\n    basis_vector[one_index] = 1\n    return basis_vector\n\n\ndef jw_hartree_fock_state(n_electrons, n_orbitals):\n    \"\"\"Function to produce Hartree-Fock state in JW representation.\"\"\"\n    hartree_fock_state = jw_configuration_state(range(n_electrons), n_orbitals)\n    return hartree_fock_state\n\n\ndef jw_number_indices(n_electrons, n_qubits):\n    \"\"\"Return the indices for n_electrons in n_qubits under JW encoding\n\n    Calculates the indices for all possible arrangements of n-electrons\n        within n-qubit orbitals when a Jordan-Wigner encoding is used.\n        Useful for restricting generic operators or vectors to a particular\n        particle number space when desired\n\n    Args:\n        n_electrons(int): Number of particles to restrict the operator to\n        n_qubits(int): Number of qubits defining the total state\n\n    Returns:\n        indices(list): List of indices in a 2^n length array that indicate\n            the indices of constant particle number within n_qubits\n            in a Jordan-Wigner encoding.\n    \"\"\"\n    occupations = itertools.combinations(range(n_qubits), n_electrons)\n    indices = [sum([2**n for n in occupation]) for occupation in occupations]\n    return indices\n\n\ndef jw_sz_indices(sz_value, n_qubits, n_electrons=None, up_index=up_index, down_index=down_index):\n    r\"\"\"Return the indices of basis vectors with fixed Sz under JW encoding.\n\n    The returned indices label computational basis vectors which lie within\n    the corresponding eigenspace of the Sz operator,\n\n    $$\n        \\begin{align}\n        S^{z} = \\frac{1}{2}\\sum_{i = 1}^{n}(n_{i, \\alpha} - n_{i, \\beta})\n        \\end{align}\n    $$\n\n    Args:\n        sz_value(float): Desired Sz value. Should be an integer or\n            half-integer.\n        n_qubits(int): Number of qubits defining the total state\n        n_electrons(int, optional): Number of particles to restrict the\n            operator to, if such a restriction is desired\n        up_index (Callable, optional): Function that maps a spatial index\n            to the index of the corresponding up site\n        down_index (Callable, optional): Function that maps a spatial index\n            to the index of the corresponding down site\n\n    Returns:\n        indices(list): The list of indices\n    \"\"\"\n    if n_qubits % 2 != 0:\n        raise ValueError('Number of qubits must be even')\n\n    if not (2.0 * sz_value).is_integer():\n        raise ValueError('Sz value must be an integer or half-integer')\n\n    n_sites = n_qubits // 2\n    sz_integer = int(2.0 * sz_value)\n    indices = []\n\n    if n_electrons is not None:\n        # Particle number is fixed, so the number of spin-up electrons\n        # (as well as the number of spin-down electrons) is fixed\n        if (n_electrons + sz_integer) % 2 != 0 or n_electrons < abs(sz_integer):\n            raise ValueError('The specified particle number and sz value are ' 'incompatible.')\n        num_up = (n_electrons + sz_integer) // 2\n        num_down = n_electrons - num_up\n        up_occupations = itertools.combinations(range(n_sites), num_up)\n        down_occupations = list(itertools.combinations(range(n_sites), num_down))\n        # Each arrangement of up spins can be paired with an arrangement\n        # of down spins\n        for up_occupation in up_occupations:\n            up_occupation = [up_index(index) for index in up_occupation]\n            for down_occupation in down_occupations:\n                down_occupation = [down_index(index) for index in down_occupation]\n                occupation = up_occupation + down_occupation\n                indices.append(sum(2 ** (n_qubits - 1 - k) for k in occupation))\n    else:\n        # Particle number is not fixed\n        if sz_integer < 0:\n            # There are more down spins than up spins\n            more_map = down_index\n            less_map = up_index\n        else:\n            # There are at least as many up spins as down spins\n            more_map = up_index\n            less_map = down_index\n        for n in range(abs(sz_integer), n_sites + 1):\n            # Choose n of the 'more' spin and n - abs(sz_integer) of the\n            # 'less' spin\n            more_occupations = itertools.combinations(range(n_sites), n)\n            less_occupations = list(itertools.combinations(range(n_sites), n - abs(sz_integer)))\n            # Each arrangement of the 'more' spins can be paired with an\n            # arrangement of the 'less' spin\n            for more_occupation in more_occupations:\n                more_occupation = [more_map(index) for index in more_occupation]\n                for less_occupation in less_occupations:\n                    less_occupation = [less_map(index) for index in less_occupation]\n                    occupation = more_occupation + less_occupation\n                    indices.append(sum(2 ** (n_qubits - 1 - k) for k in occupation))\n\n    return indices\n\n\ndef jw_number_restrict_operator(operator, n_electrons, n_qubits=None):\n    \"\"\"Restrict a Jordan-Wigner encoded operator to a given particle number\n\n    Args:\n        sparse_operator(ndarray or sparse): Numpy operator acting on\n            the space of n_qubits.\n        n_electrons(int): Number of particles to restrict the operator to\n        n_qubits(int): Number of qubits defining the total state\n\n    Returns:\n        new_operator(ndarray or sparse): Numpy operator restricted to\n            acting on states with the same particle number.\n    \"\"\"\n    if n_qubits is None:\n        n_qubits = int(numpy.log2(operator.shape[0]))\n\n    select_indices = jw_number_indices(n_electrons, n_qubits)\n    return operator[numpy.ix_(select_indices, select_indices)]\n\n\ndef jw_sz_restrict_operator(\n    operator, sz_value, n_electrons=None, n_qubits=None, up_index=up_index, down_index=down_index\n):\n    \"\"\"Restrict a Jordan-Wigner encoded operator to a given Sz value\n\n    Args:\n        operator(ndarray or sparse): Numpy operator acting on\n            the space of n_qubits.\n        sz_value(float): Desired Sz value. Should be an integer or\n            half-integer.\n        n_electrons(int, optional): Number of particles to restrict the\n            operator to, if such a restriction is desired.\n        n_qubits(int, optional): Number of qubits defining the total state\n        up_index (Callable, optional): Function that maps a spatial index\n            to the index of the corresponding up site\n        down_index (Callable, optional): Function that maps a spatial index\n            to the index of the corresponding down site\n\n    Returns:\n        new_operator(ndarray or sparse): Numpy operator restricted to\n            acting on states with the desired Sz value.\n    \"\"\"\n    if n_qubits is None:\n        n_qubits = int(numpy.log2(operator.shape[0]))\n\n    select_indices = jw_sz_indices(\n        sz_value, n_qubits, n_electrons=n_electrons, up_index=up_index, down_index=down_index\n    )\n    return operator[numpy.ix_(select_indices, select_indices)]\n\n\ndef jw_number_restrict_state(state, n_electrons, n_qubits=None):\n    \"\"\"Restrict a Jordan-Wigner encoded state to a given particle number\n\n    Args:\n        state(ndarray or sparse): Numpy vector in\n            the space of n_qubits.\n        n_electrons(int): Number of particles to restrict the state to\n        n_qubits(int): Number of qubits defining the total state\n\n    Returns:\n        new_operator(ndarray or sparse): Numpy vector restricted to\n            states with the same particle number. May not be normalized.\n    \"\"\"\n    if n_qubits is None:\n        n_qubits = int(numpy.log2(state.shape[0]))\n\n    select_indices = jw_number_indices(n_electrons, n_qubits)\n    return state[select_indices]\n\n\ndef jw_sz_restrict_state(\n    state, sz_value, n_electrons=None, n_qubits=None, up_index=up_index, down_index=down_index\n):\n    \"\"\"Restrict a Jordan-Wigner encoded state to a given Sz value\n\n    Args:\n        state(ndarray or sparse): Numpy vector in\n            the space of n_qubits.\n        sz_value(float): Desired Sz value. Should be an integer or\n            half-integer.\n        n_electrons(int, optional): Number of particles to restrict the\n            operator to, if such a restriction is desired.\n        n_qubits(int, optional): Number of qubits defining the total state\n        up_index (Callable, optional): Function that maps a spatial index\n            to the index of the corresponding up site\n        down_index (Callable, optional): Function that maps a spatial index\n            to the index of the corresponding down site\n\n    Returns:\n        new_operator(ndarray or sparse): Numpy vector restricted to\n            states with the desired Sz value. May not be normalized.\n    \"\"\"\n    if n_qubits is None:\n        n_qubits = int(numpy.log2(state.shape[0]))\n\n    select_indices = jw_sz_indices(\n        sz_value, n_qubits, n_electrons=n_electrons, up_index=up_index, down_index=down_index\n    )\n    return state[select_indices]\n\n\ndef jw_get_ground_state_at_particle_number(sparse_operator, particle_number):\n    \"\"\"Compute ground energy and state at a specified particle number.\n\n    Assumes the Jordan-Wigner transform. The input operator should be Hermitian\n    and particle-number-conserving.\n\n    Args:\n        sparse_operator(sparse): A Jordan-Wigner encoded sparse matrix.\n        particle_number(int): The particle number at which to compute the ground\n            energy and states\n\n    Returns:\n        ground_energy(float): The lowest eigenvalue of sparse_operator within\n            the eigenspace of the number operator corresponding to\n            particle_number.\n        ground_state(ndarray): The ground state at the particle number\n    \"\"\"\n\n    n_qubits = int(numpy.log2(sparse_operator.shape[0]))\n\n    # Get the operator restricted to the subspace of the desired particle number\n    restricted_operator = jw_number_restrict_operator(sparse_operator, particle_number, n_qubits)\n\n    # Compute eigenvalues and eigenvectors\n    if restricted_operator.shape[0] - 1 <= 1:\n        # Restricted operator too small for sparse eigensolver\n        dense_restricted_operator = restricted_operator.toarray()\n        eigvals, eigvecs = numpy.linalg.eigh(dense_restricted_operator)\n    else:\n        eigvals, eigvecs = scipy.sparse.linalg.eigsh(restricted_operator, k=1, which='SA')\n\n    # Expand the state\n    state = eigvecs[:, 0]\n    expanded_state = numpy.zeros(2**n_qubits, dtype=complex)\n    expanded_state[jw_number_indices(particle_number, n_qubits)] = state\n\n    return eigvals[0], expanded_state\n\n\ndef jw_sparse_givens_rotation(i, j, theta, phi, n_qubits):\n    \"\"\"Return the matrix (acting on a full wavefunction) that performs a\n    Givens rotation of modes i and j in the Jordan-Wigner encoding.\"\"\"\n    if j != i + 1:\n        raise ValueError('Only adjacent modes can be rotated.')\n    if j > n_qubits - 1:\n        raise ValueError('Too few qubits requested.')\n\n    cosine = numpy.cos(theta)\n    sine = numpy.sin(theta)\n    phase = numpy.exp(1.0j * phi)\n\n    # Create the two-qubit rotation matrix\n    rotation_matrix = scipy.sparse.csc_matrix(\n        (\n            [1.0, phase * cosine, -phase * sine, sine, cosine, phase],\n            ((0, 1, 1, 2, 2, 3), (0, 1, 2, 1, 2, 3)),\n        ),\n        shape=(4, 4),\n        dtype=numpy.complex128,\n    )\n\n    # Initialize identity operators\n    left_eye = scipy.sparse.eye(2**i, format='csc')\n    right_eye = scipy.sparse.eye(2 ** (n_qubits - 1 - j), format='csc')\n\n    # Construct the matrix and return\n    givens_matrix = kronecker_operators([left_eye, rotation_matrix, right_eye])\n\n    return givens_matrix\n\n\ndef jw_sparse_particle_hole_transformation_last_mode(n_qubits):\n    \"\"\"Return the matrix (acting on a full wavefunction) that performs a\n    particle-hole transformation on the last mode in the Jordan-Wigner\n    encoding.\n    \"\"\"\n    left_eye = scipy.sparse.eye(2 ** (n_qubits - 1), format='csc')\n    return kronecker_operators([left_eye, pauli_matrix_map['X']])\n\n\ndef get_density_matrix(states, probabilities):\n    n_qubits = states[0].shape[0]\n    density_matrix = scipy.sparse.csc_matrix((n_qubits, n_qubits), dtype=complex)\n    for state, probability in zip(states, probabilities):\n        state = scipy.sparse.csc_matrix(state.reshape((len(state), 1)))\n        density_matrix = density_matrix + probability * state * state.getH()\n    return density_matrix\n\n\ndef get_ground_state(sparse_operator, initial_guess=None):\n    \"\"\"Compute lowest eigenvalue and eigenstate.\n\n    Args:\n        sparse_operator (LinearOperator): Operator to find the ground state of.\n        initial_guess (ndarray): Initial guess for ground state.  A good\n            guess dramatically reduces the cost required to converge.\n\n    Returns\n    -------\n        eigenvalue:\n            The lowest eigenvalue, a float.\n        eigenstate:\n            The lowest eigenstate in scipy.sparse csc format.\n    \"\"\"\n    values, vectors = scipy.sparse.linalg.eigsh(\n        sparse_operator, k=1, v0=initial_guess, which='SA', maxiter=1e7\n    )\n\n    order = numpy.argsort(values)\n    values = values[order]\n    vectors = vectors[:, order]\n    eigenvalue = values[0]\n    eigenstate = vectors[:, 0]\n    return eigenvalue, eigenstate.T\n\n\ndef eigenspectrum(operator, n_qubits=None):\n    \"\"\"Compute the eigenspectrum of an operator.\n\n    WARNING: This function has cubic runtime in dimension of\n        Hilbert space operator, which might be exponential.\n\n    NOTE: This function does not currently support\n        QuadOperator and BosonOperator.\n\n    Args:\n        operator: QubitOperator, InteractionOperator, FermionOperator,\n            PolynomialTensor, or InteractionRDM.\n        n_qubits (int): number of qubits/modes in operator. if None, will\n            be counted.\n\n    Returns:\n        spectrum: dense numpy array of floats giving eigenspectrum.\n    \"\"\"\n    if isinstance(operator, (QuadOperator, BosonOperator)):\n        raise TypeError('Operator of invalid type.')\n    sparse_operator = get_sparse_operator(operator, n_qubits)\n    spectrum = sparse_eigenspectrum(sparse_operator)\n    return spectrum\n\n\ndef sparse_eigenspectrum(sparse_operator):\n    \"\"\"Perform a dense diagonalization.\n\n    Returns:\n        eigenspectrum: The lowest eigenvalues in a numpy array.\n    \"\"\"\n    dense_operator = sparse_operator.todense()\n    if is_hermitian(sparse_operator):\n        eigenspectrum = numpy.linalg.eigvalsh(dense_operator)\n    else:\n        eigenspectrum = numpy.linalg.eigvals(dense_operator)\n    return numpy.sort(eigenspectrum)\n\n\ndef expectation(operator, state):\n    \"\"\"Compute the expectation value of an operator with a state.\n\n    Args:\n        operator(scipy.sparse.spmatrix or scipy.sparse.linalg.LinearOperator):\n            The operator whose expectation value is desired.\n        state(numpy.ndarray or scipy.sparse.spmatrix): A numpy array\n            representing a pure state or a sparse matrix representing a density\n            matrix. If `operator` is a LinearOperator, then this must be a\n            numpy array.\n\n    Returns:\n        A complex number giving the expectation value.\n\n    Raises:\n        ValueError: Input state has invalid format.\n    \"\"\"\n\n    if isinstance(state, scipy.sparse.spmatrix):\n        # Handle density matrix.\n        if isinstance(operator, scipy.sparse.linalg.LinearOperator):\n            raise ValueError(\n                'Taking the expectation of a LinearOperator with '\n                'a density matrix is not supported.'\n            )\n        product = state * operator\n        expectation = numpy.sum(product.diagonal())\n\n    elif isinstance(state, numpy.ndarray):\n        # Handle state vector.\n        if len(state.shape) == 1:\n            # Row vector\n            expectation = numpy.dot(numpy.conjugate(state), operator * state)\n        else:\n            # Column vector\n            expectation = numpy.dot(numpy.conjugate(state.T), operator * state)[0, 0]\n\n    else:\n        # Handle exception.\n        raise ValueError('Input state must be a numpy array or a sparse matrix.')\n\n    # Return.\n    return expectation\n\n\ndef variance(operator, state):\n    \"\"\"Compute variance of operator with a state.\n\n    Args:\n        operator(scipy.sparse.spmatrix or scipy.sparse.linalg.LinearOperator):\n            The operator whose expectation value is desired.\n        state(numpy.ndarray or scipy.sparse.spmatrix): A numpy array\n            representing a pure state or a sparse matrix representing a density\n            matrix.\n\n    Returns:\n        A complex number giving the variance.\n\n    Raises:\n        ValueError: Input state has invalid format.\n    \"\"\"\n    return expectation(operator**2, state) - expectation(operator, state) ** 2\n\n\ndef expectation_computational_basis_state(operator, computational_basis_state):\n    \"\"\"Compute expectation value of operator with a  state.\n\n    Args:\n        operator: Qubit or FermionOperator to evaluate expectation value of.\n                  If operator is a FermionOperator, it must be normal-ordered.\n        computational_basis_state (scipy.sparse vector / list): normalized\n            computational basis state (if scipy.sparse vector), or list of\n            occupied orbitals.\n\n    Returns:\n        A real float giving expectation value.\n\n    Raises:\n        TypeError: Incorrect operator or state type.\n    \"\"\"\n    if isinstance(operator, QubitOperator):\n        raise NotImplementedError('Not yet implemented for QubitOperators.')\n\n    if not isinstance(operator, FermionOperator):\n        raise TypeError('operator must be a FermionOperator.')\n\n    occupied_orbitals = computational_basis_state\n\n    if not isinstance(occupied_orbitals, list):\n        computational_basis_state_index = occupied_orbitals.nonzero()[0][0]\n\n        occupied_orbitals = [digit == '1' for digit in bin(computational_basis_state_index)[2:]][\n            ::-1\n        ]\n\n    expectation_value = operator.terms.get((), 0.0)\n\n    for i in range(len(occupied_orbitals)):\n        if occupied_orbitals[i]:\n            expectation_value += operator.terms.get(((i, 1), (i, 0)), 0.0)\n\n            for j in range(i + 1, len(occupied_orbitals)):\n                expectation_value -= operator.terms.get(((j, 1), (i, 1), (j, 0), (i, 0)), 0.0)\n\n    return expectation_value\n\n\ndef expectation_db_operator_with_pw_basis_state(\n    operator, plane_wave_occ_orbitals, n_spatial_orbitals, grid, spinless\n):\n    \"\"\"Compute expectation value of a dual basis operator with a plane\n    wave computational basis state.\n\n    Args:\n        operator: Dual-basis representation of FermionOperator to evaluate\n                  expectation value of. Can have at most 3-body terms.\n        plane_wave_occ_orbitals (list): list of occupied plane-wave orbitals.\n        n_spatial_orbitals (int): Number of spatial orbitals.\n        grid (openfermion.utils.Grid): The grid used for discretization.\n        spinless (bool): Whether the system is spinless.\n\n    Returns:\n        A real float giving the expectation value.\n    \"\"\"\n    expectation_value = operator.terms.get((), 0.0)\n\n    for single_action, coefficient in operator.terms.items():\n        if len(single_action) == 2:\n            expectation_value += coefficient * (\n                expectation_one_body_db_operator_computational_basis_state(\n                    single_action, plane_wave_occ_orbitals, grid, spinless\n                )\n                / n_spatial_orbitals\n            )\n\n        elif len(single_action) == 4:\n            expectation_value += coefficient * (\n                expectation_two_body_db_operator_computational_basis_state(\n                    single_action, plane_wave_occ_orbitals, grid, spinless\n                )\n                / n_spatial_orbitals**2\n            )\n\n        elif len(single_action) == 6:\n            expectation_value += coefficient * (\n                expectation_three_body_db_operator_computational_basis_state(\n                    single_action, plane_wave_occ_orbitals, grid, spinless\n                )\n                / n_spatial_orbitals**3\n            )\n\n    return expectation_value\n\n\ndef expectation_one_body_db_operator_computational_basis_state(\n    dual_basis_action, plane_wave_occ_orbitals, grid, spinless\n):\n    \"\"\"Compute expectation value of a 1-body dual-basis operator with a\n    plane wave computational basis state.\n\n    Args:\n        dual_basis_action: Dual-basis action of FermionOperator to\n                           evaluate expectation value of.\n        plane_wave_occ_orbitals (list): list of occupied plane-wave orbitals.\n        grid (openfermion.utils.Grid): The grid used for discretization.\n        spinless (bool): Whether the system is spinless.\n\n    Returns:\n        A real float giving the expectation value.\n    \"\"\"\n    expectation_value = 0.0\n\n    r_p = grid.position_vector(grid.grid_indices(dual_basis_action[0][0], spinless))\n    r_q = grid.position_vector(grid.grid_indices(dual_basis_action[1][0], spinless))\n\n    for orbital in plane_wave_occ_orbitals:\n        # If there's spin, p and q have to have the same parity (spin),\n        # and the new orbital has to have the same spin as these.\n        k_orbital = grid.momentum_vector(grid.grid_indices(orbital, spinless))\n        # The Fourier transform is spin-conserving. This means that p, q,\n        # and the new orbital all have to have the same spin (parity).\n        if spinless or (dual_basis_action[0][0] % 2 == dual_basis_action[1][0] % 2 == orbital % 2):\n            expectation_value += numpy.exp(-1j * k_orbital.dot(r_p - r_q))\n\n    return expectation_value\n\n\ndef expectation_two_body_db_operator_computational_basis_state(\n    dual_basis_action, plane_wave_occ_orbitals, grid, spinless\n):\n    \"\"\"Compute expectation value of a 2-body dual-basis operator with a\n    plane wave computational basis state.\n\n    Args:\n        dual_basis_action: Dual-basis action of FermionOperator to\n                           evaluate expectation value of.\n        plane_wave_occ_orbitals (list): list of occupied plane-wave orbitals.\n        grid (openfermion.utils.Grid): The grid used for discretization.\n        spinless (bool): Whether the system is spinless.\n\n    Returns:\n        A float giving the expectation value.\n    \"\"\"\n    expectation_value = 0.0\n\n    r = {}\n    for i in range(4):\n        r[i] = grid.position_vector(grid.grid_indices(dual_basis_action[i][0], spinless))\n\n    rr = {}\n    k_map = {}\n    for i in range(2):\n        rr[i] = {}\n        k_map[i] = {}\n        for j in range(2, 4):\n            rr[i][j] = r[i] - r[j]\n            k_map[i][j] = {}\n\n    # Pre-computations.\n    for o in plane_wave_occ_orbitals:\n        k = grid.momentum_vector(grid.grid_indices(o, spinless))\n        for i in range(2):\n            for j in range(2, 4):\n                k_map[i][j][o] = k.dot(rr[i][j])\n\n    for orbital1 in plane_wave_occ_orbitals:\n        k1ac = k_map[0][2][orbital1]\n        k1ad = k_map[0][3][orbital1]\n\n        for orbital2 in plane_wave_occ_orbitals:\n            if orbital1 != orbital2:\n                k2bc = k_map[1][2][orbital2]\n                k2bd = k_map[1][3][orbital2]\n\n                # The Fourier transform is spin-conserving. This means that\n                # the parity of the orbitals involved in the transition must\n                # be the same.\n                if spinless or (\n                    (dual_basis_action[0][0] % 2 == dual_basis_action[3][0] % 2 == orbital1 % 2)\n                    and (dual_basis_action[1][0] % 2 == dual_basis_action[2][0] % 2 == orbital2 % 2)\n                ):\n                    value = numpy.exp(-1j * (k1ad + k2bc))\n\n                    # Add because it came from two anti-commutations.\n                    expectation_value += value\n\n                # The Fourier transform is spin-conserving. This means that\n                # the parity of the orbitals involved in the transition must\n                # be the same.\n                if spinless or (\n                    (dual_basis_action[0][0] % 2 == dual_basis_action[2][0] % 2 == orbital1 % 2)\n                    and (dual_basis_action[1][0] % 2 == dual_basis_action[3][0] % 2 == orbital2 % 2)\n                ):\n                    value = numpy.exp(-1j * (k1ac + k2bd))\n\n                    # Subtract because it came from a single anti-commutation.\n                    expectation_value -= value\n\n    return expectation_value\n\n\ndef expectation_three_body_db_operator_computational_basis_state(\n    dual_basis_action, plane_wave_occ_orbitals, grid, spinless\n):\n    \"\"\"Compute expectation value of a 3-body dual-basis operator with a\n    plane wave computational basis state.\n\n    Args:\n        dual_basis_action: Dual-basis action of FermionOperator to\n                           evaluate expectation value of.\n        plane_wave_occ_orbitals (list): list of occupied plane-wave orbitals.\n        grid (openfermion.utils.Grid): The grid used for discretization.\n        spinless (bool): Whether the system is spinless.\n\n    Returns:\n        A float giving the expectation value.\n    \"\"\"\n    expectation_value = 0.0\n\n    r = {}\n    for i in range(6):\n        r[i] = grid.position_vector(grid.grid_indices(dual_basis_action[i][0], spinless))\n\n    rr = {}\n    k_map = {}\n    for i in range(3):\n        rr[i] = {}\n        k_map[i] = {}\n        for j in range(3, 6):\n            rr[i][j] = r[i] - r[j]\n            k_map[i][j] = {}\n\n    # Pre-computations.\n    for o in plane_wave_occ_orbitals:\n        k = grid.momentum_vector(grid.grid_indices(o, spinless))\n        for i in range(3):\n            for j in range(3, 6):\n                k_map[i][j][o] = k.dot(rr[i][j])\n\n    for orbital1 in plane_wave_occ_orbitals:\n        k1ad = k_map[0][3][orbital1]\n        k1ae = k_map[0][4][orbital1]\n        k1af = k_map[0][5][orbital1]\n\n        for orbital2 in plane_wave_occ_orbitals:\n            if orbital1 != orbital2:\n                k2bd = k_map[1][3][orbital2]\n                k2be = k_map[1][4][orbital2]\n                k2bf = k_map[1][5][orbital2]\n\n                for orbital3 in plane_wave_occ_orbitals:\n                    if orbital1 != orbital3 and orbital2 != orbital3:\n                        k3cd = k_map[2][3][orbital3]\n                        k3ce = k_map[2][4][orbital3]\n                        k3cf = k_map[2][5][orbital3]\n\n                        # Handle \\delta_{ad} \\delta_{bf} \\delta_{ce} after FT.\n                        # The Fourier transform is spin-conserving.\n                        if spinless or (\n                            (\n                                dual_basis_action[0][0] % 2\n                                == dual_basis_action[3][0] % 2\n                                == orbital1 % 2\n                            )\n                            and (\n                                dual_basis_action[1][0] % 2\n                                == dual_basis_action[5][0] % 2\n                                == orbital2 % 2\n                            )\n                            and (\n                                dual_basis_action[2][0] % 2\n                                == dual_basis_action[4][0] % 2\n                                == orbital3 % 2\n                            )\n                        ):\n                            expectation_value += numpy.exp(-1j * (k1ad + k2bf + k3ce))\n\n                        # Handle -\\delta_{ad} \\delta_{be} \\delta_{cf} after FT.\n                        # The Fourier transform is spin-conserving.\n                        if spinless or (\n                            (\n                                dual_basis_action[0][0] % 2\n                                == dual_basis_action[3][0] % 2\n                                == orbital1 % 2\n                            )\n                            and (\n                                dual_basis_action[1][0] % 2\n                                == dual_basis_action[4][0] % 2\n                                == orbital2 % 2\n                            )\n                            and (\n                                dual_basis_action[2][0] % 2\n                                == dual_basis_action[5][0] % 2\n                                == orbital3 % 2\n                            )\n                        ):\n                            expectation_value -= numpy.exp(-1j * (k1ad + k2be + k3cf))\n\n                        # Handle -\\delta_{ae} \\delta_{bf} \\delta_{cd} after FT.\n                        # The Fourier transform is spin-conserving.\n                        if spinless or (\n                            (\n                                dual_basis_action[0][0] % 2\n                                == dual_basis_action[4][0] % 2\n                                == orbital1 % 2\n                            )\n                            and (\n                                dual_basis_action[1][0] % 2\n                                == dual_basis_action[5][0] % 2\n                                == orbital2 % 2\n                            )\n                            and (\n                                dual_basis_action[2][0] % 2\n                                == dual_basis_action[3][0] % 2\n                                == orbital3 % 2\n                            )\n                        ):\n                            expectation_value -= numpy.exp(-1j * (k1ae + k2bf + k3cd))\n\n                        # Handle \\delta_{ae} \\delta_{bd} \\delta_{cf} after FT.\n                        # The Fourier transform is spin-conserving.\n                        if spinless or (\n                            (\n                                dual_basis_action[0][0] % 2\n                                == dual_basis_action[4][0] % 2\n                                == orbital1 % 2\n                            )\n                            and (\n                                dual_basis_action[1][0] % 2\n                                == dual_basis_action[3][0] % 2\n                                == orbital2 % 2\n                            )\n                            and (\n                                dual_basis_action[2][0] % 2\n                                == dual_basis_action[5][0] % 2\n                                == orbital3 % 2\n                            )\n                        ):\n                            expectation_value += numpy.exp(-1j * (k1ae + k2bd + k3cf))\n\n                        # Handle \\delta_{af} \\delta_{be} \\delta_{cd} after FT.\n                        # The Fourier transform is spin-conserving.\n                        if spinless or (\n                            (\n                                dual_basis_action[0][0] % 2\n                                == dual_basis_action[5][0] % 2\n                                == orbital1 % 2\n                            )\n                            and (\n                                dual_basis_action[1][0] % 2\n                                == dual_basis_action[4][0] % 2\n                                == orbital2 % 2\n                            )\n                            and (\n                                dual_basis_action[2][0] % 2\n                                == dual_basis_action[3][0] % 2\n                                == orbital3 % 2\n                            )\n                        ):\n                            expectation_value += numpy.exp(-1j * (k1af + k2be + k3cd))\n\n                        # Handle -\\delta_{af} \\delta_{bd} \\delta_{ce} after FT.\n                        # The Fourier transform is spin-conserving.\n                        if spinless or (\n                            (\n                                dual_basis_action[0][0] % 2\n                                == dual_basis_action[5][0] % 2\n                                == orbital1 % 2\n                            )\n                            and (\n                                dual_basis_action[1][0] % 2\n                                == dual_basis_action[3][0] % 2\n                                == orbital2 % 2\n                            )\n                            and (\n                                dual_basis_action[2][0] % 2\n                                == dual_basis_action[4][0] % 2\n                                == orbital3 % 2\n                            )\n                        ):\n                            expectation_value -= numpy.exp(-1j * (k1af + k2bd + k3ce))\n\n    return expectation_value\n\n\ndef get_gap(sparse_operator, initial_guess=None):\n    \"\"\"Compute gap between lowest eigenvalue and first excited state.\n\n    Args:\n        sparse_operator (LinearOperator): Operator to find the ground state of.\n        initial_guess (ndarray): Initial guess for eigenspace.  A good\n            guess dramatically reduces the cost required to converge.\n    Returns: A real float giving eigenvalue gap.\n    \"\"\"\n    if not is_hermitian(sparse_operator):\n        raise ValueError('sparse_operator must be Hermitian.')\n\n    values, _ = scipy.sparse.linalg.eigsh(\n        sparse_operator, k=2, v0=initial_guess, which='SA', maxiter=1e7\n    )\n\n    gap = abs(values[1] - values[0])\n    return gap\n\n\ndef inner_product(state_1, state_2):\n    \"\"\"Compute inner product of two states.\"\"\"\n    return numpy.dot(state_1.conjugate(), state_2)\n\n\ndef boson_ladder_sparse(n_modes, mode, ladder_type, trunc):\n    r\"\"\"Make a matrix representation of a singular bosonic ladder operator\n    in the Fock space.\n\n    Since the bosonic operator lies in an infinite Fock space,\n    a truncation value needs to be provide so that a sparse matrix\n    of finite size can be returned.\n\n    Args:\n        n_modes (int): Number of modes in the system Hilbert space.\n        mode (int): The mode the ladder operator targets.\n        ladder_type (int): This is a nonzero integer. 0 indicates a lowering\n            operator, 1 a raising operator.\n        trunc (int): The size at which the Fock space should be truncated\n            when returning the matrix representing the ladder operator.\n\n    Returns:\n        The corresponding trunc x trunc Scipy sparse matrix.\n    \"\"\"\n    if trunc < 1 or not isinstance(trunc, int):\n        raise ValueError(\"Fock space truncation must be a positive integer.\")\n\n    if ladder_type:\n        lop = scipy.sparse.spdiags(numpy.sqrt(range(1, trunc)), -1, trunc, trunc, format='csc')\n    else:\n        lop = scipy.sparse.spdiags(numpy.sqrt(range(trunc)), 1, trunc, trunc, format='csc')\n\n    Id = [scipy.sparse.identity(trunc, format='csc', dtype=complex)]\n    operator_list = Id * mode + [lop] + Id * (n_modes - mode - 1)\n    operator = kronecker_operators(operator_list)\n\n    return operator\n\n\ndef single_quad_op_sparse(n_modes, mode, quadrature, hbar, trunc):\n    r\"\"\"Make a matrix representation of a singular quadrature\n    operator in the Fock space.\n\n    Since the bosonic operators lie in an infinite Fock space,\n    a truncation value needs to be provide so that a sparse matrix\n    of finite size can be returned.\n\n    Args:\n        n_modes (int): Number of modes in the system Hilbert space.\n        mode (int): The mode the ladder operator targets.\n        quadrature (str): 'q' for the canonical position operator,\n            'p' for the canonical moment]um operator.\n        hbar (float): the value of hbar to use in the definition of the\n            canonical commutation relation [q_i, p_j] = \\delta_{ij} i hbar.\n        trunc (int): The size at which the Fock space should be truncated\n            when returning the matrix representing the ladder operator.\n\n    Returns:\n        The corresponding trunc x trunc Scipy sparse matrix.\n    \"\"\"\n    if trunc < 1 or not isinstance(trunc, int):\n        raise ValueError(\"Fock space truncation must be a positive integer.\")\n\n    b = boson_ladder_sparse(1, 0, 0, trunc)\n\n    if quadrature == 'q':\n        op = numpy.sqrt(hbar / 2) * (b + b.conj().T)\n    elif quadrature == 'p':\n        op = -1j * numpy.sqrt(hbar / 2) * (b - b.conj().T)\n\n    Id = [scipy.sparse.identity(trunc, dtype=complex, format='csc')]\n    operator_list = Id * mode + [op] + Id * (n_modes - mode - 1)\n    operator = kronecker_operators(operator_list)\n\n    return operator\n\n\ndef boson_operator_sparse(operator, trunc, hbar=1.0):\n    r\"\"\"Initialize a Scipy sparse matrix in the Fock space\n    from a bosonic operator.\n\n    Since the bosonic operators lie in an infinite Fock space,\n    a truncation value needs to be provide so that a sparse matrix\n    of finite size can be returned.\n\n    Args:\n        operator: One of either BosonOperator or QuadOperator.\n        trunc (int): The size at which the Fock space should be truncated\n            when returning the matrix representing the ladder operator.\n        hbar (float): the value of hbar to use in the definition of the\n            canonical commutation relation [q_i, p_j] = \\delta_{ij} i hbar.\n            This only applies if calcualating the sparse representation of\n            a quadrature operator.\n\n    Returns:\n        The corresponding Scipy sparse matrix of size [trunc, trunc].\n    \"\"\"\n    if isinstance(operator, QuadOperator):\n        from openfermion.transforms.opconversions import get_boson_operator\n\n        boson_operator = get_boson_operator(operator, hbar)\n    elif isinstance(operator, BosonOperator):\n        boson_operator = operator\n    else:\n        raise ValueError(\"Only BosonOperator and QuadOperator are supported.\")\n\n    if trunc < 1 or not isinstance(trunc, int):\n        raise ValueError(\"Fock space truncation must be a positive integer.\")\n\n    # count the number of modes\n    n_modes = 0\n    for term in boson_operator.terms:\n        for ladder_operator in term:\n            if ladder_operator[0] + 1 > n_modes:\n                n_modes = ladder_operator[0] + 1\n\n    # Construct the Scipy sparse matrix.\n    n_hilbert = trunc**n_modes\n    values_list = [[]]\n    row_list = [[]]\n    column_list = [[]]\n\n    # Loop through the terms.\n    for term in boson_operator.terms:\n        coefficient = boson_operator.terms[term]\n        term_operator = coefficient * scipy.sparse.identity(n_hilbert, dtype=complex, format='csc')\n\n        for ladder_op in term:\n            # Add actual operator to the list.\n            b = boson_ladder_sparse(n_modes, ladder_op[0], ladder_op[1], trunc)\n            term_operator = term_operator.dot(b)\n\n        # Extract triplets from sparse_term.\n        values_list.append(term_operator.tocoo(copy=False).data)\n        (row, column) = term_operator.nonzero()\n        column_list.append(column)\n        row_list.append(row)\n\n    # Create sparse operator.\n    values_list = numpy.concatenate(values_list)\n    row_list = numpy.concatenate(row_list)\n    column_list = numpy.concatenate(column_list)\n    sparse_operator = scipy.sparse.coo_matrix(\n        (values_list, (row_list, column_list)), shape=(n_hilbert, n_hilbert)\n    ).tocsc(copy=False)\n    sparse_operator.eliminate_zeros()\n    return sparse_operator\n\n\ndef get_sparse_operator(operator, n_qubits=None, trunc=None, hbar=1.0):\n    r\"\"\"Map an operator to a sparse matrix.\n\n    If the input is not a QubitOperator, the Jordan-Wigner Transform is used.\n\n    Args:\n        operator: Currently supported operators include:\n            FermionOperator, QubitOperator, DiagonalCoulombHamiltonian,\n            PolynomialTensor, BosonOperator, QuadOperator.\n        n_qubits(int): Number qubits in the system Hilbert space.\n            Applicable only to fermionic systems.\n        trunc (int): The size at which the Fock space should be truncated.\n            Applicable only to bosonic systems.\n        hbar (float): the value of hbar to use in the definition of the\n            canonical commutation relation [q_i, p_j] = \\delta_{ij} i hbar.\n            Applicable only to the QuadOperator.\n    \"\"\"\n    from openfermion.transforms.opconversions import get_fermion_operator\n\n    if isinstance(operator, (DiagonalCoulombHamiltonian, PolynomialTensor)):\n        return jordan_wigner_sparse(get_fermion_operator(operator))\n    elif isinstance(operator, FermionOperator):\n        return jordan_wigner_sparse(operator, n_qubits)\n    elif isinstance(operator, QubitOperator):\n        return qubit_operator_sparse(operator, n_qubits)\n    elif isinstance(operator, (BosonOperator, QuadOperator)):\n        return boson_operator_sparse(operator, trunc, hbar)\n    else:\n        raise TypeError(\n            'Failed to convert a {} to a sparse matrix.'.format(type(operator).__name__)\n        )\n\n\ndef get_number_preserving_sparse_operator(\n    fermion_op,\n    num_qubits,\n    num_electrons,\n    spin_preserving=False,\n    reference_determinant=None,\n    excitation_level=None,\n):\n    \"\"\"Initialize a Scipy sparse matrix in a specific symmetry sector.\n\n    This method initializes a Scipy sparse matrix from a FermionOperator,\n    explicitly working in a particular particle number sector. Optionally, it\n    can also restrict the space to contain only states with a particular Sz.\n\n    Finally, the Hilbert space can also be restricted to only those states\n    which are reachable by excitations up to a fixed rank from an initial\n    reference determinant.\n\n    Args:\n        fermion_op(FermionOperator): An instance of the FermionOperator class.\n            It should not contain terms which do not preserve particle number.\n            If spin_preserving is set to True it should also not contain terms\n            which do not preserve the Sz (it is assumed that the ordering of\n            the indices goes alpha, beta, alpha, beta, ...).\n        num_qubits(int): The total number of qubits / spin-orbitals in the\n            system.\n        num_electrons(int): The number of particles in the desired Hilbert\n            space.\n        spin_preserving(bool): Whether or not the constructed operator should\n            be defined in a space which has support only on states with the\n            same Sz value as the reference_determinant.\n        reference_determinant(list(bool)): A list, whose length is equal to\n            num_qubits, which specifies which orbitals should be occupied in\n            the reference state. If spin_preserving is set to True then the Sz\n            value of this reference state determines the Sz value of the\n            symmetry sector in which the generated operator acts. If a value\n            for excitation_level is provided then the excitations are generated\n            with respect to the reference state. In any case, the ordering of\n            the states in the matrix representation of the operator depends on\n            reference_determinant and the state corresponding to\n            reference_determinant is the vector [1.0, 0.0, 0.0 ... 0.0]. Can be\n            set to None in order to take the first num_electrons orbitals to be\n            the occupied orbitals.\n        excitation_level(int): The number of excitations from the reference\n            state which should be included in the generated operator's matrix\n            representation. Can be set to None to include all levels of\n            excitation.\n\n    Returns:\n        sparse_op(scipy.sparse.csc_matrix): A sparse matrix representation of\n            fermion_op in the basis set by the arguments.\n    \"\"\"\n\n    # We use the Hartree-Fock determinant as a reference if none is provided.\n    if reference_determinant is None:\n        reference_determinant = numpy.array([i < num_electrons for i in range(num_qubits)])\n    else:\n        reference_determinant = numpy.asarray(reference_determinant)\n\n    if excitation_level is None:\n        excitation_level = num_electrons\n\n    state_array = numpy.asarray(\n        list(_iterate_basis_(reference_determinant, excitation_level, spin_preserving))\n    )\n    # Create a 1d array with each determinant encoded\n    # as an integer for sorting purposes.\n    int_state_array = state_array.dot(1 << numpy.arange(state_array.shape[1])[::-1])\n    sorting_indices = numpy.argsort(int_state_array)\n\n    space_size = state_array.shape[0]\n\n    fermion_op = normal_ordered(fermion_op)\n\n    sparse_op = scipy.sparse.csc_matrix((space_size, space_size), dtype=float)\n\n    for term, coefficient in fermion_op.terms.items():\n        if len(term) == 0:\n            constant = coefficient * scipy.sparse.identity(space_size, dtype=float, format='csc')\n\n            sparse_op += constant\n\n        else:\n            term_op = _build_term_op_(term, state_array, int_state_array, sorting_indices)\n\n            sparse_op += coefficient * term_op\n\n    return sparse_op\n\n\ndef _iterate_basis_(reference_determinant, excitation_level, spin_preserving):\n    \"\"\"A helper method which iterates over the specified basis states.\n\n    Note that this method always yields the states in order of their excitation\n    rank from the reference_determinant.\n\n    Args:\n        reference_determinant(list(bool)): A list of bools which indicates\n            which orbitals are occupied and which are unoccupied in the\n            reference state.\n        excitation_level(int): The maximum excitation rank to iterate over.\n        spin_preserving(bool): A bool which, if set to True, constrains the\n            method to iterate over only those states which have the same Sz as\n            reference_determinant.\n\n    Yields:\n        Lists of bools which indicate which orbitals are occupied and which are\n            unoccupied in the current determinant.\n    \"\"\"\n    if not spin_preserving:\n        for order in range(excitation_level + 1):\n            for determinant in _iterate_basis_order_(reference_determinant, order):\n                yield determinant\n\n    else:\n        alpha_excitation_level = min((numpy.sum(reference_determinant[::2]), excitation_level))\n        beta_excitation_level = min((numpy.sum(reference_determinant[1::2]), excitation_level))\n\n        for order in range(excitation_level + 1):\n            for alpha_order in range(alpha_excitation_level + 1):\n                beta_order = order - alpha_order\n                if beta_order < 0 or beta_order > beta_excitation_level:\n                    continue\n\n                for determinant in _iterate_basis_spin_order_(\n                    reference_determinant, alpha_order, beta_order\n                ):\n                    yield determinant\n\n\ndef _iterate_basis_order_(reference_determinant, order):\n    \"\"\"A helper for iterating over determinants of a fixed excitation rank.\n\n    Args:\n        reference_determinant(list(bool)): The reference state with respect to\n            which we are iterating over excited determinants.\n        order(int): The number of excitations from the modes which are occupied\n            in the reference_determinant.\n\n    Yields:\n        Lists of bools which indicate which orbitals are occupied and which are\n            unoccupied in the current determinant.\n    \"\"\"\n    occupied_indices = numpy.where(reference_determinant)[0]\n    unoccupied_indices = numpy.where(numpy.invert(reference_determinant))[0]\n\n    for occ_ind, unocc_ind in itertools.product(\n        itertools.combinations(occupied_indices, order),\n        itertools.combinations(unoccupied_indices, order),\n    ):\n        basis_state = reference_determinant.copy()\n\n        occ_ind = list(occ_ind)\n        unocc_ind = list(unocc_ind)\n\n        basis_state[occ_ind] = False\n        basis_state[unocc_ind] = True\n\n        yield basis_state\n\n\ndef _iterate_basis_spin_order_(reference_determinant, alpha_order, beta_order):\n    \"\"\"Iterates over states with a fixed excitation rank for each spin sector.\n\n    This helper method assumes that the two spin sectors are interleaved:\n    [1_alpha, 1_beta, 2_alpha, 2_beta, ...].\n\n    Args:\n        reference_determinant(list(bool)): The reference state with respect to\n            which we are iterating over excited determinants.\n        alpha_order(int): The number of excitations from the alpha spin sector\n            of the reference_determinant.\n        beta_order(int): The number of excitations from the beta spin sector of\n            the reference_determinant.\n\n    Yields:\n        Lists of bools which indicate which orbitals are occupied and which are\n            unoccupied in the current determinant.\n    \"\"\"\n    occupied_alpha_indices = numpy.where(reference_determinant[::2])[0] * 2\n    unoccupied_alpha_indices = numpy.where(numpy.invert(reference_determinant[::2]))[0] * 2\n    occupied_beta_indices = numpy.where(reference_determinant[1::2])[0] * 2 + 1\n    unoccupied_beta_indices = numpy.where(numpy.invert(reference_determinant[1::2]))[0] * 2 + 1\n\n    for alpha_occ_ind, alpha_unocc_ind, beta_occ_ind, beta_unocc_ind in itertools.product(\n        itertools.combinations(occupied_alpha_indices, alpha_order),\n        itertools.combinations(unoccupied_alpha_indices, alpha_order),\n        itertools.combinations(occupied_beta_indices, beta_order),\n        itertools.combinations(unoccupied_beta_indices, beta_order),\n    ):\n        basis_state = reference_determinant.copy()\n\n        alpha_occ_ind = list(alpha_occ_ind)\n        alpha_unocc_ind = list(alpha_unocc_ind)\n        beta_occ_ind = list(beta_occ_ind)\n        beta_unocc_ind = list(beta_unocc_ind)\n\n        basis_state[alpha_occ_ind] = False\n        basis_state[alpha_unocc_ind] = True\n        basis_state[beta_occ_ind] = False\n        basis_state[beta_unocc_ind] = True\n\n        yield basis_state\n\n\ndef _build_term_op_(term, state_array, int_state_array, sorting_indices):\n    \"\"\"Builds a scipy sparse representation of a term from a FermionOperator.\n\n    Args:\n        term(tuple of tuple(int, int)s): The argument is a tuple of tuples\n            representing a product of normal ordered fermionic creation and\n            annihilation operators, each of which is of the form (int, int)\n            where the first int indicates which site the operator acts on and\n            the second int indicates whether the operator is a creation\n            operator (1) or an annihilation operator (0). See the\n            implementation of FermionOperator for more details.\n        state_array(ndarray(bool)): A Numpy array which encodes each of the\n            determinants in the space we are working in a bools which indicate\n            the occupation of each mode. See the implementation of\n            get_number_preserving_sparse_operator for more details.\n        int_state_array(ndarray(int)): A one dimensional Numpy array which\n            encodes the intFeger representation of the binary number\n            corresponding to each determinant in state_array.\n        sorting_indices(ndarray.view): A Numpy view which sorts\n            int_state_array. This, together with int_state_array, allows for a\n            quick lookup of the position of a particular determinant in\n            state_array by converting it to its integer representation and\n            searching through the sorted int_state_array.\n\n    Raises:\n        ValueError: If term does not represent a particle number conserving\n            operator.\n\n    Returns:\n        A scipy.sparse.csc_matrix which corresponds to the operator specified\n            by term expressed in the basis corresponding to the other arguments\n            of the method.\"\"\"\n\n    space_size = state_array.shape[0]\n\n    needs_to_be_occupied = []\n    needs_to_be_unoccupied = []\n\n    # We keep track of the number of creation and annihilation operators and\n    # ensure that there are an equal number of them in order to help detect\n    # invalid inputs.\n    delta = 0\n    for index, op_type in reversed(term):\n        if op_type == 0:\n            needs_to_be_occupied.append(index)\n            delta -= 1\n        else:\n            if index not in needs_to_be_occupied:\n                needs_to_be_unoccupied.append(index)\n            delta += 1\n\n    if delta != 0:\n        raise ValueError(\"The supplied operator doesn't preserve particle number\")\n\n    # We search for every state which has the necessary orbitals occupied and\n    # unoccupied in order to not be immediately zeroed out based on the\n    # creation and annihilation operators specified in term.\n    maybe_valid_states = numpy.where(\n        numpy.logical_and(\n            numpy.all(state_array[:, needs_to_be_occupied], axis=1),\n            numpy.logical_not(numpy.any(state_array[:, needs_to_be_unoccupied], axis=1)),\n        )\n    )[0]\n\n    data = []\n    row_ind = []\n    col_ind = []\n    shape = (space_size, space_size)\n\n    # For each state that is not immediately zeroed out by the action of our\n    # operator we check to see if the determinant which this state gets mapped\n    # to is in the space we are considering.\n    # Note that a failure to find any state does not necessarily indicate that\n    # term specifies an invalid operator. For example, if we are restricting\n    # ourselves to double excitations from a fixed reference state then the\n    # action of term on some of our basis states may lead to determinants with\n    # more than two excitations from the reference. These more than double\n    # excited determinants are not included in the matrix representation (and\n    # hence, will not be present in state_array).\n    for _, state in enumerate(maybe_valid_states):\n        determinant = state_array[state, :]\n        target_determinant = determinant.copy()\n\n        parity = 1\n        for i, _ in reversed(term):\n            area_to_check = target_determinant[0:i]\n            parity *= (-1) ** numpy.sum(area_to_check)\n\n            target_determinant[i] = not target_determinant[i]\n\n        int_encoding = target_determinant.dot(1 << numpy.arange(target_determinant.size)[::-1])\n\n        target_state_index_sorted = numpy.searchsorted(\n            int_state_array, int_encoding, sorter=sorting_indices\n        )\n\n        target_state = sorting_indices[target_state_index_sorted]\n\n        if int_state_array[target_state] == int_encoding:\n            # Then target state is in the space considered:\n            data.append(parity)\n            row_ind.append(target_state)\n            col_ind.append(state)\n\n    data = numpy.asarray(data)\n    row_ind = numpy.asarray(row_ind)\n    col_ind = numpy.asarray(col_ind)\n\n    term_op = scipy.sparse.csc_matrix((data, (row_ind, col_ind)), shape=shape)\n\n    return term_op\n", "import_text": ["itertools", "functools.reduce", "numpy.linalg", "numpy", "scipy", "scipy.sparse", "scipy.sparse.linalg", "openfermion.ops.operators.FermionOperator", "openfermion.ops.operators.QubitOperator", "openfermion.ops.operators.BosonOperator", "openfermion.ops.operators.QuadOperator", "openfermion.ops.representations.DiagonalCoulombHamiltonian", "openfermion.ops.representations.PolynomialTensor", "openfermion.transforms.opconversions.normal_ordered", "openfermion.utils.indexing.up_index", "openfermion.utils.indexing.down_index", "openfermion.utils.operator_utils.count_qubits", "openfermion.utils.operator_utils.is_hermitian"], "prompt": "\"\"\"\nDescription: This function converts a fermion operator to a sparse matrix representation using the Jordan-Wigner transformation.\n\nArgs:\n    fermion_operator (type): The fermion operator to be transformed.\n    n_qubits (type, optional): The number of qubits. If not provided, it is calculated from the fermion operator.\n\nReturns:\n    scipy.sparse.coo_matrix: The sparse matrix representation of the fermion operator.\n\"\"\"", "comment": "    r\"\"\"Initialize a Scipy sparse matrix from a FermionOperator.\n\n    Operators are mapped as follows:\n    a_j^\\dagger -> Z_0 .. Z_{j-1} (X_j - iY_j) / 2\n    a_j -> Z_0 .. Z_{j-1} (X_j + iY_j) / 2\n\n    Args:\n        fermion_operator(FermionOperator): instance of the FermionOperator\n            class.\n        n_qubits(int): Number of qubits.\n\n    Returns:\n        The corresponding Scipy sparse matrix.\n    \"\"\"", "prompt_is_gen_from_api": true, "function_dependencies": ["openfermion.utils.operator_utils.count_qubits", "scipy.sparse.identity", "numpy.concatenate", "scipy.sparse.coo_matrix", "scipy.sparse.coo_matrix.tocsc", "scipy.sparse.coo_matrix.tocsc.eliminate_zeros"], "project_create_time": "2017-09-21T22:10:28+00:00", "project_update_time": "2024-04-16T10:59:35+00:00", "file_create_time": "2020-07-23T23:38:20Z", "file_update_time": "2023-11-22T18:22:17Z", "function_update_time": "2020-07-23T23:38:20Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["scipy.sparse.coo_matrix"], "test_function": [{"file_path": "/OpenFermion-v1.6.1/OpenFermion-1.6.1/src/openfermion/linalg/sparse_tools_test.py", "class_name": "SparseOperatorTest", "function_name": "test_qubit_jw_fermion_integration", "code": "\n    def test_qubit_jw_fermion_integration(self):\n        # Initialize a random fermionic operator.\n        fermion_operator = FermionOperator(((3, 1), (2, 1), (1, 0), (0, 0)), -4.3)\n        fermion_operator += FermionOperator(((3, 1), (1, 0)), 8.17)\n        fermion_operator += 3.2 * FermionOperator()\n\n        # Map to qubits and compare matrix versions.\n        qubit_operator = jordan_wigner(fermion_operator)\n        qubit_sparse = get_sparse_operator(qubit_operator)\n        qubit_spectrum = sparse_eigenspectrum(qubit_sparse)\n        fermion_sparse = jordan_wigner_sparse(fermion_operator)\n        fermion_spectrum = sparse_eigenspectrum(fermion_sparse)\n        self.assertAlmostEqual(0.0, numpy.amax(numpy.absolute(fermion_spectrum - qubit_spectrum)))"}, {"file_path": "/OpenFermion-v1.6.1/OpenFermion-1.6.1/src/openfermion/linalg/sparse_tools_test.py", "class_name": "JWNumberRestrictOperatorTest", "function_name": "test_jw_restrict_operator", "code": "    def test_jw_restrict_operator(self):\n        # Make a Hamiltonian that cares mostly about number of electrons\n        n_qubits = 4\n        target_electrons = 2\n        penalty_const = 10.0\n        number_sparse = jordan_wigner_sparse(number_operator(n_qubits))\n        bias_sparse = jordan_wigner_sparse(\n            sum(\n                [FermionOperator(((i, 1), (i, 0)), 1.0) for i in range(n_qubits)], FermionOperator()\n            )\n        )\n        hamiltonian_sparse = (\n            penalty_const\n            * (number_sparse - target_electrons * scipy.sparse.identity(2**n_qubits)).dot(\n                number_sparse - target_electrons * scipy.sparse.identity(2**n_qubits)\n            )\n            + bias_sparse\n        )\n\n        restricted_hamiltonian = jw_number_restrict_operator(\n            hamiltonian_sparse, target_electrons, n_qubits\n        )\n        true_eigvals, _ = eigh(hamiltonian_sparse.A)\n        test_eigvals, _ = eigh(restricted_hamiltonian.A)\n\n        self.assertAlmostEqual(norm(true_eigvals[:6] - test_eigvals[:6]), 0.0)"}, {"file_path": "/OpenFermion-v1.6.1/OpenFermion-1.6.1/src/openfermion/linalg/sparse_tools_test.py", "class_name": "JWNumberRestrictOperatorTest", "function_name": "test_jw_restrict_operator_interaction_to_2_particles", "code": "\n    def test_jw_restrict_operator_interaction_to_2_particles(self):\n        interaction = FermionOperator('3^ 2^ 4 1') + FermionOperator('4^ 1^ 3 2')\n        interaction_sparse = jordan_wigner_sparse(interaction, n_qubits=6)\n        interaction_restrict = jw_number_restrict_operator(interaction_sparse, 2, n_qubits=6)\n\n        dim = 6 * 5 // 2  # shape of new sparse array\n\n        # 3^ 2^ 4 1 maps 2**4 + 2 = 18 to 2**3 + 2**2 = 12 and vice versa;\n        # in the 2-particle subspace (1, 4) and (2, 3) are 7th and 9th.\n        expected = csc_matrix(([-1, -1], ([7, 9], [9, 7])), shape=(dim, dim))\n\n        self.assertTrue(numpy.allclose(interaction_restrict.A, expected.A))"}, {"file_path": "/OpenFermion-v1.6.1/OpenFermion-1.6.1/src/openfermion/linalg/sparse_tools_test.py", "class_name": "JWNumberRestrictOperatorTest", "function_name": "test_jw_restrict_operator_hopping_to_1_particle_default_nqubits", "code": "\n    def test_jw_restrict_operator_hopping_to_1_particle_default_nqubits(self):\n        interaction = FermionOperator('3^ 2^ 4 1') + FermionOperator('4^ 1^ 3 2')\n        interaction_sparse = jordan_wigner_sparse(interaction, n_qubits=6)\n        # n_qubits should default to 6\n        interaction_restrict = jw_number_restrict_operator(interaction_sparse, 2)\n\n        dim = 6 * 5 // 2  # shape of new sparse array\n\n        # 3^ 2^ 4 1 maps 2**4 + 2 = 18 to 2**3 + 2**2 = 12 and vice versa;\n        # in the 2-particle subspace (1, 4) and (2, 3) are 7th and 9th.\n        expected = csc_matrix(([-1, -1], ([7, 9], [9, 7])), shape=(dim, dim))\n\n        self.assertTrue(numpy.allclose(interaction_restrict.A, expected.A))"}, {"file_path": "/OpenFermion-v1.6.1/OpenFermion-1.6.1/src/openfermion/linalg/sparse_tools_test.py", "class_name": "JWNumberRestrictOperatorTest", "function_name": "test_jw_restrict_jellium_ground_state_integration", "code": "\n    def test_jw_restrict_jellium_ground_state_integration(self):\n        n_qubits = 4\n        grid = Grid(dimensions=1, length=n_qubits, scale=1.0)\n        jellium_hamiltonian = jordan_wigner_sparse(jellium_model(grid, spinless=False))\n\n        #  2 * n_qubits because of spin\n        number_sparse = jordan_wigner_sparse(number_operator(2 * n_qubits))\n\n        restricted_number = jw_number_restrict_operator(number_sparse, 2)\n        restricted_jellium_hamiltonian = jw_number_restrict_operator(jellium_hamiltonian, 2)\n\n        _, ground_state = get_ground_state(restricted_jellium_hamiltonian)\n\n        number_expectation = expectation(restricted_number, ground_state)\n        self.assertAlmostEqual(number_expectation, 2)"}]}, {"git_group": "vanderschaarlab", "git_name": "synthcity", "version": "v0.2.10", "language": "Python", "project_name": "synthcity-v0.2.10.zip", "file_path": "/synthcity-v0.2.10/synthcity-0.2.10/src/synthcity/plugins/core/models/convnet.py", "file_name": "convnet.py", "focal_class": null, "focal_name": "suggest_image_generator_discriminator_arch", "focal_parameter": [], "solution": "def suggest_image_generator_discriminator_arch(\n    n_units_latent: int,\n    n_channels: int,\n    height: int,\n    width: int,\n    generator_dropout: float = 0.2,\n    generator_nonlin: str = \"prelu\",\n    generator_n_residual_units: int = 2,\n    discriminator_dropout: float = 0.2,\n    discriminator_nonlin: str = \"prelu\",\n    discriminator_n_residual_units: int = 2,\n    device: Any = DEVICE,\n    strategy: str = \"predefined\",\n    cond: Optional[torch.Tensor] = None,\n    cond_embedding_n_units_hidden: int = 100,\n) -> Tuple[ConditionalGenerator, ConditionalDiscriminator]:\n    cond_weight = 1 if cond is None else 2\n    if strategy == \"predefined\":\n        if height == 32 and width == 32:\n            start_shape_gen = 4\n            start_stride_disc = 2\n        elif height == 64 and width == 64:\n            start_shape_gen = 8\n            start_stride_disc = 4\n        elif height == 128 and width == 128:\n            start_shape_gen = 16\n            start_stride_disc = 8\n        else:\n            raise ValueError(\n                f\"Unsupported predefined arch : ({n_channels}, {height}, {width})\"\n            )\n\n        generator = nn.Sequential(\n            Generator(\n                latent_shape=(n_units_latent, cond_weight * n_channels),\n                start_shape=(64, start_shape_gen, start_shape_gen),\n                channels=[64, 32, 16, n_channels],\n                strides=[2, 2, 2, 1],\n                kernel_size=3,\n                dropout=generator_dropout,\n                act=map_nonlin(generator_nonlin),\n                num_res_units=generator_n_residual_units,\n            ),\n            nn.Tanh(),\n        ).to(device)\n        discriminator = Discriminator(\n            in_shape=(cond_weight * n_channels, height, width),\n            channels=[16, 32, 64, 1],\n            strides=[start_stride_disc, 2, 2, 2],\n            kernel_size=3,\n            last_act=None,\n            dropout=discriminator_dropout,\n            act=map_nonlin(generator_nonlin),\n            num_res_units=discriminator_n_residual_units,\n        ).to(device)\n\n        return ConditionalGenerator(\n            model=generator,\n            n_channels=n_channels,\n            n_units_latent=n_units_latent,\n            cond=cond,\n            cond_embedding_n_units_hidden=cond_embedding_n_units_hidden,\n            device=device,\n        ), ConditionalDiscriminator(\n            discriminator,\n            n_channels=n_channels,\n            height=height,\n            width=width,\n            cond=cond,\n            cond_embedding_n_units_hidden=cond_embedding_n_units_hidden,\n            device=device,\n        )\n\n    raise ValueError(f\"unsupported image arch : ({n_channels}, {height}, {width})\")", "function_signature": "def suggest_image_generator_discriminator_arch(\n    n_units_latent: int,\n    n_channels: int,\n    height: int,\n    width: int,\n    generator_dropout: float = 0.2,\n    generator_nonlin: str = \"prelu\",\n    generator_n_residual_units: int = 2,\n    discriminator_dropout: float = 0.2,\n    discriminator_nonlin: str = \"prelu\",\n    discriminator_n_residual_units: int = 2,\n    device: Any = DEVICE,\n    strategy: str = \"predefined\",\n    cond: Optional[torch.Tensor] = None,\n    cond_embedding_n_units_hidden: int = 100,\n) -> Tuple[ConditionalGenerator, ConditionalDiscriminator] :", "left_context": "# stdlib\nfrom typing import Any, Optional, Tuple\n\n# third party\nimport numpy as np\nimport torch\nfrom monai.networks.layers.factories import Act\nfrom monai.networks.nets import Classifier, Discriminator, Generator\nfrom pydantic import validate_arguments\nfrom torch import nn\n\n# synthcity absolute\nimport synthcity.logger as log\nfrom synthcity.utils.constants import DEVICE\nfrom synthcity.utils.reproducibility import enable_reproducible_results\n\n\ndef map_nonlin(nonlin: str) -> Act:\n    if nonlin == \"relu\":\n        return Act.RELU\n    elif nonlin == \"elu\":\n        return Act.ELU\n    elif nonlin == \"prelu\":\n        return Act.PRELU\n    elif nonlin == \"leaky_relu\":\n        return Act.LEAKYRELU\n    elif nonlin == \"sigmoid\":\n        return Act.SIGMOID\n    elif nonlin == \"softmax\":\n        return Act.SOFTMAX\n    elif nonlin == \"tanh\":\n        return Act.TANH\n\n    raise ValueError(f\"Unknown activation {nonlin}\")\n\n\nclass ConvNet(nn.Module):\n    \"\"\"\n    Wrapper for convolutional nets for classification and regression.\n\n    Parameters\n    ----------\n    task_type: str\n        classifier or regression\n    model: nn.Module\n        classification or regression model implementation\n    lr: float\n        learning rate for optimizer.\n    weight_decay: float\n        l2 (ridge) penalty for the weights.\n    n_iter: int\n        Maximum number of iterations.\n    batch_size: int\n        Batch size\n    n_iter_print: int\n        Number of iterations after which to print updates and check the validation loss.\n    random_state: int\n        random_state used\n    patience: int\n        Number of iterations to wait before early stopping after decrease in validation loss\n    n_iter_min: int\n        Minimum number of iterations to go through before starting early stopping\n    clipping_value: int, default 1\n        Gradients clipping value\n    early_stopping: bool\n        Enable/disable early stopping\n    \"\"\"\n\n    @validate_arguments(config=dict(arbitrary_types_allowed=True))\n    def __init__(\n        self,\n        task_type: str,  # classification/regression\n        model: nn.Module,\n        lr: float = 1e-3,\n        weight_decay: float = 1e-3,\n        opt_betas: tuple = (0.9, 0.999),\n        n_iter: int = 1000,\n        batch_size: int = 500,\n        n_iter_print: int = 100,\n        random_state: int = 0,\n        patience: int = 10,\n        n_iter_min: int = 100,\n        clipping_value: int = 1,\n        early_stopping: bool = True,\n        device: Any = DEVICE,\n    ) -> None:\n        super(ConvNet, self).__init__()\n\n        if task_type not in [\"classification\", \"regression\"]:\n            raise ValueError(f\"Invalid task type {task_type}\")\n\n        enable_reproducible_results(random_state)\n\n        self.task_type = task_type\n        self.device = device\n        self.model = model\n        self.random_state = random_state\n\n        # optimizer\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.opt_betas = opt_betas\n        self.optimizer = torch.optim.Adam(\n            self.parameters(),\n            lr=self.lr,\n            weight_decay=self.weight_decay,\n            betas=self.opt_betas,\n        )\n\n        # training\n        self.n_iter = n_iter\n        self.n_iter_print = n_iter_print\n        self.n_iter_min = n_iter_min\n        self.batch_size = batch_size\n        self.patience = patience\n        self.clipping_value = clipping_value\n        self.early_stopping = early_stopping\n        if task_type == \"classification\":\n            self.loss = nn.CrossEntropyLoss()\n        else:\n            self.loss = nn.MSELoss()\n\n    def fit(self, X: torch.utils.data.Dataset) -> \"ConvNet\":\n        train_size = int(0.8 * len(X))\n        test_size = len(X) - train_size\n        train_dataset, test_dataset = torch.utils.data.random_split(\n            X, [train_size, test_size]\n        )\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset, batch_size=self.batch_size, pin_memory=False\n        )\n        test_loader = torch.utils.data.DataLoader(\n            test_dataset, batch_size=len(test_dataset)\n        )\n\n        # Setup the network and optimizer\n\n        val_loss_best = 999999\n        patience = 0\n\n        # do training\n        for i in range(self.n_iter):\n            train_loss = self._train_epoch(train_loader)\n\n            if self.early_stopping or i % self.n_iter_print == 0:\n                with torch.no_grad():\n                    X_val, y_val = next(iter(test_loader))\n                    X_val = self._check_tensor(X_val)\n                    y_val = self._check_tensor(y_val).long()\n\n                    preds = self.forward(X_val).squeeze()\n                    val_loss = self.loss(preds, y_val)\n\n                    if self.early_stopping:\n                        if val_loss_best > val_loss:\n                            val_loss_best = val_loss\n                            patience = 0\n                        else:\n                            patience += 1\n\n                        if patience > self.patience and i > self.n_iter_min:\n                            break\n\n                    if i % self.n_iter_print == 0:\n                        log.debug(\n                            f\"Epoch: {i}, val loss: {val_loss}, train_loss: {train_loss}\"\n                        )\n\n        return self\n\n    @validate_arguments(config=dict(arbitrary_types_allowed=True))\n    def predict_proba(self, X: torch.Tensor) -> torch.Tensor:\n        if self.task_type != \"classification\":\n            raise ValueError(f\"Invalid task type for predict_proba {self.task_type}\")\n\n        with torch.no_grad():\n            Xt = self._check_tensor(X)\n\n            yt = self.forward(Xt)\n\n            return yt.cpu()\n\n    @validate_arguments(config=dict(arbitrary_types_allowed=True))\n    def predict(self, X: torch.Tensor) -> torch.Tensor:\n        with torch.no_grad():\n            Xt = self._check_tensor(X)\n\n            yt = self.forward(Xt)\n\n            if self.task_type == \"classification\":\n                return torch.argmax(yt.cpu(), -1).squeeze()\n            else:\n                return yt.cpu()\n\n    def score(self, X: torch.Tensor, y: torch.Tensor) -> float:\n        y_pred = self.predict(X)\n        if self.task_type == \"classification\":\n            return torch.mean(y_pred == y)\n        else:\n            return torch.mean(torch.inner(y - y_pred, y - y_pred) / 2.0)\n\n    @validate_arguments(config=dict(arbitrary_types_allowed=True))\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        X = self._check_tensor(X)\n\n        return self.model(X.float())\n\n    def _train_epoch(self, loader: torch.utils.data.DataLoader) -> float:\n        train_loss = []\n\n        for batch_ndx, sample in enumerate(loader):\n            self.optimizer.zero_grad()\n\n            X_next, y_next = sample\n\n            X_next = self._check_tensor(X_next)\n            y_next = self._check_tensor(y_next).long()\n\n            if len(X_next) < 2:\n                continue\n\n            preds = self.forward(X_next).squeeze()\n\n            batch_loss = self.loss(preds, y_next)\n\n            batch_loss.backward()\n\n            if self.clipping_value > 0:\n                torch.nn.utils.clip_grad_norm_(self.parameters(), self.clipping_value)\n\n            self.optimizer.step()\n\n            train_loss.append(batch_loss.detach())\n\n        return torch.mean(torch.Tensor(train_loss))\n\n    def _check_tensor(self, X: torch.Tensor) -> torch.Tensor:\n        if isinstance(X, torch.Tensor):\n            return X.to(self.device)\n        else:\n            return torch.from_numpy(np.asarray(X)).to(self.device)\n\n    def __len__(self) -> int:\n        return len(self.model)\n\n\nclass ConditionalGenerator(nn.Module):\n    \"\"\"Wrapper for making existing CNN generator conditional. Useful for Conditional GANs\n\n    Args:\n        model: nn.Module\n            Core model.\n        n_channels: int\n            Number of channels in images\n        n_units_latent: int\n            Noise size for the input\n        cond: torch.Tensor\n            The reference conditional\n        cond_embedding_n_units_hidden: int\n            Size of the conditional embedding layer\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        n_channels: int,\n        n_units_latent: int,\n        cond: Optional[torch.Tensor] = None,\n        cond_embedding_n_units_hidden: int = 100,\n        device: Any = DEVICE,\n    ) -> None:\n        super(ConditionalGenerator, self).__init__()\n\n        self.model = model\n        self.cond = cond\n        self.n_channels = n_channels\n        self.n_units_latent = n_units_latent\n        self.device = device\n\n        self.label_conditioned_generator: Optional[nn.Module] = None\n        if cond is not None:\n            classes = torch.unique(self.cond)\n            self.label_conditioned_generator = nn.Sequential(\n                nn.Embedding(len(classes), cond_embedding_n_units_hidden),\n                nn.Linear(cond_embedding_n_units_hidden, n_channels * n_units_latent),\n            ).to(device)\n\n    def forward(\n        self, noise: torch.Tensor, cond: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        if cond is None and self.cond is not None:\n            perm = torch.randperm(self.cond.size(0))\n            cond = self.cond[perm[: len(noise)]]\n\n        if self.label_conditioned_generator is not None and cond is not None:\n            cond_emb = self.label_conditioned_generator(cond.long()).view(\n                -1, self.n_units_latent, self.n_channels, 1\n            )\n            noise = torch.cat(\n                (noise, cond_emb), dim=2\n            )  # add another channel with the conditional\n        return self.model(noise)\n\n\nclass ConditionalDiscriminator(nn.Module):\n    \"\"\"\n\n    Args:\n        model: nn.Module\n            Core model.\n        n_channels: int\n            Number of channels in images\n        height: int\n            Image height\n        width: int\n            Image width\n        cond: torch.Tensor\n            The reference conditional\n        cond_embedding_n_units_hidden: int\n            Size of the conditional embedding layer\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        n_channels: int,\n        height: int,\n        width: int,\n        cond: Optional[torch.Tensor] = None,\n        cond_embedding_n_units_hidden: int = 100,\n        device: Any = DEVICE,\n    ) -> None:\n        super(ConditionalDiscriminator, self).__init__()\n        self.model = model\n        self.cond = cond\n\n        self.n_channels = n_channels\n        self.height = height\n        self.width = width\n\n        self.device = device\n\n        self.label_conditioned_generator: Optional[nn.Module] = None\n        if cond is not None:\n            classes = torch.unique(self.cond)\n            self.label_conditioned_generator = nn.Sequential(\n                nn.Embedding(len(classes), cond_embedding_n_units_hidden),\n                nn.Linear(cond_embedding_n_units_hidden, n_channels * height * width),\n            ).to(device)\n\n    def forward(\n        self, X: torch.Tensor, cond: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        if cond is None and self.cond is not None:\n            perm = torch.randperm(self.cond.size(0))\n            cond = self.cond[perm[: len(X)]]\n\n        if self.label_conditioned_generator is not None and cond is not None:\n            cond_emb = self.label_conditioned_generator(cond.long()).view(\n                -1, self.n_channels, self.height, self.width\n            )\n            X = torch.cat(\n                (X, cond_emb), dim=1\n            )  # add another channel with the conditional\n\n        return self.model(X)\n\n\n# TODO: add more image processing architectures for better performance", "right_context": "\n\ndef suggest_image_classifier_arch(\n    n_channels: int,\n    height: int,\n    width: int,\n    classes: int,\n    n_residual_units: int = 2,\n    nonlin: str = \"prelu\",\n    dropout: float = 0.2,\n    last_nonlin: str = \"softmax\",\n    device: Any = DEVICE,\n    strategy: str = \"predefined\",\n    # training\n    lr: float = 1e-3,\n    weight_decay: float = 1e-3,\n    opt_betas: tuple = (0.9, 0.999),\n    n_iter: int = 1000,\n    batch_size: int = 500,\n    n_iter_print: int = 100,\n    random_state: int = 0,\n    patience: int = 10,\n    n_iter_min: int = 100,\n    clipping_value: int = 1,\n    early_stopping: bool = True,\n) -> ConvNet:\n    \"\"\"Helper for selecting compatible architecture for image classifiers.\n\n    Args:\n        n_channels: int\n            Number of channels in the image\n        height: int\n            Image height\n        width: int\n            Image width\n        classes: int\n            Number of output classes\n        nonlin: str\n            name of the activation activation layers. Can be relu, elu, prelu or leaky_relu\n        last_act: str\n            output activation\n        dropout: float = 0.2\n            Dropout value\n        n_residual_units: int\n             integer stating number of convolutions in residual units, 0 means no residual units\n        device: str\n            PyTorch device. cpu, cuda\n     # Training\n        lr: float\n            learning rate for optimizer.\n        weight_decay: float\n            l2 (ridge) penalty for the weights.\n        n_iter: int\n            Maximum number of iterations.\n        batch_size: int\n            Batch size\n        n_iter_print: int\n            Number of iterations after which to print updates and check the validation loss.\n        random_state: int\n            random_state used\n        patience: int\n            Number of iterations to wait before early stopping after decrease in validation loss\n        n_iter_min: int\n            Minimum number of iterations to go through before starting early stopping\n        clipping_value: int, default 1\n            Gradients clipping value\n        early_stopping: bool\n            Enable/disable early stopping\n\n\n    \"\"\"\n    if strategy == \"predefined\":\n        if height == 32 and width == 32:\n            start_stride = 2\n        elif height == 64 and width == 64:\n            start_stride = 4\n        elif height == 128 and width == 128:\n            start_stride = 8\n        else:\n            raise ValueError(\n                f\"Unsupported predefined arch : ({n_channels}, {height}, {width})\"\n            )\n\n        clf = Classifier(\n            in_shape=(n_channels, height, width),\n            classes=classes,\n            channels=[16, 32, 64, 1],\n            strides=[start_stride, 2, 2, 2],\n            act=map_nonlin(nonlin),\n            last_act=map_nonlin(last_nonlin),\n            dropout=dropout,\n            num_res_units=n_residual_units,\n        ).to(device)\n        return ConvNet(\n            task_type=\"classification\",\n            model=clf,\n            device=device,\n            lr=lr,\n            weight_decay=weight_decay,\n            opt_betas=opt_betas,\n            n_iter=n_iter,\n            batch_size=batch_size,\n            n_iter_print=n_iter_print,\n            random_state=random_state,\n            patience=patience,\n            n_iter_min=n_iter_min,\n            clipping_value=clipping_value,\n            early_stopping=early_stopping,\n        )\n\n    raise ValueError(f\"unsupported image arch : ({n_channels}, {height}, {width})\")\n", "import_text": ["typing.Any", "typing.Optional", "typing.Tuple", "numpy", "torch", "monai.networks.layers.factories.Act", "monai.networks.nets.Classifier", "monai.networks.nets.Discriminator", "monai.networks.nets.Generator", "pydantic.validate_arguments", "torch.nn", "synthcity.logger", "synthcity.utils.constants.DEVICE", "synthcity.utils.reproducibility.enable_reproducible_results"], "prompt": "\"\"\"\nDescription: This function is for generating a generator and discriminator architecture for an image generator and discriminator.\n\nArgs:\n    n_units_latent (int): The number of units in the latent space.\n    n_channels (int): The number of channels in the input image.\n    height (int): The height of the input image.\n    width (int): The width of the input image.\n    generator_dropout (float, optional): The dropout rate for the generator. Defaults to 0.2.\n    generator_nonlin (str, optional): The non-linear activation function for the generator. Defaults to \"prelu\".\n    generator_n_residual_units (int, optional): The number of residual units in the generator. Defaults to 2.\n    discriminator_dropout (float, optional): The dropout rate for the discriminator. Defaults to 0.2.\n    discriminator_nonlin (str, optional): The non-linear activation function for the discriminator. Defaults to \"prelu\".\n    discriminator_n_residual_units (int, optional): The number of residual units in the discriminator. Defaults to 2.\n    device (Any, optional): The device to use for the generator and discriminator. Defaults to DEVICE.\n    strategy (str, optional): The strategy to use for the generator and discriminator. Defaults to \"predefined\".\n    cond (Optional[torch.Tensor], optional): The conditional input to the generator and discriminator. Defaults to None.\n    cond_embedding_n_units_hidden (int, optional): The number of hidden units in the conditional embedding. Defaults to 100.\n\nReturns:\n    Tuple[ConditionalGenerator, ConditionalDiscriminator]: A tuple containing the generator and discriminator.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Helper for selecting compatible architecture for image generators and discriminators.\n\n    Args:\n        n_units_latent: int,\n            Input size for the generator\n        n_channels: int\n            Number of channels in the image\n        height: int\n            Image height\n        width: int\n            Image width\n        generator_dropout: float = 0.2\n            Dropout value for the generator\n        generator_nonlin: str\n            name of the activation activation layers in the generator. Can be relu, elu, prelu or leaky_relu\n        generator_n_residual_units: int\n             integer stating number of convolutions in residual units for the generator, 0 means no residual units\n        discriminator_dropout: float = 0.2\n            Dropout value for the discriminator\n        discriminator_nonlin: str\n            name of the activation activation layers in the discriminator. Can be relu, elu, prelu or leaky_relu\n        discriminator_n_residual_units: int\n             integer stating number of convolutions in residual units for the discriminator, 0 means no residual units\n        device: str\n            PyTorch device. cpu, cuda\n        strategy: str\n            Which suggestion to use. Options:\n                - predefined: a few hardcoded architectures for certain image shapes.\n                - ...\n    \"\"\"", "function_dependencies": ["torch.nn.Sequential", "torch.nn.Sequential.to", "monai.networks.nets.Generator", "torch.nn.Tanh", "monai.networks.nets.Discriminator", "monai.networks.nets.Discriminator.to"], "project_create_time": "2022-03-18T15:26:36+00:00", "project_update_time": "2024-04-17T03:22:55+00:00", "file_create_time": "2023-02-27T06:11:59Z", "file_update_time": "2023-04-14T16:05:46Z", "function_update_time": "2023-02-27T06:11:59Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["torch.nn.Sequential"], "test_function": [{"file_path": "/synthcity-v0.2.10/synthcity-0.2.10/tests/plugins/core/models/test_convnet.py", "class_name": null, "function_name": "test_suggest_gan", "code": "\ndef test_suggest_gan(n_channels: int, height: int) -> None:\n    n_units_latent = 100\n    gen, disc = suggest_image_generator_discriminator_arch(\n        n_units_latent=n_units_latent,\n        n_channels=n_channels,\n        height=height,\n        width=height,\n    )\n\n    dummy_noise = torch.rand((10, n_units_latent, n_channels, 1), device=DEVICE)\n    gen(dummy_noise)\n\n    dummy_in = torch.rand((10, n_channels, height, height), device=DEVICE)\n    disc(dummy_in)"}]}, {"git_group": "holzschu", "git_name": "python3_ios", "version": "v1.0", "language": "Python", "project_name": "python3_ios-v1.0.zip", "file_path": "/python3_ios-v1.0/python3_ios-1.0/site-packages/sympy/solvers/pde.py", "file_name": "pde.py", "focal_class": null, "focal_name": "classify_pde", "focal_parameter": ["eq"], "solution": "def classify_pde(eq, func=None, dict=False, **kwargs):\n\n    prep = kwargs.pop('prep', True)\n\n    if func and len(func.args) != 2:\n        raise NotImplementedError(\"Right now only partial \"\n            \"differential equations of two variables are supported\")\n\n    if prep or func is None:\n        prep, func_ = _preprocess(eq, func)\n        if func is None:\n            func = func_\n\n    if isinstance(eq, Equality):\n        if eq.rhs != 0:\n            return classify_pde(eq.lhs - eq.rhs, func)\n        eq = eq.lhs\n\n    f = func.func\n    x = func.args[0]\n    y = func.args[1]\n    fx = f(x,y).diff(x)\n    fy = f(x,y).diff(y)\n\n    # TODO : For now pde.py uses support offered by the ode_order function\n    # to find the order with respect to a multi-variable function. An\n    # improvement could be to classify the order of the PDE on the basis of\n    # individual variables.\n    order = ode_order(eq, f(x,y))\n\n    # hint:matchdict or hint:(tuple of matchdicts)\n    # Also will contain \"default\":<default hint> and \"order\":order items.\n    matching_hints = {'order': order}\n\n    if not order:\n        if dict:\n            matching_hints[\"default\"] = None\n            return matching_hints\n        else:\n            return ()\n\n    eq = expand(eq)\n\n    a = Wild('a', exclude = [f(x,y)])\n    b = Wild('b', exclude = [f(x,y), fx, fy, x, y])\n    c = Wild('c', exclude = [f(x,y), fx, fy, x, y])\n    d = Wild('d', exclude = [f(x,y), fx, fy, x, y])\n    e = Wild('e', exclude = [f(x,y), fx, fy])\n    n = Wild('n', exclude = [x, y])\n    # Try removing the smallest power of f(x,y)\n    # from the highest partial derivatives of f(x,y)\n    reduced_eq = None\n    if eq.is_Add:\n        var = set(combinations_with_replacement((x,y), order))\n        dummyvar = var.copy()\n        power = None\n        for i in var:\n            coeff = eq.coeff(f(x,y).diff(*i))\n            if coeff != 1:\n                match = coeff.match(a*f(x,y)**n)\n                if match and match[a]:\n                    power = match[n]\n                    dummyvar.remove(i)\n                    break\n            dummyvar.remove(i)\n        for i in dummyvar:\n            coeff = eq.coeff(f(x,y).diff(*i))\n            if coeff != 1:\n                match = coeff.match(a*f(x,y)**n)\n                if match and match[a] and match[n] < power:\n                    power = match[n]\n        if power:\n            den = f(x,y)**power\n            reduced_eq = Add(*[arg/den for arg in eq.args])\n    if not reduced_eq:\n        reduced_eq = eq\n\n    if order == 1:\n        reduced_eq = collect(reduced_eq, f(x, y))\n        r = reduced_eq.match(b*fx + c*fy + d*f(x,y) + e)\n        if r:\n            if not r[e]:\n                ## Linear first-order homogeneous partial-differential\n                ## equation with constant coefficients\n                r.update({'b': b, 'c': c, 'd': d})\n                matching_hints[\"1st_linear_constant_coeff_homogeneous\"] = r\n            else:\n                if r[b]**2 + r[c]**2 != 0:\n                    ## Linear first-order general partial-differential\n                    ## equation with constant coefficients\n                    r.update({'b': b, 'c': c, 'd': d, 'e': e})\n                    matching_hints[\"1st_linear_constant_coeff\"] = r\n                    matching_hints[\n                        \"1st_linear_constant_coeff_Integral\"] = r\n\n        else:\n            b = Wild('b', exclude=[f(x, y), fx, fy])\n            c = Wild('c', exclude=[f(x, y), fx, fy])\n            d = Wild('d', exclude=[f(x, y), fx, fy])\n            r = reduced_eq.match(b*fx + c*fy + d*f(x,y) + e)\n            if r:\n                r.update({'b': b, 'c': c, 'd': d, 'e': e})\n                matching_hints[\"1st_linear_variable_coeff\"] = r\n\n    # Order keys based on allhints.\n    retlist = []\n    for i in allhints:\n        if i in matching_hints:\n            retlist.append(i)\n\n    if dict:\n        # Dictionaries are ordered arbitrarily, so make note of which\n        # hint would come first for pdsolve().  Use an ordered dict in Py 3.\n        matching_hints[\"default\"] = None\n        matching_hints[\"ordered_hints\"] = tuple(retlist)\n        for i in allhints:\n            if i in matching_hints:\n                matching_hints[\"default\"] = i\n                break\n        return matching_hints\n    else:\n        return tuple(retlist)", "function_signature": "def classify_pde(eq, func=None, dict=False, **kwargs) :", "left_context": "\"\"\"\nThis module contains pdsolve() and different helper functions that it\nuses. It is heavily inspired by the ode module and hence the basic\ninfrastructure remains the same.\n\n**Functions in this module**\n\n    These are the user functions in this module:\n\n    - pdsolve()     - Solves PDE's\n    - classify_pde() - Classifies PDEs into possible hints for dsolve().\n    - pde_separate() - Separate variables in partial differential equation either by\n                       additive or multiplicative separation approach.\n\n    These are the helper functions in this module:\n\n    - pde_separate_add() - Helper function for searching additive separable solutions.\n    - pde_separate_mul() - Helper function for searching multiplicative\n                           separable solutions.\n\n**Currently implemented solver methods**\n\nThe following methods are implemented for solving partial differential\nequations.  See the docstrings of the various pde_hint() functions for\nmore information on each (run help(pde)):\n\n  - 1st order linear homogeneous partial differential equations\n    with constant coefficients.\n  - 1st order linear general partial differential equations\n    with constant coefficients.\n  - 1st order linear partial differential equations with\n    variable coefficients.\n\n\"\"\"\nfrom __future__ import print_function, division\n\nfrom itertools import combinations_with_replacement\nfrom sympy.simplify import simplify\nfrom sympy.core import Add, S\nfrom sympy.core.compatibility import (reduce, is_sequence, range)\nfrom sympy.core.function import Function, expand, AppliedUndef, Subs\nfrom sympy.core.relational import Equality, Eq\nfrom sympy.core.symbol import Symbol, Wild, symbols\nfrom sympy.functions import exp\nfrom sympy.integrals.integrals import Integral\nfrom sympy.utilities.iterables import has_dups\nfrom sympy.utilities.misc import filldedent\n\nfrom sympy.solvers.deutils import _preprocess, ode_order, _desolve\nfrom sympy.solvers.solvers import solve\nfrom sympy.simplify.radsimp import collect\n\nimport operator\n\n\nallhints = (\n    \"1st_linear_constant_coeff_homogeneous\",\n    \"1st_linear_constant_coeff\",\n    \"1st_linear_constant_coeff_Integral\",\n    \"1st_linear_variable_coeff\"\n    )\n\n\ndef pdsolve(eq, func=None, hint='default', dict=False, solvefun=None, **kwargs):\n    \"\"\"\n    Solves any (supported) kind of partial differential equation.\n\n    **Usage**\n\n        pdsolve(eq, f(x,y), hint) -> Solve partial differential equation\n        eq for function f(x,y), using method hint.\n\n    **Details**\n\n        ``eq`` can be any supported partial differential equation (see\n            the pde docstring for supported methods).  This can either\n            be an Equality, or an expression, which is assumed to be\n            equal to 0.\n\n        ``f(x,y)`` is a function of two variables whose derivatives in that\n            variable make up the partial differential equation. In many\n            cases it is not necessary to provide this; it will be autodetected\n            (and an error raised if it couldn't be detected).\n\n        ``hint`` is the solving method that you want pdsolve to use.  Use\n            classify_pde(eq, f(x,y)) to get all of the possible hints for\n            a PDE.  The default hint, 'default', will use whatever hint\n            is returned first by classify_pde().  See Hints below for\n            more options that you can use for hint.\n\n        ``solvefun`` is the convention used for arbitrary functions returned\n            by the PDE solver. If not set by the user, it is set by default\n            to be F.\n\n    **Hints**\n\n        Aside from the various solving methods, there are also some\n        meta-hints that you can pass to pdsolve():\n\n        \"default\":\n                This uses whatever hint is returned first by\n                classify_pde(). This is the default argument to\n                pdsolve().\n\n        \"all\":\n                To make pdsolve apply all relevant classification hints,\n                use pdsolve(PDE, func, hint=\"all\").  This will return a\n                dictionary of hint:solution terms.  If a hint causes\n                pdsolve to raise the NotImplementedError, value of that\n                hint's key will be the exception object raised.  The\n                dictionary will also include some special keys:\n\n                - order: The order of the PDE.  See also ode_order() in\n                  deutils.py\n                - default: The solution that would be returned by\n                  default.  This is the one produced by the hint that\n                  appears first in the tuple returned by classify_pde().\n\n        \"all_Integral\":\n                This is the same as \"all\", except if a hint also has a\n                corresponding \"_Integral\" hint, it only returns the\n                \"_Integral\" hint.  This is useful if \"all\" causes\n                pdsolve() to hang because of a difficult or impossible\n                integral.  This meta-hint will also be much faster than\n                \"all\", because integrate() is an expensive routine.\n\n        See also the classify_pde() docstring for more info on hints,\n        and the pde docstring for a list of all supported hints.\n\n    **Tips**\n        - You can declare the derivative of an unknown function this way:\n\n            >>> from sympy import Function, Derivative\n            >>> from sympy.abc import x, y # x and y are the independent variables\n            >>> f = Function(\"f\")(x, y) # f is a function of x and y\n            >>> # fx will be the partial derivative of f with respect to x\n            >>> fx = Derivative(f, x)\n            >>> # fy will be the partial derivative of f with respect to y\n            >>> fy = Derivative(f, y)\n\n        - See test_pde.py for many tests, which serves also as a set of\n          examples for how to use pdsolve().\n        - pdsolve always returns an Equality class (except for the case\n          when the hint is \"all\" or \"all_Integral\"). Note that it is not possible\n          to get an explicit solution for f(x, y) as in the case of ODE's\n        - Do help(pde.pde_hintname) to get help more information on a\n          specific hint\n\n\n    Examples\n    ========\n\n    >>> from sympy.solvers.pde import pdsolve\n    >>> from sympy import Function, diff, Eq\n    >>> from sympy.abc import x, y\n    >>> f = Function('f')\n    >>> u = f(x, y)\n    >>> ux = u.diff(x)\n    >>> uy = u.diff(y)\n    >>> eq = Eq(1 + (2*(ux/u)) + (3*(uy/u)))\n    >>> pdsolve(eq)\n    Eq(f(x, y), F(3*x - 2*y)*exp(-2*x/13 - 3*y/13))\n\n    \"\"\"\n\n    given_hint = hint  # hint given by the user.\n\n    if not solvefun:\n        solvefun = Function('F')\n\n    # See the docstring of _desolve for more details.\n    hints = _desolve(eq, func=func,\n        hint=hint, simplify=True, type='pde', **kwargs)\n    eq = hints.pop('eq', False)\n    all_ = hints.pop('all', False)\n\n    if all_:\n        # TODO : 'best' hint should be implemented when adequate\n        # number of hints are added.\n        pdedict = {}\n        failed_hints = {}\n        gethints = classify_pde(eq, dict=True)\n        pdedict.update({'order': gethints['order'],\n            'default': gethints['default']})\n        for hint in hints:\n            try:\n                rv = _helper_simplify(eq, hint, hints[hint]['func'],\n                    hints[hint]['order'], hints[hint][hint], solvefun)\n            except NotImplementedError as detail:\n                failed_hints[hint] = detail\n            else:\n                pdedict[hint] = rv\n        pdedict.update(failed_hints)\n        return pdedict\n\n    else:\n        return _helper_simplify(eq, hints['hint'],\n            hints['func'], hints['order'], hints[hints['hint']], solvefun)\n\n\ndef _helper_simplify(eq, hint, func, order, match, solvefun):\n    \"\"\"Helper function of pdsolve that calls the respective\n    pde functions to solve for the partial differential\n    equations. This minimizes the computation in\n    calling _desolve multiple times.\n    \"\"\"\n\n    if hint.endswith(\"_Integral\"):\n        solvefunc = globals()[\n            \"pde_\" + hint[:-len(\"_Integral\")]]\n    else:\n        solvefunc = globals()[\"pde_\" + hint]\n    return _handle_Integral(solvefunc(eq, func, order,\n        match, solvefun), func, order, hint)\n\n\ndef _handle_Integral(expr, func, order, hint):\n    r\"\"\"\n    Converts a solution with integrals in it into an actual solution.\n\n    Simplifies the integral mainly using doit()\n    \"\"\"\n    if hint.endswith(\"_Integral\"):\n        return expr\n\n    elif hint == \"1st_linear_constant_coeff\":\n        return simplify(expr.doit())\n\n    else:\n        return expr\n\n", "right_context": "\n\ndef checkpdesol(pde, sol, func=None, solve_for_func=True):\n    \"\"\"\n    Checks if the given solution satisfies the partial differential\n    equation.\n\n    pde is the partial differential equation which can be given in the\n    form of an equation or an expression. sol is the solution for which\n    the pde is to be checked. This can also be given in an equation or\n    an expression form. If the function is not provided, the helper\n    function _preprocess from deutils is used to identify the function.\n\n    If a sequence of solutions is passed, the same sort of container will be\n    used to return the result for each solution.\n\n    The following methods are currently being implemented to check if the\n    solution satisfies the PDE:\n\n        1. Directly substitute the solution in the PDE and check. If the\n           solution hasn't been solved for f, then it will solve for f\n           provided solve_for_func hasn't been set to False.\n\n    If the solution satisfies the PDE, then a tuple (True, 0) is returned.\n    Otherwise a tuple (False, expr) where expr is the value obtained\n    after substituting the solution in the PDE. However if a known solution\n    returns False, it may be due to the inability of doit() to simplify it to zero.\n\n    Examples\n    ========\n\n    >>> from sympy import Function, symbols, diff\n    >>> from sympy.solvers.pde import checkpdesol, pdsolve\n    >>> x, y = symbols('x y')\n    >>> f = Function('f')\n    >>> eq = 2*f(x,y) + 3*f(x,y).diff(x) + 4*f(x,y).diff(y)\n    >>> sol = pdsolve(eq)\n    >>> assert checkpdesol(eq, sol)[0]\n    >>> eq = x*f(x,y) + f(x,y).diff(x)\n    >>> checkpdesol(eq, sol)\n    (False, (x*F(4*x - 3*y) - 6*F(4*x - 3*y)/25 + 4*Subs(Derivative(F(_xi_1), _xi_1), (_xi_1,), (4*x - 3*y,)))*exp(-6*x/25 - 8*y/25))\n    \"\"\"\n\n    # Converting the pde into an equation\n    if not isinstance(pde, Equality):\n        pde = Eq(pde, 0)\n\n    # If no function is given, try finding the function present.\n    if func is None:\n        try:\n            _, func = _preprocess(pde.lhs)\n        except ValueError:\n            funcs = [s.atoms(AppliedUndef) for s in (\n                sol if is_sequence(sol, set) else [sol])]\n            funcs = set().union(funcs)\n            if len(funcs) != 1:\n                raise ValueError(\n                    'must pass func arg to checkpdesol for this case.')\n            func = funcs.pop()\n\n    # If the given solution is in the form of a list or a set\n    # then return a list or set of tuples.\n    if is_sequence(sol, set):\n        return type(sol)([checkpdesol(\n            pde, i, func=func,\n            solve_for_func=solve_for_func) for i in sol])\n\n    # Convert solution into an equation\n    if not isinstance(sol, Equality):\n        sol = Eq(func, sol)\n    elif sol.rhs == func:\n        sol = sol.reversed\n\n    # Try solving for the function\n    solved = sol.lhs == func and not sol.rhs.has(func)\n    if solve_for_func and not solved:\n        solved = solve(sol, func)\n        if solved:\n            if len(solved) == 1:\n                return checkpdesol(pde, Eq(func, solved[0]),\n                    func=func, solve_for_func=False)\n            else:\n                return checkpdesol(pde, [Eq(func, t) for t in solved],\n                    func=func, solve_for_func=False)\n\n    # try direct substitution of the solution into the PDE and simplify\n    if sol.lhs == func:\n        pde = pde.lhs - pde.rhs\n        s = simplify(pde.subs(func, sol.rhs).doit())\n        return s is S.Zero, s\n\n    raise NotImplementedError(filldedent('''\n        Unable to test if %s is a solution to %s.''' % (sol, pde)))\n\n\n\ndef pde_1st_linear_constant_coeff_homogeneous(eq, func, order, match, solvefun):\n    r\"\"\"\n    Solves a first order linear homogeneous\n    partial differential equation with constant coefficients.\n\n    The general form of this partial differential equation is\n\n    .. math:: a \\frac{df(x,y)}{dx} + b \\frac{df(x,y)}{dy} + c f(x,y) = 0\n\n    where `a`, `b` and `c` are constants.\n\n    The general solution is of the form::\n\n        >>> from sympy.solvers import pdsolve\n        >>> from sympy.abc import x, y, a, b, c\n        >>> from sympy import Function, pprint\n        >>> f = Function('f')\n        >>> u = f(x,y)\n        >>> ux = u.diff(x)\n        >>> uy = u.diff(y)\n        >>> genform = a*ux + b*uy + c*u\n        >>> pprint(genform)\n          d               d\n        a*--(f(x, y)) + b*--(f(x, y)) + c*f(x, y)\n          dx              dy\n\n        >>> pprint(pdsolve(genform))\n                                 -c*(a*x + b*y)\n                                 ---------------\n                                      2    2\n                                     a  + b\n        f(x, y) = F(-a*y + b*x)*e\n\n    Examples\n    ========\n\n    >>> from sympy.solvers.pde import (\n    ... pde_1st_linear_constant_coeff_homogeneous)\n    >>> from sympy import pdsolve\n    >>> from sympy import Function, diff, pprint\n    >>> from sympy.abc import x,y\n    >>> f = Function('f')\n    >>> pdsolve(f(x,y) + f(x,y).diff(x) + f(x,y).diff(y))\n    Eq(f(x, y), F(x - y)*exp(-x/2 - y/2))\n    >>> pprint(pdsolve(f(x,y) + f(x,y).diff(x) + f(x,y).diff(y)))\n                          x   y\n                        - - - -\n                          2   2\n    f(x, y) = F(x - y)*e\n\n    References\n    ==========\n\n    - Viktor Grigoryan, \"Partial Differential Equations\"\n      Math 124A - Fall 2010, pp.7\n\n    \"\"\"\n    # TODO : For now homogeneous first order linear PDE's having\n    # two variables are implemented. Once there is support for\n    # solving systems of ODE's, this can be extended to n variables.\n\n    f = func.func\n    x = func.args[0]\n    y = func.args[1]\n    b = match[match['b']]\n    c = match[match['c']]\n    d = match[match['d']]\n    return Eq(f(x,y), exp(-S(d)/(b**2 + c**2)*(b*x + c*y))*solvefun(c*x - b*y))\n\n\ndef pde_1st_linear_constant_coeff(eq, func, order, match, solvefun):\n    r\"\"\"\n    Solves a first order linear partial differential equation\n    with constant coefficients.\n\n    The general form of this partial differential equation is\n\n    .. math:: a \\frac{df(x,y)}{dx} + b \\frac{df(x,y)}{dy} + c f(x,y) = G(x,y)\n\n    where `a`, `b` and `c` are constants and `G(x, y)` can be an arbitrary\n    function in `x` and `y`.\n\n    The general solution of the PDE is::\n\n        >>> from sympy.solvers import pdsolve\n        >>> from sympy.abc import x, y, a, b, c\n        >>> from sympy import Function, pprint\n        >>> f = Function('f')\n        >>> G = Function('G')\n        >>> u = f(x,y)\n        >>> ux = u.diff(x)\n        >>> uy = u.diff(y)\n        >>> genform = a*u + b*ux + c*uy - G(x,y)\n        >>> pprint(genform)\n                  d               d\n        a*f(x, y) + b*--(f(x, y)) + c*--(f(x, y)) - G(x, y)\n                  dx              dy\n        >>> pprint(pdsolve(genform, hint='1st_linear_constant_coeff_Integral'))\n                  //          b*x + c*y                                             \\\n                  ||              /                                                 |\n                  ||             |                                                  |\n                  ||             |                                       a*xi       |\n                  ||             |                                     -------      |\n                  ||             |                                      2    2      |\n                  ||             |      /b*xi + c*eta  -b*eta + c*xi\\  b  + c       |\n                  ||             |     G|------------, -------------|*e        d(xi)|\n                  ||             |      |   2    2         2    2   |               |\n                  ||             |      \\  b  + c         b  + c    /               |\n                  ||             |                                                  |\n                  ||            /                                                   |\n                  ||                                                                |\n        f(x, y) = ||F(eta) + -------------------------------------------------------|*\n                  ||                                  2    2                        |\n                  \\\\                                 b  + c                         /\n        <BLANKLINE>\n                \\|\n                ||\n                ||\n                ||\n                ||\n                ||\n                ||\n                ||\n                ||\n          -a*xi ||\n         -------||\n          2    2||\n         b  + c ||\n        e       ||\n                ||\n                /|eta=-b*y + c*x, xi=b*x + c*y\n\n\n    Examples\n    ========\n\n    >>> from sympy.solvers.pde import pdsolve\n    >>> from sympy import Function, diff, pprint, exp\n    >>> from sympy.abc import x,y\n    >>> f = Function('f')\n    >>> eq = -2*f(x,y).diff(x) + 4*f(x,y).diff(y) + 5*f(x,y) - exp(x + 3*y)\n    >>> pdsolve(eq)\n    Eq(f(x, y), (F(4*x + 2*y) + exp(x/2 + 4*y)/15)*exp(x/2 - y))\n\n    References\n    ==========\n\n    - Viktor Grigoryan, \"Partial Differential Equations\"\n      Math 124A - Fall 2010, pp.7\n\n    \"\"\"\n\n    # TODO : For now homogeneous first order linear PDE's having\n    # two variables are implemented. Once there is support for\n    # solving systems of ODE's, this can be extended to n variables.\n    xi, eta = symbols(\"xi eta\")\n    f = func.func\n    x = func.args[0]\n    y = func.args[1]\n    b = match[match['b']]\n    c = match[match['c']]\n    d = match[match['d']]\n    e = -match[match['e']]\n    expterm = exp(-S(d)/(b**2 + c**2)*xi)\n    functerm = solvefun(eta)\n    solvedict = solve((b*x + c*y - xi, c*x - b*y - eta), x, y)\n    # Integral should remain as it is in terms of xi,\n    # doit() should be done in _handle_Integral.\n    genterm = (1/S(b**2 + c**2))*Integral(\n        (1/expterm*e).subs(solvedict), (xi, b*x + c*y))\n    return Eq(f(x,y), Subs(expterm*(functerm + genterm),\n        (eta, xi), (c*x - b*y, b*x + c*y)))\n\n\ndef pde_1st_linear_variable_coeff(eq, func, order, match, solvefun):\n    r\"\"\"\n    Solves a first order linear partial differential equation\n    with variable coefficients. The general form of this partial differential equation is\n\n    .. math:: a(x, y) \\frac{df(x, y)}{dx} + a(x, y) \\frac{df(x, y)}{dy}\n                + c(x, y) f(x, y) - G(x, y)\n\n    where `a(x, y)`, `b(x, y)`, `c(x, y)` and `G(x, y)` are arbitrary functions\n    in `x` and `y`. This PDE is converted into an ODE by making the following transformation.\n\n    1] `\\xi` as `x`\n\n    2] `\\eta` as the constant in the solution to the differential equation\n    `\\frac{dy}{dx} = -\\frac{b}{a}`\n\n    Making the following substitutions reduces it to the linear ODE\n\n    .. math:: a(\\xi, \\eta)\\frac{du}{d\\xi} + c(\\xi, \\eta)u - d(\\xi, \\eta) = 0\n\n    which can be solved using dsolve.\n\n    The general form of this PDE is::\n\n        >>> from sympy.solvers.pde import pdsolve\n        >>> from sympy.abc import x, y\n        >>> from sympy import Function, pprint\n        >>> a, b, c, G, f= [Function(i) for i in ['a', 'b', 'c', 'G', 'f']]\n        >>> u = f(x,y)\n        >>> ux = u.diff(x)\n        >>> uy = u.diff(y)\n        >>> genform = a(x, y)*u + b(x, y)*ux + c(x, y)*uy - G(x,y)\n        >>> pprint(genform)\n                                             d                     d\n        -G(x, y) + a(x, y)*f(x, y) + b(x, y)*--(f(x, y)) + c(x, y)*--(f(x, y))\n                                             dx                    dy\n\n    Examples\n    ========\n\n    >>> from sympy.solvers.pde import pdsolve\n    >>> from sympy import Function, diff, pprint, exp\n    >>> from sympy.abc import x,y\n    >>> f = Function('f')\n    >>> eq =  x*(u.diff(x)) - y*(u.diff(y)) + y**2*u - y**2\n    >>> pdsolve(eq)\n    Eq(f(x, y), F(x*y)*exp(y**2/2) + 1)\n\n    References\n    ==========\n\n    - Viktor Grigoryan, \"Partial Differential Equations\"\n      Math 124A - Fall 2010, pp.7\n\n    \"\"\"\n    from sympy.integrals.integrals import integrate\n    from sympy.solvers.ode import dsolve\n\n    xi, eta = symbols(\"xi eta\")\n    f = func.func\n    x = func.args[0]\n    y = func.args[1]\n    b = match[match['b']]\n    c = match[match['c']]\n    d = match[match['d']]\n    e = -match[match['e']]\n\n\n    if not d:\n         # To deal with cases like b*ux = e or c*uy = e\n         if not (b and c):\n            if c:\n                try:\n                    tsol = integrate(e/c, y)\n                except NotImplementedError:\n                    raise NotImplementedError(\"Unable to find a solution\"\n                        \" due to inability of integrate\")\n                else:\n                    return Eq(f(x,y), solvefun(x) + tsol)\n            if b:\n                try:\n                    tsol = integrate(e/b, x)\n                except NotImplementedError:\n                    raise NotImplementedError(\"Unable to find a solution\"\n                        \" due to inability of integrate\")\n                else:\n                    return Eq(f(x,y), solvefun(y) + tsol)\n\n    if not c:\n        # To deal with cases when c is 0, a simpler method is used.\n        # The PDE reduces to b*(u.diff(x)) + d*u = e, which is a linear ODE in x\n        plode = f(x).diff(x)*b + d*f(x) - e\n        sol = dsolve(plode, f(x))\n        syms = sol.free_symbols - plode.free_symbols - {x, y}\n        rhs = _simplify_variable_coeff(sol.rhs, syms, solvefun, y)\n        return Eq(f(x, y), rhs)\n\n    if not b:\n        # To deal with cases when b is 0, a simpler method is used.\n        # The PDE reduces to c*(u.diff(y)) + d*u = e, which is a linear ODE in y\n        plode = f(y).diff(y)*c + d*f(y) - e\n        sol = dsolve(plode, f(y))\n        syms = sol.free_symbols - plode.free_symbols - {x, y}\n        rhs = _simplify_variable_coeff(sol.rhs, syms, solvefun, x)\n        return Eq(f(x, y), rhs)\n\n    dummy = Function('d')\n    h = (c/b).subs(y, dummy(x))\n    sol = dsolve(dummy(x).diff(x) - h, dummy(x))\n    if isinstance(sol, list):\n        sol = sol[0]\n    solsym = sol.free_symbols - h.free_symbols - {x, y}\n    if len(solsym) == 1:\n        solsym = solsym.pop()\n        etat = (solve(sol, solsym)[0]).subs(dummy(x), y)\n        ysub = solve(eta - etat, y)[0]\n        deq = (b*(f(x).diff(x)) + d*f(x) - e).subs(y, ysub)\n        final = (dsolve(deq, f(x), hint='1st_linear')).rhs\n        if isinstance(final, list):\n            final = final[0]\n        finsyms = final.free_symbols - deq.free_symbols - {x, y}\n        rhs = _simplify_variable_coeff(final, finsyms, solvefun, etat)\n        return Eq(f(x, y), rhs)\n\n    else:\n        raise NotImplementedError(\"Cannot solve the partial differential equation due\"\n            \" to inability of constantsimp\")\n\n\ndef _simplify_variable_coeff(sol, syms, func, funcarg):\n    r\"\"\"\n    Helper function to replace constants by functions in 1st_linear_variable_coeff\n    \"\"\"\n    eta = Symbol(\"eta\")\n    if len(syms) == 1:\n        sym = syms.pop()\n        final = sol.subs(sym, func(funcarg))\n\n    else:\n        fname = func.__name__\n        for key, sym in enumerate(syms):\n            tempfun = Function(fname + str(key))\n            final = sol.subs(sym, func(funcarg))\n\n    return simplify(final.subs(eta, funcarg))\n\n\ndef pde_separate(eq, fun, sep, strategy='mul'):\n    \"\"\"Separate variables in partial differential equation either by additive\n    or multiplicative separation approach. It tries to rewrite an equation so\n    that one of the specified variables occurs on a different side of the\n    equation than the others.\n\n    :param eq: Partial differential equation\n\n    :param fun: Original function F(x, y, z)\n\n    :param sep: List of separated functions [X(x), u(y, z)]\n\n    :param strategy: Separation strategy. You can choose between additive\n        separation ('add') and multiplicative separation ('mul') which is\n        default.\n\n    Examples\n    ========\n\n    >>> from sympy import E, Eq, Function, pde_separate, Derivative as D\n    >>> from sympy.abc import x, t\n    >>> u, X, T = map(Function, 'uXT')\n\n    >>> eq = Eq(D(u(x, t), x), E**(u(x, t))*D(u(x, t), t))\n    >>> pde_separate(eq, u(x, t), [X(x), T(t)], strategy='add')\n    [exp(-X(x))*Derivative(X(x), x), exp(T(t))*Derivative(T(t), t)]\n\n    >>> eq = Eq(D(u(x, t), x, 2), D(u(x, t), t, 2))\n    >>> pde_separate(eq, u(x, t), [X(x), T(t)], strategy='mul')\n    [Derivative(X(x), (x, 2))/X(x), Derivative(T(t), (t, 2))/T(t)]\n\n    See Also\n    ========\n    pde_separate_add, pde_separate_mul\n    \"\"\"\n\n    do_add = False\n    if strategy == 'add':\n        do_add = True\n    elif strategy == 'mul':\n        do_add = False\n    else:\n        raise ValueError('Unknown strategy: %s' % strategy)\n\n    if isinstance(eq, Equality):\n        if eq.rhs != 0:\n            return pde_separate(Eq(eq.lhs - eq.rhs), fun, sep, strategy)\n    else:\n        return pde_separate(Eq(eq, 0), fun, sep, strategy)\n\n    if eq.rhs != 0:\n        raise ValueError(\"Value should be 0\")\n\n    # Handle arguments\n    orig_args = list(fun.args)\n    subs_args = []\n    for s in sep:\n        for j in range(0, len(s.args)):\n            subs_args.append(s.args[j])\n\n    if do_add:\n        functions = reduce(operator.add, sep)\n    else:\n        functions = reduce(operator.mul, sep)\n\n    # Check whether variables match\n    if len(subs_args) != len(orig_args):\n        raise ValueError(\"Variable counts do not match\")\n    # Check for duplicate arguments like  [X(x), u(x, y)]\n    if has_dups(subs_args):\n        raise ValueError(\"Duplicate substitution arguments detected\")\n    # Check whether the variables match\n    if set(orig_args) != set(subs_args):\n        raise ValueError(\"Arguments do not match\")\n\n    # Substitute original function with separated...\n    result = eq.lhs.subs(fun, functions).doit()\n\n    # Divide by terms when doing multiplicative separation\n    if not do_add:\n        eq = 0\n        for i in result.args:\n            eq += i/functions\n        result = eq\n\n    svar = subs_args[0]\n    dvar = subs_args[1:]\n    return _separate(result, svar, dvar)\n\n\ndef pde_separate_add(eq, fun, sep):\n    \"\"\"\n    Helper function for searching additive separable solutions.\n\n    Consider an equation of two independent variables x, y and a dependent\n    variable w, we look for the product of two functions depending on different\n    arguments:\n\n    `w(x, y, z) = X(x) + y(y, z)`\n\n    Examples\n    ========\n\n    >>> from sympy import E, Eq, Function, pde_separate_add, Derivative as D\n    >>> from sympy.abc import x, t\n    >>> u, X, T = map(Function, 'uXT')\n\n    >>> eq = Eq(D(u(x, t), x), E**(u(x, t))*D(u(x, t), t))\n    >>> pde_separate_add(eq, u(x, t), [X(x), T(t)])\n    [exp(-X(x))*Derivative(X(x), x), exp(T(t))*Derivative(T(t), t)]\n\n    \"\"\"\n    return pde_separate(eq, fun, sep, strategy='add')\n\n\ndef pde_separate_mul(eq, fun, sep):\n    \"\"\"\n    Helper function for searching multiplicative separable solutions.\n\n    Consider an equation of two independent variables x, y and a dependent\n    variable w, we look for the product of two functions depending on different\n    arguments:\n\n    `w(x, y, z) = X(x)*u(y, z)`\n\n    Examples\n    ========\n\n    >>> from sympy import Function, Eq, pde_separate_mul, Derivative as D\n    >>> from sympy.abc import x, y\n    >>> u, X, Y = map(Function, 'uXY')\n\n    >>> eq = Eq(D(u(x, y), x, 2), D(u(x, y), y, 2))\n    >>> pde_separate_mul(eq, u(x, y), [X(x), Y(y)])\n    [Derivative(X(x), (x, 2))/X(x), Derivative(Y(y), (y, 2))/Y(y)]\n\n    \"\"\"\n    return pde_separate(eq, fun, sep, strategy='mul')\n\n\ndef _separate(eq, dep, others):\n    \"\"\"Separate expression into two parts based on dependencies of variables.\"\"\"\n\n    # FIRST PASS\n    # Extract derivatives depending our separable variable...\n    terms = set()\n    for term in eq.args:\n        if term.is_Mul:\n            for i in term.args:\n                if i.is_Derivative and not i.has(*others):\n                    terms.add(term)\n                    continue\n        elif term.is_Derivative and not term.has(*others):\n            terms.add(term)\n    # Find the factor that we need to divide by\n    div = set()\n    for term in terms:\n        ext, sep = term.expand().as_independent(dep)\n        # Failed?\n        if sep.has(*others):\n            return None\n        div.add(ext)\n    # FIXME: Find lcm() of all the divisors and divide with it, instead of\n    # current hack :(\n    # https://github.com/sympy/sympy/issues/4597\n    if len(div) > 0:\n        final = 0\n        for term in eq.args:\n            eqn = 0\n            for i in div:\n                eqn += term / i\n            final += simplify(eqn)\n        eq = final\n\n    # SECOND PASS - separate the derivatives\n    div = set()\n    lhs = rhs = 0\n    for term in eq.args:\n        # Check, whether we have already term with independent variable...\n        if not term.has(*others):\n            lhs += term\n            continue\n        # ...otherwise, try to separate\n        temp, sep = term.expand().as_independent(dep)\n        # Failed?\n        if sep.has(*others):\n            return None\n        # Extract the divisors\n        div.add(sep)\n        rhs -= term.expand()\n    # Do the division\n    fulldiv = reduce(operator.add, div)\n    lhs = simplify(lhs/fulldiv).expand()\n    rhs = simplify(rhs/fulldiv).expand()\n    # ...and check whether we were successful :)\n    if lhs.has(*others) or rhs.has(dep):\n        return None\n    return [lhs, rhs]\n", "import_text": ["itertools.combinations_with_replacement", "sympy.simplify.simplify", "sympy.core.Add", "sympy.core.S", "sympy.core.compatibility.reduce", "sympy.core.compatibility.is_sequence", "sympy.core.compatibility.range", "sympy.core.function.Function", "sympy.core.function.expand", "sympy.core.function.AppliedUndef", "sympy.core.function.Subs", "sympy.core.relational.Equality", "sympy.core.relational.Eq", "sympy.core.symbol.Symbol", "sympy.core.symbol.Wild", "sympy.core.symbol.symbols", "sympy.functions.exp", "sympy.integrals.integrals.Integral", "sympy.utilities.iterables.has_dups", "sympy.utilities.misc.filldedent", "sympy.solvers.deutils._preprocess", "sympy.solvers.deutils.ode_order", "sympy.solvers.deutils._desolve", "sympy.solvers.solvers.solve", "sympy.simplify.radsimp.collect", "operator"], "prompt": "\"\"\"\nDescription: This function classifies a partial differential equation (PDE) based on its order and coefficients.\n\nArgs:\n    eq (sympy.Eq): The equation to be classified.\n    func (sympy.Function, optional): The function to be used in the PDE. Defaults to None.\n    dict (bool, optional): If True, the function returns a dictionary with classification hints. Defaults to False.\n    **kwargs: Additional keyword arguments.\n\nReturns:\n    tuple or dict: A tuple of classification hints or a dictionary with classification hints, depending on the 'dict' parameter.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Returns a tuple of possible pdsolve() classifications for a PDE.\n\n    The tuple is ordered so that first item is the classification that\n    pdsolve() uses to solve the PDE by default.  In general,\n    classifications near the beginning of the list will produce\n    better solutions faster than those near the end, though there are\n    always exceptions.  To make pdsolve use a different classification,\n    use pdsolve(PDE, func, hint=<classification>).  See also the pdsolve()\n    docstring for different meta-hints you can use.\n\n    If ``dict`` is true, classify_pde() will return a dictionary of\n    hint:match expression terms. This is intended for internal use by\n    pdsolve().  Note that because dictionaries are ordered arbitrarily,\n    this will most likely not be in the same order as the tuple.\n\n    You can get help on different hints by doing help(pde.pde_hintname),\n    where hintname is the name of the hint without \"_Integral\".\n\n    See sympy.pde.allhints or the sympy.pde docstring for a list of all\n    supported hints that can be returned from classify_pde.\n\n\n    Examples\n    ========\n\n    >>> from sympy.solvers.pde import classify_pde\n    >>> from sympy import Function, diff, Eq\n    >>> from sympy.abc import x, y\n    >>> f = Function('f')\n    >>> u = f(x, y)\n    >>> ux = u.diff(x)\n    >>> uy = u.diff(y)\n    >>> eq = Eq(1 + (2*(ux/u)) + (3*(uy/u)))\n    >>> classify_pde(eq)\n    ('1st_linear_constant_coeff_homogeneous',)\n    \"\"\"", "function_dependencies": ["sympy.solvers.deutils._preprocess", "sympy.solvers.deutils.ode_order", "sympy.core.function.expand", "sympy.core.symbol.Wild", "itertools.combinations_with_replacement", "sympy.core.function.expand.coeff", "sympy.core.function.expand.coeff.match", "sympy.core.Add", "sympy.simplify.radsimp.collect", "sympy.simplify.radsimp.collect.match", "sympy.simplify.radsimp.collect.match.update"], "project_create_time": "2018-01-08T14:28:29+00:00", "project_update_time": "2024-04-16T22:36:42+00:00", "file_create_time": "2019-01-24T21:57:23Z", "file_update_time": "2019-01-24T21:57:23Z", "function_update_time": "2019-01-24T21:57:23Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["sympy.core.function.expand"], "test_function": [{"file_path": "/python3_ios-v1.0/python3_ios-1.0/site-packages/sympy/solvers/tests/test_pde.py", "class_name": null, "function_name": "test_pde_classify", "code": "\ndef test_pde_classify():\n    # When more number of hints are added, add tests for classifying here.\n    f = Function('f')\n    eq1 = a*f(x,y) + b*f(x,y).diff(x) + c*f(x,y).diff(y)\n    eq2 = 3*f(x,y) + 2*f(x,y).diff(x) + f(x,y).diff(y)\n    eq3 = a*f(x,y) + b*f(x,y).diff(x) + 2*f(x,y).diff(y)\n    eq4 = x*f(x,y) + f(x,y).diff(x) + 3*f(x,y).diff(y)\n    eq5 = x**2*f(x,y) + x*f(x,y).diff(x) + x*y*f(x,y).diff(y)\n    eq6 = y*x**2*f(x,y) + y*f(x,y).diff(x) + f(x,y).diff(y)\n    for eq in [eq1, eq2, eq3]:\n        assert classify_pde(eq) == ('1st_linear_constant_coeff_homogeneous',)\n    for eq in [eq4, eq5, eq6]:\n        assert classify_pde(eq) == ('1st_linear_variable_coeff',)"}, {"file_path": "/python3_ios-v1.0/python3_ios-1.0/site-packages/sympy/solvers/tests/test_pde.py", "class_name": null, "function_name": "test_pde_1st_linear_constant_coeff_homogeneous", "code": "\ndef test_pde_1st_linear_constant_coeff_homogeneous():\n    f, F = map(Function, ['f', 'F'])\n    u = f(x, y)\n    eq = 2*u + u.diff(x) + u.diff(y)\n    assert classify_pde(eq) == ('1st_linear_constant_coeff_homogeneous',)\n    sol = pdsolve(eq)\n    assert sol == Eq(u, F(x - y)*exp(-x - y))\n    assert checkpdesol(eq, sol)[0]\n\n    eq = 4 + (3*u.diff(x)/u) + (2*u.diff(y)/u)\n    assert classify_pde(eq) == ('1st_linear_constant_coeff_homogeneous',)\n    sol = pdsolve(eq)\n    assert sol == Eq(u, F(2*x - 3*y)*exp(-S(12)*x/13 - S(8)*y/13))\n    assert checkpdesol(eq, sol)[0]\n\n    eq = u + (6*u.diff(x)) + (7*u.diff(y))\n    assert classify_pde(eq) == ('1st_linear_constant_coeff_homogeneous',)\n    sol = pdsolve(eq)\n    assert sol == Eq(u, F(7*x - 6*y)*exp(-6*x/S(85) - 7*y/S(85)))\n    assert checkpdesol(eq, sol)[0]\n\n    eq = a*u + b*u.diff(x) + c*u.diff(y)\n    sol = pdsolve(eq)\n    assert checkpdesol(eq, sol)[0]"}, {"file_path": "/python3_ios-v1.0/python3_ios-1.0/site-packages/sympy/solvers/tests/test_pde.py", "class_name": null, "function_name": "test_pde_1st_linear_constant_coeff", "code": "\ndef test_pde_1st_linear_constant_coeff():\n    f, F = map(Function, ['f', 'F'])\n    u = f(x,y)\n    eq = -2*u.diff(x) + 4*u.diff(y) + 5*u - exp(x + 3*y)\n    sol = pdsolve(eq)\n    assert sol == Eq(f(x,y),\n    (F(4*x + 2*y) + exp(x/S(2) + 4*y)/S(15))*exp(x/S(2) - y))\n    assert classify_pde(eq) == ('1st_linear_constant_coeff',\n    '1st_linear_constant_coeff_Integral')\n    assert checkpdesol(eq, sol)[0]\n\n    eq = (u.diff(x)/u) + (u.diff(y)/u) + 1 - (exp(x + y)/u)\n    sol = pdsolve(eq)\n    assert sol == Eq(f(x, y), F(x - y)*exp(-x/2 - y/2) + exp(x + y)/S(3))\n    assert classify_pde(eq) == ('1st_linear_constant_coeff',\n    '1st_linear_constant_coeff_Integral')\n    assert checkpdesol(eq, sol)[0]\n\n    eq = 2*u + -u.diff(x) + 3*u.diff(y) + sin(x)\n    sol = pdsolve(eq)\n    assert sol == Eq(f(x, y),\n         F(3*x + y)*exp(x/S(5) - 3*y/S(5)) - 2*sin(x)/S(5) - cos(x)/S(5))\n    assert classify_pde(eq) == ('1st_linear_constant_coeff',\n    '1st_linear_constant_coeff_Integral')\n    assert checkpdesol(eq, sol)[0]\n\n    eq = u + u.diff(x) + u.diff(y) + x*y\n    sol = pdsolve(eq)\n    assert sol == Eq(f(x, y),\n        -x*y + x + y + F(x - y)*exp(-x/S(2) - y/S(2)) - 2)\n    assert classify_pde(eq) == ('1st_linear_constant_coeff',\n    '1st_linear_constant_coeff_Integral')\n    assert checkpdesol(eq, sol)[0]\n\n    eq = u + u.diff(x) + u.diff(y) + log(x)\n    assert classify_pde(eq) == ('1st_linear_constant_coeff',\n    '1st_linear_constant_coeff_Integral')"}]}, {"git_group": "amundsen-io", "git_name": "amundsen", "version": "search-4.2.0", "language": "Python", "project_name": "amundsen-search-4.2.0.zip", "file_path": "/amundsen-search-4.2.0/amundsen-search-4.2.0/databuilder/databuilder/extractor/glue_extractor.py", "file_name": "glue_extractor.py", "focal_class": "GlueExtractor", "focal_name": "init", "focal_parameter": [], "solution": "\n    def init(self, conf: ConfigTree) -> None:\n        conf = conf.with_fallback(GlueExtractor.DEFAULT_CONFIG)\n        self._cluster = conf.get_string(GlueExtractor.CLUSTER_KEY)\n        self._filters = conf.get(GlueExtractor.FILTER_KEY)\n        self._max_results = conf.get(GlueExtractor.MAX_RESULTS_KEY)\n        self._resource_share_type = conf.get(GlueExtractor.RESOURCE_SHARE_TYPE)\n        self._region_name = conf.get(GlueExtractor.REGION_NAME_KEY)\n        self._partition_badge_label = conf.get(GlueExtractor.PARTITION_BADGE_LABEL_KEY)\n        if self._region_name is not None:\n            self._glue = boto3.client('glue', region_name=self._region_name)\n        else:\n            self._glue = boto3.client('glue')\n        self._extract_iter: Union[None, Iterator] = None", "function_signature": "def init(self, conf: ConfigTree) -> None :", "left_context": "# Copyright Contributors to the Amundsen project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom typing import (\n    Any, Dict, Iterator, List, Union,\n)\n\nimport boto3\nfrom pyhocon import ConfigFactory, ConfigTree\n\nfrom databuilder.extractor.base_extractor import Extractor\nfrom databuilder.models.table_metadata import ColumnMetadata, TableMetadata\n\n\nclass GlueExtractor(Extractor):\n    \"\"\"\n    Extracts tables and columns metadata from AWS Glue metastore\n    \"\"\"\n\n    CLUSTER_KEY = 'cluster'\n    FILTER_KEY = 'filters'\n    MAX_RESULTS_KEY = 'max_results'\n    RESOURCE_SHARE_TYPE = 'resource_share_type'\n    REGION_NAME_KEY = \"region\"\n    PARTITION_BADGE_LABEL_KEY = \"partition_badge_label\"\n\n    DEFAULT_CONFIG = ConfigFactory.from_dict({\n        CLUSTER_KEY: 'gold',\n        FILTER_KEY: None,\n        MAX_RESULTS_KEY: 500,\n        RESOURCE_SHARE_TYPE: \"ALL\",\n        REGION_NAME_KEY: None,\n        PARTITION_BADGE_LABEL_KEY: None,\n    })\n", "right_context": "\n    def extract(self) -> Union[TableMetadata, None]:\n        if not self._extract_iter:\n            self._extract_iter = self._get_extract_iter()\n        try:\n            return next(self._extract_iter)\n        except StopIteration:\n            return None\n\n    def get_scope(self) -> str:\n        return 'extractor.glue'\n\n    def _get_extract_iter(self) -> Iterator[TableMetadata]:\n        \"\"\"\n        It gets all tables and yields TableMetadata\n        :return:\n        \"\"\"\n        for row in self._get_raw_extract_iter():\n            columns, i = [], 0\n\n            if 'StorageDescriptor' not in row:\n                continue\n\n            for column in row['StorageDescriptor']['Columns']:\n                columns.append(ColumnMetadata(\n                    name=column[\"Name\"],\n                    description=column.get(\"Comment\"),\n                    col_type=column[\"Type\"],\n                    sort_order=i,\n                ))\n                i += 1\n\n            for column in row.get('PartitionKeys', []):\n                columns.append(ColumnMetadata(\n                    name=column[\"Name\"],\n                    description=column.get(\"Comment\"),\n                    col_type=column[\"Type\"],\n                    sort_order=i,\n                    badges=[self._partition_badge_label] if self._partition_badge_label else None,\n                ))\n                i += 1\n\n            yield TableMetadata(\n                'glue',\n                self._cluster,\n                row['DatabaseName'],\n                row['Name'],\n                row.get('Description') or row.get('Parameters', {}).get('comment'),\n                columns,\n                row.get('TableType') == 'VIRTUAL_VIEW',\n            )\n\n    def _get_raw_extract_iter(self) -> Iterator[Dict[str, Any]]:\n        \"\"\"\n        Provides iterator of results row from glue client\n        :return:\n        \"\"\"\n        tables = self._search_tables()\n        return iter(tables)\n\n    def _search_tables(self) -> List[Dict[str, Any]]:\n        tables = []\n        kwargs = {}\n        if self._filters is not None:\n            kwargs['Filters'] = self._filters\n            kwargs['MaxResults'] = self._max_results\n        if self._resource_share_type:\n            kwargs['ResourceShareType'] = self._resource_share_type\n        data = self._glue.search_tables(**kwargs)\n        tables += data['TableList']\n        while 'NextToken' in data:\n            token = data['NextToken']\n            kwargs['NextToken'] = token\n            data = self._glue.search_tables(**kwargs)\n            tables += data['TableList']\n        return tables\n", "import_text": ["typing.Any", "typing.Dict", "typing.Iterator", "typing.List", "typing.Union", "boto3", "pyhocon.ConfigFactory", "pyhocon.ConfigTree", "databuilder.extractor.base_extractor.Extractor", "databuilder.models.table_metadata.ColumnMetadata", "databuilder.models.table_metadata.TableMetadata"], "prompt": "\"\"\"\nDescription: This function initializes an instance of the class with a configuration tree.\n\nArgs:\n    self: The instance of the class.\n    conf (ConfigTree): The configuration tree to be used for initialization.\n\nReturns:\n    None: This function does not return any value.\n\"\"\"", "comment": null, "prompt_is_gen_from_api": true, "function_dependencies": ["boto3.client"], "project_create_time": "2019-05-14T15:12:40+00:00", "project_update_time": "2024-04-17T09:08:12+00:00", "file_create_time": "2019-10-24T19:21:43Z", "file_update_time": "2022-02-12T21:20:00Z", "function_update_time": "2019-10-24T19:21:43Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["boto3.client"], "test_function": [{"file_path": "/amundsen-search-4.2.0/amundsen-search-4.2.0/databuilder/tests/unit/extractor/test_glue_extractor.py", "class_name": "TestGlueExtractor", "function_name": "test_extraction_with_single_result", "code": "\n    def test_extraction_with_single_result(self) -> None:\n        with patch.object(GlueExtractor, '_search_tables') as mock_search:\n            mock_search.return_value = [\n                {\n                    'Name': 'test_table',\n                    'DatabaseName': 'test_schema',\n                    'Description': 'a table for testing',\n                    'StorageDescriptor': {\n                        'Columns': [\n                            {\n                                'Name': 'col_id1',\n                                'Type': 'bigint',\n                                'Comment': 'description of id1'\n                            },\n                            {\n                                'Name': 'col_id2',\n                                'Type': 'bigint',\n                                'Comment': 'description of id2'\n                            },\n                            {\n                                'Name': 'is_active',\n                                'Type': 'boolean'\n                            },\n                            {\n                                'Name': 'source',\n                                'Type': 'varchar',\n                                'Comment': 'description of source'\n                            },\n                            {\n                                'Name': 'etl_created_at',\n                                'Type': 'timestamp',\n                                'Comment': 'description of etl_created_at'\n                            },\n                            {\n                                'Name': 'ds',\n                                'Type': 'varchar'\n                            }\n                        ]\n                    },\n                    'PartitionKeys': [\n                        {\n                            'Name': 'partition_key1',\n                            'Type': 'string',\n                            'Comment': 'description of partition_key1'\n                        },\n                    ],\n                    'TableType': 'EXTERNAL_TABLE',\n                }\n            ]\n\n            extractor = GlueExtractor()\n            extractor.init(self.conf)\n            actual = extractor.extract()\n            expected = TableMetadata('glue', 'gold', 'test_schema', 'test_table', 'a table for testing',\n                                     [ColumnMetadata('col_id1', 'description of id1', 'bigint', 0),\n                                      ColumnMetadata('col_id2', 'description of id2', 'bigint', 1),\n                                      ColumnMetadata('is_active', None, 'boolean', 2),\n                                      ColumnMetadata('source', 'description of source', 'varchar', 3),\n                                      ColumnMetadata('etl_created_at', 'description of etl_created_at', 'timestamp', 4),\n                                      ColumnMetadata('ds', None, 'varchar', 5),\n                                      ColumnMetadata('partition_key1', 'description of partition_key1', 'string', 6),\n                                      ], False)\n            self.assertEqual(expected.__repr__(), actual.__repr__())\n            self.assertIsNone(extractor.extract())"}, {"file_path": "/amundsen-search-4.2.0/amundsen-search-4.2.0/databuilder/tests/unit/extractor/test_glue_extractor.py", "class_name": "TestGlueExtractor", "function_name": "test_extraction_with_multiple_result", "code": "\n    def test_extraction_with_multiple_result(self) -> None:\n        with patch.object(GlueExtractor, '_search_tables') as mock_search:\n            mock_search.return_value = [\n                test_table,\n                {\n                    'Name': 'test_table2',\n                    'DatabaseName': 'test_schema1',\n                    'Description': 'test table 2',\n                    'StorageDescriptor': {\n                        'Columns': [\n                            {\n                                'Name': 'col_name',\n                                'Type': 'varchar',\n                                'Comment': 'description of col_name'\n                            },\n                            {\n                                'Name': 'col_name2',\n                                'Type': 'varchar',\n                                'Comment': 'description of col_name2'\n                            }\n                        ]\n                    },\n                    'TableType': 'EXTERNAL_TABLE',\n                },\n                {\n                    'Name': 'test_table3',\n                    'DatabaseName': 'test_schema2',\n                    'StorageDescriptor': {\n                        'Columns': [\n                            {\n                                'Name': 'col_id3',\n                                'Type': 'varchar',\n                                'Comment': 'description of col_id3'\n                            },\n                            {\n                                'Name': 'col_name3',\n                                'Type': 'varchar',\n                                'Comment': 'description of col_name3'\n                            }\n                        ]\n                    },\n                    'Parameters': {'comment': 'description of test table 3 from comment'},\n                    'TableType': 'EXTERNAL_TABLE',\n                },\n                {\n                    'Name': 'test_view1',\n                    'DatabaseName': 'test_schema1',\n                    'Description': 'test view 1',\n                    'StorageDescriptor': {\n                        'Columns': [\n                            {\n                                'Name': 'col_id3',\n                                'Type': 'varchar',\n                                'Comment': 'description of col_id3'\n                            },\n                            {\n                                'Name': 'col_name3',\n                                'Type': 'varchar',\n                                'Comment': 'description of col_name3'\n                            }\n                        ]\n                    },\n                    'TableType': 'VIRTUAL_VIEW',\n                },\n            ]\n\n            extractor = GlueExtractor()\n            extractor.init(self.conf)\n\n            expected = TableMetadata('glue', 'gold', 'test_schema', 'test_table', 'a table for testing',\n                                     [ColumnMetadata('col_id1', 'description of id1', 'bigint', 0),\n                                      ColumnMetadata('col_id2', 'description of id2', 'bigint', 1),\n                                      ColumnMetadata('is_active', None, 'boolean', 2),\n                                      ColumnMetadata('source', 'description of source', 'varchar', 3),\n                                      ColumnMetadata('etl_created_at', 'description of etl_created_at', 'timestamp', 4),\n                                      ColumnMetadata('ds', None, 'varchar', 5),\n                                      ColumnMetadata('partition_key1', 'description of partition_key1', 'string', 6),\n                                      ], False)\n            self.assertEqual(expected.__repr__(), extractor.extract().__repr__())\n\n            expected = TableMetadata('glue', 'gold', 'test_schema1', 'test_table2', 'test table 2',\n                                     [ColumnMetadata('col_name', 'description of col_name', 'varchar', 0),\n                                      ColumnMetadata('col_name2', 'description of col_name2', 'varchar', 1)], False)\n            self.assertEqual(expected.__repr__(), extractor.extract().__repr__())\n\n            expected = TableMetadata('glue', 'gold', 'test_schema2', 'test_table3',\n                                     'description of test table 3 from comment',\n                                     [ColumnMetadata('col_id3', 'description of col_id3', 'varchar', 0),\n                                      ColumnMetadata('col_name3', 'description of col_name3',\n                                                     'varchar', 1)], False)\n            self.assertEqual(expected.__repr__(), extractor.extract().__repr__())\n\n            expected = TableMetadata('glue', 'gold', 'test_schema1', 'test_view1', 'test view 1',\n                                     [ColumnMetadata('col_id3', 'description of col_id3', 'varchar', 0),\n                                      ColumnMetadata('col_name3', 'description of col_name3',\n                                                     'varchar', 1)], True)\n            self.assertEqual(expected.__repr__(), extractor.extract().__repr__())\n\n            self.assertIsNone(extractor.extract())\n            self.assertIsNone(extractor.extract())"}, {"file_path": "/amundsen-search-4.2.0/amundsen-search-4.2.0/databuilder/tests/unit/extractor/test_glue_extractor.py", "class_name": "TestGlueExtractor", "function_name": "test_extraction_with_resource_link_result", "code": "\n    def test_extraction_with_resource_link_result(self) -> None:\n        with patch.object(GlueExtractor, '_search_tables') as mock_search:\n            mock_search.return_value = [\n                test_table,\n                {\n                    \"Name\": \"test_resource_link\",\n                    \"DatabaseName\": \"test_schema\",\n                    \"TargetTable\": {\n                        \"CatalogId\": \"111111111111\",\n                        \"DatabaseName\": \"test_schema_external\",\n                        \"Name\": \"test_table\"\n                    },\n                    \"CatalogId\": \"222222222222\"\n                }\n            ]\n\n            extractor = GlueExtractor()\n            extractor.init(self.conf)\n            actual = extractor.extract()\n            expected = TableMetadata('glue', 'gold', 'test_schema', 'test_table', 'a table for testing',\n                                     [ColumnMetadata('col_id1', 'description of id1', 'bigint', 0),\n                                      ColumnMetadata('col_id2', 'description of id2', 'bigint', 1),\n                                      ColumnMetadata('is_active', None, 'boolean', 2),\n                                      ColumnMetadata('source', 'description of source', 'varchar', 3),\n                                      ColumnMetadata('etl_created_at', 'description of etl_created_at', 'timestamp', 4),\n                                      ColumnMetadata('ds', None, 'varchar', 5),\n                                      ColumnMetadata('partition_key1', 'description of partition_key1', 'string', 6),\n                                      ], False)\n            self.assertEqual(expected.__repr__(), actual.__repr__())\n            self.assertIsNone(extractor.extract())"}, {"file_path": "/amundsen-search-4.2.0/amundsen-search-4.2.0/databuilder/tests/unit/extractor/test_glue_extractor.py", "class_name": "TestGlueExtractor", "function_name": "test_extraction_with_partition_badge", "code": "\n    def test_extraction_with_partition_badge(self) -> None:\n        with patch.object(GlueExtractor, '_search_tables') as mock_search:\n            mock_search.return_value = [test_table]\n\n            extractor = GlueExtractor()\n            extractor.init(conf=ConfigFactory.from_dict({\n                GlueExtractor.PARTITION_BADGE_LABEL_KEY: \"partition_key\",\n            }))\n            actual = extractor.extract()\n            expected = TableMetadata('glue', 'gold', 'test_schema', 'test_table', 'a table for testing',\n                                     [ColumnMetadata('col_id1', 'description of id1', 'bigint', 0),\n                                      ColumnMetadata('col_id2', 'description of id2', 'bigint', 1),\n                                      ColumnMetadata('is_active', None, 'boolean', 2),\n                                      ColumnMetadata('source', 'description of source', 'varchar', 3),\n                                      ColumnMetadata('etl_created_at', 'description of etl_created_at', 'timestamp', 4),\n                                      ColumnMetadata('ds', None, 'varchar', 5),\n                                      ColumnMetadata(\n                                          'partition_key1',\n                                          'description of partition_key1',\n                                          'string',\n                                          6,\n                                          [\"partition_key\"],\n                                     ),\n                                     ], False)\n            self.assertEqual(expected.__repr__(), actual.__repr__())"}]}, {"git_group": "DoubleML", "git_name": "doubleml-for-py", "version": "0.7.1", "language": "Python", "project_name": "doubleml-for-py-0.7.1.zip", "file_path": "/doubleml-for-py-0.7.1/doubleml-for-py-0.7.1/doubleml/datasets.py", "file_name": "datasets.py", "focal_class": null, "focal_name": "make_pliv_multiway_cluster_CKMS2021", "focal_parameter": [], "solution": "def make_pliv_multiway_cluster_CKMS2021(N=25, M=25, dim_X=100, theta=1., return_type='DoubleMLClusterData', **kwargs):\n    # additional parameters specifiable via kwargs\n    pi_10 = kwargs.get('pi_10', 1.0)\n\n    xx = np.arange(1, dim_X + 1)\n    zeta_0 = kwargs.get('zeta_0', np.power(0.5, xx))\n    pi_20 = kwargs.get('pi_20', np.power(0.5, xx))\n    xi_0 = kwargs.get('xi_0', np.power(0.5, xx))\n\n    omega_X = kwargs.get('omega_X', np.array([0.25, 0.25]))\n    omega_epsilon = kwargs.get('omega_epsilon', np.array([0.25, 0.25]))\n    omega_v = kwargs.get('omega_v', np.array([0.25, 0.25]))\n    omega_V = kwargs.get('omega_V', np.array([0.25, 0.25]))\n\n    s_X = kwargs.get('s_X', 0.25)\n    s_epsilon_v = kwargs.get('s_epsilon_v', 0.25)\n\n    # use np.tile() and np.repeat() for repeating vectors in different styles, i.e.,\n    # np.tile([v1, v2, v3], 2) [v1, v2, v3, v1, v2, v3]\n    # np.repeat([v1, v2, v3], 2) [v1, v1, v2, v2, v3, v3]\n\n    alpha_V = np.random.normal(size=(N * M))\n    alpha_V_i = np.repeat(np.random.normal(size=N), M)\n    alpha_V_j = np.tile(np.random.normal(size=M), N)\n\n    cov_mat = np.array([[1, s_epsilon_v], [s_epsilon_v, 1]])\n    alpha_eps_v = np.random.multivariate_normal(np.zeros(2), cov_mat, size=[N * M, ])\n    alpha_eps = alpha_eps_v[:, 0]\n    alpha_v = alpha_eps_v[:, 1]\n\n    alpha_eps_v_i = np.random.multivariate_normal(np.zeros(2), cov_mat, size=[N, ])\n    alpha_eps_i = np.repeat(alpha_eps_v_i[:, 0], M)\n    alpha_v_i = np.repeat(alpha_eps_v_i[:, 1], M)\n\n    alpha_eps_v_j = np.random.multivariate_normal(np.zeros(2), cov_mat, size=[M, ])\n    alpha_eps_j = np.tile(alpha_eps_v_j[:, 0], N)\n    alpha_v_j = np.tile(alpha_eps_v_j[:, 1], N)\n\n    cov_mat = toeplitz([np.power(s_X, k) for k in range(dim_X)])\n    alpha_X = np.random.multivariate_normal(np.zeros(dim_X), cov_mat, size=[N * M, ])\n    alpha_X_i = np.repeat(np.random.multivariate_normal(np.zeros(dim_X), cov_mat, size=[N, ]),\n                          M, axis=0)\n    alpha_X_j = np.tile(np.random.multivariate_normal(np.zeros(dim_X), cov_mat, size=[M, ]),\n                        (N, 1))\n\n    # generate variables\n    x = (1 - omega_X[0] - omega_X[1]) * alpha_X \\\n        + omega_X[0] * alpha_X_i + omega_X[1] * alpha_X_j\n\n    eps = (1 - omega_epsilon[0] - omega_epsilon[1]) * alpha_eps \\\n        + omega_epsilon[0] * alpha_eps_i + omega_epsilon[1] * alpha_eps_j\n\n    v = (1 - omega_v[0] - omega_v[1]) * alpha_v \\\n        + omega_v[0] * alpha_v_i + omega_v[1] * alpha_v_j\n\n    V = (1 - omega_V[0] - omega_V[1]) * alpha_V \\\n        + omega_V[0] * alpha_V_i + omega_V[1] * alpha_V_j\n\n    z = np.matmul(x, xi_0) + V\n    d = z * pi_10 + np.matmul(x, pi_20) + v\n    y = d * theta + np.matmul(x, zeta_0) + eps\n\n    cluster_cols = ['cluster_var_i', 'cluster_var_j']\n    cluster_vars = pd.MultiIndex.from_product([range(N), range(M)]).to_frame(name=cluster_cols).reset_index(drop=True)\n\n    if return_type in _array_alias:\n        return x, y, d, cluster_vars.values, z\n    elif return_type in _data_frame_alias + _dml_cluster_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_X)]\n        data = pd.concat((cluster_vars,\n                          pd.DataFrame(np.column_stack((x, y, d, z)), columns=x_cols + ['Y', 'D', 'Z'])),\n                         axis=1)\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLClusterData(data, 'Y', 'D', cluster_cols, x_cols, 'Z')\n    else:\n        raise ValueError('Invalid return_type.')", "function_signature": "def make_pliv_multiway_cluster_CKMS2021(N=25, M=25, dim_X=100, theta=1., return_type='DoubleMLClusterData', **kwargs) :", "left_context": "import pandas as pd\nimport numpy as np\n\nfrom scipy.linalg import toeplitz\nfrom scipy.optimize import minimize_scalar\n\nfrom sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\nfrom sklearn.datasets import make_spd_matrix\n\nfrom .double_ml_data import DoubleMLData, DoubleMLClusterData\n\n_array_alias = ['array', 'np.ndarray', 'np.array', np.ndarray]\n_data_frame_alias = ['DataFrame', 'pd.DataFrame', pd.DataFrame]\n_dml_data_alias = ['DoubleMLData', DoubleMLData]\n_dml_cluster_data_alias = ['DoubleMLClusterData', DoubleMLClusterData]\n\n\ndef fetch_401K(return_type='DoubleMLData', polynomial_features=False):\n    \"\"\"\n    Data set on financial wealth and 401(k) plan participation.\n\n    Parameters\n    ----------\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n    polynomial_features :\n        If ``True`` polynomial features are added (see replication files of Chernozhukov et al. (2018)).\n\n    References\n    ----------\n    Abadie, A. (2003), Semiparametric instrumental variable estimation of treatment response models. Journal of\n    Econometrics, 113(2): 231-263.\n\n    Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W. and Robins, J. (2018),\n    Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21: C1-C68.\n    doi:`10.1111/ectj.12097 <https://doi.org/10.1111/ectj.12097>`_.\n    \"\"\"\n    url = 'https://github.com/VC2015/DMLonGitHub/raw/master/sipp1991.dta'\n    raw_data = pd.read_stata(url)\n\n    y_col = 'net_tfa'\n    d_cols = ['e401']\n    x_cols = ['age', 'inc', 'educ', 'fsize', 'marr', 'twoearn', 'db', 'pira', 'hown']\n\n    data = raw_data.copy()\n\n    if polynomial_features:\n        raise NotImplementedError('polynomial_features os not implemented yet for fetch_401K.')\n\n    if return_type in _data_frame_alias + _dml_data_alias:\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, y_col, d_cols, x_cols)\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef fetch_bonus(return_type='DoubleMLData', polynomial_features=False):\n    \"\"\"\n    Data set on the Pennsylvania Reemployment Bonus experiment.\n\n    Parameters\n    ----------\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n    polynomial_features :\n        If ``True`` polynomial features are added (see replication files of Chernozhukov et al. (2018)).\n\n    References\n    ----------\n    Bilias Y. (2000), Sequential Testing of Duration Data: The Case of Pennsylvania 'Reemployment Bonus' Experiment.\n    Journal of Applied Econometrics, 15(6): 575-594.\n\n    Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W. and Robins, J. (2018),\n    Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21: C1-C68.\n    doi:`10.1111/ectj.12097 <https://doi.org/10.1111/ectj.12097>`_.\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/VC2015/DMLonGitHub/master/penn_jae.dat'\n    raw_data = pd.read_csv(url, delim_whitespace=True)\n\n    ind = (raw_data['tg'] == 0) | (raw_data['tg'] == 4)\n    data = raw_data.copy()[ind]\n    data.reset_index(inplace=True)\n    data['tg'].replace(4, 1, inplace=True)\n    data['inuidur1'] = np.log(data['inuidur1'])\n\n    # variable dep as factor (dummy encoding)\n    dummy_enc = OneHotEncoder(drop='first', categories='auto').fit(data.loc[:, ['dep']])\n    xx = dummy_enc.transform(data.loc[:, ['dep']]).toarray()\n    data['dep1'] = xx[:, 0]\n    data['dep2'] = xx[:, 1]\n\n    y_col = 'inuidur1'\n    d_cols = ['tg']\n    x_cols = ['female', 'black', 'othrace',\n              'dep1', 'dep2',\n              'q2', 'q3', 'q4', 'q5', 'q6',\n              'agelt35', 'agegt54', 'durable', 'lusd', 'husd']\n\n    if polynomial_features:\n        poly = PolynomialFeatures(2, include_bias=False)\n        data_transf = poly.fit_transform(data[x_cols])\n        x_cols = list(poly.get_feature_names_out(x_cols))\n\n        data_transf = pd.DataFrame(data_transf, columns=x_cols)\n        data = pd.concat((data[[y_col] + d_cols], data_transf),\n                         axis=1, sort=False)\n\n    if return_type in _data_frame_alias + _dml_data_alias:\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, y_col, d_cols, x_cols)\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef _g(x):\n    return np.power(np.sin(x), 2)\n\n\ndef _m(x, nu=0., gamma=1.):\n    return 0.5/np.pi*(np.sinh(gamma))/(np.cosh(gamma)-np.cos(x-nu))\n\n\ndef make_plr_CCDDHNR2018(n_obs=500, dim_x=20, alpha=0.5, return_type='DoubleMLData', **kwargs):\n    \"\"\"\n    Generates data from a partially linear regression model used in Chernozhukov et al. (2018) for Figure 1.\n    The data generating process is defined as\n\n    .. math::\n\n        d_i &= m_0(x_i) + s_1 v_i, & &v_i \\\\sim \\\\mathcal{N}(0,1),\n\n        y_i &= \\\\alpha d_i + g_0(x_i) + s_2 \\\\zeta_i, & &\\\\zeta_i \\\\sim \\\\mathcal{N}(0,1),\n\n\n    with covariates :math:`x_i \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`, where  :math:`\\\\Sigma` is a matrix with entries\n    :math:`\\\\Sigma_{kj} = 0.7^{|j-k|}`.\n    The nuisance functions are given by\n\n    .. math::\n\n        m_0(x_i) &= a_0 x_{i,1} + a_1 \\\\frac{\\\\exp(x_{i,3})}{1+\\\\exp(x_{i,3})},\n\n        g_0(x_i) &= b_0 \\\\frac{\\\\exp(x_{i,1})}{1+\\\\exp(x_{i,1})} + b_1 x_{i,3}.\n\n    Parameters\n    ----------\n    n_obs :\n        The number of observations to simulate.\n    dim_x :\n        The number of covariates.\n    alpha :\n        The value of the causal parameter.\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s ``(x, y, d)``.\n    **kwargs\n        Additional keyword arguments to set non-default values for the parameters\n        :math:`a_0=1`, :math:`a_1=0.25`, :math:`s_1=1`, :math:`b_0=1`, :math:`b_1=0.25` or :math:`s_2=1`.\n\n    References\n    ----------\n    Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W. and Robins, J. (2018),\n    Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21: C1-C68.\n    doi:`10.1111/ectj.12097 <https://doi.org/10.1111/ectj.12097>`_.\n    \"\"\"\n    a_0 = kwargs.get('a_0', 1.)\n    a_1 = kwargs.get('a_1', 0.25)\n    s_1 = kwargs.get('s_1', 1.)\n\n    b_0 = kwargs.get('b_0', 1.)\n    b_1 = kwargs.get('b_1', 0.25)\n    s_2 = kwargs.get('s_2', 1.)\n\n    cov_mat = toeplitz([np.power(0.7, k) for k in range(dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x), cov_mat, size=[n_obs, ])\n\n    d = a_0 * x[:, 0] + a_1 * np.divide(np.exp(x[:, 2]), 1 + np.exp(x[:, 2])) \\\n        + s_1 * np.random.standard_normal(size=[n_obs, ])\n    y = alpha * d + b_0 * np.divide(np.exp(x[:, 0]), 1 + np.exp(x[:, 0])) \\\n        + b_1 * x[:, 2] + s_2 * np.random.standard_normal(size=[n_obs, ])\n\n    if return_type in _array_alias:\n        return x, y, d\n    elif return_type in _data_frame_alias + _dml_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_x)]\n        data = pd.DataFrame(np.column_stack((x, y, d)),\n                            columns=x_cols + ['y', 'd'])\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, 'y', 'd', x_cols)\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef make_plr_turrell2018(n_obs=100, dim_x=20, theta=0.5, return_type='DoubleMLData', **kwargs):\n    \"\"\"\n    Generates data from a partially linear regression model used in a blog article by Turrell (2018).\n    The data generating process is defined as\n\n    .. math::\n\n        d_i &= m_0(x_i' b) + v_i, & &v_i \\\\sim \\\\mathcal{N}(0,1),\n\n        y_i &= \\\\theta d_i + g_0(x_i' b) + u_i, & &u_i \\\\sim \\\\mathcal{N}(0,1),\n\n\n    with covariates :math:`x_i \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`, where  :math:`\\\\Sigma` is a random symmetric,\n    positive-definite matrix generated with :py:meth:`sklearn.datasets.make_spd_matrix`.\n    :math:`b` is a vector with entries :math:`b_j=\\\\frac{1}{j}` and the nuisance functions are given by\n\n    .. math::\n\n        m_0(x_i) &= \\\\frac{1}{2 \\\\pi} \\\\frac{\\\\sinh(\\\\gamma)}{\\\\cosh(\\\\gamma) - \\\\cos(x_i-\\\\nu)},\n\n        g_0(x_i) &= \\\\sin(x_i)^2.\n\n    Parameters\n    ----------\n    n_obs :\n        The number of observations to simulate.\n    dim_x :\n        The number of covariates.\n    theta :\n        The value of the causal parameter.\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s ``(x, y, d)``.\n    **kwargs\n        Additional keyword arguments to set non-default values for the parameters\n        :math:`\\\\nu=0`, or :math:`\\\\gamma=1`.\n\n    References\n    ----------\n    Turrell, A. (2018), Econometrics in Python part I - Double machine learning, Markov Wanderer: A blog on economics,\n    science, coding and data. `https://aeturrell.com/blog/posts/econometrics-in-python-parti-ml/\n    <https://aeturrell.com/blog/posts/econometrics-in-python-parti-ml/>`_.\n    \"\"\"\n    nu = kwargs.get('nu', 0.)\n    gamma = kwargs.get('gamma', 1.)\n\n    b = [1 / k for k in range(1, dim_x + 1)]\n    sigma = make_spd_matrix(dim_x)\n\n    x = np.random.multivariate_normal(np.zeros(dim_x), sigma, size=[n_obs, ])\n    G = _g(np.dot(x, b))\n    M = _m(np.dot(x, b), nu=nu, gamma=gamma)\n    d = M + np.random.standard_normal(size=[n_obs, ])\n    y = np.dot(theta, d) + G + np.random.standard_normal(size=[n_obs, ])\n\n    if return_type in _array_alias:\n        return x, y, d\n    elif return_type in _data_frame_alias + _dml_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_x)]\n        data = pd.DataFrame(np.column_stack((x, y, d)),\n                            columns=x_cols + ['y', 'd'])\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, 'y', 'd', x_cols)\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef make_irm_data(n_obs=500, dim_x=20, theta=0, R2_d=0.5, R2_y=0.5, return_type='DoubleMLData'):\n    \"\"\"\n    Generates data from a interactive regression (IRM) model.\n    The data generating process is defined as\n\n    .. math::\n\n        d_i &= 1\\\\left\\\\lbrace \\\\frac{\\\\exp(c_d x_i' \\\\beta)}{1+\\\\exp(c_d x_i' \\\\beta)} > v_i \\\\right\\\\rbrace, & &v_i\n        \\\\sim \\\\mathcal{U}(0,1),\n\n        y_i &= \\\\theta d_i + c_y x_i' \\\\beta d_i + \\\\zeta_i, & &\\\\zeta_i \\\\sim \\\\mathcal{N}(0,1),\n\n    with covariates :math:`x_i \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`, where  :math:`\\\\Sigma` is a matrix with entries\n    :math:`\\\\Sigma_{kj} = 0.5^{|j-k|}`.\n    :math:`\\\\beta` is a `dim_x`-vector with entries :math:`\\\\beta_j=\\\\frac{1}{j^2}` and the constants :math:`c_y` and\n    :math:`c_d` are given by\n\n    .. math::\n\n        c_y = \\\\sqrt{\\\\frac{R_y^2}{(1-R_y^2) \\\\beta' \\\\Sigma \\\\beta}}, \\\\qquad c_d =\n        \\\\sqrt{\\\\frac{(\\\\pi^2 /3) R_d^2}{(1-R_d^2) \\\\beta' \\\\Sigma \\\\beta}}.\n\n    The data generating process is inspired by a process used in the simulation experiment (see Appendix P) of Belloni\n    et al. (2017).\n\n    Parameters\n    ----------\n    n_obs :\n        The number of observations to simulate.\n    dim_x :\n        The number of covariates.\n    theta :\n        The value of the causal parameter.\n    R2_d :\n        The value of the parameter :math:`R_d^2`.\n    R2_y :\n        The value of the parameter :math:`R_y^2`.\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s ``(x, y, d)``.\n\n    References\n    ----------\n    Belloni, A., Chernozhukov, V., Fern\u00e1ndez\u2010Val, I. and Hansen, C. (2017). Program Evaluation and Causal Inference With\n    High\u2010Dimensional Data. Econometrica, 85: 233-298.\n    \"\"\"\n    # inspired by https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA12723, see suplement\n    v = np.random.uniform(size=[n_obs, ])\n    zeta = np.random.standard_normal(size=[n_obs, ])\n\n    cov_mat = toeplitz([np.power(0.5, k) for k in range(dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x), cov_mat, size=[n_obs, ])\n\n    beta = [1 / (k**2) for k in range(1, dim_x + 1)]\n    b_sigma_b = np.dot(np.dot(cov_mat, beta), beta)\n    c_y = np.sqrt(R2_y/((1-R2_y) * b_sigma_b))\n    c_d = np.sqrt(np.pi**2 / 3. * R2_d/((1-R2_d) * b_sigma_b))\n\n    xx = np.exp(np.dot(x, np.multiply(beta, c_d)))\n    d = 1. * ((xx/(1+xx)) > v)\n\n    y = d * theta + d * np.dot(x, np.multiply(beta, c_y)) + zeta\n\n    if return_type in _array_alias:\n        return x, y, d\n    elif return_type in _data_frame_alias + _dml_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_x)]\n        data = pd.DataFrame(np.column_stack((x, y, d)),\n                            columns=x_cols + ['y', 'd'])\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, 'y', 'd', x_cols)\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef make_iivm_data(n_obs=500, dim_x=20, theta=1., alpha_x=0.2, return_type='DoubleMLData'):\n    \"\"\"\n    Generates data from a interactive IV regression (IIVM) model.\n    The data generating process is defined as\n\n    .. math::\n\n        d_i &= 1\\\\left\\\\lbrace \\\\alpha_x Z + v_i > 0 \\\\right\\\\rbrace,\n\n        y_i &= \\\\theta d_i + x_i' \\\\beta + u_i,\n\n    with :math:`Z \\\\sim \\\\text{Bernoulli}(0.5)` and\n\n    .. math::\n\n        \\\\left(\\\\begin{matrix} u_i \\\\\\\\ v_i \\\\end{matrix} \\\\right) \\\\sim\n        \\\\mathcal{N}\\\\left(0, \\\\left(\\\\begin{matrix} 1 & 0.3 \\\\\\\\ 0.3 & 1 \\\\end{matrix} \\\\right) \\\\right).\n\n    The covariates :math:`x_i \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`, where  :math:`\\\\Sigma` is a matrix with entries\n    :math:`\\\\Sigma_{kj} = 0.5^{|j-k|}` and :math:`\\\\beta` is a `dim_x`-vector with entries\n    :math:`\\\\beta_j=\\\\frac{1}{j^2}`.\n\n    The data generating process is inspired by a process used in the simulation experiment of Farbmacher, Gruber and\n    Klaassen (2020).\n\n    Parameters\n    ----------\n    n_obs :\n        The number of observations to simulate.\n    dim_x :\n        The number of covariates.\n    theta :\n        The value of the causal parameter.\n    alpha_x :\n        The value of the parameter :math:`\\\\alpha_x`.\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s ``(x, y, d, z)``.\n\n    References\n    ----------\n    Farbmacher, H., Guber, R. and Klaa\u00dfen, S. (2020). Instrument Validity Tests with Causal Forests. MEA Discussion\n    Paper No. 13-2020. Available at SSRN: http://dx.doi.org/10.2139/ssrn.3619201.\n    \"\"\"\n    # inspired by https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3619201\n    xx = np.random.multivariate_normal(np.zeros(2),\n                                       np.array([[1., 0.3], [0.3, 1.]]),\n                                       size=[n_obs, ])\n    u = xx[:, 0]\n    v = xx[:, 1]\n\n    cov_mat = toeplitz([np.power(0.5, k) for k in range(dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x), cov_mat, size=[n_obs, ])\n\n    beta = [1 / (k**2) for k in range(1, dim_x + 1)]\n\n    z = np.random.binomial(p=0.5, n=1, size=[n_obs, ])\n    d = 1. * (alpha_x * z + v > 0)\n\n    y = d * theta + np.dot(x, beta) + u\n\n    if return_type in _array_alias:\n        return x, y, d, z\n    elif return_type in _data_frame_alias + _dml_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_x)]\n        data = pd.DataFrame(np.column_stack((x, y, d, z)),\n                            columns=x_cols + ['y', 'd', 'z'])\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, 'y', 'd', x_cols, 'z')\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef _make_pliv_data(n_obs=100, dim_x=20, theta=0.5, gamma_z=0.4, return_type='DoubleMLData'):\n    b = [1/k for k in range(1, dim_x+1)]\n    sigma = make_spd_matrix(dim_x)\n\n    x = np.random.multivariate_normal(np.zeros(dim_x), sigma, size=[n_obs, ])\n    G = _g(np.dot(x, b))\n    # instrument\n    z = _m(np.dot(x, b)) + np.random.standard_normal(size=[n_obs, ])\n    # treatment\n    M = _m(gamma_z * z + np.dot(x, b))\n    d = M + np.random.standard_normal(size=[n_obs, ])\n    y = np.dot(theta, d) + G + np.random.standard_normal(size=[n_obs, ])\n\n    if return_type in _array_alias:\n        return x, y, d, z\n    elif return_type in _data_frame_alias + _dml_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_x)]\n        data = pd.DataFrame(np.column_stack((x, y, d, z)),\n                            columns=x_cols + ['y', 'd', 'z'])\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, 'y', 'd', x_cols, 'z')\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef make_pliv_CHS2015(n_obs, alpha=1., dim_x=200, dim_z=150, return_type='DoubleMLData'):\n    \"\"\"\n    Generates data from a partially linear IV regression model used in Chernozhukov, Hansen and Spindler (2015).\n    The data generating process is defined as\n\n    .. math::\n\n        z_i &= \\\\Pi x_i + \\\\zeta_i,\n\n        d_i &= x_i' \\\\gamma + z_i' \\\\delta + u_i,\n\n        y_i &= \\\\alpha d_i + x_i' \\\\beta + \\\\varepsilon_i,\n\n    with\n\n    .. math::\n\n        \\\\left(\\\\begin{matrix} \\\\varepsilon_i \\\\\\\\ u_i \\\\\\\\ \\\\zeta_i \\\\\\\\ x_i \\\\end{matrix} \\\\right) \\\\sim\n        \\\\mathcal{N}\\\\left(0, \\\\left(\\\\begin{matrix} 1 & 0.6 & 0 & 0 \\\\\\\\ 0.6 & 1 & 0 & 0 \\\\\\\\\n        0 & 0 & 0.25 I_{p_n^z} & 0 \\\\\\\\ 0 & 0 & 0 & \\\\Sigma \\\\end{matrix} \\\\right) \\\\right)\n\n    where  :math:`\\\\Sigma` is a :math:`p_n^x \\\\times p_n^x` matrix with entries\n    :math:`\\\\Sigma_{kj} = 0.5^{|j-k|}` and :math:`I_{p_n^z}` is the :math:`p_n^z \\\\times p_n^z` identity matrix.\n    :math:`\\\\beta = \\\\gamma` is a :math:`p_n^x`-vector with entries :math:`\\\\beta_j=\\\\frac{1}{j^2}`,\n    :math:`\\\\delta` is a :math:`p_n^z`-vector with entries :math:`\\\\delta_j=\\\\frac{1}{j^2}`\n    and :math:`\\\\Pi = (I_{p_n^z}, 0_{p_n^z \\\\times (p_n^x - p_n^z)})`.\n\n    Parameters\n    ----------\n    n_obs :\n        The number of observations to simulate.\n    alpha :\n        The value of the causal parameter.\n    dim_x :\n        The number of covariates.\n    dim_z :\n        The number of instruments.\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s ``(x, y, d, z)``.\n\n    References\n    ----------\n    Chernozhukov, V., Hansen, C. and Spindler, M. (2015), Post-Selection and Post-Regularization Inference in Linear\n    Models with Many Controls and Instruments. American Economic Review: Papers and Proceedings, 105 (5): 486-90.\n    \"\"\"\n    assert dim_x >= dim_z\n    # see https://assets.aeaweb.org/asset-server/articles-attachments/aer/app/10505/P2015_1022_app.pdf\n    xx = np.random.multivariate_normal(np.zeros(2),\n                                       np.array([[1., 0.6], [0.6, 1.]]),\n                                       size=[n_obs, ])\n    epsilon = xx[:, 0]\n    u = xx[:, 1]\n\n    sigma = toeplitz([np.power(0.5, k) for k in range(0, dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x),\n                                      sigma,\n                                      size=[n_obs, ])\n\n    I_z = np.eye(dim_z)\n    xi = np.random.multivariate_normal(np.zeros(dim_z),\n                                       0.25*I_z,\n                                       size=[n_obs, ])\n\n    beta = [1 / (k**2) for k in range(1, dim_x + 1)]\n    gamma = beta\n    delta = [1 / (k**2) for k in range(1, dim_z + 1)]\n    Pi = np.hstack((I_z, np.zeros((dim_z, dim_x-dim_z))))\n\n    z = np.dot(x, np.transpose(Pi)) + xi\n    d = np.dot(x, gamma) + np.dot(z, delta) + u\n    y = alpha * d + np.dot(x, beta) + epsilon\n\n    if return_type in _array_alias:\n        return x, y, d, z\n    elif return_type in _data_frame_alias + _dml_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_x)]\n        z_cols = [f'Z{i + 1}' for i in np.arange(dim_z)]\n        data = pd.DataFrame(np.column_stack((x, y, d, z)),\n                            columns=x_cols + ['y', 'd'] + z_cols)\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, 'y', 'd', x_cols, z_cols)\n    else:\n        raise ValueError('Invalid return_type.')\n\n", "right_context": "\n\ndef make_did_SZ2020(n_obs=500, dgp_type=1, cross_sectional_data=False, return_type='DoubleMLData', **kwargs):\n    \"\"\"\n    Generates data from a difference-in-differences model used in Sant'Anna and Zhao (2020).\n    The data generating process is defined as follows. For a generic :math:`W=(W_1, W_2, W_3, W_4)^T`, let\n\n    .. math::\n\n        f_{reg}(W) &= 210 + 27.4 \\\\cdot W_1 +13.7 \\\\cdot (W_2 + W_3 + W_4),\n\n        f_{ps}(W) &= 0.75 \\\\cdot (-W_1 + 0.5 \\\\cdot W_2 -0.25 \\\\cdot W_3 - 0.1 \\\\cdot W_4).\n\n\n    Let :math:`X= (X_1, X_2, X_3, X_4)^T \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`, where  :math:`\\\\Sigma` is a matrix with entries\n    :math:`\\\\Sigma_{kj} = c^{|j-k|}`. The default value is  :math:`c = 0`, corresponding to the identity matrix.\n    Further, define :math:`Z_j = (\\\\tilde{Z_j} - \\\\mathbb{E}[\\\\tilde{Z}_j]) / \\\\sqrt{\\\\text{Var}(\\\\tilde{Z}_j)}`,\n    where :math:`\\\\tilde{Z}_1 = \\\\exp(0.5 \\\\cdot X_1)`, :math:`\\\\tilde{Z}_2 = 10 + X_2/(1 + \\\\exp(X_1))`,\n    :math:`\\\\tilde{Z}_3 = (0.6 + X_1 \\\\cdot X_3 / 25)^3` and :math:`\\\\tilde{Z}_4 = (20 + X_2 + X_4)^2`.\n    At first define\n\n    .. math::\n\n        Y_0(0) &= f_{reg}(W_{reg}) + \\\\nu(W_{reg}, D) + \\\\varepsilon_0,\n\n        Y_1(d) &= 2 \\\\cdot f_{reg}(W_{reg}) + \\\\nu(W_{reg}, D) + \\\\varepsilon_1(d),\n\n        p(W_{ps}) &= \\\\frac{\\\\exp(f_{ps}(W_{ps}))}{1 + \\\\exp(f_{ps}(W_{ps}))},\n\n        D &= 1\\\\{p(W_{ps}) \\\\ge U\\\\},\n\n    where :math:`\\\\varepsilon_0, \\\\varepsilon_1(d), d=0, 1` are independent standard normal random variables,\n    :math:`U \\\\sim \\\\mathcal{U}[0, 1]` is a independent standard uniform\n    and :math:`\\\\nu(W_{reg}, D)\\\\sim \\\\mathcal{N}(D \\\\cdot f_{reg}(W_{reg}),1)`.\n    The different data generating processes are defined via\n\n    .. math::\n\n        DGP1:\\\\quad W_{reg} &= Z \\\\quad W_{ps} = Z\n\n        DGP2:\\\\quad W_{reg} &= Z \\\\quad W_{ps} = X\n\n        DGP3:\\\\quad W_{reg} &= X \\\\quad W_{ps} = Z\n\n        DGP4:\\\\quad W_{reg} &= X \\\\quad W_{ps} = X\n\n        DGP5:\\\\quad W_{reg} &= Z \\\\quad W_{ps} = 0\n\n        DGP6:\\\\quad W_{reg} &= X \\\\quad W_{ps} = 0,\n\n    such that the last two settings correspond to an experimental setting with treatment probability\n    of :math:`P(D=1) = \\\\frac{1}{2}.`\n    For the panel data the outcome is already defined as the difference :math:`Y = Y_1(D) - Y_0(0)`.\n    For cross-sectional data the flag ``cross_sectional_data`` has to be set to ``True``.\n    Then the outcome will be defined to be\n\n    .. math::\n\n        Y = T \\\\cdot Y_1(D) + (1-T) \\\\cdot Y_0(0),\n\n    where :math:`T = 1\\\\{U_T\\\\le \\\\lambda_T \\\\}` with :math:`U_T\\\\sim \\\\mathcal{U}[0, 1]` and :math:`\\\\lambda_T=0.5`.\n    The true average treatment effect on the treated is zero for all data generating processes.\n\n    Parameters\n    ----------\n    n_obs :\n        The number of observations to simulate.\n    dgp_type :\n        The DGP to be used. Default value is ``1`` (integer).\n    cross_sectional_data :\n        Indicates whether the setting is uses cross-sectional or panel data. Default value is ``False``.\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s ``(x, y, d)``\n        or ``(x, y, d, t)``.\n    **kwargs\n        Additional keyword arguments to set non-default values for the parameter\n        :math:`xi=0.75`, :math:`c=0.0` and :math:`\\\\lambda_T=0.5`.\n\n    References\n    ----------\n    Sant\u2019Anna, P. H. and Zhao, J. (2020),\n    Doubly robust difference-in-differences estimators. Journal of Econometrics, 219(1), 101-122.\n    doi:`10.1016/j.jeconom.2020.06.003 <https://doi.org/10.1016/j.jeconom.2020.06.003>`_.\n    \"\"\"\n    xi = kwargs.get('xi', 0.75)\n    c = kwargs.get('c', 0.0)\n    lambda_t = kwargs.get('lambda_t', 0.5)\n\n    def f_reg(w):\n        res = 210 + 27.4*w[:, 0] + 13.7*(w[:, 1] + w[:, 2] + w[:, 3])\n        return res\n\n    def f_ps(w, xi):\n        res = xi*(-w[:, 0] + 0.5*w[:, 1] - 0.25*w[:, 2] - 0.1*w[:, 3])\n        return res\n\n    dim_x = 4\n    cov_mat = toeplitz([np.power(c, k) for k in range(dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x), cov_mat, size=[n_obs, ])\n\n    z_tilde_1 = np.exp(0.5*x[:, 0])\n    z_tilde_2 = 10 + x[:, 1] / (1 + np.exp(x[:, 0]))\n    z_tilde_3 = (0.6 + x[:, 0]*x[:, 2]/25)**3\n    z_tilde_4 = (20 + x[:, 1] + x[:, 3])**2\n\n    z_tilde = np.column_stack((z_tilde_1, z_tilde_2, z_tilde_3, z_tilde_4))\n    z = (z_tilde - np.mean(z_tilde, axis=0)) / np.std(z_tilde, axis=0)\n\n    # error terms\n    epsilon_0 = np.random.normal(loc=0, scale=1, size=n_obs)\n    epsilon_1 = np.random.normal(loc=0, scale=1, size=[n_obs, 2])\n\n    if dgp_type == 1:\n        features_ps = z\n        features_reg = z\n    elif dgp_type == 2:\n        features_ps = x\n        features_reg = z\n    elif dgp_type == 3:\n        features_ps = z\n        features_reg = x\n    elif dgp_type == 4:\n        features_ps = x\n        features_reg = x\n    elif dgp_type == 5:\n        features_ps = None\n        features_reg = z\n    elif dgp_type == 6:\n        features_ps = None\n        features_reg = x\n    else:\n        raise ValueError('The dgp_type is not valid.')\n\n    # treatment and propensities\n    is_experimental = (dgp_type == 5) or (dgp_type == 6)\n    if is_experimental:\n        # Set D to be experimental\n        p = 0.5 * np.ones(n_obs)\n    else:\n        p = np.exp(f_ps(features_ps, xi)) / (1 + np.exp(f_ps(features_ps, xi)))\n    u = np.random.uniform(low=0, high=1, size=n_obs)\n    d = 1.0 * (p >= u)\n\n    # potential outcomes\n    nu = np.random.normal(loc=d*f_reg(features_reg), scale=1, size=n_obs)\n    y0 = f_reg(features_reg) + nu + epsilon_0\n    y1_d0 = 2 * f_reg(features_reg) + nu + epsilon_1[:, 0]\n    y1_d1 = 2 * f_reg(features_reg) + nu + epsilon_1[:, 1]\n    y1 = d * y1_d1 + (1-d) * y1_d0\n\n    if not cross_sectional_data:\n        y = y1 - y0\n\n        if return_type in _array_alias:\n            return z, y, d\n        elif return_type in _data_frame_alias + _dml_data_alias:\n            z_cols = [f'Z{i + 1}' for i in np.arange(dim_x)]\n            data = pd.DataFrame(np.column_stack((z, y, d)),\n                                columns=z_cols + ['y', 'd'])\n            if return_type in _data_frame_alias:\n                return data\n            else:\n                return DoubleMLData(data, 'y', 'd', z_cols)\n        else:\n            raise ValueError('Invalid return_type.')\n\n    else:\n        u_t = np.random.uniform(low=0, high=1, size=n_obs)\n        t = 1.0 * (u_t <= lambda_t)\n        y = t * y1 + (1-t)*y0\n\n        if return_type in _array_alias:\n            return z, y, d, t\n        elif return_type in _data_frame_alias + _dml_data_alias:\n            z_cols = [f'Z{i + 1}' for i in np.arange(dim_x)]\n            data = pd.DataFrame(np.column_stack((z, y, d, t)),\n                                columns=z_cols + ['y', 'd', 't'])\n            if return_type in _data_frame_alias:\n                return data\n            else:\n                return DoubleMLData(data, 'y', 'd', z_cols, t_col='t')\n        else:\n            raise ValueError('Invalid return_type.')\n\n\ndef make_confounded_irm_data(n_obs=500, theta=5.0, cf_y=0.04, cf_d=0.04):\n    \"\"\"\n    Generates counfounded data from an interactive regression model.\n\n    The data generating process is defined as follows (similar to the Monte Carlo simulation used\n    in Sant'Anna and Zhao (2020)).\n\n    Let :math:`X= (X_1, X_2, X_3, X_4, X_5)^T \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`, where  :math:`\\\\Sigma` corresponds\n    to the identity matrix.\n    Further, define :math:`Z_j = (\\\\tilde{Z_j} - \\\\mathbb{E}[\\\\tilde{Z}_j]) / \\\\sqrt{\\\\text{Var}(\\\\tilde{Z}_j)}`,\n    where\n\n    .. math::\n\n        \\\\tilde{Z}_1 &= \\\\exp(0.5 \\\\cdot X_1)\n\n        \\\\tilde{Z}_2 &= 10 + X_2/(1 + \\\\exp(X_1))\n\n        \\\\tilde{Z}_3 &= (0.6 + X_1 \\\\cdot X_3 / 25)^3\n\n        \\\\tilde{Z}_4 &= (20 + X_2 + X_4)^2\n\n        \\\\tilde{Z}_5 &= X_5.\n\n    Additionally, generate a confounder :math:`A \\\\sim \\\\mathcal{U}[-1, 1]`.\n    At first, define the propensity score as\n\n    .. math::\n\n        m(X, A) = P(D=1|X,A) = 0.5 + \\\\gamma_A \\\\cdot A\n\n    and generate the treatment :math:`D = 1\\\\{m(X, A) \\\\ge U\\\\}` with :math:`U \\\\sim \\\\mathcal{U}[0, 1]`.\n    Since :math:`A` is independent of :math:`X`, the short form of the propensity score is given as\n\n    .. math::\n\n        P(D=1|X) = 0.5.\n\n    Further, generate the outcome of interest :math:`Y` as\n\n    .. math::\n\n        Y &= \\\\theta \\\\cdot D (Z_5 + 1) + g(Z) + \\\\beta_A \\\\cdot A + \\\\varepsilon\n\n        g(Z) &= 210 + 27.4 \\\\cdot Z_1 +13.7 \\\\cdot (Z_2 + Z_3 + Z_4)\n\n    where :math:`\\\\varepsilon \\\\sim \\\\mathcal{N}(0,5)`.\n    This implies an average treatment effect of :math:`\\\\theta`. Additionally, the long and short forms of\n    the conditional expectation take the following forms\n\n    .. math::\n\n        \\\\mathbb{E}[Y|D, X, A] &= \\\\theta \\\\cdot D (Z_5 + 1) + g(Z) + \\\\beta_A \\\\cdot A\n\n        \\\\mathbb{E}[Y|D, X] &= (\\\\theta + \\\\beta_A \\\\frac{\\\\mathrm{Cov}(A, D(Z_5 + 1))}{\\\\mathrm{Var}(D(Z_5 + 1))})\n            \\\\cdot D (Z_5 + 1) + g(Z).\n\n    Consequently, the strength of confounding is determined via :math:`\\\\gamma_A` and :math:`\\\\beta_A`.\n    Both are chosen to obtain the desired confounding of the outcome and Riesz Representer (in sample).\n\n    The observed data is given as :math:`W = (Y, D, X)`.\n    Further, orcale values of the confounder :math:`A`, the transformed covariated :math:`Z`,\n    the potential outcomes of :math:`Y`, the coefficients :math:`\\\\gamma_a`, :math:`\\\\beta_a`, the\n    long and short forms of the main regression and the propensity score\n    are returned in a dictionary.\n\n    Parameters\n    ----------\n    n_obs : int\n        The number of observations to simulate.\n        Default is ``500``.\n    theta : float or int\n        Average treatment effect.\n        Default is ``5.0``.\n    cf_y : float\n        Percentage of the residual variation of the outcome explained by latent/confounding variable.\n        Default is ``0.04``.\n    cf_d : float\n        Percentage gains in the variation of the Riesz Representer generated by latent/confounding variable.\n        Default is ``0.04``.\n\n    Returns\n    -------\n    res_dict : dictionary\n       Dictionary with entries ``x``, ``y``, ``d`` and ``oracle_values``.\n\n    References\n    ----------\n    Sant\u2019Anna, P. H. and Zhao, J. (2020),\n    Doubly robust difference-in-differences estimators. Journal of Econometrics, 219(1), 101-122.\n    doi:`10.1016/j.jeconom.2020.06.003 <https://doi.org/10.1016/j.jeconom.2020.06.003>`_.\n    \"\"\"\n    c = 0.0  # the confounding strength is only valid for c=0\n    dim_x = 5\n\n    # observed covariates\n    cov_mat = toeplitz([np.power(c, k) for k in range(dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x), cov_mat, size=[n_obs, ])\n\n    z_tilde_1 = np.exp(0.5*x[:, 0])\n    z_tilde_2 = 10 + x[:, 1] / (1 + np.exp(x[:, 0]))\n    z_tilde_3 = (0.6 + x[:, 0] * x[:, 2]/25)**3\n    z_tilde_4 = (20 + x[:, 1] + x[:, 3])**2\n\n    z_tilde = np.column_stack((z_tilde_1, z_tilde_2, z_tilde_3, z_tilde_4, x[:, 4:]))\n    z = (z_tilde - np.mean(z_tilde, axis=0)) / np.std(z_tilde, axis=0)\n\n    # error terms and unobserved confounder\n    var_eps_y = 5\n    eps_y = np.random.normal(loc=0, scale=np.sqrt(var_eps_y), size=n_obs)\n\n    # unobserved confounder\n    a_bounds = (-1, 1)\n    a = np.random.uniform(low=a_bounds[0], high=a_bounds[1], size=n_obs)\n\n    # get the required impact of the confounder on the propensity score\n    possible_coefs = np.arange(0.001, 0.4999, 0.001)\n    gamma_a = possible_coefs[(np.arctanh(2*possible_coefs) / (2*possible_coefs)) - 1 - cf_d/(1 - cf_d) >= 0][0]\n\n    # compute short and long form of riesz representer\n    m_long = 0.5 + gamma_a*a\n    m_short = 0.5 * np.ones_like(m_long)\n\n    u = np.random.uniform(low=0, high=1, size=n_obs)\n    d = 1.0 * (m_long >= u)\n\n    # short and long version of g\n    g_partial_reg = 210 + 27.4*z[:, 0] + 13.7*(z[:, 1] + z[:, 2] + z[:, 3])\n\n    dx = d * (x[:, 4] + 1)\n    d1x = x[:, 4] + 1\n    var_dx = np.var(dx)\n    cov_adx = np.cov(a, dx)[0, 1]\n\n    def f_g(beta_a):\n        g_diff = beta_a * (a - cov_adx / var_dx)\n        y_diff = eps_y + g_diff\n        return np.square(np.mean(np.square(g_diff)) / np.mean(np.square(y_diff)) - cf_y)\n    beta_a = minimize_scalar(f_g).x\n\n    g_short_d0 = g_partial_reg\n    g_short_d1 = (theta + beta_a * cov_adx / var_dx) * d1x + g_partial_reg\n    g_short = d * g_short_d1 + (1.0-d) * g_short_d0\n\n    g_long_d0 = g_partial_reg + beta_a * a\n    g_long_d1 = theta * d1x + g_partial_reg + beta_a * a\n    g_long = d * g_long_d1 + (1.0-d) * g_long_d0\n\n    y0 = g_long_d0 + eps_y\n    y1 = g_long_d1 + eps_y\n\n    y = d * y1 + (1.0-d) * y0\n\n    oracle_values = {'g_long': g_long,\n                     'g_short': g_short,\n                     'm_long': m_long,\n                     'm_short': m_short,\n                     'gamma_a': gamma_a,\n                     'beta_a': beta_a,\n                     'a': a,\n                     'y0': y0,\n                     'y1': y1,\n                     'z': z}\n\n    res_dict = {'x': x,\n                'y': y,\n                'd': d,\n                'oracle_values': oracle_values}\n\n    return res_dict\n\n\ndef make_confounded_plr_data(n_obs=500, theta=5.0, cf_y=0.04, cf_d=0.04, **kwargs):\n    \"\"\"\n    Generates counfounded data from an partially linear regression model.\n\n    The data generating process is defined as follows (similar to the Monte Carlo simulation used\n    in Sant'Anna and Zhao (2020)). Let :math:`X= (X_1, X_2, X_3, X_4, X_5)^T \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`,\n    where  :math:`\\\\Sigma` is a matrix with entries\n    :math:`\\\\Sigma_{kj} = c^{|j-k|}`. The default value is  :math:`c = 0`, corresponding to the identity matrix.\n    Further, define :math:`Z_j = (\\\\tilde{Z_j} - \\\\mathbb{E}[\\\\tilde{Z}_j]) / \\\\sqrt{\\\\text{Var}(\\\\tilde{Z}_j)}`,\n    where\n\n    .. math::\n\n        \\\\tilde{Z}_1 &= \\\\exp(0.5 \\\\cdot X_1)\n\n        \\\\tilde{Z}_2 &= 10 + X_2/(1 + \\\\exp(X_1))\n\n        \\\\tilde{Z}_3 &= (0.6 + X_1 \\\\cdot X_3 / 25)^3\n\n        \\\\tilde{Z}_4 &= (20 + X_2 + X_4)^2.\n\n    Additionally, generate a confounder :math:`A \\\\sim \\\\mathcal{U}[-1, 1]`.\n    At first, define the treatment as\n\n    .. math::\n\n        D = -Z_1 + 0.5 \\\\cdot Z_2 - 0.25 \\\\cdot Z_3 - 0.1 \\\\cdot Z_4 + \\\\gamma_A \\\\cdot A + \\\\varepsilon_D\n\n    and with :math:`\\\\varepsilon \\\\sim \\\\mathcal{N}(0,1)`.\n    Since :math:`A` is independent of :math:`X`, the long and short form of the treatment regression are given as\n\n    .. math::\n\n        E[D|X,A] = -Z_1 + 0.5 \\\\cdot Z_2 - 0.25 \\\\cdot Z_3 - 0.1 \\\\cdot Z_4 + \\\\gamma_A \\\\cdot A\n\n        E[D|X] = -Z_1 + 0.5 \\\\cdot Z_2 - 0.25 \\\\cdot Z_3 - 0.1 \\\\cdot Z_4.\n\n    Further, generate the outcome of interest :math:`Y` as\n\n    .. math::\n\n        Y &= \\\\theta \\\\cdot D + g(Z) + \\\\beta_A \\\\cdot A + \\\\varepsilon\n\n        g(Z) &= 210 + 27.4 \\\\cdot Z_1 +13.7 \\\\cdot (Z_2 + Z_3 + Z_4)\n\n    where :math:`\\\\varepsilon \\\\sim \\\\mathcal{N}(0,5)`.\n    This implies an average treatment effect of :math:`\\\\theta`. Additionally, the long and short forms of\n    the conditional expectation take the following forms\n\n    .. math::\n\n        \\\\mathbb{E}[Y|D, X, A] &= \\\\theta \\\\cdot D + g(Z) + \\\\beta_A \\\\cdot A\n\n        \\\\mathbb{E}[Y|D, X] &= (\\\\theta + \\\\gamma_A\\\\beta_A \\\\frac{\\\\mathrm{Var}(A)}{\\\\mathrm{Var}(D)}) \\\\cdot D + g(Z).\n\n    Consequently, the strength of confounding is determined via :math:`\\\\gamma_A` and :math:`\\\\beta_A`.\n    Both are chosen to obtain the desired confounding of the outcome and Riesz Representer (in sample).\n\n    The observed data is given as :math:`W = (Y, D, X)`.\n    Further, orcale values of the confounder :math:`A`, the transformed covariated :math:`Z`, the effect :math:`\\\\theta`,\n    the coefficients :math:`\\\\gamma_a`, :math:`\\\\beta_a`, the long and short forms of the main regression and\n    the propensity score are returned in a dictionary.\n\n    Parameters\n    ----------\n    n_obs : int\n        The number of observations to simulate.\n        Default is ``500``.\n    theta : float or int\n        Average treatment effect.\n        Default is ``5.0``.\n    cf_y : float\n        Percentage of the residual variation of the outcome explained by latent/confounding variable.\n        Default is ``0.04``.\n    cf_d : float\n        Percentage gains in the variation of the Riesz Representer generated by latent/confounding variable.\n        Default is ``0.04``.\n\n    Returns\n    -------\n    res_dict : dictionary\n       Dictionary with entries ``x``, ``y``, ``d`` and ``oracle_values``.\n\n    References\n    ----------\n    Sant\u2019Anna, P. H. and Zhao, J. (2020),\n    Doubly robust difference-in-differences estimators. Journal of Econometrics, 219(1), 101-122.\n    doi:`10.1016/j.jeconom.2020.06.003 <https://doi.org/10.1016/j.jeconom.2020.06.003>`_.\n    \"\"\"\n    c = kwargs.get('c', 0.0)\n    dim_x = kwargs.get('dim_x', 4)\n\n    # observed covariates\n    cov_mat = toeplitz([np.power(c, k) for k in range(dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x), cov_mat, size=[n_obs, ])\n\n    z_tilde_1 = np.exp(0.5*x[:, 0])\n    z_tilde_2 = 10 + x[:, 1] / (1 + np.exp(x[:, 0]))\n    z_tilde_3 = (0.6 + x[:, 0] * x[:, 2]/25)**3\n    z_tilde_4 = (20 + x[:, 1] + x[:, 3])**2\n\n    z_tilde = np.column_stack((z_tilde_1, z_tilde_2, z_tilde_3, z_tilde_4, x[:, 4:]))\n    z = (z_tilde - np.mean(z_tilde, axis=0)) / np.std(z_tilde, axis=0)\n\n    # error terms\n    var_eps_y = 5\n    eps_y = np.random.normal(loc=0, scale=np.sqrt(var_eps_y), size=n_obs)\n    var_eps_d = 1\n    eps_d = np.random.normal(loc=0, scale=np.sqrt(var_eps_d), size=n_obs)\n\n    # unobserved confounder\n    a_bounds = (-1, 1)\n    a = np.random.uniform(low=a_bounds[0], high=a_bounds[1], size=n_obs)\n    var_a = np.square(a_bounds[1] - a_bounds[0]) / 12\n\n    # get the required impact of the confounder on the propensity score\n    m_short = -z[:, 0] + 0.5*z[:, 1] - 0.25*z[:, 2] - 0.1*z[:, 3]\n\n    def f_m(gamma_a):\n        rr_long = eps_d / var_eps_d\n        rr_short = (gamma_a * a + eps_d) / (gamma_a**2 * var_a + var_eps_d)\n        C2_D = (np.mean(np.square(rr_long)) - np.mean(np.square(rr_short))) / np.mean(np.square(rr_short))\n        return np.square(C2_D / (1 + C2_D) - cf_d)\n\n    gamma_a = minimize_scalar(f_m).x\n    m_long = m_short + gamma_a*a\n    d = m_long + eps_d\n\n    # short and long version of g\n    g_partial_reg = 210 + 27.4*z[:, 0] + 13.7*(z[:, 1] + z[:, 2] + z[:, 3])\n\n    var_d = np.var(d)\n\n    def f_g(beta_a):\n        g_diff = beta_a * (a - gamma_a * (var_a/var_d) * d)\n        y_diff = eps_y + g_diff\n        return np.square(np.mean(np.square(g_diff)) / np.mean(np.square(y_diff)) - cf_y)\n\n    beta_a = minimize_scalar(f_g).x\n\n    g_long = theta*d + g_partial_reg + beta_a*a\n    g_short = (theta + gamma_a*beta_a * var_a / var_d)*d + g_partial_reg\n\n    y = g_long + eps_y\n\n    oracle_values = {'g_long': g_long,\n                     'g_short': g_short,\n                     'm_long': m_long,\n                     'm_short': m_short,\n                     'theta': theta,\n                     'gamma_a': gamma_a,\n                     'beta_a': beta_a,\n                     'a': a,\n                     'z': z}\n\n    res_dict = {'x': x,\n                'y': y,\n                'd': d,\n                'oracle_values': oracle_values}\n\n    return res_dict\n\n\ndef make_heterogeneous_data(n_obs=200, p=30, support_size=5, n_x=1, binary_treatment=False):\n    \"\"\"\n    Creates a simple synthetic example for heterogeneous treatment effects.\n    The data generating process is based on the Monte Carlo simulation from Oprescu et al. (2019).\n\n    The data is generated as\n\n    .. math::\n\n        Y_i & = \\\\theta_0(X_i)D_i + \\\\langle X_i,\\\\gamma_0\\\\rangle + \\\\epsilon_i\n\n        D_i & = \\\\langle X_i,\\\\beta_0\\\\rangle + \\\\eta_i,\n\n    where :math:`X_i\\\\sim\\\\mathcal{U}[0,1]^{p}` and :math:`\\\\epsilon_i,\\\\eta_i\n    \\\\sim\\\\mathcal{U}[-1,1]`.\n    If the treatment is set to be binary, the treatment is generated as\n\n    .. math::\n        D_i = 1\\\\{\\\\langle X_i,\\\\beta_0\\\\rangle \\\\ge \\\\eta_i\\\\}.\n\n    The coefficient vectors :math:`\\\\gamma_0` and :math:`\\\\beta_0` both have small random (identical) support\n    which values are drawn independently from :math:`\\\\mathcal{U}[0,1]` and :math:`\\\\mathcal{U}[0,0.3]`.\n    Further, :math:`\\\\theta_0(x)` defines the conditional treatment effect, which is defined differently depending\n    on the dimension of :math:`x`.\n\n    If the heterogeneity is univariate the conditional treatment effect takes the following form\n\n    .. math::\n            \\\\theta_0(x) = \\\\exp(2x_0) + 3\\\\sin(4x_0),\n\n    whereas for the two-dimensional case the conditional treatment effect is defined as\n\n    .. math::\n        \\\\theta_0(x) = \\\\exp(2x_0) + 3\\\\sin(4x_1).\n\n    Parameters\n    ----------\n    n_obs : int\n        Number of observations to simulate.\n        Default is ``200``.\n\n    p : int\n        Dimension of covariates.\n        Default is ``30``.\n\n    support_size : int\n        Number of relevant (confounding) covariates.\n        Default is ``5``.\n\n    n_x : int\n        Dimension of the heterogeneity. Can be either ``1`` or ``2``.\n        Default is ``1``.\n\n    binary_treatment : bool\n        Indicates whether the treatment is binary.\n        Default is ``False``.\n\n    Returns\n    -------\n    res_dict : dictionary\n       Dictionary with entries ``data``, ``effects``, ``treatment_effect``.\n\n    \"\"\"\n    # simple input checks\n    assert n_x in [1, 2], 'n_x must be either 1 or 2.'\n    assert support_size <= p, 'support_size must be smaller than p.'\n    assert isinstance(binary_treatment, bool), 'binary_treatment must be a boolean.'\n\n    # define treatment effects\n    if n_x == 1:\n        def treatment_effect(x):\n            return np.exp(2 * x[:, 0]) + 3 * np.sin(4 * x[:, 0])\n    else:\n        assert n_x == 2\n\n        # redefine treatment effect\n        def treatment_effect(x):\n            return np.exp(2 * x[:, 0]) + 3 * np.sin(4 * x[:, 1])\n\n    # Outcome support and coefficients\n    support_y = np.random.choice(np.arange(p), size=support_size, replace=False)\n    coefs_y = np.random.uniform(0, 1, size=support_size)\n    # treatment support and coefficients\n    support_d = support_y\n    coefs_d = np.random.uniform(0, 0.3, size=support_size)\n\n    # noise\n    epsilon = np.random.uniform(-1, 1, size=n_obs)\n    eta = np.random.uniform(-1, 1, size=n_obs)\n\n    # Generate controls, covariates, treatments and outcomes\n    x = np.random.uniform(0, 1, size=(n_obs, p))\n    # Heterogeneous treatment effects\n    te = treatment_effect(x)\n    if binary_treatment:\n        d = 1.0 * (np.dot(x[:, support_d], coefs_d) >= eta)\n    else:\n        d = np.dot(x[:, support_d], coefs_d) + eta\n    y = te * d + np.dot(x[:, support_y], coefs_y) + epsilon\n\n    # Now we build the dataset\n    y_df = pd.DataFrame({'y': y})\n    d_df = pd.DataFrame({'d': d})\n    x_df = pd.DataFrame(\n        data=x,\n        index=np.arange(x.shape[0]),\n        columns=[f'X_{i}' for i in range(x.shape[1])]\n    )\n\n    data = pd.concat([y_df, d_df, x_df], axis=1)\n    res_dict = {\n        'data': data,\n        'effects': te,\n        'treatment_effect': treatment_effect}\n    return res_dict\n", "import_text": ["pandas", "numpy", "scipy.linalg.toeplitz", "scipy.optimize.minimize_scalar", "sklearn.preprocessing.PolynomialFeatures", "sklearn.preprocessing.OneHotEncoder", "sklearn.datasets.make_spd_matrix"], "prompt": "\"\"\"\nDescription: This function generates simulated data for a multiway cluster data set as per the PLIV model.\n\nArgs:\n    N (int): Number of clusters for the first dimension. Default is 25.\n    M (int): Number of clusters for the second dimension. Default is 25.\n    dim_X (int): Dimension of the covariates. Default is 100.\n    theta (float): Parameter of the PLIV model. Default is 1.0.\n    return_type (str): Type of the return object. Default is 'DoubleMLClusterData'.\n    **kwargs: Additional parameters specifiable via keyword arguments.\n        pi_10 (float): Parameter of the PLIV model. Default is 1.0.\n        zeta_0 (np.ndarray): Parameter of the PLIV model. Default is np.power(0.5, xx).\n        pi_20 (np.ndarray): Parameter of the PLIV model. Default is np.power(0.5, xx).\n        xi_0 (np.ndarray): Parameter of the PLIV model. Default is np.power(0.5, xx).\n        omega_X (np.ndarray): Parameter of the PLIV model. Default is np.array([0.25, 0.25]).\n        omega_epsilon (np.ndarray): Parameter of the PLIV model. Default is np.array([0.25, 0.25]).\n        omega_v (np.ndarray): Parameter of the PLIV model. Default is np.array([0.25, 0.25]).\n        omega_V (np.ndarray): Parameter of the PLIV model. Default is np.array([0.25, 0.25]).\n        s_X (float): Parameter of the PLIV model. Default is 0.25.\n        s_epsilon_v (float): Parameter of the PLIV model. Default is 0.25.\n\nReturns:\n    Depending on the return_type, the function returns simulated data for x, y, d, cluster_vars, and z.\n    If return_type is 'DoubleMLClusterData', it returns a DoubleMLClusterData object.\n    If return_type is in _array_alias, it returns a tuple of numpy arrays.\n    If return_type is in _data_frame_alias, it returns a pandas DataFrame.\n    If return_type is not one of the above, it raises a ValueError.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Generates data from a partially linear IV regression model with multiway cluster sample used in Chiang et al.\n    (2021). The data generating process is defined as\n\n    .. math::\n\n        Z_{ij} &= X_{ij}' \\\\xi_0 + V_{ij},\n\n        D_{ij} &= Z_{ij}' \\\\pi_{10} + X_{ij}' \\\\pi_{20} + v_{ij},\n\n        Y_{ij} &= D_{ij} \\\\theta + X_{ij}' \\\\zeta_0 + \\\\varepsilon_{ij},\n\n    with\n\n    .. math::\n\n        X_{ij} &= (1 - \\\\omega_1^X - \\\\omega_2^X) \\\\alpha_{ij}^X\n        + \\\\omega_1^X \\\\alpha_{i}^X + \\\\omega_2^X \\\\alpha_{j}^X,\n\n        \\\\varepsilon_{ij} &= (1 - \\\\omega_1^\\\\varepsilon - \\\\omega_2^\\\\varepsilon) \\\\alpha_{ij}^\\\\varepsilon\n        + \\\\omega_1^\\\\varepsilon \\\\alpha_{i}^\\\\varepsilon + \\\\omega_2^\\\\varepsilon \\\\alpha_{j}^\\\\varepsilon,\n\n        v_{ij} &= (1 - \\\\omega_1^v - \\\\omega_2^v) \\\\alpha_{ij}^v\n        + \\\\omega_1^v \\\\alpha_{i}^v + \\\\omega_2^v \\\\alpha_{j}^v,\n\n        V_{ij} &= (1 - \\\\omega_1^V - \\\\omega_2^V) \\\\alpha_{ij}^V\n        + \\\\omega_1^V \\\\alpha_{i}^V + \\\\omega_2^V \\\\alpha_{j}^V,\n\n    and :math:`\\\\alpha_{ij}^X, \\\\alpha_{i}^X, \\\\alpha_{j}^X \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`\n    where  :math:`\\\\Sigma` is a :math:`p_x \\\\times p_x` matrix with entries\n    :math:`\\\\Sigma_{kj} = s_X^{|j-k|}`.\n    Further\n\n    .. math::\n\n        \\\\left(\\\\begin{matrix} \\\\alpha_{ij}^\\\\varepsilon \\\\\\\\ \\\\alpha_{ij}^v \\\\end{matrix}\\\\right),\n        \\\\left(\\\\begin{matrix} \\\\alpha_{i}^\\\\varepsilon \\\\\\\\ \\\\alpha_{i}^v \\\\end{matrix}\\\\right),\n        \\\\left(\\\\begin{matrix} \\\\alpha_{j}^\\\\varepsilon \\\\\\\\ \\\\alpha_{j}^v \\\\end{matrix}\\\\right)\n        \\\\sim \\\\mathcal{N}\\\\left(0, \\\\left(\\\\begin{matrix} 1 & s_{\\\\varepsilon v} \\\\\\\\\n        s_{\\\\varepsilon v} & 1 \\\\end{matrix} \\\\right) \\\\right)\n\n\n    and :math:`\\\\alpha_{ij}^V, \\\\alpha_{i}^V, \\\\alpha_{j}^V \\\\sim \\\\mathcal{N}(0, 1)`.\n\n    Parameters\n    ----------\n    N :\n        The number of observations (first dimension).\n    M :\n        The number of observations (second dimension).\n    dim_X :\n        The number of covariates.\n    theta :\n        The value of the causal parameter.\n    return_type :\n        If ``'DoubleMLClusterData'`` or ``DoubleMLClusterData``, returns a ``DoubleMLClusterData`` object where\n        ``DoubleMLClusterData.data`` is a ``pd.DataFrame``.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s\n        ``(x, y, d, cluster_vars, z)``.\n    **kwargs\n        Additional keyword arguments to set non-default values for the parameters\n        :math:`\\\\pi_{10}=1.0`, :math:`\\\\omega_X = \\\\omega_{\\\\varepsilon} = \\\\omega_V = \\\\omega_v = (0.25, 0.25)`,\n        :math:`s_X = s_{\\\\varepsilon v} = 0.25`,\n        or the :math:`p_x`-vectors :math:`\\\\zeta_0 = \\\\pi_{20} = \\\\xi_0` with default entries\n        :math:`(\\\\zeta_{0})_j = 0.5^j`.\n\n    References\n    ----------\n    Chiang, H. D., Kato K., Ma, Y. and Sasaki, Y. (2021), Multiway Cluster Robust Double/Debiased Machine Learning,\n    Journal of Business & Economic Statistics,\n    doi: `10.1080/07350015.2021.1895815 <https://doi.org/10.1080/07350015.2021.1895815>`_,\n    arXiv:`1909.03489 <https://arxiv.org/abs/1909.03489>`_.\n    \"\"\"", "function_dependencies": ["numpy.arange", "numpy.power", "numpy.array", "numpy.random.normal", "numpy.repeat", "numpy.tile", "numpy.random.multivariate_normal", "numpy.zeros", "scipy.linalg.toeplitz", "numpy.matmul", "pandas.MultiIndex.from_product", "pandas.MultiIndex.from_product.to_frame", "pandas.MultiIndex.from_product.to_frame.reset_index", "pandas.concat", "pandas.DataFrame", "numpy.column_stack"], "project_create_time": "2020-09-09T13:27:25+00:00", "project_update_time": "2024-04-14T19:25:09+00:00", "file_create_time": "2020-09-09T14:06:05Z", "file_update_time": "2023-12-04T13:44:45Z", "function_update_time": "2020-09-09T14:06:05Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["numpy.tile", "numpy.random.normal", "numpy.repeat", "numpy.matmul", "pandas.MultiIndex.from_product"], "test_function": [{"file_path": "/doubleml-for-py-0.7.1/doubleml-for-py-0.7.1/doubleml/tests/test_datasets.py", "class_name": null, "function_name": "test_make_pliv_multiway_cluster_CKMS2021_return_types", "code": "\ndef test_make_pliv_multiway_cluster_CKMS2021_return_types():\n    np.random.seed(3141)\n    res = make_pliv_multiway_cluster_CKMS2021(N=10, M=10, return_type='DoubleMLClusterData')\n    assert isinstance(res, DoubleMLClusterData)\n    res = make_pliv_multiway_cluster_CKMS2021(N=10, M=10, return_type='DataFrame')\n    assert isinstance(res, pd.DataFrame)\n    x, y, d, cluster_vars, z = make_pliv_multiway_cluster_CKMS2021(N=10, M=10, return_type='array')\n    assert isinstance(x, np.ndarray)\n    assert isinstance(y, np.ndarray)\n    assert isinstance(d, np.ndarray)\n    assert isinstance(cluster_vars, np.ndarray)\n    assert isinstance(z, np.ndarray)\n    with pytest.raises(ValueError, match=msg_inv_return_type):\n        _ = make_pliv_multiway_cluster_CKMS2021(N=10, M=10, return_type='matrix')"}]}, {"git_group": "google", "git_name": "uncertainty-baselines", "version": "main", "language": "Python", "project_name": "uncertainty-baselines-main.zip", "file_path": "/uncertainty-baselines-main/uncertainty-baselines-main/experimental/plex/colab_utils.py", "file_name": "colab_utils.py", "focal_class": null, "focal_name": "process_fewshot_for_moe_comparison", "focal_parameter": [], "solution": "def process_fewshot_for_moe_comparison(\n    measurements_dict: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n\n  def _parse_column(c):\n    match = re.fullmatch(r'z/(.*)_(\\d*)shot_(.*)$', c)\n    is_valid = match is not None and 'best_l2' not in c\n    if not is_valid:\n      return None\n    else:\n      dataset, shot, metric_type = match.groups()\n      column_name = (f'{shot}shot_{metric_type}', f'few-shot {dataset}')\n      return column_name\n\n  rows = []\n  for model_name in ('MoE', '[Det]_4', '[MoE]_4'):\n    # We average over the different seeds.\n    series = measurements_dict[model_name].mean(axis=0)\n    assert isinstance(series, pd.Series)\n    fewshot_dict = series.to_dict()\n    # We format the names of the columns.\n    column_names = {c: _parse_column(c) for c in fewshot_dict}\n    fewshot_dict = {\n        column_name: fewshot_dict[key]\n        for key, column_name in column_names.items()\n        if column_name is not None\n    }\n    fewshot_dict['model_name'] = model_name\n    rows.append(fewshot_dict)\n\n  df = pd.DataFrame(rows).set_index('model_name')\n  df.columns = pd.MultiIndex.from_tuples(\n      df.columns, names=['metric', 'dataset'])\n  return df", "function_signature": "def process_fewshot_for_moe_comparison(\n    measurements_dict: Dict[str, pd.DataFrame]) -> pd.DataFrame :", "left_context": "# coding=utf-8\n# Copyright 2024 The Uncertainty Baselines Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utility functions used to process xmanager experiments.\"\"\"\n\nimport enum\nimport itertools\nimport math\nimport re\nfrom typing import Any, Dict, Iterable, List, Optional, Tuple\n\nimport immutabledict\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\n_HPARAM_PREFIX = 'config.'\n_RANDOM_SEED_COL = _HPARAM_PREFIX + 'seed'\n_DATASET_COL = _HPARAM_PREFIX + 'dataset'\n_MODEL_COL = 'model'\n\n# In-distribution metrics across train/validation/test splits.\n_SPLIT_METRICS = ('loss', 'prec@1', 'ece', 'calib_auc')\n# Metrics specific to the diabetic retinopathy dataset.\n_RETINA_METRICS = ('accuracy', 'negative_log_likelihood', 'ece',\n                   'retention_auroc_auc', 'retention_accuracy_auc')\n# Compute cost metrics.\n_COMPUTE_METRICS = ('exaflops', 'tpu_days', 'gflops', 'ms_step')\n# Shifted dataset metrics.\n_SHIFTED_METRICS = ('nll', 'ece', 'brier', 'accuracy')\n\n# Pretraining datasets.\n_UPSTREAM_DATASETS = ('jft/entity:1.0.0', 'imagenet21k')\n# Fewshot datasets of interest.\n_FEWSHOT_DATASETS = ('imagenet', 'pets', 'birds', 'col_hist', 'cifar100',\n                     'caltech', 'cars', 'dtd', 'uc_merced')\n\n_NUM_CLASSES_BY_DATASET = immutabledict.immutabledict({\n    'cifar10': 10,\n    'cifar100': 100,\n    'imagenet2012': 1000,\n    'imagenet21k': 21841,\n    'jft/entity:1.0.0': 18291,\n    'retina_country': 2,\n    'retina_severity': 2,\n    # TODO(zmariet, dusenberrymw): Update for the specific ImageNet-Vid and\n    # YTBB datasets, which use a subset of the 1000 Imagenet classes.\n    'imagenet_variants': 1000,\n    'few-shot birds': 200,\n    'few-shot caltech': 102,\n    'few-shot cars': 196,\n    'few-shot cifar100': 100,\n    'few-shot col_hist': 8,\n    'few-shot dtd': 47,\n    'few-shot imagenet': 1000,\n    'few-shot pets': 37,\n    'few-shot uc_merced': 21,\n\n\n})\n\n\nclass MetricCategory(enum.Enum):\n  PREDICTION = enum.auto()  # Prediction metrics, e.g., prec@1.\n  UNCERTAINTY = enum.auto()  # Uncertainty metrics, e.g., ECE.\n  ADAPTATION = enum.auto()  # Adaptation metrics, e.g., fewshot metrics.\n\n\ndef random_seed_col() -> str:\n  \"\"\"Returns the name of the column containing the experimental random seed.\"\"\"\n  return _RANDOM_SEED_COL\n\n\ndef dataset_col() -> str:\n  \"\"\"Returns the name of the column containing the training dataset.\"\"\"\n  return _DATASET_COL\n\n\ndef model_col() -> str:\n  \"\"\"Returns the name of the column containing the model name.\"\"\"\n  return _MODEL_COL\n\n\ndef upstream_datasets() -> Tuple[str, ...]:\n  \"\"\"Returns the datasets used for upstream pretraining.\"\"\"\n  return _UPSTREAM_DATASETS\n\n\ndef default_fewshot_datasets() -> Tuple[str, ...]:\n  \"\"\"Returns the default fewshot datasets used for reporting results.\"\"\"\n  return _FEWSHOT_DATASETS\n\n\ndef compute_metrics() -> Tuple[str, ...]:\n  \"\"\"Returns the metrics corresponding to computational requirements.\"\"\"\n  return _COMPUTE_METRICS\n\n\ndef ood_related_metrics() -> List[str]:\n  \"\"\"Returns the list of OOD metrics we care about for the open set recognition.\"\"\"\n  metrics = []\n  # Out of distribution detection metrics\n  metrics.extend(\n      f'ood_{dset}_msp_auroc'\n      for dset in ['cifar10', 'cifar100', 'svhn_cropped', 'places365_small'])\n  metrics.extend(\n      f'ood_{dset}_entropy_auroc'\n      for dset in ['cifar10', 'cifar100', 'svhn_cropped', 'places365_small'])\n  metrics.extend(\n      f'ood_{dset}_mlogit_auroc'\n      for dset in ['cifar10', 'cifar100', 'svhn_cropped', 'places365_small'])\n  metrics.extend(f'ood_{dset}_maha_auroc'\n                 for dset in ['cifar10', 'cifar100', 'svhn_cropped'])\n  metrics.extend(f'ood_{dset}_rmaha_auroc'\n                 for dset in ['cifar10', 'cifar100', 'svhn_cropped'])\n  return metrics\n\n\ndef default_selected_metrics() -> List[str]:\n  \"\"\"Returns the list of metrics we care about for the big paper.\"\"\"\n  metrics = list(_COMPUTE_METRICS)\n\n  # In-distribution metrics\n  metrics.extend(f'test_{m}' for m in _SPLIT_METRICS)\n\n  # Out of distribution metrics\n  metrics.extend(f'{dset}_{m}' for dset, m in itertools.product(\n      ['cifar_10h', 'imagenet_real'], _SPLIT_METRICS))\n\n  # Out of distribution detection metrics\n  metrics.extend(\n      f'ood_{dset}_msp_auroc'\n      for dset in ['cifar10', 'cifar100', 'svhn_cropped', 'places365_small']\n  )\n\n  # Shifted metrics\n  metrics.extend(\n      f'{dset}/{m}/mean'\n      for dset, m in itertools.product(['imagenet_c'], _SHIFTED_METRICS))\n  metrics.extend(f'imagenet_c/{m}' for m in ['mce', 'relative_mce'])\n  metrics.extend(f'{dset}/{m}' for dset, m in itertools.product(\n      ['imagenet_a', 'imagenet_r', 'imagenet_v2'], _SHIFTED_METRICS))\n  metrics.extend(f'{dset}/{m}' for dset, m in itertools.product(\n      ['imagenet_vid_robust', 'ytbb_robust'],\n      ['accuracy_drop', 'accuracy_pmk', 'anchor_accuracy']))\n\n  # Fewshot metrics\n  metrics.extend(\n      f'z/{dset}_{k}shot'\n      for dset, k in itertools.product(_FEWSHOT_DATASETS, [1, 5, 10, 25])\n  )\n\n  # Retina metrics\n  metrics.extend(\n      f'{prefix}/{metric}' for metric, prefix in itertools.product(\n          _RETINA_METRICS, ['in_domain_test', 'ood_test'])\n  )\n\n  return metrics\n\n\ndef get_base_metric(metric: str) -> str:\n  \"\"\"Validates `metric` and returns the base computed metric.\n\n  Args:\n    metric: A metric name that may contain auxiliary information such as\n      the dataset it was computed on. For example: \"ood_cifar10_msp_auroc\".\n\n  Returns:\n    The base metric; for example, \"auroc\".\n\n  Raises:\n    ValueError: If metric does not follow the expected syntax.\n  \"\"\"\n  if metric in _COMPUTE_METRICS:\n    return metric\n  # NOTE: `(?<!...)` means not preceded by `...`, and `(?!...)` means not\n  # followed by `...`. `(?P<metric>)` gives the name `metric` to the group.\n  pattern = (r'.*[_/](?P<metric>loss|likelihood|ece|auc|auroc|prec@1|\\d+shot'\n             r'|nll|brier|relative\\_mce|(?<!relative\\_)mce|accuracy\\_drop'\n             r'|accuracy\\_pmk|anchor\\_accuracy'\n             r'|(?<!anchor\\_)accuracy(?!(:\\_pmk|\\_drop)))')\n  match = re.search(pattern, metric)\n  if match is not None:\n    return match.group('metric')\n  else:\n    raise ValueError(f'Unrecognized metric {metric}!')\n\n\ndef get_metric_category(metric: str) -> MetricCategory:\n  \"\"\"Returns which category `metric` belongs to for scoring purposes.\"\"\"\n  base_metric = get_base_metric(metric)\n  if base_metric in [\n      'loss', 'accuracy', 'likelihood', 'nll', 'brier', 'mce', 'relative_mce',\n      'accuracy_drop', 'accuracy_pmk', 'anchor_accuracy'\n  ]:\n    if 'shot' in metric:\n      return MetricCategory.ADAPTATION\n    else:\n      return MetricCategory.PREDICTION\n  elif base_metric in ['auc', 'auroc', 'ece']:\n    if 'shot' in metric:\n      return MetricCategory.ADAPTATION\n    else:\n      return MetricCategory.UNCERTAINTY\n  elif base_metric == 'prec@1':\n    if 'shot' in metric:\n      return MetricCategory.ADAPTATION\n    else:\n      return MetricCategory.PREDICTION\n  raise ValueError(f'Metric {metric} is not used for scoring.')\n\n\ndef is_higher_better(metric: str) -> bool:\n  \"\"\"Returns True if the metric is to be maximized (e.g., precision).\"\"\"\n  maximized_metrics = [\n      'prec@1', 'accuracy', 'auc', 'auroc', 'anchor_accuracy', 'accuracy_pmk'\n  ]\n  minimized_metrics = [\n      'loss', 'likelihood', 'ece', 'nll', 'brier', 'mce', 'relative_mce',\n      'accuracy_drop'\n  ]\n  base_metric = get_base_metric(metric)\n  if base_metric in maximized_metrics or 'shot' in base_metric:\n    return True\n  elif base_metric in minimized_metrics:\n    return False\n  else:\n    raise ValueError(f'Metric {metric} is unrecognized. It was parsed as a '\n                     f'{base_metric} base metric.')\n\n\ndef get_unique_value(df: pd.DataFrame, col: str) -> Any:\n  \"\"\"Returns the unique value in a dataframe column.\n\n  Args:\n    df: A pd.DataFrame.\n    col: The column of interest.\n\n  Returns:\n    The unique value.\n\n  Raises:\n    ValueError: if the column does not have an unique value.\n  \"\"\"\n  unique_values = df[col].unique()\n  if len(unique_values) != 1:\n    raise ValueError(\n        f'Expected unique value in column {col}; got values {unique_values}!'\n    )\n  return unique_values[0]\n\n\ndef row_wise_unique_non_nan(df: pd.DataFrame) -> pd.Series:\n  \"\"\"Checks there is exactly one non-NA in each row, and returns its value.\"\"\"\n  non_nan_counts = df.notna().sum(axis=1)\n  if not (non_nan_counts <= 1).all():\n    num_incorrect_rows = int(np.sum(non_nan_counts > 1))\n    raise ValueError(f'{num_incorrect_rows} rows have multiple set values!')\n  return df.fillna(axis=1, method='bfill').iloc[:, 0]\n\n\ndef is_hyperparameter(\n    column: str, auxiliary_hparams: Iterable[str] = ('learning_rate',)) -> bool:\n  \"\"\"Returns True if the column corresponds to a hyperparameter.\"\"\"\n  return column.startswith(_HPARAM_PREFIX) or column in auxiliary_hparams\n\n\ndef get_sweeped_hyperparameters(\n    df,\n    marginalization_hparams: Iterable[str] = (_RANDOM_SEED_COL,)) -> List[Any]:\n  \"\"\"Identifies the columns that correspond to a hyperparameter tuning sweep.\"\"\"\n  hparams = [c for c in df.columns if is_hyperparameter(c)]\n  return [\n      h for h in hparams\n      if len(df[h].unique()) > 1 and h not in marginalization_hparams\n  ]\n\n\ndef get_best_hyperparameters(df: pd.DataFrame,\n                             tuning_metric: str,\n                             marginalization_hparams: Iterable[str] = (\n                                 _RANDOM_SEED_COL,),\n                             verbose: bool = True) -> Dict[str, Any]:\n  \"\"\"Returns the best choice of hyperparameters for a given model and dataset.\n\n  Each row of a dataframe corresponds to a single experiment; each column is\n  either a standard hyperparameter (e.g., learning rate), a hyperparameter to\n  marginalize over (e.g., random seed), or a metric (e.g., test loss).\n\n  For each choice of non-marginalization hyperparameters, the tuning metric\n  (e.g., the validation loss) is averaged over all marginalization\n  hyperparameters; the hyperparameters achieving the best average metric are\n  returned.\n\n  In the unlikely event that separate hyperparameter values produce an optimal\n  tuning metric, the first hyperparameter choice is returned.\n\n  Args:\n    df: A pd.DataFrame corresponding to a single model and dataset; each row\n      corresponds to a different choice of hyperparameters and / or parameters\n      to aggregate over (e.g., the random seed).\n    tuning_metric: The metric over which hyperparameters are tuned.\n    marginalization_hparams: Columns of the dataframe that correspond to\n      hyperparameters to aggregate over rather than tune (e.g., random seed).\n    verbose: If True, logs to stdout which hyperparameters were swept over, and\n      which values are optimal.\n\n  Returns:\n    A dictionary mapping hyperparameters to their optimal values.\n  \"\"\"\n  dataset = get_unique_value(df, _DATASET_COL)\n  model = get_unique_value(df, _MODEL_COL)\n  hps = get_sweeped_hyperparameters(df, marginalization_hparams)\n\n  if not hps:  # There is no hyperparameter tuning.\n    if verbose:\n      print(f'No hyperparameter tuning for {model} on {dataset}.')\n    return {}\n\n  aggregated_results = df.groupby(hps)[tuning_metric].agg('mean').reset_index()\n\n  if is_higher_better(tuning_metric):\n    best_value_idx = aggregated_results[tuning_metric].idxmax()\n  else:\n    best_value_idx = aggregated_results[tuning_metric].idxmin()\n\n  best_hps = aggregated_results.loc[best_value_idx][hps].to_dict()\n  if verbose:\n    print(f'For {model} on {dataset}, found {len(hps)} hyperparameters: {hps}.')\n    print(f'\\tBest hyperparameters: {best_hps}.')\n  return best_hps\n\n\ndef get_tuned_results(df: pd.DataFrame,\n                      tuning_metric: str,\n                      marginalization_hparams: Iterable[str] = (\n                          _RANDOM_SEED_COL,),\n                      verbose: bool = True) -> pd.DataFrame:\n  \"\"\"Returns dataframe rows corresponding to optimal hyperparameter choices.\n\n  Args:\n    df: pd.DataFrame corresponding to different evaluations of a single model on\n      a single dataset. Each row corresponds to a single experiment. This\n      dataframe must contain at minimum columns [MODEL_COL, DATASET_COL,\n      tuning_metric].\n    tuning_metric: The metric over which hyperparameters are tuned.\n    marginalization_hparams: Columns of the dataframe that correspond to\n      hyperparameters to aggregate over rather than tune (e.g., random seed).\n    verbose: if True, logs to stdout which hyperparameters were swept over, and\n      which values are optimal.\n\n  Returns:\n    A subset of `df` corresponding to the experimental runs with optimal\n    hyperparameters.\n  \"\"\"\n  df = df.copy()\n  best_hps = get_best_hyperparameters(\n      df,\n      tuning_metric,\n      marginalization_hparams=marginalization_hparams,\n      verbose=verbose)\n  for k, v in best_hps.items():\n    df = df[df[k] == v]\n  return df\n\n\ndef _fill_upstream_test_metrics(df: pd.DataFrame) -> pd.DataFrame:\n  \"\"\"Copies validation metrics to test metrics on upstream datasets.\n\n  Upstream datasets (Imagenet21K and JFT) have no dedicated test set, since they\n  are used to pretrain models rather than evaluate them. To simplify downstream\n  plotting analysis, we consider upstream validation metrics to be \"test\"\n  metrics since validation metrics are typically dropped.\n\n  Args:\n    df: pd.DataFrame where rows correspond to model classes, and columns to\n      reported metrics.\n\n  Returns:\n    A copy of `df` where the rows corresponding to models trained on upstream\n    datasets have their in-distribution test metrics filled with in-distribution\n    validation metrics.\n  \"\"\"\n  df = df.copy()\n  for m in _SPLIT_METRICS:\n    if f'val_{m}' in df.columns:\n      idx = df[_DATASET_COL].isin(_UPSTREAM_DATASETS)\n      df.loc[idx, f'test_{m}'] = df.loc[idx, f'val_{m}']\n  return df\n\n\ndef process_tuned_results(\n    df: pd.DataFrame,\n    relevant_metrics: Optional[Iterable[str]] = None,\n) -> pd.DataFrame:\n  \"\"\"Cleans and reformats the dataframe of all results used for the big paper.\n\n  Args:\n    df: pd.DataFrame where each row represents the average performance of a\n      (model, train_dataset) pair with optimal hyperparameters across a variety\n      of metrics. This dataframe is expected to have columns `[_MODEL_COL,\n      _DATASET_COL]`, as well as all chosen metrics to analyze\n      (`relevant_metrics`).\n    relevant_metrics: Optional list of all metrics to analyze; defaults to\n      metrics produced by `default_selected_metrics()`. These metrics must all\n      be columns of `df`.\n\n  Returns:\n    pd.DataFrame where each row corresponds to a model in `df`, and each column\n    is a 2-level multiindex of stucture `(metric, dataset)`, where metrics\n    correspond to `relevant_metrics` and datasets are the available training\n    sets. Note that not all `(metric, dataset)` columns exist, since some\n    metrics are only reported on a subset of available training sets (for\n    example, we don't report Cifar100 OOD numbers when training on Cifar100).\n  \"\"\"\n  df = _fill_upstream_test_metrics(df)\n\n  if relevant_metrics is None:\n    relevant_metrics = default_selected_metrics()\n  relevant_metrics = [m for m in relevant_metrics if m in df.columns]\n\n  df = df.groupby([_MODEL_COL,\n                   _DATASET_COL])[relevant_metrics].mean().reset_index()\n\n  df = df.pivot(index=_MODEL_COL, columns=_DATASET_COL, values=relevant_metrics)\n  df = df.dropna(axis=1, how='all')\n  df.columns = df.columns.set_names(['metric', 'dataset'])\n\n  # Fewshot metrics are only reported on upstream datasets. For each fewshot\n  # metric `m`, `df[m]` is a dataframe with two columns: one for each possible\n  # upstream dataset. Each row of `df[m]` only has one column with a non NaN\n  # value, so we collapse both columns into a single column.\n  fewshot_metrics = (c for c in df.columns.levels[0] if c.startswith('z/'))\n  for fewshot_metric in fewshot_metrics:\n    fewshot_match = re.match(r'z/(.*)_(\\d*)shot', fewshot_metric)\n    if fewshot_match is not None:\n      dset, k = fewshot_match.groups()\n    else:\n      raise ValueError(f'Unconsistent fewshot metric {fewshot_metric}.')\n    df[f'{k}shot_prec@1',\n       f'few-shot {dset}'] = row_wise_unique_non_nan(df[fewshot_metric])\n\n    df = df.drop(columns=fewshot_metric, level=0)\n\n  # For now, we only care about compute metrics on upstream datasets, and only\n  # one of the two upstream compute columns has a non-NaN value for each model.\n  # We process the compute metrics similarly to the fewshot metrics.\n  compute_cols = [c for c in df.columns.levels[0] if c in _COMPUTE_METRICS]\n  for metric in compute_cols:\n    compute_vals = row_wise_unique_non_nan(df[metric][list(_UPSTREAM_DATASETS)])\n    df = df.drop(metric, axis=1, level=0)\n    df[metric, 'compute'] = compute_vals\n\n  return df\n\n\ndef _uniform_entropy(num_classes: int) -> float:\n  \"\"\"Entropy of the uniform categorical distribution over `num_classes`.\"\"\"\n  # TODO(zmariet, jsnoek): Consider switching to log2.\n  return np.log(num_classes)  # Typically written as -n * 1/n * log(1/n).\n\n\ndef _normalize_scores(df: pd.DataFrame) -> pd.DataFrame:\n  \"\"\"Normalizes metrics to [0, 1] (except for multiclass NLL); higher is better.\n\n  All metrics are normalized to [0, 1] except for NLL on multiclass datasets;\n  those are normalized to [0, max_entropy / U], where U is the entropy of the\n  categorical uniform distribution. U does not bound multiclass entropy;\n  however, the multiclass uniform entropy is too large to be a meaningful bound.\n\n  Args:\n    df: pd.DataFrame indexed by model name; each column corresponds to a metric,\n      and each row corresponds to a model choice.\n\n  Returns:\n    A copy of `df` where all scores have been normalized and higher values\n    indicate better performance.\n  \"\"\"\n  df = df.copy()\n  for column in df.columns:\n    metric, dataset = column\n    metric_type = get_base_metric(metric)\n    if metric_type == 'ece':\n      df[column] = 1. - df[column]\n\n    elif metric_type in ['loss', 'likelihood', 'nll']:\n      num_classes = _NUM_CLASSES_BY_DATASET[dataset]\n      df[column] = 1. - df[column] / _uniform_entropy(num_classes)\n  return df\n\n\ndef _drop_unused_measurements(\n    df: pd.DataFrame,\n    drop_compute: bool,\n    drop_1shot: bool,\n    drop_incomplete_measurements: bool,\n    datasets: Optional[Iterable[str]] = None) -> pd.DataFrame:\n  \"\"\"Drops rows and columns that will not be used for analysis.\n\n  Args:\n    df: pd.DataFrame where each row corresponds to a model in `df`, and each\n      column is a 2-level multiindex of stucture `(metric, dataset)`; in typical\n      usage, `df` was obtained by calling `process_tuned_results`.\n    drop_compute: Whether to drop metrics for compute cost.\n    drop_1shot: Whether to include fewshot@1 results, which tend to have high\n      variance.\n    drop_incomplete_measurements: If True, only models which report values\n      across all metrics will be considered for scoring. Otherwise, only models\n      that report no measurements at all are dropped.\n    datasets: Optional datasets of interest. If None, all datasets are kept.\n\n  Returns:\n    A pd.DataFrame indexed by model name with columns corresponding to scores.\n  \"\"\"\n  df = df.copy()\n\n  if drop_compute:\n    df = df.drop(columns='compute', level=1, errors='ignore')\n  if drop_1shot:\n    cols_to_drop = [c for c in df.columns.levels[0] if c.startswith('1shot_')]\n    df = df.drop(columns=cols_to_drop, level=0, errors='ignore')\n  if datasets:\n    df = df.drop(\n        columns=[c for c in df.columns.levels[1] if c not in datasets],\n        level=1)\n\n  if drop_incomplete_measurements:\n    df = df.dropna(how='any')\n  else:\n    df = df.dropna(how='all')\n\n  # Level-0 column indices remain even if level-1 has been removed.\n  df.columns = df.columns.remove_unused_levels()\n  return df\n\n\ndef compute_score(df: pd.DataFrame,\n                  drop_incomplete_measurements: bool,\n                  baseline_model: Optional[str] = None,\n                  drop_1shot: bool = True,\n                  datasets: Optional[Iterable[str]] = None) -> pd.DataFrame:\n  \"\"\"Computes aggregate prediction, uncertainty, and adaptation scores.\n\n  Args:\n    df: pd.DataFrame where each row corresponds to a model in `df`, and each\n      column is a 2-level multiindex of stucture `(metric, dataset)`; in typical\n      usage, `df` was obtained by calling `process_tuned_results`.\n    drop_incomplete_measurements: If True, only models which report values\n      across all metrics will be considered for scoring. Otherwise, only models\n      that report no measurements at all are dropped.\n    baseline_model: If provided, scores will be computed relatively to the\n      metrics of this model.\n    drop_1shot: Whether to include fewshot@1 results, which tend to have high\n      variance.\n    datasets: Optional datasets of interest. If None, all datasets are kept.\n\n  Returns:\n    A pd.DataFrame indexed by model name with columns corresponding to scores.\n  \"\"\"\n  df = _drop_unused_measurements(\n      df,\n      drop_compute=True,\n      drop_1shot=drop_1shot,\n      datasets=datasets,\n      drop_incomplete_measurements=drop_incomplete_measurements)\n  df = _normalize_scores(df)\n\n  if baseline_model:\n    df /= df.loc[baseline_model]\n\n  categories = {category.name: [] for category in MetricCategory}\n  for metric in df.columns.levels[0]:\n    categories[get_metric_category(metric).name].append(metric)\n\n  series = df.mean(axis=1, skipna=False)\n  assert isinstance(series, pd.Series)\n  scores = series.to_frame(name='score')\n  for category, metrics in categories.items():\n    # Don't compute scores for methods that don't report all metrics in the\n    # current category, since this would make metrics such as ranking\n    # meaningless.\n    category_df = df[metrics].dropna(how='any')\n    if category_df.empty:\n      continue\n\n    # Average score per category.\n    scores[f'score_{category.lower()}'] = category_df.mean(axis=1)\n\n    # Number of times each model was the best per category.\n    best_per_task = category_df.idxmax()\n    col_name = f'#_best_{category.lower()}'\n    scores[col_name] = best_per_task.groupby(best_per_task).count()\n    # Fill in column with 0s for tasks that are never the best.\n    scores[col_name] = scores[col_name].fillna(\n        {model: 0 for model in category_df.index})\n\n    # Average rank per category.\n    scores[f'mean_rank_{category.lower()}'] = category_df.rank(\n        ascending=False).mean(axis=1)\n\n  return scores.sort_values(by='score', ascending=False)\n\n\ndef rank_models(df: pd.DataFrame,\n                drop_incomplete_measurements: bool,\n                drop_1shot: bool = True,\n                datasets: Optional[Iterable[str]] = None) -> pd.DataFrame:\n  \"\"\"Ranks models across all metrics of interest; lower rank is better.\"\"\"\n  df = _drop_unused_measurements(\n      df,\n      drop_compute=True,\n      drop_1shot=drop_1shot,\n      drop_incomplete_measurements=drop_incomplete_measurements,\n      datasets=datasets)\n  df = _normalize_scores(df)\n  return df.rank(ascending=False)\n\n\ndef rank_models_by_category(\n    df: pd.DataFrame,\n    drop_incomplete_measurements: bool,\n    drop_1shot: bool = True,\n    datasets: Optional[Iterable[str]] = None) -> Dict[str, pd.DataFrame]:\n  \"\"\"Returns a dictionary mapping MetricCategory to per-metric model ranking.\"\"\"\n  df = _drop_unused_measurements(\n      df,\n      drop_compute=True,\n      drop_1shot=drop_1shot,\n      datasets=datasets,\n      drop_incomplete_measurements=drop_incomplete_measurements)\n  df = _normalize_scores(df)\n\n  categories = {category.name: [] for category in MetricCategory}\n  for metric in df.columns.levels[0]:\n    categories[get_metric_category(metric).name].append(metric)\n\n  return {\n      category.lower(): df[metrics].rank(ascending=False)\n      for category, metrics in categories.items()\n  }\n\n\ndef make_radar_plot(df,\n                    row,\n                    color,\n                    max_val,\n                    ax,\n                    xticklabels=None,\n                    yscales=None,\n                    fontfamily='serif',\n                    fontsize=40):\n  \"\"\"Generate a radar plot given a dataframe of results.\n\n  Args:\n    df: A pandas dataframe with methods as rows and tasks as columns\n    row: A string indicating the row name to plot.\n    color: The color to use for this row in the plot.\n    max_val: The maximum value over all columns. It's recommended to normalize\n      the columns and then set this to 1.\n    ax: A pyplot axis handle corresponding to axis to plot on.\n    xticklabels: List of strings with labels for the x-ticks around the\n      perimeter of the plot.\n    yscales: List of tuples containing (min, max) indicating the ranges for\n      each y-axis in the plot (corresponding to xticklabels).\n    fontfamily: String indicating the matplotlib font family to use.\n    fontsize: Integer indicating the font size.\n  \"\"\"\n\n  categories = list(df)[0:]\n  num_categories = len(categories)\n\n  nticks = 5\n  angles = [\n      n / float(num_categories) * 2 * math.pi for n in range(num_categories)\n  ]\n  angles += angles[:1]\n\n  ax.set_theta_offset(math.pi / 2)\n  ax.set_theta_direction(-1)\n\n  xticklabels = xticklabels if xticklabels is not None else categories\n  plt.xticks(\n      angles[:-1],\n      xticklabels,\n      color='black',\n      size=fontsize,\n      fontweight='normal',\n      fontname=fontfamily)\n\n  ax.set_rlabel_position(0)\n\n  #  Rescale the values to fit in each y-axis.\n  values = df.loc[row].values.tolist()[0:].copy()\n  rescaled_values = []\n  if yscales is not None:\n    for i, k in enumerate(yscales):\n      min_val, max_val = k\n      rescaled_values.append((float(values[i]) - min_val) / (max_val - min_val))\n  values = rescaled_values\n  values += values[:1]\n\n  max_val += 0.2  # Adds some padding for ticklabels\n  ticks = np.linspace(0, max_val, nticks)\n  scaled_ticks = np.linspace(yscales[0][0], yscales[0][1], nticks)\n  ticklabels = ['%.2f' % i for i in scaled_ticks]\n  ax.set_yticks(ticks, ticklabels)\n  ax.set_ylim(0.0, max_val)\n  ax.set_yticklabels([])\n\n  ax.plot(angles, values, color=color, linewidth=1, linestyle='solid')\n  ax.fill(angles, values, color=color, alpha=0.5)\n\n  ax.spines['polar'].set_visible(False)\n  gridlines = ax.yaxis.get_gridlines()\n  gridlines[-1].set_visible(False)\n  ax.patch.set_alpha(0.01)\n\n  def add_new_yaxis(min_range, max_range, angle):\n    # Add ticks along the other axes.\n    ax2 = ax.figure.add_axes(\n        ax.get_position(),\n        projection='polar',\n        label='twin',\n        frameon=False,\n        theta_direction=ax.get_theta_direction(),\n        theta_offset=ax.get_theta_offset(),\n        zorder=0,  # zorder seems to be buggy for polar plots\n        alpha=0.1)\n    ax2.xaxis.set_visible(False)\n    scaled_ticks = np.linspace(min_range, max_range, nticks)\n    ticklabels = ['%.2f' % i for i in scaled_ticks]\n    ax2.set_yticks(ticks, ticklabels)\n    ax2.set_yticklabels(\n        ticklabels,\n        fontdict={\n            'fontsize': fontsize * .66,\n            'color': 'grey'\n        },\n        zorder=1)\n    ax2.set_yticklabels(\n        ticklabels, fontdict={'fontsize': fontsize * .8}, zorder=1)\n    ax2.tick_params(zorder=0.5)\n    ax2.set_ylim(0.0, max_val)\n    ax2.set_theta_zero_location('W', offset=-np.rad2deg(angle) + 22.5 - 90)\n\n    # Remove the tick label at zero\n    ax2.yaxis.get_major_ticks()[0].label1.set_visible(False)\n    ax2.yaxis.get_major_ticks()[-1].set_zorder(0.1)\n    ax2.spines['polar'].set_visible(False)\n    ax2.xaxis.set_visible(False)\n    ax2.yaxis.set_zorder(0.1)\n    ax2.yaxis.grid(False)\n    ax2.xaxis.grid(False)\n\n  for i in range(0, num_categories):\n    add_new_yaxis(yscales[i][0], yscales[i][1], angles[i])\n\n", "right_context": "", "import_text": ["enum", "itertools", "math", "re", "typing.Any", "typing.Dict", "typing.Iterable", "typing.List", "typing.Optional", "typing.Tuple", "immutabledict", "matplotlib.pyplot", "numpy", "pandas"], "prompt": "\"\"\"\nDescription: This function processes few-shot measurements for comparison between models.\n\nArgs:\n    measurements_dict (Dict[str, pd.DataFrame]): A dictionary where keys are model names and values are DataFrames containing measurements.\n\nReturns:\n    pd.DataFrame: A DataFrame with multi-index columns, where the first level is the metric and the second level is the dataset. The index is the model name.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "  \"\"\"Format the fewshot results for 'MoE', '[Det]_4' and '[MoE]_4'.\n\n  This function is not needed for Det, BE and Het since they had dedicated xm\n  jobs for their fewshot evaluations.\n\n  Args:\n    measurements_dict: dictionary of pd.DataFrame's where one row corresponds\n      to one seed for one given model.\n\n  Returns:\n    A pd.DataFrame indexed by model name with multindex columns corresponding to\n    pairs of (fewshot metric, fewshot dataset).\n  \"\"\"", "function_dependencies": ["re.fullmatch", "re.fullmatch.groups", "pandas.DataFrame", "pandas.DataFrame.set_index", "pandas.MultiIndex.from_tuples"], "project_create_time": "2020-07-16T01:54:32+00:00", "project_update_time": "2024-04-15T15:05:26+00:00", "file_create_time": "2022-07-14T01:02:22Z", "file_update_time": "2024-02-26T18:14:03Z", "function_update_time": "2022-07-14T01:02:22Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["pandas.MultiIndex.from_tuples"], "test_function": [{"file_path": "/uncertainty-baselines-main/uncertainty-baselines-main/experimental/plex/colab_utils_test.py", "class_name": "ColabUtilsTest", "function_name": "test_process_fewshot_for_moe_comparison", "code": "\n  def test_process_fewshot_for_moe_comparison(self):\n    df_from_dict = pd.DataFrame.from_dict\n\n    column_names = [\n        'IGNORED-INVALID-NAME',\n        'z/MY-DATASET1_666shot_MY-SUPER-METRIC',\n        'z/MY-DATASET2_666shot_MY-SUPER-METRIC',\n        'z/MY-DATASET1_3shot_best_l2'\n    ]\n\n    input_df = {\n        '[Det]_4': df_from_dict({c: [3., np.nan, 1.] for c in column_names}),\n        '[MoE]_4': df_from_dict({c: [np.nan, -1., 1.] for c in column_names}),\n        'MoE': df_from_dict({c: [0., 10., np.nan] for c in column_names}),\n    }\n\n    expected_cols = [('666shot_MY-SUPER-METRIC', 'few-shot MY-DATASET1'),\n                     ('666shot_MY-SUPER-METRIC', 'few-shot MY-DATASET2')]\n    expected_df = [\n        # The values correspond to the averages: (3+1)/2, (1-1)/2 and (0+10)/2.\n        {'model_name': 'MoE', expected_cols[0]: 5., expected_cols[1]: 5.},\n        {'model_name': '[Det]_4', expected_cols[0]: 2., expected_cols[1]: 2.},\n        {'model_name': '[MoE]_4', expected_cols[0]: 0., expected_cols[1]: 0.}\n    ]\n    expected_df = pd.DataFrame(expected_df).set_index('model_name')\n    expected_df.columns = pd.MultiIndex.from_tuples(\n        expected_df.columns, names=['metric', 'dataset'])\n\n    result_df = colab_utils.process_fewshot_for_moe_comparison(input_df)\n    pd.testing.assert_frame_equal(result_df, expected_df)"}]}, {"git_group": "holoviz", "git_name": "datashader", "version": "website", "language": "Python", "project_name": "datashader-website.zip", "file_path": "/datashader-website/datashader-website/datashader/tiles.py", "file_name": "tiles.py", "focal_class": null, "focal_name": "render_tiles", "focal_parameter": ["full_extent", "levels", "load_data_func", "rasterize_func", "shader_func", "post_render_func", "output_path"], "solution": "\ndef render_tiles(full_extent, levels, load_data_func,\n                 rasterize_func, shader_func,\n                 post_render_func, output_path, color_ranging_strategy='fullscan'):\n\n    #TODO: get full extent once at beginning for all levels\n    results = dict()\n    for level in levels:\n        print('calculating statistics for level {}'.format(level))\n        span = calculate_zoom_level_stats(full_extent, level,\n                                          load_data_func, rasterize_func,\n                                          color_ranging_strategy='fullscan')\n\n        super_tiles = list(gen_super_tiles(full_extent, level, span))\n        print('rendering {} supertiles for zoom level {} with span={}'.format(len(super_tiles), level, span))\n        b = db.from_sequence(super_tiles)\n        b.map(render_super_tile, output_path, load_data_func, rasterize_func, shader_func, post_render_func).compute()\n        results[level] = dict(success=True, stats=span, supertile_count=len(super_tiles))\n\n    return results", "function_signature": "def render_tiles(full_extent, levels, load_data_func,\n                 rasterize_func, shader_func,\n                 post_render_func, output_path, color_ranging_strategy='fullscan') :", "left_context": "from __future__ import absolute_import, division, print_function\nfrom io import BytesIO\n\nimport math\nimport os\n\nimport dask\nimport dask.bag as db\n\nimport numpy as np\n\nfrom PIL.Image import fromarray\n\n\n__all__ = ['render_tiles', 'MercatorTileDefinition']\n\n\n\n# helpers ---------------------------------------------------------------------\ndef _create_dir(path):\n\n    import os, errno\n\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\n\ndef _get_super_tile_min_max(tile_info, load_data_func, rasterize_func):\n    tile_size = tile_info['tile_size']\n    df = load_data_func(tile_info['x_range'], tile_info['y_range'])\n    agg = rasterize_func(df, x_range=tile_info['x_range'],\n                         y_range=tile_info['y_range'],\n                         height=tile_size, width=tile_size)\n    return [np.nanmin(agg.data), np.nanmax(agg.data)]\n\n\ndef calculate_zoom_level_stats(full_extent, level, load_data_func,\n                               rasterize_func,\n                               color_ranging_strategy='fullscan'):\n    if color_ranging_strategy == 'fullscan':\n        b = db.from_sequence(list(gen_super_tiles(full_extent,level)))\n        b = b.map(_get_super_tile_min_max, load_data_func, rasterize_func).flatten()\n        return dask.compute(b.min(), b.max())\n    else:\n        raise ValueError('Invalid color_ranging_strategy option')\n\n", "right_context": "\n\ndef gen_super_tiles(extent, zoom_level, span=None):\n    xmin, ymin, xmax, ymax = extent\n    super_tile_size = min(2**4 * 256,\n                         (2 ** zoom_level) * 256)\n    super_tile_def = MercatorTileDefinition(x_range=(xmin, xmax), y_range=(ymin, ymax), tile_size=super_tile_size)\n    super_tiles = super_tile_def.get_tiles_by_extent(extent, zoom_level)\n    for s in super_tiles:\n        st_extent = s[3]\n        x_range = (st_extent[0], st_extent[2])\n        y_range = (st_extent[1], st_extent[3])\n        yield {'level': zoom_level,\n                'x_range': x_range,\n                'y_range': y_range,\n                'tile_size': super_tile_def.tile_size,\n                'span': span}\n\n\ndef render_super_tile(tile_info, output_path, load_data_func,\n                      rasterize_func, shader_func, post_render_func):\n    tile_size = tile_info['tile_size']\n    level = tile_info['level']\n    df = load_data_func(tile_info['x_range'], tile_info['y_range'])\n    agg = rasterize_func(df, x_range=tile_info['x_range'],\n                         y_range=tile_info['y_range'], height=tile_size,\n                         width=tile_size)\n    ds_img = shader_func(agg, span=tile_info['span'])\n    return create_sub_tiles(ds_img, level, tile_info, output_path, post_render_func)\n\ndef create_sub_tiles(data_array, level, tile_info, output_path, post_render_func=None):\n\n    # validate / createoutput_dir\n    _create_dir(output_path)\n\n    # create tile source\n    tile_def = MercatorTileDefinition(x_range=tile_info['x_range'],\n                                      y_range=tile_info['y_range'],\n                                      tile_size=256)\n\n    # create Tile Renderer\n    if output_path.startswith('s3:'):\n        renderer = S3TileRenderer(tile_def, output_location=output_path,\n                                  post_render_func=post_render_func)\n    else:\n        renderer = FileSystemTileRenderer(tile_def, output_location=output_path,\n                                          post_render_func=post_render_func)\n\n    return renderer.render(data_array, level=level)\n\n\n# TODO: change name from source to definition\nclass MercatorTileDefinition(object):\n    ''' Implementation of mercator tile source\n    In general, tile sources are used as a required input for ``TileRenderer``.\n\n    Parameters\n    ----------\n\n    x_range : tuple\n      full extent of x dimension in data units\n\n    y_range : tuple\n      full extent of y dimension in data units\n\n    max_zoom : int\n      A maximum zoom level for the tile layer. This is the most zoomed-in level.\n\n    min_zoom : int\n      A minimum zoom level for the tile layer. This is the most zoomed-out level.\n\n    max_zoom : int\n      A maximum zoom level for the tile layer. This is the most zoomed-in level.\n\n    x_origin_offset : int\n      An x-offset in plot coordinates.\n\n    y_origin_offset : int\n      An y-offset in plot coordinates.\n\n    initial_resolution : int\n      Resolution (plot_units / pixels) of minimum zoom level of tileset\n      projection. None to auto-compute.\n\n    format : int\n      An y-offset in plot coordinates.\n\n    Output\n    ------\n    tileScheme: MercatorTileSource\n\n    '''\n\n    def __init__(self, x_range, y_range, tile_size=256, min_zoom=0, max_zoom=30,\n                 x_origin_offset=20037508.34, y_origin_offset=20037508.34,\n                 initial_resolution=156543.03392804097):\n        self.x_range = x_range\n        self.y_range = y_range\n        self.tile_size = tile_size\n        self.min_zoom = min_zoom\n        self.max_zoom = max_zoom\n        self.x_origin_offset = x_origin_offset\n        self.y_origin_offset = y_origin_offset\n        self.initial_resolution = initial_resolution\n        self._resolutions = [self._get_resolution(z) for z in range(self.min_zoom, self.max_zoom+1)]\n\n    def to_ogc_tile_metadata(self, output_file_path):\n        '''\n        Create OGC tile metadata XML\n        '''\n        pass\n\n\n    def to_esri_tile_metadata(self, output_file_path):\n        '''\n        Create ESRI tile metadata JSON\n        '''\n        pass\n\n\n    def is_valid_tile(self, x, y, z):\n\n        if x < 0 or x >= math.pow(2, z):\n            return False\n\n        if y < 0 or y >= math.pow(2, z):\n            return False\n\n        return True\n\n\n    # TODO ngjit?\n    def _get_resolution(self, z):\n        return self.initial_resolution / (2 ** z)\n\n\n    def get_resolution_by_extent(self, extent, height, width):\n        x_rs = (extent[2] - extent[0]) / width\n        y_rs = (extent[3] - extent[1]) / height\n        return [x_rs, y_rs]\n\n\n    def get_level_by_extent(self, extent, height, width):\n        x_rs = (extent[2] - extent[0]) / width\n        y_rs = (extent[3] - extent[1]) / height\n        resolution = max(x_rs, y_rs)\n\n        # TODO: refactor this...\n        i = 0\n        for r in self._resolutions:\n            if resolution > r:\n                if i == 0:\n                    return 0\n                if i > 0:\n                    return i - 1\n            i += 1\n        return (i-1)\n\n\n    def pixels_to_meters(self, px, py, level):\n        res = self._get_resolution(level)\n        mx = (px * res) - self.x_origin_offset\n        my = (py * res) - self.y_origin_offset\n        return (mx, my)\n\n\n    def meters_to_pixels(self, mx, my, level):\n        res = self._get_resolution(level)\n        px = (mx + self.x_origin_offset) / res\n        py = (my + self.y_origin_offset) / res\n        return (px, py)\n\n\n    def pixels_to_tile(self, px, py):\n        tx = math.ceil(px / self.tile_size)\n        tx = tx if tx == 0 else tx - 1\n        ty = max(math.ceil(py / self.tile_size) - 1, 0)\n        return (int(tx), int(ty))\n\n\n    def pixels_to_raster(self, px, py, level):\n        map_size = self.tile_size << level\n        return (px, map_size - py)\n\n\n    def meters_to_tile(self, mx, my, level):\n        px, py = self.meters_to_pixels(mx, my, level)\n        return self.pixels_to_tile(px, py)\n\n\n    def get_tiles_by_extent(self, extent, level):\n\n        # unpack extent and convert to tile coordinates\n        xmin, ymin, xmax, ymax = extent\n        txmin, tymin = self.meters_to_tile(xmin, ymin, level)\n        txmax, tymax = self.meters_to_tile(xmax, ymax, level)\n\n        # TODO: vectorize?\n        tiles = []\n        for ty in range(tymin, tymax + 1):\n            for tx in range(txmin, txmax + 1):\n                if self.is_valid_tile(tx, ty, level):\n                    t = (tx, ty, level, self.get_tile_meters(tx, ty, level))\n                    tiles.append(t)\n\n        return tiles\n\n\n    def get_tile_meters(self, tx, ty, level):\n        xmin, ymin = self.pixels_to_meters(tx * self.tile_size, ty * self.tile_size, level)\n        xmax, ymax = self.pixels_to_meters((tx + 1) * self.tile_size, (ty + 1) * self.tile_size, level)\n        return (xmin, ymin, xmax, ymax)\n\n\nclass TileRenderer(object):\n\n    def __init__(self, tile_definition, output_location, tile_format='PNG',\n                 post_render_func=None):\n\n        self.tile_def = tile_definition\n        self.output_location = output_location\n        self.tile_format = tile_format\n        self.post_render_func = post_render_func\n\n        # TODO: add all the formats supported by PIL\n        if self.tile_format not in ('PNG', 'JPG'):\n            raise ValueError('Invalid output format')\n\n    def render(self, da, level):\n        xmin, xmax = self.tile_def.x_range\n        ymin, ymax = self.tile_def.y_range\n        extent = xmin, ymin, xmax, ymax\n\n        tiles = self.tile_def.get_tiles_by_extent(extent, level)\n        for t in tiles:\n            x, y, z, data_extent = t\n            dxmin, dymin, dxmax, dymax = data_extent\n            arr = da.loc[{'x':slice(dxmin, dxmax), 'y':slice(dymin, dymax)}]\n\n            if 0 in arr.shape:\n                continue\n\n            img = fromarray(arr.data, 'RGBA')\n\n            if self.post_render_func:\n                extras = dict(x=x, y=y, z=z)\n                img = self.post_render_func(img, **extras)\n\n            yield (img, x, y, z)\n\n\ndef tile_previewer(full_extent, tileset_url,\n                   output_dir=None,\n                   filename='index.html',\n                   title='Datashader Tileset',\n                   min_zoom=0, max_zoom=40,\n                   height=None, width=None,\n                   **kwargs):\n    '''Helper function for creating a simple Bokeh figure with\n    a WMTS Tile Source.\n\n    Notes\n    -----\n    - if you don't supply height / width, stretch_both sizing_mode is used.\n    - supply an output_dir to write figure to disk.\n    '''\n\n    try:\n        from bokeh.plotting import figure\n        from bokeh.models.tiles import WMTSTileSource\n        from bokeh.io import output_file, save\n        from os import path\n    except ImportError:\n        raise ImportError('conda install bokeh to enable creation of simple tile viewer')\n\n    if output_dir:\n        output_file(filename=path.join(output_dir, filename),\n                    title=title)\n\n    xmin, ymin, xmax, ymax = full_extent\n\n    if height and width:\n        p = figure(width=width, height=height,\n                   x_range=(xmin, xmax),\n                   y_range=(ymin, ymax),\n                   tools=\"pan,wheel_zoom,reset\", **kwargs)\n    else:\n        p = figure(sizing_mode='stretch_both',\n                   x_range=(xmin, xmax),\n                   y_range=(ymin, ymax),\n                   tools=\"pan,wheel_zoom,reset\", **kwargs)\n\n    p.background_fill_color = 'black'\n    p.grid.grid_line_alpha = 0\n    p.axis.visible = True\n\n    tile_source = WMTSTileSource(url=tileset_url,\n                                 min_zoom=min_zoom,\n                                 max_zoom=max_zoom)\n    p.add_tile(tile_source, render_parents=False)\n\n    if output_dir:\n        save(p)\n\n    return p\n\n\nclass FileSystemTileRenderer(TileRenderer):\n\n    def render(self, da, level):\n        for img, x, y, z in super(FileSystemTileRenderer, self).render(da, level):\n            tile_file_name = '{}.{}'.format(y, self.tile_format.lower())\n            tile_directory = os.path.join(self.output_location, str(z), str(x))\n            output_file = os.path.join(tile_directory, tile_file_name)\n            _create_dir(tile_directory)\n            img.save(output_file, self.tile_format)\n\nclass S3TileRenderer(TileRenderer):\n\n    def render(self, da, level):\n\n        try:\n            import boto3\n        except ImportError:\n            raise ImportError('conda install boto3 to enable rendering to S3')\n\n        try:\n            from urlparse import urlparse\n        except ImportError:\n            from urllib.parse import urlparse\n\n        s3_info = urlparse(self.output_location)\n        bucket = s3_info.netloc\n        client = boto3.client('s3')\n        for img, x, y, z in super(S3TileRenderer, self).render(da, level):\n            tile_file_name = '{}.{}'.format(y, self.tile_format.lower())\n            key = os.path.join(s3_info.path, str(z), str(x), tile_file_name).lstrip('/')\n            output_buf = BytesIO()\n            img.save(output_buf, self.tile_format)\n            output_buf.seek(0)\n            client.put_object(Body=output_buf, Bucket=bucket, Key=key, ACL='public-read')\n\n        return 'https://{}.s3.amazonaws.com/{}'.format(bucket, s3_info.path)\n", "import_text": ["io.BytesIO", "math", "os", "dask", "dask.bag", "numpy", "PIL.Image.fromarray"], "prompt": "\"\"\"\nDescription: This function renders tiles for a given extent and levels.\n\nArgs:\n    full_extent (type): The full extent of the data to be rendered.\n    levels (type): The levels of the tiles to be rendered.\n    load_data_func (type): A function to load the data.\n    rasterize_func (type): A function to rasterize the data.\n    shader_func (type): A function to apply shaders to the data.\n    post_render_func (type): A function to post-render the data.\n    output_path (type): The path to output the rendered tiles.\n    color_ranging_strategy (type, optional): The strategy for color ranging. Defaults to 'fullscan'.\n\nReturns:\n    dict: A dictionary containing the results of the rendering process.\n\"\"\"", "comment": null, "prompt_is_gen_from_api": true, "function_dependencies": ["dask.bag.from_sequence", "dask.bag.from_sequence.map", "dask.bag.from_sequence.map.compute"], "project_create_time": "2015-12-23T18:02:20+00:00", "project_update_time": "2024-04-15T03:02:09+00:00", "file_create_time": "2018-10-25T19:01:17Z", "file_update_time": "2019-08-23T19:54:40Z", "function_update_time": "2018-10-25T19:01:17Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["dask.bag.from_sequence"], "test_function": [{"file_path": "/datashader-website/datashader-website/datashader/tests/test_tiles.py", "class_name": null, "function_name": "test_render_tiles", "code": "\ndef test_render_tiles():\n    full_extent_of_data = (-500000, -500000,\n                           500000, 500000)\n    levels = list(range(2))\n    output_path = 'test_tiles_output'\n    results = render_tiles(full_extent_of_data,\n                           levels,\n                           load_data_func=mock_load_data_func,\n                           rasterize_func=mock_rasterize_func,\n                           shader_func=mock_shader_func,\n                           post_render_func=mock_post_render_func,\n                           output_path=output_path)\n\n    assert results\n    assert isinstance(results, dict)\n\n    for l in levels:\n        assert l in results\n        assert isinstance(results[l], dict)\n\n    assert results[0]['success']\n    assert results[0]['stats']\n    assert results[0]['supertile_count']"}]}, {"git_group": "facebookresearch", "git_name": "multimodal", "version": "v2024.04.22.00", "language": "Python", "project_name": "multimodal-v2024.04.22.00.zip", "file_path": "/multimodal-v2024.04.22.00/multimodal-2024.04.22.00/torchmultimodal/modules/layers/attention.py", "file_name": "attention.py", "focal_class": null, "focal_name": "scaled_dot_product_attention", "focal_parameter": [], "solution": "def scaled_dot_product_attention(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    attention_mask: Optional[Tensor] = None,\n    head_mask: Optional[Tensor] = None,\n    attn_dropout: float = 0.0,\n) -> Tuple[Tensor, Tensor]:\n\n    # Take the dot product between \"query\" and \"key\" and scale to get the raw attention scores.\n    attn = torch.matmul(q, k.transpose(-1, -2))\n    attn = attn / torch.sqrt(torch.tensor(q.shape[-1]))\n    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n    # masked positions, this operation will create a tensor with the computed attention weights\n    # at the positions we want to attend and -inf for masked positions.\n    # Since we are adding it to the raw scores before the softmax, this is\n    # effectively the same as removing these entirely.\n    if attention_mask is not None:\n        attn = attn.masked_fill(attention_mask == 0, float(\"-inf\"))\n    # Normalize the attention scores to probabilities\n    attn_float = F.softmax(attn, dim=-1)\n    attn = attn_float.type_as(attn)  # b, h, d1, ..., q_dn, k_dn\n    # This is actually dropping out entire tokens to attend to, which might\n    # seem a bit unusual, but is taken from the original Transformer paper.\n    attn = F.dropout(attn, p=attn_dropout)\n    # Mask heads if we want to\n    if head_mask is not None:\n        attn = attn * head_mask\n    # For each query sum over the key/value dim with attention weights\n    a = torch.matmul(attn, v)  # b, h, d1, ..., q_dn, c\n\n    return a, attn", "function_signature": "def scaled_dot_product_attention(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    attention_mask: Optional[Tensor] = None,\n    head_mask: Optional[Tensor] = None,\n    attn_dropout: float = 0.0,\n) -> Tuple[Tensor, Tensor] :", "left_context": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport torch\nfrom torch import nn, Tensor\nfrom torch.nn import functional as F\nfrom torchmultimodal.utils.common import shift_dim\n\n\nclass SelfAttention(nn.Module):\n    \"\"\"Computes attention over the entire n-dimensional input.\n\n    Args:\n        attn_dropout (float, optional): Probability of dropout after softmax. Default is ``0.0``.\n    \"\"\"\n\n    def __init__(self, attn_dropout: float = 0.0) -> None:\n        super().__init__()\n        self.attn_dropout = attn_dropout\n\n    def forward(\n        self,\n        q: Tensor,\n        k: Tensor,\n        v: Tensor,\n        attention_mask: Optional[Tensor] = None,\n        head_mask: Optional[Tensor] = None,\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            q (Tensor): Query input of shape ``(b, h, d1, ..., dn, dim_q)`` where ``h`` is the number of\n                attention heads, ``(d1, ..., dn)`` are the query latent dimensions and ``dim_q`` is the dimension\n                of the query embeddings.\n            k, v (Tensor): Key/value input of shape ``(b, h, d1', ..., dn', dim_kv)`` where ``h`` is the number\n                of attention heads, ``(d1', ..., dn')`` are the key/value latent dimensions and ``dim_kv`` is\n                the dimension of the key/value embeddings.\n            attention_mask (Tensor, optional): Tensor of shape ``(b, h, q_dn, k_dn)`` where ``q_dn`` is the\n                dimension of the flattened query input along its latent dimensions and ``k_dn`` that of the\n                flattened key input. Contains 1s for positions to attend to and 0s for masked positions.\n            head_mask (Tensor, optional): Tensor of shape ``(b, h, q_dn, k_dn)``.\n                Contains 1s for positions to attend to and 0s for masked positions.\n\n        Returns:\n            A tuple of output tensor and attention probabilities.\n        \"\"\"\n        _, _, *shape, _ = q.shape\n\n        # flatten to b, h, (d1, ..., dn), dim_q/dim_kv\n        q = q.flatten(start_dim=2, end_dim=-2)\n        k = k.flatten(start_dim=2, end_dim=-2)\n        v = v.flatten(start_dim=2, end_dim=-2)\n\n        out, attn_probs = scaled_dot_product_attention(\n            q,\n            k,\n            v,\n            attention_mask=attention_mask,\n            head_mask=head_mask,\n            attn_dropout=self.attn_dropout if self.training else 0.0,\n        )\n\n        return out.unflatten(2, shape), attn_probs\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Computes multihead attention with flexible attention mechanism and caching for fast decoding.\n\n    Multihead attention linearly projects and divides queries, keys, and values into\n    multiple 'heads'. This enables the computation of attention multiple times in\n    parallel, creating more varied representations and allows the model to jointly\n    attend to information from different representation subspaces at different positions,\n    as described in `\"Attention Is All You Need (Vaswani et al. 2017)\"<https://arxiv.org/pdf/1706.03762.pdf>`_.\n\n    Args:\n        dim_q (int): Dimensionality of query input. Also the embedding dimension of the model.\n        dim_kv (int): Dimensionality of key/value input. Projects to the embedding dimension of the model, ``dim_q``.\n        n_head (int): Number of attention heads.\n        attn_module (nn.Module): Module of attention mechanism to use. Default is ``SelfAttention``.\n            See :class:`~torchmultimodal.modules.layers.attention.SelfAttention` for API details.\n        add_bias (bool): Whether to add bias to the q, k, v, linear layers or not. Default is ``True``.\n\n    Attributes:\n        cache (Dict[str, Tensor]): Dictionary that stores past key/value vectors.\n\n    Raises:\n        ValueError: When ``dim_q`` or ``dim_kv`` is not divisible by ``n_head``.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim_q: int,\n        dim_kv: int,\n        n_head: int,\n        attn_module: nn.Module = SelfAttention(),\n        add_bias: bool = True,\n    ) -> None:\n        super().__init__()\n        if dim_q % n_head != 0 or dim_kv % n_head != 0:\n            raise ValueError(\n                \"The hidden size of q, k, v must be a multiple of the number of attention heads.\"\n            )\n\n        self.dim_q = dim_q\n        self.dim_kv = dim_kv\n        self.n_head = n_head\n        self.query = nn.Linear(dim_q, dim_q, bias=add_bias)  # q\n        self.key = nn.Linear(dim_kv, dim_q, bias=add_bias)  # k\n        self.value = nn.Linear(dim_kv, dim_q, bias=add_bias)  # v\n        self.output = nn.Linear(dim_q, dim_q, bias=True)  # c\n\n        self.attn = attn_module\n\n        self.cache: Optional[Dict[str, Tensor]] = None\n\n    def forward(\n        self,\n        q: Tensor,\n        kv: Optional[Tensor] = None,\n        return_attn_weights: bool = False,\n        use_cache: bool = False,\n        causal: bool = False,\n        **attn_kwargs: Any,\n    ) -> Union[Tensor, Tuple[Tensor, Tensor]]:\n        \"\"\"\n        Args:\n            q (Tensor): Query of shape ``(b, d1, ..., dn, dim_q)`` or ``(b, seq_len, dim_q)``\n                (for autoregressive decoding it's typical to pass in flattened tensors).\n            kv (Tensor, optional): Key (and value) of shape ``(b, d1', ..., dn', dim_kv)`` or\n                ``(b, seq_len', dim_kv)``. If this argument is specified, cross-attention will be applied.\n                Default is ``None``.\n            use_cache (bool): If ``True``, caches past ``k`` and ``v`` tensors for faster decoding.\n                If ``False``, recomputes ``k`` and ``v`` for each decoding step. Default is ``False``.\n            causal (bool): Whether to use causal attention or not. Default is ``False``.\n\n        Returns:\n            * If ``return_attn_weights`` is ``True``: A tuple of output tensor and attention probabilities.\n            * If ``return_attn_weights`` is ``False``: A single output tensor.\n        \"\"\"\n        # If kv is specified use those inputs for cross-attention, otherwise use q\n        k = v = q if kv is None else kv\n        # compute q\n        q = split_multihead(self.query(q), self.n_head)\n\n        # For causal k, v are provided step-wise so we should always compute them\n        # For non-causal skip computing k, v if they have been cached\n        if causal or not self.cache:\n            k = split_multihead(self.key(k), self.n_head)\n            v = split_multihead(self.value(v), self.n_head)\n\n        # fast decoding by caching past key, value tensors\n        if use_cache:\n            if not self.cache:\n                # initialize the cache with the present k, v\n                self.cache = dict(k=k.clone(), v=v.clone())\n            else:\n                if causal:\n                    # append present k, v to past k, v\n                    # for autoregressive decoding inputs are flattened as 1D sequences\n                    # so are the cached tensors: (b, n_heads, seq_len, c)\n                    k_, v_ = self.cache[\"k\"], self.cache[\"v\"]\n                    self.cache[\"k\"] = torch.cat([k_, k], dim=2)\n                    self.cache[\"v\"] = torch.cat([v_, v], dim=2)\n                # override the present k, v with the cache\n                k, v = self.cache[\"k\"], self.cache[\"v\"]\n\n        attn_out = self.attn(q, k, v, **attn_kwargs)\n        attn_probs = None\n        # Unpack if attn module also returns attn probs\n        if isinstance(attn_out, tuple):\n            attn_out, attn_probs = attn_out\n        a = merge_multihead(attn_out)\n        a = self.output(a)\n\n        if return_attn_weights:\n            return a, attn_probs\n        else:\n            return a\n\n", "right_context": "\n\ndef split_multihead(x: Tensor, n_head: int) -> Tensor:\n    \"\"\"Splits channel dimension of input tensor of size (b, d1, ..., dn, c)\n    into multiple heads, (b, n_head, d1, ..., dn, c // n_head)\"\"\"\n    x = x.unflatten(-1, (n_head, -1))\n    # Rearrange to put head dim first, (b, n_head, d1, ..., dn, c // n_head)\n    x = shift_dim(x, -2, 1)\n    return x\n\n\ndef merge_multihead(x: Tensor) -> Tensor:\n    \"\"\"Moves head dim back to original location and concatenates heads\n    (b, n_head, d1, ..., dn, c // n_head) -> (b, d1, ..., dn, c)\"\"\"\n    return shift_dim(x, 1, -2).flatten(start_dim=-2)\n", "import_text": ["typing.Any", "typing.Dict", "typing.Optional", "typing.Tuple", "typing.Union", "torch", "torch.nn", "torch.Tensor", "torch.nn.functional", "torchmultimodal.utils.common.shift_dim"], "prompt": "\"\"\"\nDescription: This function implements the scaled dot-product attention mechanism as described in the Transformer model.\n\nArgs:\n    q (Tensor): Query tensor of shape (batch_size, num_heads, ..., query_dim).\n    k (Tensor): Key tensor of shape (batch_size, num_heads, ..., key_dim).\n    v (Tensor): Value tensor of shape (batch_size, num_heads, ..., value_dim).\n    attention_mask (Optional[Tensor]): Attention mask tensor of shape (batch_size, ..., query_dim), where 1.0 indicates positions to attend to and 0.0 indicates masked positions.\n    head_mask (Optional[Tensor]): Head mask tensor of shape (num_heads), where 1.0 indicates heads to attend to and 0.0 indicates masked heads.\n    attn_dropout (float): Dropout probability for the attention weights.\n\nReturns:\n    Tuple[Tensor, Tensor]: A tuple containing the output tensor of shape (batch_size, num_heads, ..., query_dim, value_dim) and the attention weights tensor of shape (batch_size, num_heads, ..., query_dim, key_dim).\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Similar to PyTorch Core's _scaled_dot_product_attention but generalized\n    to handle n-dimensional input tokens (images, video) and support multihead.\n    Computes attention as described in Attention Is All You Need (Vaswani et al. 2017)\n\n    Args:\n        q (Tensor): Query of shape ``(b, h, d1, ..., dn, dim_qk)`` or ``(b, h, seq_len, dim_qk)`` where\n            ``h`` is number of attention heads, ``d1, ..., dn`` are latent dimensions and ``dim_qk` is\n            the embedding dim of the query tensor.\n        k (Tensor): Key of shape ``(b, h, d1', ...., dn', dim_qk)`` or ``(b, h, seq_len', dim_qk)`` where\n            ``h`` is the number of attention heads, ``d1', ..., dn'` are latent dimensions and ``dim_qk``\n            is the key embedding dim aligned with query embedding dim,\n            see :class:`~torchmultimodal.modules.layers.attention.MultiHeadAttention`.\n        v (Tensor): Value of shape ``(b, h, d1', ..., dn', dim_v)`` or ``(b, h, seq_len', dim_v)`` where\n            ``h`` is the number of attention heads, ``d1', ..., dn'`` are latent dimensions and ``dim_v``\n            is the embedding dim of the value tensor.\n        attention_mask (Tensor, optional): Tensor of shape ``(b, h, d1, ..., q_dn, k_dn)``.\n            Contains 1s for positions to attend to and 0s for masked positions. Applied before softmax.\n        head_mask (Tensor, optional): Tensor of shape ``(b, h, d1, ..., q_dn, k_dn)``.\n            Contains 1s for positions to attend to and 0s for masked positions.\n            Applied after dropout, before matrix multiplication with values.\n        attn_dropout (float): Probability of dropout after softmax. Default is ``0.0``.\n\n    Returns:\n        A tuple of output tensor and attention probabilities.\n    \"\"\"", "function_dependencies": ["torch.matmul", "torch.sqrt", "torch.tensor", "torch.matmul.masked_fill", "torch.nn.functional.softmax", "torch.nn.functional.softmax.type_as", "torch.nn.functional.dropout"], "project_create_time": "2022-01-27T20:01:00+00:00", "project_update_time": "2024-04-16T15:14:52+00:00", "file_create_time": "2022-06-10T18:47:21Z", "file_update_time": "2023-10-03T19:49:56Z", "function_update_time": "2022-07-16T00:00:44Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["torch.matmul", "torch.nn.functional.softmax", "torch.sqrt", "torch.nn.functional.dropout"], "test_function": [{"file_path": "/multimodal-v2024.04.22.00/multimodal-2024.04.22.00/tests/modules/layers/test_attention.py", "class_name": "TestScaledDotProductAttention", "function_name": "test_scaled_dot_product_attention", "code": "\n    def test_scaled_dot_product_attention(self, q, kv):\n        output, weights = scaled_dot_product_attention(q, kv, kv)\n        actual = output\n        expected = torch.tensor(\n            [\n                [\n                    [\n                        [\n                            [[-0.5862, 1.7955, 1.0711], [-0.2718, 1.2177, 1.4946]],\n                            [[-0.0613, 0.1774, 0.4893], [0.6899, -0.0650, 0.2909]],\n                        ],\n                        [\n                            [[0.2950, 1.2029, 1.7035], [0.2735, 0.5582, 0.6797]],\n                            [[-1.1558, 1.0143, 0.1598], [0.7875, 0.0928, -0.7952]],\n                        ],\n                    ],\n                ],\n            ]\n        )\n        assert_expected(actual, expected, rtol=0, atol=1e-4)\n        actual = weights\n        expected = torch.tensor(\n            [\n                [\n                    [\n                        [\n                            [[0.8797, 0.1203], [0.5595, 0.4405]],\n                            [[0.0553, 0.9447], [0.4549, 0.5451]],\n                        ],\n                        [\n                            [[0.0419, 0.9581], [0.4391, 0.5609]],\n                            [[0.0297, 0.9703], [0.7313, 0.2687]],\n                        ],\n                    ]\n                ]\n            ]\n        )\n        assert_expected(actual, expected, rtol=0, atol=1e-4)"}, {"file_path": "/multimodal-v2024.04.22.00/multimodal-2024.04.22.00/tests/modules/layers/test_attention.py", "class_name": "TestScaledDotProductAttention", "function_name": "test_scaled_dot_product_attention_with_attention_mask", "code": "\n    def test_scaled_dot_product_attention_with_attention_mask(self, q, kv):\n        attn_shape = torch.Size([1, 1, 2, 2, 2, 2])\n        mask = torch.tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1]).view(\n            attn_shape\n        )\n        actual, _ = scaled_dot_product_attention(q, kv, kv, attention_mask=mask)\n        expected = torch.tensor(\n            [\n                [\n                    [\n                        [\n                            [[-0.7042, 2.0126, 0.9120], [-0.2718, 1.2177, 1.4946]],\n                            [[-0.1652, 0.2109, 0.5167], [1.7146, -0.3956, 0.0204]],\n                        ],\n                        [\n                            [[0.2950, 1.2029, 1.7035], [0.2973, 1.2710, 1.8117]],\n                            [[1.5320, -0.2602, -1.1611], [0.7875, 0.0928, -0.7952]],\n                        ],\n                    ]\n                ]\n            ]\n        )\n        assert_expected(actual, expected, rtol=0, atol=1e-4)"}, {"file_path": "/multimodal-v2024.04.22.00/multimodal-2024.04.22.00/tests/modules/layers/test_attention.py", "class_name": "TestScaledDotProductAttention", "function_name": "test_scaled_dot_product_attention_with_head_mask", "code": "\n    def test_scaled_dot_product_attention_with_head_mask(self, q, kv):\n        attn_shape = torch.Size([1, 1, 2, 2, 2, 2])\n        mask = torch.tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1]).view(\n            attn_shape\n        )\n        actual, _ = scaled_dot_product_attention(q, kv, kv, head_mask=mask)\n        expected = torch.tensor(\n            [\n                [\n                    [\n                        [\n                            [[-0.6195, 1.7705, 0.8023], [-0.2718, 1.2177, 1.4946]],\n                            [[-0.1561, 0.1993, 0.4882], [0.7800, -0.1800, 0.0093]],\n                        ],\n                        [\n                            [[0.2950, 1.2029, 1.7035], [0.1668, 0.7129, 1.0162]],\n                            [[0.0455, -0.0077, -0.0345], [0.7875, 0.0928, -0.7952]],\n                        ],\n                    ]\n                ]\n            ]\n        )\n        assert_expected(actual, expected, rtol=0, atol=1e-4)"}, {"file_path": "/multimodal-v2024.04.22.00/multimodal-2024.04.22.00/tests/modules/layers/test_attention.py", "class_name": "TestScaledDotProductAttention", "function_name": "test_scaled_dot_product_attention_with_dropout", "code": "\n    def test_scaled_dot_product_attention_with_dropout(self, q, kv):\n        actual, _ = scaled_dot_product_attention(q, kv, kv, attn_dropout=0.3)\n        expected = torch.tensor(\n            [\n                [\n                    [\n                        [\n                            [\n                                [0.0000e00, 0.0000e00, 0.0000e00],\n                                [-5.6284e-01, 1.6085e00, 7.2891e-01],\n                            ],\n                            [\n                                [1.3536e-01, -3.1232e-02, 1.6106e-03],\n                                [9.8563e-01, -9.2847e-02, 4.1562e-01],\n                            ],\n                        ],\n                        [\n                            [\n                                [4.2149e-01, 1.7184e00, 2.4336e00],\n                                [2.3824e-01, 1.0184e00, 1.4517e00],\n                            ],\n                            [\n                                [-1.6511e00, 1.4490e00, 2.2828e-01],\n                                [1.1250e00, 1.3256e-01, -1.1361e00],\n                            ],\n                        ],\n                    ]\n                ]\n            ]\n        )\n        assert_expected(actual, expected, rtol=0, atol=1e-4)"}]}, {"git_group": "bashtage", "git_name": "linearmodels", "version": "v6.0", "language": "Python", "project_name": "linearmodels-v6.0.zip", "file_path": "/linearmodels-v6.0/linearmodels-6.0/linearmodels/system/_utility.py", "file_name": "_utility.py", "focal_class": null, "focal_name": "blocked_diag_product", "focal_parameter": [], "solution": "def blocked_diag_product(x: ArraySequence, s: Float64Array) -> Float64Array:\n    k = len(x)\n    out = []\n    for i in range(k):\n        row = []\n        for j in range(k):\n            row.append(s[i, j] * x[j])\n        row_arr = np.hstack(row)\n        out.append(row_arr)\n\n    return np.vstack(out)", "function_signature": "def blocked_diag_product(x: ArraySequence, s: Float64Array) -> Float64Array :", "left_context": "from __future__ import annotations\n\nfrom typing import cast\n\nimport numpy as np\nfrom numpy.linalg import inv, matrix_rank\nimport pandas as pd\n\nfrom linearmodels.typing import ArraySequence, Float64Array\n\n\ndef blocked_column_product(x: ArraySequence, s: Float64Array) -> Float64Array:\n    \"\"\"\n    Parameters\n    ----------\n    x : list of ndarray\n        k-element list of arrays to construct the inner product\n    s : ndarray\n        Weighting matrix (k by k)\n\n    Returns\n    -------\n    ndarray\n        Blocked product.  k x nobs rows and the number of columns is the same\n        the number of columns as any member of x.\n    \"\"\"\n    k = len(x)\n    out = []\n    for i in range(k):\n        val = s[i, 0] * x[0]\n        for j in range(1, k):\n            val += s[i, j] * x[j]\n        out.append(val)\n    return np.vstack(out)\n\n", "right_context": "\n\ndef blocked_inner_prod(x: ArraySequence, s: Float64Array) -> Float64Array:\n    r\"\"\"\n    Parameters\n    ----------\n    x : list of ndarray\n        k-element list of arrays to construct the inner product\n    s : ndarray\n        Weighting matrix (k by k)\n\n    Returns\n    -------\n    ndarray\n        Weighted inner product constructed from x and s\n\n    Notes\n    -----\n    Memory efficient implementation of high-dimensional inner product\n\n    .. math::\n\n      X'(S \\otimes I_n)X\n\n    where n is the number of observations in the sample\n    \"\"\"\n    k = len(x)\n    widths = [m.shape[1] for m in x]\n    s_is_diag = np.all(np.asarray((s - np.diag(np.diag(s))) == 0.0))\n\n    w0 = widths[0]\n    homogeneous = all([w == w0 for w in widths])\n    if homogeneous and not s_is_diag:\n        # Fast path when all x have same number of columns\n        # Slower than diag case when k is large since many 0s\n        xa = np.hstack(x)\n        return xa.T @ xa * np.kron(s, np.ones((w0, w0)))\n\n    cum_width = np.cumsum([0] + widths)\n    total = sum(widths)\n    out: np.ndarray = np.zeros((total, total))\n\n    for i in range(k):\n        xi = x[i]\n        sel_i = slice(cum_width[i], cum_width[i + 1])\n        s_ii = s[i, i]\n        prod = s_ii * (xi.T @ xi)\n        out[sel_i, sel_i] = prod\n\n    # Short circuit if identity\n    if s_is_diag:\n        return out\n\n    for i in range(k):\n        xi = x[i]\n        sel_i = slice(cum_width[i], cum_width[i + 1])\n        for j in range(i + 1, k):\n            sel_j = slice(cum_width[j], cum_width[j + 1])\n            xj = x[j]\n            s_ij = s[i, j]\n            prod = s_ij * (xi.T @ xj)\n            out[sel_i, sel_j] = prod\n            out[sel_j, sel_i] = prod.T\n\n    return cast(np.ndarray, out)\n\n\ndef blocked_cross_prod(\n    x: ArraySequence, z: ArraySequence, s: Float64Array\n) -> Float64Array:\n    r\"\"\"\n    Parameters\n    ----------\n    x : list of ndarray\n        k-element list of arrays to use as the left side of the cross-product\n    z : list of ndarray\n        k-element list of arrays to use as the right side of the cross-product\n    s : ndarray\n        Weighting matrix (k by k)\n\n    Returns\n    -------\n    ndarray\n        Weighted cross product constructed from x and s\n\n    Notes\n    -----\n    Memory efficient implementation of high-dimensional cross product\n\n    .. math::\n\n      X'(S \\otimes I_N)Z\n\n    where n is the number of observations in the sample\n    \"\"\"\n    k = len(x)\n    xp = []\n    for i in range(k):\n        row = []\n        for j in range(k):\n            s_ij = s[i, j]\n            row.append(s_ij * (x[i].T @ z[j]))\n        xp.append(np.concatenate(row, 1))\n    return np.concatenate(xp, 0)\n\n\ndef blocked_full_inner_product(x: Float64Array, s: Float64Array) -> Float64Array:\n    r\"\"\"\n    Parameters\n    ----------\n        x : ndarray\n            Array of shape KT by KT\n        s : ndarray\n            Array of shape K by K\n\n    Notes\n    -----\n    Computes the quantity\n\n    .. math ::\n\n        x^\\prime (S \\otimes I_N)x\n    \"\"\"\n    k = s.shape[0]\n    t = x.shape[0] // k\n    sx = np.empty_like(x)\n    for i in range(k):\n        v = s[i, 0] * x[0:t]\n        for j in range(1, k):\n            v += s[i, j] * x[j * t : (j + 1) * t]\n        sx[i * t : (i + 1) * t] = v\n    return x.T @ sx\n\n\ndef inv_matrix_sqrt(s: Float64Array) -> Float64Array:\n    vecs, vals = np.linalg.eigh(s)\n    vecs = 1.0 / np.sqrt(vecs)\n    out = vals @ np.diag(vecs) @ vals.T\n    return (out + out.T) / 2\n\n\nclass LinearConstraint:\n    r\"\"\"\n    Linear constraint for regression estimation\n\n    Parameters\n    ----------\n    r : {ndarray, DataFrame}\n        Restriction loading matrix\n    q : {ndarray, Series}\n        Restriction value\n    num_params : int\n        Number of model parameter.  Used to test for correctness\n    require_pandas : bool\n        Flag indicating whether r and q must be pandas\n\n    Notes\n    -----\n    Used to impose the constraints\n\n    .. math ::\n\n        r \\beta = q\n    \"\"\"\n\n    def __init__(\n        self,\n        r: pd.DataFrame | np.ndarray,\n        q: pd.Series | np.ndarray | None = None,\n        num_params: int | None = None,\n        require_pandas: bool = True,\n    ) -> None:\n        if not isinstance(r, (pd.DataFrame, np.ndarray)):\n            raise TypeError(\"r must be an array or DataFrame\")\n        elif require_pandas and not isinstance(r, pd.DataFrame):\n            raise TypeError(\"r must be a DataFrame\")\n        if r.ndim != 2:\n            raise ValueError(\"r must be 2-dimensional\")\n        r_pd = pd.DataFrame(r)\n        ra = np.asarray(r, dtype=np.float64)\n        self._r_pd = r_pd\n        self._ra = ra\n        if q is not None:\n            if require_pandas and not isinstance(q, pd.Series):\n                raise TypeError(\"q must be a Series\")\n            elif not isinstance(q, (pd.Series, np.ndarray)):\n                raise TypeError(\"q must be a Series or an array\")\n            if r.shape[0] != q.shape[0]:\n                raise ValueError(\"Constraint inputs are not shape compatible\")\n            q_pd = pd.Series(q, index=r_pd.index)\n        else:\n            q_pd = pd.Series(np.zeros(r_pd.shape[0]), index=r_pd.index)\n        self._q_pd = q_pd\n        self._qa = np.asarray(q_pd)\n        self._computed = False\n        self._t = np.empty((0, 0))\n        self._l = np.empty((0, 0))\n        self._a = np.empty((0, 0))\n        self._num_params = num_params\n        self._verify_constraints()\n\n    def __repr__(self) -> str:\n        return self.__str__() + \"\\nid: \" + str(hex(id(self)))\n\n    def __str__(self) -> str:\n        return f\"Linear Constraint with {self._ra.shape[0]} constraints\"\n\n    def _verify_constraints(self) -> None:\n        r = self._ra\n        q = self._qa\n        if self._num_params is not None:\n            if r.shape[1] != self._num_params:\n                raise ValueError(\n                    \"r is incompatible with the number of model \" \"parameters\"\n                )\n        rq = np.c_[r, q[:, None]]\n        if not np.all(np.isfinite(rq)) or matrix_rank(rq) < rq.shape[0]:\n            raise ValueError(\"Constraints must be non-redundant\")\n        qr = np.linalg.qr(rq)\n        if matrix_rank(qr[1][:, :-1]) != matrix_rank(qr[1]):\n            raise ValueError(\"One or more constraints are infeasible\")\n\n    def _compute_transform(self) -> None:\n        r = self._ra\n        c, k = r.shape\n        m = np.eye(k) - r.T @ inv(r @ r.T) @ r\n        vals, vecs = np.linalg.eigh(m)\n        vals = np.real(vals)\n        vecs = np.real(vecs)\n        idx = np.argsort(vals)[::-1]\n        vecs = vecs[:, idx]\n        t, left = vecs[:, : k - c], vecs[:, k - c :]\n        q = self._qa[:, None]\n        a = q.T @ inv(left.T @ r.T) @ left.T\n        self._t, self._l, self._a = t, left, a\n        self._computed = True\n\n    @property\n    def r(self) -> pd.DataFrame:\n        \"\"\"Constraint loading matrix\"\"\"\n        return self._r_pd\n\n    @property\n    def t(self) -> Float64Array:\n        \"\"\"\n        Constraint transformation matrix\n\n        Returns\n        -------\n        ndarray\n            Constraint transformation matrix\n\n        Notes\n        -----\n        Constrained regressors are constructed as x @ t\n        \"\"\"\n        if not self._computed:\n            self._compute_transform()\n        assert isinstance(self._t, np.ndarray)\n        return self._t\n\n    @property\n    def a(self) -> Float64Array:\n        r\"\"\"\n        Transformed constraint target\n\n        Returns\n        -------\n        ndarray\n            Transformed target\n\n        Notes\n        -----\n        Has two uses.  The first is to translate the restricted parameters\n        back to the full parameter vector using\n\n        .. math ::\n\n           beta = t  beta_c + ^\\prime\n\n        Also used in estimation or restricted model to demean the dependent\n        variable\n\n        .. math ::\n\n            \\tilde{y} = y - x  a^\\prime\n        \"\"\"\n        if not self._computed:\n            self._compute_transform()\n        assert isinstance(self._a, np.ndarray)\n        return self._a\n\n    @property\n    def q(self) -> pd.Series | np.ndarray:\n        \"\"\"Constrain target values\"\"\"\n        return self._q_pd\n", "import_text": ["typing.cast", "numpy", "numpy.linalg.inv", "numpy.linalg.matrix_rank", "pandas", "linearmodels.typing.ArraySequence", "linearmodels.typing.Float64Array"], "prompt": "\"\"\"\nDescription: This function computes the product of a sequence and a 2D array along the diagonal, and stacks the results vertically.\n\nArgs:\n    x (ArraySequence): A sequence of elements to be multiplied with the diagonal elements of the 2D array.\n    s (Float64Array): A 2D array from which the diagonal elements are to be taken.\n\nReturns:\n    Float64Array: A 2D array where each row is the product of the corresponding element in 'x' and the diagonal elements of 's', stacked vertically.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Parameters\n    ----------\n    x : list of ndarray\n        k-element list of arrays to construct the inner product\n    s : ndarray\n        Weighting matrix (k by k)\n\n    Returns\n    -------\n    ndarray\n        Blocked product.  k x nobs rows and the number of columns is the same\n        as the total number of columns in x.\n    \"\"\"", "function_dependencies": ["numpy.hstack", "numpy.vstack"], "project_create_time": "2017-02-17T11:39:15+00:00", "project_update_time": "2024-04-17T10:03:03+00:00", "file_create_time": "2017-06-15T08:57:57Z", "file_update_time": "2022-07-01T16:19:59Z", "function_update_time": "2021-05-13T07:50:04Z", "license": {"key": "ncsa", "name": "University of Illinois/NCSA Open Source License", "spdx_id": "NCSA", "url": "https://api.github.com/licenses/ncsa", "node_id": "MDc6TGljZW5zZTI5"}, "reference_api": ["numpy.vstack"], "test_function": [{"file_path": "/linearmodels-v6.0/linearmodels-6.0/linearmodels/tests/system/test_utility.py", "class_name": null, "function_name": "test_diag_product", "code": "\ndef test_diag_product(data):\n    y, x, sigma = data\n    efficient = blocked_diag_product(x, sigma)\n    nobs = x[0].shape[0]\n    omega = np.kron(sigma, np.eye(nobs))\n    k = len(x)\n    bigx = []\n    for i in range(k):\n        row = []\n        for j in range(k):\n            if i == j:\n                row.append(x[i])\n            else:\n                row.append(np.zeros((nobs, x[j].shape[1])))\n        bigx.append(np.hstack(row))\n    bigx = np.vstack(bigx)\n    expected = omega @ bigx\n    assert_allclose(efficient, expected)"}]}, {"git_group": "krishnaik06", "git_name": "The-Grand-Complete-Data-Science-Materials", "version": "main", "language": "Python", "project_name": "The-Grand-Complete-Data-Science-Materials-main.zip", "file_path": "/The-Grand-Complete-Data-Science-Materials-main/The-Grand-Complete-Data-Science-Materials-main/ML Projects/NLP_WebAPP_Twitter_Sentiment_Analysis_knowledge_graph/venv/Lib/site-packages/pandas/core/ops/array_ops.py", "file_name": "array_ops.py", "focal_class": null, "focal_name": "comparison_op", "focal_parameter": ["op"], "solution": "def comparison_op(left: ArrayLike, right: Any, op) -> ArrayLike:\n    # NB: We assume extract_array has already been called on left and right\n    lvalues = ensure_wrapped_if_datetimelike(left)\n    rvalues = ensure_wrapped_if_datetimelike(right)\n\n    rvalues = lib.item_from_zerodim(rvalues)\n    if isinstance(rvalues, list):\n        # We don't catch tuple here bc we may be comparing e.g. MultiIndex\n        #  to a tuple that represents a single entry, see test_compare_tuple_strs\n        rvalues = np.asarray(rvalues)\n\n    if isinstance(rvalues, (np.ndarray, ABCExtensionArray)):\n        # TODO: make this treatment consistent across ops and classes.\n        #  We are not catching all listlikes here (e.g. frozenset, tuple)\n        #  The ambiguous case is object-dtype.  See GH#27803\n        if len(lvalues) != len(rvalues):\n            raise ValueError(\n                \"Lengths must match to compare\", lvalues.shape, rvalues.shape\n            )\n\n    if should_extension_dispatch(lvalues, rvalues) or (\n        (isinstance(rvalues, (Timedelta, BaseOffset, Timestamp)) or right is NaT)\n        and not is_object_dtype(lvalues.dtype)\n    ):\n        # Call the method on lvalues\n        res_values = op(lvalues, rvalues)\n\n    elif is_scalar(rvalues) and isna(rvalues):  # TODO: but not pd.NA?\n        # numpy does not like comparisons vs None\n        if op is operator.ne:\n            res_values = np.ones(lvalues.shape, dtype=bool)\n        else:\n            res_values = np.zeros(lvalues.shape, dtype=bool)\n\n    elif is_numeric_v_string_like(lvalues, rvalues):\n        # GH#36377 going through the numexpr path would incorrectly raise\n        return invalid_comparison(lvalues, rvalues, op)\n\n    elif is_object_dtype(lvalues.dtype) or isinstance(rvalues, str):\n        res_values = comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\n    else:\n        res_values = _na_arithmetic_op(lvalues, rvalues, op, is_cmp=True)\n\n    return res_values", "function_signature": "def comparison_op(left: ArrayLike, right: Any, op) -> ArrayLike :", "left_context": "\"\"\"\nFunctions for arithmetic and comparison operations on NumPy arrays and\nExtensionArrays.\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime\nfrom functools import partial\nimport operator\nfrom typing import Any\n\nimport numpy as np\n\nfrom pandas._libs import (\n    NaT,\n    Timedelta,\n    Timestamp,\n    lib,\n    ops as libops,\n)\nfrom pandas._libs.tslibs import BaseOffset\nfrom pandas._typing import (\n    ArrayLike,\n    Shape,\n)\n\nfrom pandas.core.dtypes.cast import (\n    construct_1d_object_array_from_listlike,\n    find_common_type,\n)\nfrom pandas.core.dtypes.common import (\n    ensure_object,\n    is_bool_dtype,\n    is_integer_dtype,\n    is_list_like,\n    is_numeric_v_string_like,\n    is_object_dtype,\n    is_scalar,\n)\nfrom pandas.core.dtypes.generic import (\n    ABCExtensionArray,\n    ABCIndex,\n    ABCSeries,\n)\nfrom pandas.core.dtypes.missing import (\n    isna,\n    notna,\n)\n\nfrom pandas.core.computation import expressions\nfrom pandas.core.construction import ensure_wrapped_if_datetimelike\nfrom pandas.core.ops import (\n    missing,\n    roperator,\n)\nfrom pandas.core.ops.dispatch import should_extension_dispatch\nfrom pandas.core.ops.invalid import invalid_comparison\n\n\ndef comp_method_OBJECT_ARRAY(op, x, y):\n    if isinstance(y, list):\n        y = construct_1d_object_array_from_listlike(y)\n\n    if isinstance(y, (np.ndarray, ABCSeries, ABCIndex)):\n        if not is_object_dtype(y.dtype):\n            y = y.astype(np.object_)\n\n        if isinstance(y, (ABCSeries, ABCIndex)):\n            y = y._values\n\n        if x.shape != y.shape:\n            raise ValueError(\"Shapes must match\", x.shape, y.shape)\n        result = libops.vec_compare(x.ravel(), y.ravel(), op)\n    else:\n        result = libops.scalar_compare(x.ravel(), y, op)\n    return result.reshape(x.shape)\n\n\ndef _masked_arith_op(x: np.ndarray, y, op):\n    \"\"\"\n    If the given arithmetic operation fails, attempt it again on\n    only the non-null elements of the input array(s).\n\n    Parameters\n    ----------\n    x : np.ndarray\n    y : np.ndarray, Series, Index\n    op : binary operator\n    \"\"\"\n    # For Series `x` is 1D so ravel() is a no-op; calling it anyway makes\n    # the logic valid for both Series and DataFrame ops.\n    xrav = x.ravel()\n    assert isinstance(x, np.ndarray), type(x)\n    if isinstance(y, np.ndarray):\n        dtype = find_common_type([x.dtype, y.dtype])\n        result = np.empty(x.size, dtype=dtype)\n\n        if len(x) != len(y):\n            raise ValueError(x.shape, y.shape)\n        ymask = notna(y)\n\n        # NB: ravel() is only safe since y is ndarray; for e.g. PeriodIndex\n        #  we would get int64 dtype, see GH#19956\n        yrav = y.ravel()\n        mask = notna(xrav) & ymask.ravel()\n\n        # See GH#5284, GH#5035, GH#19448 for historical reference\n        if mask.any():\n            result[mask] = op(xrav[mask], yrav[mask])\n\n    else:\n        if not is_scalar(y):\n            raise TypeError(\n                f\"Cannot broadcast np.ndarray with operand of type { type(y) }\"\n            )\n\n        # mask is only meaningful for x\n        result = np.empty(x.size, dtype=x.dtype)\n        mask = notna(xrav)\n\n        # 1 ** np.nan is 1. So we have to unmask those.\n        if op is pow:\n            mask = np.where(x == 1, False, mask)\n        elif op is roperator.rpow:\n            mask = np.where(y == 1, False, mask)\n\n        if mask.any():\n            result[mask] = op(xrav[mask], y)\n\n    np.putmask(result, ~mask, np.nan)\n    result = result.reshape(x.shape)  # 2D compat\n    return result\n\n\ndef _na_arithmetic_op(left: np.ndarray, right, op, is_cmp: bool = False):\n    \"\"\"\n    Return the result of evaluating op on the passed in values.\n\n    If native types are not compatible, try coercion to object dtype.\n\n    Parameters\n    ----------\n    left : np.ndarray\n    right : np.ndarray or scalar\n        Excludes DataFrame, Series, Index, ExtensionArray.\n    is_cmp : bool, default False\n        If this a comparison operation.\n\n    Returns\n    -------\n    array-like\n\n    Raises\n    ------\n    TypeError : invalid operation\n    \"\"\"\n    if isinstance(right, str):\n        # can never use numexpr\n        func = op\n    else:\n        func = partial(expressions.evaluate, op)\n\n    try:\n        result = func(left, right)\n    except TypeError:\n        if not is_cmp and (is_object_dtype(left.dtype) or is_object_dtype(right)):\n            # For object dtype, fallback to a masked operation (only operating\n            #  on the non-missing values)\n            # Don't do this for comparisons, as that will handle complex numbers\n            #  incorrectly, see GH#32047\n            result = _masked_arith_op(left, right, op)\n        else:\n            raise\n\n    if is_cmp and (is_scalar(result) or result is NotImplemented):\n        # numpy returned a scalar instead of operating element-wise\n        # e.g. numeric array vs str\n        # TODO: can remove this after dropping some future numpy version?\n        return invalid_comparison(left, right, op)\n\n    return missing.dispatch_fill_zeros(op, left, right, result)\n\n\ndef arithmetic_op(left: ArrayLike, right: Any, op):\n    \"\"\"\n    Evaluate an arithmetic operation `+`, `-`, `*`, `/`, `//`, `%`, `**`, ...\n\n    Note: the caller is responsible for ensuring that numpy warnings are\n    suppressed (with np.errstate(all=\"ignore\")) if needed.\n\n    Parameters\n    ----------\n    left : np.ndarray or ExtensionArray\n    right : object\n        Cannot be a DataFrame or Index.  Series is *not* excluded.\n    op : {operator.add, operator.sub, ...}\n        Or one of the reversed variants from roperator.\n\n    Returns\n    -------\n    ndarray or ExtensionArray\n        Or a 2-tuple of these in the case of divmod or rdivmod.\n    \"\"\"\n    # NB: We assume that extract_array and ensure_wrapped_if_datetimelike\n    #  have already been called on `left` and `right`,\n    #  and `maybe_prepare_scalar_for_op` has already been called on `right`\n    # We need to special-case datetime64/timedelta64 dtypes (e.g. because numpy\n    # casts integer dtypes to timedelta64 when operating with timedelta64 - GH#22390)\n\n    if (\n        should_extension_dispatch(left, right)\n        or isinstance(right, (Timedelta, BaseOffset, Timestamp))\n        or right is NaT\n    ):\n        # Timedelta/Timestamp and other custom scalars are included in the check\n        # because numexpr will fail on it, see GH#31457\n        res_values = op(left, right)\n    else:\n        # TODO we should handle EAs consistently and move this check before the if/else\n        # (https://github.com/pandas-dev/pandas/issues/41165)\n        _bool_arith_check(op, left, right)\n\n        # error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\n        # \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\n        res_values = _na_arithmetic_op(left, right, op)  # type: ignore[arg-type]\n\n    return res_values\n\n", "right_context": "\n\ndef na_logical_op(x: np.ndarray, y, op):\n    try:\n        # For exposition, write:\n        #  yarr = isinstance(y, np.ndarray)\n        #  yint = is_integer(y) or (yarr and y.dtype.kind == \"i\")\n        #  ybool = is_bool(y) or (yarr and y.dtype.kind == \"b\")\n        #  xint = x.dtype.kind == \"i\"\n        #  xbool = x.dtype.kind == \"b\"\n        # Then Cases where this goes through without raising include:\n        #  (xint or xbool) and (yint or bool)\n        result = op(x, y)\n    except TypeError:\n        if isinstance(y, np.ndarray):\n            # bool-bool dtype operations should be OK, should not get here\n            assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n            x = ensure_object(x)\n            y = ensure_object(y)\n            result = libops.vec_binop(x.ravel(), y.ravel(), op)\n        else:\n            # let null fall thru\n            assert lib.is_scalar(y)\n            if not isna(y):\n                y = bool(y)\n            try:\n                result = libops.scalar_binop(x, y, op)\n            except (\n                TypeError,\n                ValueError,\n                AttributeError,\n                OverflowError,\n                NotImplementedError,\n            ) as err:\n                typ = type(y).__name__\n                raise TypeError(\n                    f\"Cannot perform '{op.__name__}' with a dtyped [{x.dtype}] array \"\n                    f\"and scalar of type [{typ}]\"\n                ) from err\n\n    return result.reshape(x.shape)\n\n\ndef logical_op(left: ArrayLike, right: Any, op) -> ArrayLike:\n    \"\"\"\n    Evaluate a logical operation `|`, `&`, or `^`.\n\n    Parameters\n    ----------\n    left : np.ndarray or ExtensionArray\n    right : object\n        Cannot be a DataFrame, Series, or Index.\n    op : {operator.and_, operator.or_, operator.xor}\n        Or one of the reversed variants from roperator.\n\n    Returns\n    -------\n    ndarray or ExtensionArray\n    \"\"\"\n    fill_int = lambda x: x\n\n    def fill_bool(x, left=None):\n        # if `left` is specifically not-boolean, we do not cast to bool\n        if x.dtype.kind in [\"c\", \"f\", \"O\"]:\n            # dtypes that can hold NA\n            mask = isna(x)\n            if mask.any():\n                x = x.astype(object)\n                x[mask] = False\n\n        if left is None or is_bool_dtype(left.dtype):\n            x = x.astype(bool)\n        return x\n\n    is_self_int_dtype = is_integer_dtype(left.dtype)\n\n    right = lib.item_from_zerodim(right)\n    if is_list_like(right) and not hasattr(right, \"dtype\"):\n        # e.g. list, tuple\n        right = construct_1d_object_array_from_listlike(right)\n\n    # NB: We assume extract_array has already been called on left and right\n    lvalues = ensure_wrapped_if_datetimelike(left)\n    rvalues = right\n\n    if should_extension_dispatch(lvalues, rvalues):\n        # Call the method on lvalues\n        res_values = op(lvalues, rvalues)\n\n    else:\n        if isinstance(rvalues, np.ndarray):\n            is_other_int_dtype = is_integer_dtype(rvalues.dtype)\n            rvalues = rvalues if is_other_int_dtype else fill_bool(rvalues, lvalues)\n\n        else:\n            # i.e. scalar\n            is_other_int_dtype = lib.is_integer(rvalues)\n\n        # For int vs int `^`, `|`, `&` are bitwise operators and return\n        #   integer dtypes.  Otherwise these are boolean ops\n        filler = fill_int if is_self_int_dtype and is_other_int_dtype else fill_bool\n\n        res_values = na_logical_op(lvalues, rvalues, op)\n        # error: Cannot call function of unknown type\n        res_values = filler(res_values)  # type: ignore[operator]\n\n    return res_values\n\n\ndef get_array_op(op):\n    \"\"\"\n    Return a binary array operation corresponding to the given operator op.\n\n    Parameters\n    ----------\n    op : function\n        Binary operator from operator or roperator module.\n\n    Returns\n    -------\n    functools.partial\n    \"\"\"\n    if isinstance(op, partial):\n        # We get here via dispatch_to_series in DataFrame case\n        # e.g. test_rolling_consistency_var_debiasing_factors\n        return op\n\n    op_name = op.__name__.strip(\"_\").lstrip(\"r\")\n    if op_name == \"arith_op\":\n        # Reached via DataFrame._combine_frame i.e. flex methods\n        # e.g. test_df_add_flex_filled_mixed_dtypes\n        return op\n\n    if op_name in {\"eq\", \"ne\", \"lt\", \"le\", \"gt\", \"ge\"}:\n        return partial(comparison_op, op=op)\n    elif op_name in {\"and\", \"or\", \"xor\", \"rand\", \"ror\", \"rxor\"}:\n        return partial(logical_op, op=op)\n    elif op_name in {\n        \"add\",\n        \"sub\",\n        \"mul\",\n        \"truediv\",\n        \"floordiv\",\n        \"mod\",\n        \"divmod\",\n        \"pow\",\n    }:\n        return partial(arithmetic_op, op=op)\n    else:\n        raise NotImplementedError(op_name)\n\n\ndef maybe_prepare_scalar_for_op(obj, shape: Shape):\n    \"\"\"\n    Cast non-pandas objects to pandas types to unify behavior of arithmetic\n    and comparison operations.\n\n    Parameters\n    ----------\n    obj: object\n    shape : tuple[int]\n\n    Returns\n    -------\n    out : object\n\n    Notes\n    -----\n    Be careful to call this *after* determining the `name` attribute to be\n    attached to the result of the arithmetic operation.\n    \"\"\"\n    if type(obj) is datetime.timedelta:\n        # GH#22390  cast up to Timedelta to rely on Timedelta\n        # implementation; otherwise operation against numeric-dtype\n        # raises TypeError\n        return Timedelta(obj)\n    elif type(obj) is datetime.datetime:\n        # cast up to Timestamp to rely on Timestamp implementation, see Timedelta above\n        return Timestamp(obj)\n    elif isinstance(obj, np.datetime64):\n        # GH#28080 numpy casts integer-dtype to datetime64 when doing\n        #  array[int] + datetime64, which we do not allow\n        if isna(obj):\n            from pandas.core.arrays import DatetimeArray\n\n            # Avoid possible ambiguities with pd.NaT\n            obj = obj.astype(\"datetime64[ns]\")\n            right = np.broadcast_to(obj, shape)\n            return DatetimeArray(right)\n\n        return Timestamp(obj)\n\n    elif isinstance(obj, np.timedelta64):\n        if isna(obj):\n            from pandas.core.arrays import TimedeltaArray\n\n            # wrapping timedelta64(\"NaT\") in Timedelta returns NaT,\n            #  which would incorrectly be treated as a datetime-NaT, so\n            #  we broadcast and wrap in a TimedeltaArray\n            obj = obj.astype(\"timedelta64[ns]\")\n            right = np.broadcast_to(obj, shape)\n            return TimedeltaArray(right)\n\n        # In particular non-nanosecond timedelta64 needs to be cast to\n        #  nanoseconds, or else we get undesired behavior like\n        #  np.timedelta64(3, 'D') / 2 == np.timedelta64(1, 'D')\n        return Timedelta(obj)\n\n    return obj\n\n\n_BOOL_OP_NOT_ALLOWED = {\n    operator.truediv,\n    roperator.rtruediv,\n    operator.floordiv,\n    roperator.rfloordiv,\n    operator.pow,\n    roperator.rpow,\n}\n\n\ndef _bool_arith_check(op, a, b):\n    \"\"\"\n    In contrast to numpy, pandas raises an error for certain operations\n    with booleans.\n    \"\"\"\n    if op in _BOOL_OP_NOT_ALLOWED:\n        if is_bool_dtype(a.dtype) and (\n            is_bool_dtype(b) or isinstance(b, (bool, np.bool_))\n        ):\n            op_name = op.__name__.strip(\"_\").lstrip(\"r\")\n            raise NotImplementedError(\n                f\"operator '{op_name}' not implemented for bool dtypes\"\n            )\n", "import_text": ["datetime", "functools.partial", "operator", "typing.Any", "numpy", "pandas._libs.NaT", "pandas._libs.Timedelta", "pandas._libs.Timestamp", "pandas._libs.lib", "pandas._libs.ops", "pandas._libs.tslibs.BaseOffset", "pandas._typing.ArrayLike", "pandas._typing.Shape", "pandas.core.dtypes.cast.construct_1d_object_array_from_listlike", "pandas.core.dtypes.cast.find_common_type", "pandas.core.dtypes.common.ensure_object", "pandas.core.dtypes.common.is_bool_dtype", "pandas.core.dtypes.common.is_integer_dtype", "pandas.core.dtypes.common.is_list_like", "pandas.core.dtypes.common.is_numeric_v_string_like", "pandas.core.dtypes.common.is_object_dtype", "pandas.core.dtypes.common.is_scalar", "pandas.core.dtypes.generic.ABCExtensionArray", "pandas.core.dtypes.generic.ABCIndex", "pandas.core.dtypes.generic.ABCSeries", "pandas.core.dtypes.missing.isna", "pandas.core.dtypes.missing.notna", "pandas.core.computation.expressions", "pandas.core.construction.ensure_wrapped_if_datetimelike", "pandas.core.ops.missing", "pandas.core.ops.roperator", "pandas.core.ops.dispatch.should_extension_dispatch", "pandas.core.ops.invalid.invalid_comparison"], "prompt": "\"\"\"\nDescription: This function compares two arrays using a specified operation and returns the result.\n\nArgs:\n    left (ArrayLike): The left operand for the comparison.\n    right (Any): The right operand for the comparison.\n    op (function): The operation to be used for comparison.\n\nReturns:\n    ArrayLike: The result of the comparison operation.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Evaluate a comparison operation `=`, `!=`, `>=`, `>`, `<=`, or `<`.\n\n    Note: the caller is responsible for ensuring that numpy warnings are\n    suppressed (with np.errstate(all=\"ignore\")) if needed.\n\n    Parameters\n    ----------\n    left : np.ndarray or ExtensionArray\n    right : object\n        Cannot be a DataFrame, Series, or Index.\n    op : {operator.eq, operator.ne, operator.gt, operator.ge, operator.lt, operator.le}\n\n    Returns\n    -------\n    ndarray or ExtensionArray\n    \"\"\"", "function_dependencies": ["pandas.core.construction.ensure_wrapped_if_datetimelike", "pandas._libs.lib.item_from_zerodim", "numpy.asarray", "pandas.core.ops.dispatch.should_extension_dispatch", "pandas.core.dtypes.common.is_object_dtype", "pandas.core.dtypes.common.is_scalar", "pandas.core.dtypes.missing.isna", "numpy.ones", "numpy.zeros", "pandas.core.dtypes.common.is_numeric_v_string_like", "pandas.core.ops.invalid.invalid_comparison"], "project_create_time": "2023-09-19T06:19:22+00:00", "project_update_time": "2024-04-17T22:13:20+00:00", "file_create_time": "2023-12-05T18:34:56Z", "file_update_time": "2023-12-25T22:47:24Z", "function_update_time": "2023-12-05T18:34:56Z", "license": {"key": "gpl-2.0", "name": "GNU General Public License v2.0", "spdx_id": "GPL-2.0", "url": "https://api.github.com/licenses/gpl-2.0", "node_id": "MDc6TGljZW5zZTg="}, "reference_api": ["pandas.core.dtypes.common.is_scalar"], "test_function": [{"file_path": "/The-Grand-Complete-Data-Science-Materials-main/The-Grand-Complete-Data-Science-Materials-main/ML Projects/NLP_WebAPP_Twitter_Sentiment_Analysis_knowledge_graph/venv/Lib/site-packages/pandas/tests/arithmetic/test_array_ops.py", "class_name": null, "function_name": "test_object_comparison_2d", "code": "\ndef test_object_comparison_2d():\n    left = np.arange(9).reshape(3, 3).astype(object)\n    right = left.T\n\n    result = comparison_op(left, right, operator.eq)\n    expected = np.eye(3).astype(bool)\n    tm.assert_numpy_array_equal(result, expected)\n\n    # Ensure that cython doesn't raise on non-writeable arg, which\n    #  we can get from np.broadcast_to\n    right.flags.writeable = False\n    result = comparison_op(left, right, operator.ne)\n    tm.assert_numpy_array_equal(result, ~expected)"}]}, {"git_group": "Lightning-AI", "git_name": "torchmetrics", "version": "v1.3.2", "language": "Python", "project_name": "torchmetrics-v1.3.2.zip", "file_path": "/torchmetrics-v1.3.2/torchmetrics-1.3.2/src/torchmetrics/wrappers/bootstrapping.py", "file_name": "bootstrapping.py", "focal_class": null, "focal_name": "_bootstrap_sampler", "focal_parameter": [], "solution": "def _bootstrap_sampler(\n    size: int,\n    sampling_strategy: str = \"poisson\",\n) -> Tensor:\n    if sampling_strategy == \"poisson\":\n        p = torch.distributions.Poisson(1)\n        n = p.sample((size,))\n        return torch.arange(size).repeat_interleave(n.long(), dim=0)\n    if sampling_strategy == \"multinomial\":\n        return torch.multinomial(torch.ones(size), num_samples=size, replacement=True)\n    raise ValueError(\"Unknown sampling strategy\")", "function_signature": "def _bootstrap_sampler(\n    size: int,\n    sampling_strategy: str = \"poisson\",\n) -> Tensor :", "left_context": "# Copyright The Lightning team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom copy import deepcopy\nfrom typing import Any, Dict, Optional, Sequence, Union\n\nimport torch\nfrom lightning_utilities import apply_to_collection\nfrom torch import Tensor\nfrom torch.nn import ModuleList\n\nfrom torchmetrics.metric import Metric\nfrom torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE\nfrom torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE\nfrom torchmetrics.wrappers.abstract import WrapperMetric\n\nif not _MATPLOTLIB_AVAILABLE:\n    __doctest_skip__ = [\"BootStrapper.plot\"]\n\n", "right_context": "\n\nclass BootStrapper(WrapperMetric):\n    r\"\"\"Using `Turn a Metric into a Bootstrapped`_.\n\n    That can automate the process of getting confidence intervals for metric values. This wrapper\n    class basically keeps multiple copies of the same base metric in memory and whenever ``update`` or\n    ``forward`` is called, all input tensors are resampled (with replacement) along the first dimension.\n\n    Args:\n        base_metric: base metric class to wrap\n        num_bootstraps: number of copies to make of the base metric for bootstrapping\n        mean: if ``True`` return the mean of the bootstraps\n        std: if ``True`` return the standard deviation of the bootstraps\n        quantile: if given, returns the quantile of the bootstraps. Can only be used with pytorch version 1.6 or higher\n        raw: if ``True``, return all bootstrapped values\n        sampling_strategy:\n            Determines how to produce bootstrapped samplings. Either ``'poisson'`` or ``multinomial``.\n            If ``'possion'`` is chosen, the number of times each sample will be included in the bootstrap\n            will be given by :math:`n\\sim Poisson(\\lambda=1)`, which approximates the true bootstrap distribution\n            when the number of samples is large. If ``'multinomial'`` is chosen, we will apply true bootstrapping\n            at the batch level to approximate bootstrapping over the hole dataset.\n        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.\n\n    Example::\n        >>> from pprint import pprint\n        >>> from torchmetrics.wrappers import BootStrapper\n        >>> from torchmetrics.classification import MulticlassAccuracy\n        >>> _ = torch.manual_seed(123)\n        >>> base_metric = MulticlassAccuracy(num_classes=5, average='micro')\n        >>> bootstrap = BootStrapper(base_metric, num_bootstraps=20)\n        >>> bootstrap.update(torch.randint(5, (20,)), torch.randint(5, (20,)))\n        >>> output = bootstrap.compute()\n        >>> pprint(output)\n        {'mean': tensor(0.2205), 'std': tensor(0.0859)}\n\n    \"\"\"\n\n    full_state_update: Optional[bool] = True\n\n    def __init__(\n        self,\n        base_metric: Metric,\n        num_bootstraps: int = 10,\n        mean: bool = True,\n        std: bool = True,\n        quantile: Optional[Union[float, Tensor]] = None,\n        raw: bool = False,\n        sampling_strategy: str = \"poisson\",\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(**kwargs)\n        if not isinstance(base_metric, Metric):\n            raise ValueError(\n                f\"Expected base metric to be an instance of torchmetrics.Metric but received {base_metric}\"\n            )\n\n        self.metrics = ModuleList([deepcopy(base_metric) for _ in range(num_bootstraps)])\n        self.num_bootstraps = num_bootstraps\n\n        self.mean = mean\n        self.std = std\n        self.quantile = quantile\n        self.raw = raw\n\n        allowed_sampling = (\"poisson\", \"multinomial\")\n        if sampling_strategy not in allowed_sampling:\n            raise ValueError(\n                f\"Expected argument ``sampling_strategy`` to be one of {allowed_sampling}\"\n                f\" but received {sampling_strategy}\"\n            )\n        self.sampling_strategy = sampling_strategy\n\n    def update(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Update the state of the base metric.\n\n        Any tensor passed in will be bootstrapped along dimension 0.\n\n        \"\"\"\n        args_sizes = apply_to_collection(args, Tensor, len)\n        kwargs_sizes = list(apply_to_collection(kwargs, Tensor, len))\n        if len(args_sizes) > 0:\n            size = args_sizes[0]\n        elif len(kwargs_sizes) > 0:\n            size = kwargs_sizes[0]\n        else:\n            raise ValueError(\"None of the input contained tensors, so could not determine the sampling size\")\n\n        for idx in range(self.num_bootstraps):\n            sample_idx = _bootstrap_sampler(size, sampling_strategy=self.sampling_strategy).to(self.device)\n            if sample_idx.numel() == 0:\n                continue\n            new_args = apply_to_collection(args, Tensor, torch.index_select, dim=0, index=sample_idx)\n            new_kwargs = apply_to_collection(kwargs, Tensor, torch.index_select, dim=0, index=sample_idx)\n            self.metrics[idx].update(*new_args, **new_kwargs)\n\n    def compute(self) -> Dict[str, Tensor]:\n        \"\"\"Compute the bootstrapped metric values.\n\n        Always returns a dict of tensors, which can contain the following keys: ``mean``, ``std``, ``quantile`` and\n        ``raw`` depending on how the class was initialized.\n\n        \"\"\"\n        computed_vals = torch.stack([m.compute() for m in self.metrics], dim=0)\n        output_dict = {}\n        if self.mean:\n            output_dict[\"mean\"] = computed_vals.mean(dim=0)\n        if self.std:\n            output_dict[\"std\"] = computed_vals.std(dim=0)\n        if self.quantile is not None:\n            output_dict[\"quantile\"] = torch.quantile(computed_vals, self.quantile)\n        if self.raw:\n            output_dict[\"raw\"] = computed_vals\n        return output_dict\n\n    def forward(self, *args: Any, **kwargs: Any) -> Any:\n        \"\"\"Use the original forward method of the base metric class.\"\"\"\n        return super(WrapperMetric, self).forward(*args, **kwargs)\n\n    def plot(\n        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None\n    ) -> _PLOT_OUT_TYPE:\n        \"\"\"Plot a single or multiple values from the metric.\n\n        Args:\n            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.\n                If no value is provided, will automatically call `metric.compute` and plot that result.\n            ax: An matplotlib axis object. If provided will add plot to that axis\n\n        Returns:\n            Figure and Axes object\n\n        Raises:\n            ModuleNotFoundError:\n                If `matplotlib` is not installed\n\n        .. plot::\n            :scale: 75\n\n            >>> # Example plotting a single value\n            >>> import torch\n            >>> from torchmetrics.wrappers import BootStrapper\n            >>> from torchmetrics.regression import MeanSquaredError\n            >>> metric = BootStrapper(MeanSquaredError(), num_bootstraps=20)\n            >>> metric.update(torch.randn(100,), torch.randn(100,))\n            >>> fig_, ax_ = metric.plot()\n\n        .. plot::\n            :scale: 75\n\n            >>> # Example plotting multiple values\n            >>> import torch\n            >>> from torchmetrics.wrappers import BootStrapper\n            >>> from torchmetrics.regression import MeanSquaredError\n            >>> metric = BootStrapper(MeanSquaredError(), num_bootstraps=20)\n            >>> values = [ ]\n            >>> for _ in range(3):\n            ...     values.append(metric(torch.randn(100,), torch.randn(100,)))\n            >>> fig_, ax_ = metric.plot(values)\n\n        \"\"\"\n        return self._plot(val, ax)\n", "import_text": ["copy.deepcopy", "typing.Any", "typing.Dict", "typing.Optional", "typing.Sequence", "typing.Union", "torch", "lightning_utilities.apply_to_collection", "torch.Tensor", "torch.nn.ModuleList", "torchmetrics.metric.Metric", "torchmetrics.utilities.imports._MATPLOTLIB_AVAILABLE", "torchmetrics.utilities.plot._AX_TYPE", "torchmetrics.utilities.plot._PLOT_OUT_TYPE", "torchmetrics.wrappers.abstract.WrapperMetric"], "prompt": "\"\"\"\nDescription: This function generates a bootstrap sample based on the specified sampling strategy.\n\nArgs:\n    size (int): The size of the bootstrap sample to be generated.\n    sampling_strategy (str, optional): The sampling strategy to be used. Defaults to \"poisson\".\n        \"poisson\": Samples are drawn from a Poisson distribution.\n        \"multinomial\": Samples are drawn from a multinomial distribution.\n\nReturns:\n    Tensor: A bootstrap sample tensor of the specified size.\n\nRaises:\n    ValueError: If the sampling strategy is not recognized.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Resample a tensor along its first dimension with replacement.\n\n    Args:\n        size: number of samples\n        sampling_strategy: the strategy to use for sampling, either ``'poisson'`` or ``'multinomial'``\n\n    Returns:\n        resampled tensor\n\n    \"\"\"", "function_dependencies": ["torch.distributions.Poisson", "torch.distributions.Poisson.sample", "torch.arange", "torch.arange.repeat_interleave", "torch.distributions.Poisson.sample.long", "torch.multinomial", "torch.ones"], "project_create_time": "2020-12-22T20:02:42+00:00", "project_update_time": "2024-04-17T09:35:27+00:00", "file_create_time": "2022-06-01T14:41:22Z", "file_update_time": "2024-02-15T18:21:55Z", "function_update_time": "2022-06-01T14:41:22Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["torch.arange", "torch.ones"], "test_function": [{"file_path": "/torchmetrics-v1.3.2/torchmetrics-1.3.2/tests/unittests/wrappers/test_bootstrapping.py", "class_name": null, "function_name": "test_bootstrap_sampler", "code": "def test_bootstrap_sampler(sampling_strategy):\n    old_samples = torch.randn(20, 2)\n\n    # make sure that the new samples are only made up of old samples\n    idx = _bootstrap_sampler(20, sampling_strategy=sampling_strategy)\n    new_samples = old_samples[idx]\n    for ns in new_samples:\n        assert ns in old_samples\n\n    found_one = _sample_checker(old_samples, new_samples, operator.eq, 2)\n    assert found_one, \"resampling did not work because no samples were sampled twice\"\n\n    found_zero = _sample_checker(old_samples, new_samples, operator.ne, 0)\n    assert found_zero, \"resampling did not work because all samples were at least sampled once\""}]}, {"git_group": "linkedin", "git_name": "greykite", "version": "v1.0.0", "language": "Python", "project_name": "greykite-v1.0.0.zip", "file_path": "/greykite-v1.0.0/greykite-1.0.0/greykite/algo/changepoint/adalasso/auto_changepoint_params.py", "file_name": "auto_changepoint_params.py", "focal_class": null, "focal_name": "get_potential_changepoint_n", "focal_parameter": [], "solution": "def get_potential_changepoint_n(\n        n_points: int,\n        total_increment: timedelta,\n        resample_freq: str,\n        yearly_seasonality_order: int,\n        cap: int = 100) -> int:\n    try:\n        # The ``resample_freq`` is one of \"D\", \"3D\" and \"7D\".\n        n_points_after_agg = np.floor(total_increment / to_offset(resample_freq).delta)\n    except AttributeError:\n        # The ``resample_freq`` is None or other freq that is at least \"W\".\n        n_points_after_agg = n_points\n    # Sets number of potential changepoints to be at most\n    # aggregated data length - # seasonality features - 1 (intercept term) for estimability.\n    # Here we ignore dropping potential changepoints from the end.\n    # If we use the function above to infer aggregation frequency,\n    # there will be enough potential changepoints.\n    n_changepoints = max(0, n_points_after_agg - 2 * yearly_seasonality_order - 1)\n    # Caps the number of potential changepoints for speed purpose.\n    n_changepoints = min(n_changepoints, cap)\n    return n_changepoints", "function_signature": "def get_potential_changepoint_n(\n        n_points: int,\n        total_increment: timedelta,\n        resample_freq: str,\n        yearly_seasonality_order: int,\n        cap: int = 100) -> int :", "left_context": "# BSD 2-CLAUSE LICENSE\n\n# Redistribution and use in source and binary forms, with or without modification,\n# are permitted provided that the following conditions are met:\n\n# Redistributions of source code must retain the above copyright notice, this\n# list of conditions and the following disclaimer.\n# Redistributions in binary form must reproduce the above copyright notice,\n# this list of conditions and the following disclaimer in the documentation\n# and/or other materials provided with the distribution.\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n# #ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# original author: Kaixu Yang\n\"\"\"Automatically populates changepoint detection parameters from input data.\"\"\"\n\nfrom datetime import timedelta\nfrom typing import Optional\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.tseries.frequencies import to_offset\n\nfrom greykite.algo.common.seasonality_inferrer import SeasonalityInferConfig\nfrom greykite.algo.common.seasonality_inferrer import SeasonalityInferrer\nfrom greykite.algo.common.seasonality_inferrer import TrendAdjustMethodEnum\nfrom greykite.common.constants import TIME_COL\nfrom greykite.common.constants import VALUE_COL\n\n\ndef get_changepoint_resample_freq(\n        n_points: int,\n        min_increment: timedelta,\n        min_num_points_after_agg: int = 100) -> Optional[str]:\n    \"\"\"Gets the changepoint detection resample frequency parameter.\n\n    Parameters\n    ----------\n    n_points : `int`\n        The number of data points in the time series.\n    min_increment : `datetime.timedelta`\n        The minimum increment between time series points.\n    min_num_points_after_agg : `int`, default 100\n        The minimum number of observations required after aggregation.\n\n    Returns\n    -------\n    resample_freq : `str` or None\n        The resample frequency.\n        Will be one of \"D\", \"3D\" and \"7D\".\n        If None, resample will be skipped.\n    \"\"\"\n    # When ``min_increment`` is at least 7 days,\n    # the data is considered at least weekly data.\n    # In this case, we don't do aggregation.\n    if min_increment >= timedelta(days=7):\n        # Returning None for this case.\n        return None\n\n    # From now on the data is sub-weekly.\n    # The candidates of ``resample_freq`` are \"7D\", \"3D\" and \"D\".\n    # We use the longest aggregation frequency that has at least\n    # ``min_num_points_after_agg`` points after aggregation.\n    # Currently, we do not support sub-daily aggregation frequency,\n    # because it is not a common case.\n    # We also do not recommend based on detection result for speed purpose.\n    data_length = n_points * min_increment\n    if data_length >= timedelta(days=7 * min_num_points_after_agg):\n        resample_freq = \"7D\"\n    elif data_length >= timedelta(days=3 * min_num_points_after_agg):\n        resample_freq = \"3D\"\n    else:\n        resample_freq = \"D\"\n\n    return resample_freq\n\n\ndef get_yearly_seasonality_order(\n        df: pd.DataFrame,\n        time_col: str,\n        value_col: str,\n        resample_freq: str) -> int:\n    \"\"\"Infers the yearly seasonality order for changepoint detection.\n\n    Parameters\n    ----------\n    df : `pandas.DataFrame`\n        The input timeseris.\n    time_col : `str`\n        The column name for timestamps in ``df``.\n    value_col : `str`\n        The column name for values in ``df``.\n    resample_freq : `str`\n        The aggregation frequency string in changepoint detection configuration.\n\n    Returns\n    -------\n    yearly_seasonality_order : `int`\n        The inferred yearly seasonality with the best BIC score.\n    \"\"\"\n    model = SeasonalityInferrer()\n    result = model.infer_fourier_series_order(\n        df=df,\n        time_col=time_col,\n        value_col=value_col,\n        configs=[\n            SeasonalityInferConfig(\n                seas_name=\"yearly\",\n                col_name=\"toy\",\n                period=1.0,\n                max_order=30,\n                adjust_trend_method=TrendAdjustMethodEnum.seasonal_average.name,\n                adjust_trend_param=dict(trend_average_col=\"year\"),\n                fit_algorithm=\"ridge\",\n                offset=0,\n                tolerance=0.0,\n                aggregation_period=resample_freq,\n                criterion=\"bic\",\n                plotting=False\n            )\n        ]\n    )\n    return result[\"best_orders\"][\"yearly\"]\n\n", "right_context": "\n\ndef get_no_changepoint_distance_from_end(\n        min_increment: timedelta,\n        forecast_horizon: int,\n        min_num_points_after_last_changepoint: int = 1) -> str:\n    \"\"\"Gets the distance from end of time series where no changepoints will be placed.\n\n    Parameters\n    ----------\n    min_increment : `datetime.timedelta`\n        The minimum increment between time series points.\n    forecast_horizon : `int`\n        The forecast horizon.\n    min_num_points_after_last_changepoint : `int`, default 1\n        The minimum number of data points after the last potential changepoint.\n\n    Returns\n    -------\n    no_changepoint_distance_from_end : `str`\n        A string indicating the period from the end of the time series where no\n        potential changepoints will be placed.\n    \"\"\"\n    min_increment_in_days = min_increment.days\n    min_forecast_horizon_in_days = (min_increment * forecast_horizon).days\n    if min_forecast_horizon_in_days < 1:\n        min_forecast_horizon_in_days = 1\n    # Minimum distance must be at least ``min_num_points_after_last_changepoint``.\n    min_distance_days = min_increment_in_days * min_num_points_after_last_changepoint\n    # We add extra constraints according to the data frequency,\n    # and some special constraints for the commonly seen data frequencies.\n    if min_increment_in_days >= 365:\n        # At least yearly data, distance is at least 2 * forecast horizon.\n        min_distance_days = max(min_distance_days, min_forecast_horizon_in_days * 2)\n        if min_increment_in_days == 365:\n            # Exactly yearly data, between 3 years and 20 years.\n            min_distance_days = max(min_distance_days, 3 * 365)\n            min_distance_days = min(min_distance_days, 20 * 366)\n    elif min_increment_in_days >= 28:\n        # Between monthly and yearly data, distance is between 3 * forecast horizon and 6 years.\n        min_distance_days = max(min_distance_days, min_forecast_horizon_in_days * 3)\n        min_distance_days = min(min_distance_days, 6 * 366)\n        if min_increment_in_days == 90:\n            # Exactly quarterly data, at least 4 quarters.\n            min_distance_days = max(min_distance_days, 4 * 90)\n        elif min_increment_in_days == 28:\n            # Exactly monthly data, at least 3 months.\n            min_distance_days = max(min_distance_days, 3 * 28)\n    else:\n        # Sub-monthly data, distance is at least 4 * forecast horizon.\n        min_distance_days = max(min_distance_days, min_forecast_horizon_in_days * 4)\n        if min_increment_in_days == 7:\n            # Weekly data, between 8 weeks and 104 weeks (2 years).\n            min_distance_days = max(min_distance_days, 8 * 7)\n            min_distance_days = min(min_distance_days, 104 * 7)\n        elif min_increment_in_days == 1:\n            # Daily data, between 30 days and 1 year.\n            min_distance_days = max(min_distance_days, 30)\n            min_distance_days = min(min_distance_days, 365)\n        elif min_increment_in_days < 1:\n            # Sub-daily data, between 14 days and 1 year.\n            min_distance_days = max(min_distance_days, 14)\n            min_distance_days = min(min_distance_days, 365)\n\n    no_changepoint_distance_from_end = f\"{min_distance_days}D\"\n    return no_changepoint_distance_from_end\n\n\ndef get_actual_changepoint_min_distance(\n        min_increment: timedelta) -> str:\n    \"\"\"Gets the minimum distance between detected changepoints.\n\n    Parameters\n    ----------\n    min_increment : `datetime.timedelta`\n        The minimum increment between time series points.\n\n    Returns\n    -------\n    actual_changepoint_min_distance : `str`\n        The minimum distance between detected changepoints.\n    \"\"\"\n    min_distance = min_increment.days * 2  # At most every two data points.\n    if min_increment < timedelta(days=1):\n        min_distance = max(min_distance, 14)  # At least 14 days for sub-daily data.\n    elif min_increment <= timedelta(days=7):\n        min_distance = max(min_distance, 30)  # At least 30 days for daily to weekly data.\n\n    actual_changepoint_min_distance = f\"{min_distance}D\"\n    return actual_changepoint_min_distance\n\n\ndef get_regularization_strength() -> float:\n    \"\"\"Gets the regularization strength.\n\n    The regularization strength typically won't affect the result too much,\n    if set in a reasonable range (0.4-0.7).\n    Here we explicitly set it to 0.6,\n    which works well in most cases.\n    Setting it to None will trigger cross-validation,\n    but has risk to select too many changepoints.\n    \"\"\"\n    return 0.6\n\n\ndef generate_trend_changepoint_detection_params(\n        df: pd.DataFrame,\n        forecast_horizon: int,\n        time_col: str = TIME_COL,\n        value_col: str = VALUE_COL,\n        yearly_seasonality_order: Optional[int] = None) -> Optional[dict]:\n    \"\"\"Automatically generates trend changepoint detection parameters\n    based on the input data and forecast horizon.\n\n    Parameters\n    ----------\n    df : `pandas.DataFrame`\n        The input time series.\n    forecast_horizon : `int`\n        The forecast horizon.\n    time_col : `str`, default TIME_COL\n        The column name for timestamps in ``df``.\n    value_col : `str`, default VALUE_COL`\n        The column name for time series values in ``df``.\n    yearly_seasonality_order : `int` or None, default None\n        The yearly seasonality Fourier order.\n        If a known good order is given, it will be used.\n        Otherwise, it will be inferred from the algorithm.\n\n    Returns\n    -------\n    params : `dict` [`str`, any]\n        The generated trend changepoint detection parameters.\n    \"\"\"\n    df = df.copy()\n    df[time_col] = pd.to_datetime(df[time_col])\n    total_increment = df[time_col].max() - df[time_col].min()\n\n    # Auto changepoints is not active if training data is too short.\n    # In such cases, autoregression should be used.\n    if total_increment < timedelta(days=90):\n        return None\n\n    n_points = len(df)\n    min_increment = min((df[time_col] - df[time_col].shift(1)).dropna())\n\n    # Infers ``resample_freq``.\n    resample_freq = get_changepoint_resample_freq(\n        n_points=n_points,\n        min_increment=min_increment\n    )\n\n    # Infers ``yearly_seasonality_order``.\n    if yearly_seasonality_order is None:\n        yearly_seasonality_order = get_yearly_seasonality_order(\n            df=df,\n            time_col=time_col,\n            value_col=value_col,\n            resample_freq=resample_freq\n        )\n    elif yearly_seasonality_order < 0:\n        raise ValueError(f\"Yearly seasonality order must be a non-negative integer, \"\n                         f\"found {yearly_seasonality_order}.\")\n\n    # Infers ``potential_changepoint_n``.\n    potential_changepoint_n = get_potential_changepoint_n(\n        n_points=n_points,\n        total_increment=total_increment,\n        resample_freq=resample_freq,\n        yearly_seasonality_order=yearly_seasonality_order,\n        cap=100\n    )\n\n    # Infers ``no_changepoint_distance_from_end``.\n    no_changepoint_distance_from_end = get_no_changepoint_distance_from_end(\n        min_increment=min_increment,\n        forecast_horizon=forecast_horizon,\n        min_num_points_after_last_changepoint=4\n    )\n\n    # Infers ``actual_changepoint_min_distance``.\n    actual_changepoint_min_distance = get_actual_changepoint_min_distance(\n        min_increment=min_increment\n    )\n\n    # Infers ``regularization_strength``.\n    regularization_strength = get_regularization_strength()\n\n    return dict(\n        yearly_seasonality_order=yearly_seasonality_order,\n        resample_freq=resample_freq,\n        regularization_strength=regularization_strength,\n        actual_changepoint_min_distance=actual_changepoint_min_distance,\n        potential_changepoint_n=potential_changepoint_n,\n        no_changepoint_distance_from_end=no_changepoint_distance_from_end\n    )\n", "import_text": ["datetime.timedelta", "typing.Optional", "numpy", "pandas", "pandas.tseries.frequencies.to_offset", "greykite.algo.common.seasonality_inferrer.SeasonalityInferConfig", "greykite.algo.common.seasonality_inferrer.SeasonalityInferrer", "greykite.algo.common.seasonality_inferrer.TrendAdjustMethodEnum", "greykite.common.constants.TIME_COL", "greykite.common.constants.VALUE_COL"], "prompt": "\"\"\"\nDescription: This function calculates the potential number of changepoints in a time series data.\n\nArgs:\n    n_points (int): The total number of data points in the time series.\n    total_increment (timedelta): The total time increment of the time series.\n    resample_freq (str): The frequency to resample the time series. It can be \"D\", \"3D\" or \"7D\".\n    yearly_seasonality_order (int): The order of the yearly seasonality in the time series.\n    cap (int, optional): The maximum number of potential changepoints. Defaults to 100.\n\nReturns:\n    int: The potential number of changepoints after aggregation and considering the maximum number of seasonality features.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Gets the number of potential changepoints for changepoint detection.\n\n    Parameters\n    ----------\n    n_points : `int`\n        The number of data points in the time series.\n    total_increment : `datetime.timedelta`\n        The total time span of the input time series.\n    resample_freq : `str`\n        The resample frequency used in changepoint detection.\n    yearly_seasonality_order : `int`\n        The yearly seasonality order used in changepoint detection.\n    cap : `int`, default 100\n        The maximum number of potential changepoints.\n\n    Returns\n    -------\n    potential_changepoint_n : `int`\n        The number of potential changepoints used in changepoint detection.\n    \"\"\"", "function_dependencies": ["numpy.floor", "pandas.tseries.frequencies.to_offset"], "project_create_time": "2021-04-27T17:05:53+00:00", "project_update_time": "2024-04-14T06:46:02+00:00", "file_create_time": "2022-07-15T17:18:48Z", "file_update_time": "2022-07-15T17:18:48Z", "function_update_time": "2022-07-15T17:18:48Z", "license": {"key": "bsd-2-clause", "name": "BSD 2-Clause \"Simplified\" License", "spdx_id": "BSD-2-Clause", "url": "https://api.github.com/licenses/bsd-2-clause", "node_id": "MDc6TGljZW5zZTQ="}, "reference_api": ["numpy.floor", "pandas.tseries.frequencies.to_offset"], "test_function": [{"file_path": "/greykite-v1.0.0/greykite-1.0.0/greykite/tests/algo/changepoint/adalasso/test_auto_changepoint_params.py", "class_name": null, "function_name": "test_get_potential_changepoint_n", "code": "def test_get_potential_changepoint_n():\n    # Hourly data.\n    n_changepoints = get_potential_changepoint_n(\n        n_points=24*365,\n        total_increment=timedelta(days=365),\n        resample_freq=\"3D\",\n        yearly_seasonality_order=3,\n        cap=200\n    )\n    assert n_changepoints == 114\n\n    # Daily data.\n    n_changepoints = get_potential_changepoint_n(\n        n_points=365 * 2,\n        total_increment=timedelta(days=365 * 2),\n        resample_freq=\"7D\",\n        yearly_seasonality_order=15,\n        cap=100\n    )\n    assert n_changepoints == 73\n\n    # Monthly data.\n    n_changepoints = get_potential_changepoint_n(\n        n_points=12 * 5,\n        total_increment=timedelta(days=365 * 5 + 1),\n        resample_freq=None,\n        yearly_seasonality_order=15,\n        cap=100\n    )\n    assert n_changepoints == 29\n\n    # Tests cap.\n    n_changepoints = get_potential_changepoint_n(\n        n_points=12 * 5,\n        total_increment=timedelta(days=365 * 5 + 1),\n        resample_freq=None,\n        yearly_seasonality_order=15,\n        cap=20\n    )\n    assert n_changepoints == 20"}]}, {"git_group": "SpikeInterface", "git_name": "spikeinterface", "version": "0.100.5", "language": "Python", "project_name": "spikeinterface-0.100.5.zip", "file_path": "/spikeinterface-0.100.5/spikeinterface-0.100.5/src/spikeinterface/core/waveform_tools.py", "file_name": "waveform_tools.py", "focal_class": null, "focal_name": "extract_waveforms_to_buffers", "focal_parameter": ["recording", "spikes", "unit_ids", "nbefore", "nafter"], "solution": "def extract_waveforms_to_buffers(\n    recording,\n    spikes,\n    unit_ids,\n    nbefore,\n    nafter,\n    mode=\"memmap\",\n    return_scaled=False,\n    folder=None,\n    dtype=None,\n    sparsity_mask=None,\n    copy=False,\n    **job_kwargs,\n):\n    job_kwargs = fix_job_kwargs(job_kwargs)\n\n    if dtype is None:\n        if return_scaled:\n            dtype = recording.get_dtype()\n        else:\n            dtype = \"float32\"\n    dtype = np.dtype(dtype)\n\n    waveforms_by_units, arrays_info = allocate_waveforms_buffers(\n        recording, spikes, unit_ids, nbefore, nafter, mode=mode, folder=folder, dtype=dtype, sparsity_mask=sparsity_mask\n    )\n\n    distribute_waveforms_to_buffers(\n        recording,\n        spikes,\n        unit_ids,\n        arrays_info,\n        nbefore,\n        nafter,\n        return_scaled,\n        mode=mode,\n        sparsity_mask=sparsity_mask,\n        **job_kwargs,\n    )\n\n    if mode == \"memmap\":\n        return waveforms_by_units\n    elif mode == \"shared_memory\":\n        if copy:\n            waveforms_by_units = {unit_id: arr.copy() for unit_id, arr in waveforms_by_units.items()}\n            # release all sharedmem buffer\n            for unit_id in unit_ids:\n                shm = arrays_info[unit_id][0]\n                if shm is not None:\n                    # empty array have None\n                    shm.unlink()\n            return waveforms_by_units\n        else:\n            return waveforms_by_units, arrays_info", "function_signature": "def extract_waveforms_to_buffers(\n    recording,\n    spikes,\n    unit_ids,\n    nbefore,\n    nafter,\n    mode=\"memmap\",\n    return_scaled=False,\n    folder=None,\n    dtype=None,\n    sparsity_mask=None,\n    copy=False,\n    **job_kwargs,\n) :", "left_context": "\"\"\"\nThis module contains low-level functions to extract snippets of traces (aka \"spike waveforms\").\n\nThis is internally used by WaveformExtractor, but can also be used as a sorting component.\n\nIt is a 2-step approach:\n  1. allocate buffers (shared file or memory)\n  2. extract and distribute snippets into buffers (optionally in parallel)\n\n\"\"\"\n\nfrom __future__ import annotations\nfrom pathlib import Path\n\nimport numpy as np\nimport multiprocessing\n\nfrom spikeinterface.core.baserecording import BaseRecording\n\nfrom .baserecording import BaseRecording\nfrom .job_tools import ChunkRecordingExecutor, _shared_job_kwargs_doc\nfrom .core_tools import make_shared_array\nfrom .job_tools import fix_job_kwargs\n\n", "right_context": "\n\nextract_waveforms_to_buffers.__doc__ = extract_waveforms_to_buffers.__doc__.format(_shared_job_kwargs_doc)\n\n\ndef allocate_waveforms_buffers(\n    recording, spikes, unit_ids, nbefore, nafter, mode=\"memmap\", folder=None, dtype=None, sparsity_mask=None\n):\n    \"\"\"\n    Allocate memmap or shared memory buffers before snippet extraction.\n\n    Important note: for the shared memory mode arrays_info contains reference to\n    the shared memmory buffer, this variable must be reference as long as arrays as used.\n\n    Parameters\n    ----------\n    recording: recording\n        The recording object\n    spikes: 1d numpy array with several fields\n        Spikes handled as a unique vector.\n        This vector can be obtained with: `spikes = Sorting.to_spike_vector()`\n    unit_ids: list ot numpy\n        List of unit_ids\n    nbefore: int\n        N samples before spike\n    nafter: int\n        N samples after spike\n    mode: \"memmap\" | \"shared_memory\", default: \"memmap\"\n        Mode to use\n    folder: str or path\n        In case of memmap mode, folder to save npy files\n    dtype: numpy.dtype\n        dtype for waveforms buffer\n    sparsity_mask: None or array of bool\n        If not None shape must be must be (len(unit_ids), len(channel_ids)\n\n    Returns\n    -------\n    waveforms_by_units: dict of arrays\n        Arrays for all units (memmap or shared_memmep\n    arrays_info: dict of info\n        Dictionary to \"construct\" array in workers process (memmap file or sharemem)\n    \"\"\"\n\n    nsamples = nbefore + nafter\n\n    dtype = np.dtype(dtype)\n    if mode == \"shared_memory\":\n        assert folder is None\n    else:\n        folder = Path(folder)\n\n    # prepare buffers\n    waveforms_by_units = {}\n    arrays_info = {}\n    for unit_ind, unit_id in enumerate(unit_ids):\n        n_spikes = np.sum(spikes[\"unit_index\"] == unit_ind)\n        if sparsity_mask is None:\n            num_chans = recording.get_num_channels()\n        else:\n            num_chans = np.sum(sparsity_mask[unit_ind, :])\n        shape = (n_spikes, nsamples, num_chans)\n\n        if mode == \"memmap\":\n            filename = str(folder / f\"waveforms_{unit_id}.npy\")\n            arr = np.lib.format.open_memmap(filename, mode=\"w+\", dtype=dtype, shape=shape)\n            waveforms_by_units[unit_id] = arr\n            arrays_info[unit_id] = filename\n        elif mode == \"shared_memory\":\n            if n_spikes == 0 or num_chans == 0:\n                arr = np.zeros(shape, dtype=dtype)\n                shm = None\n                shm_name = None\n            else:\n                arr, shm = make_shared_array(shape, dtype)\n                shm_name = shm.name\n            waveforms_by_units[unit_id] = arr\n            arrays_info[unit_id] = (shm, shm_name, dtype.str, shape)\n        else:\n            raise ValueError(\"allocate_waveforms_buffers bad mode\")\n\n    return waveforms_by_units, arrays_info\n\n\ndef distribute_waveforms_to_buffers(\n    recording,\n    spikes,\n    unit_ids,\n    arrays_info,\n    nbefore,\n    nafter,\n    return_scaled,\n    mode=\"memmap\",\n    sparsity_mask=None,\n    job_name=None,\n    **job_kwargs,\n):\n    \"\"\"\n    Distribute snippets of traces into corresponding buffers.\n\n    Buffers must be pre-allocated with the `allocate_waveforms_buffers()` function.\n\n    Important note, for \"shared_memory\" mode arrays_info contain reference to\n    the shared memmory buffer, this variable must be reference as long as arrays as used.\n\n    Parameters\n    ----------\n    recording: recording\n        The recording object\n    spikes: 1d numpy array with several field\n        Spikes handled as a unique vector.\n        This vector can be spikes = Sorting.to_spike_vector()\n    unit_ids: list ot numpy\n        List of unit_ids\n    arrays_info: dict\n        Dictionary to \"construct\" array in workers process (memmap file or sharemem)\n    nbefore: int\n        N samples before spike\n    nafter: int\n        N samples after spike\n    return_scaled: bool\n        Scale traces before exporting to buffer or not.\n    mode: \"memmap\" | \"shared_memory\", default: \"memmap\"\n        Mode to use\n    sparsity_mask: None or array of bool\n        If not None shape must be must be (len(unit_ids), len(channel_ids)\n\n    {}\n\n    \"\"\"\n    job_kwargs = fix_job_kwargs(job_kwargs)\n\n    inds_by_unit = {}\n    for unit_ind, unit_id in enumerate(unit_ids):\n        (inds,) = np.nonzero(spikes[\"unit_index\"] == unit_ind)\n        inds_by_unit[unit_id] = inds\n\n    # and run\n    func = _worker_distribute_buffers\n    init_func = _init_worker_distribute_buffers\n\n    init_args = (\n        recording,\n        unit_ids,\n        spikes,\n        arrays_info,\n        nbefore,\n        nafter,\n        return_scaled,\n        inds_by_unit,\n        mode,\n        sparsity_mask,\n    )\n    if job_name is None:\n        job_name = f\"extract waveforms {mode} multi buffer\"\n    processor = ChunkRecordingExecutor(recording, func, init_func, init_args, job_name=job_name, **job_kwargs)\n    processor.run()\n\n\ndistribute_waveforms_to_buffers.__doc__ = distribute_waveforms_to_buffers.__doc__.format(_shared_job_kwargs_doc)\n\n\n# used by ChunkRecordingExecutor\ndef _init_worker_distribute_buffers(\n    recording, unit_ids, spikes, arrays_info, nbefore, nafter, return_scaled, inds_by_unit, mode, sparsity_mask\n):\n    # create a local dict per worker\n    worker_ctx = {}\n    if isinstance(recording, dict):\n        from spikeinterface.core import load_extractor\n\n        recording = load_extractor(recording)\n    worker_ctx[\"recording\"] = recording\n\n    if mode == \"memmap\":\n        # in memmap mode we have the \"too many open file\" problem with linux\n        # memmap file will be open on demand and not globally per worker\n        worker_ctx[\"arrays_info\"] = arrays_info\n    elif mode == \"shared_memory\":\n        from multiprocessing.shared_memory import SharedMemory\n\n        waveforms_by_units = {}\n        shms = {}\n        for unit_id, (shm, shm_name, dtype, shape) in arrays_info.items():\n            if shm_name is None:\n                arr = np.zeros(shape=shape, dtype=dtype)\n            else:\n                shm = SharedMemory(shm_name)\n                arr = np.ndarray(shape=shape, dtype=dtype, buffer=shm.buf)\n            waveforms_by_units[unit_id] = arr\n            # we need a reference to all sham otherwise we get segment fault!!!\n            shms[unit_id] = shm\n        worker_ctx[\"shms\"] = shms\n        worker_ctx[\"waveforms_by_units\"] = waveforms_by_units\n\n    worker_ctx[\"unit_ids\"] = unit_ids\n    worker_ctx[\"spikes\"] = spikes\n\n    worker_ctx[\"nbefore\"] = nbefore\n    worker_ctx[\"nafter\"] = nafter\n    worker_ctx[\"return_scaled\"] = return_scaled\n    worker_ctx[\"inds_by_unit\"] = inds_by_unit\n    worker_ctx[\"sparsity_mask\"] = sparsity_mask\n    worker_ctx[\"mode\"] = mode\n\n    return worker_ctx\n\n\n# used by ChunkRecordingExecutor\ndef _worker_distribute_buffers(segment_index, start_frame, end_frame, worker_ctx):\n    # recover variables of the worker\n    recording = worker_ctx[\"recording\"]\n    unit_ids = worker_ctx[\"unit_ids\"]\n    spikes = worker_ctx[\"spikes\"]\n    nbefore = worker_ctx[\"nbefore\"]\n    nafter = worker_ctx[\"nafter\"]\n    return_scaled = worker_ctx[\"return_scaled\"]\n    inds_by_unit = worker_ctx[\"inds_by_unit\"]\n    sparsity_mask = worker_ctx[\"sparsity_mask\"]\n\n    seg_size = recording.get_num_samples(segment_index=segment_index)\n\n    # take only spikes with the correct segment_index\n    # this is a slice so no copy!!\n    s0, s1 = np.searchsorted(spikes[\"segment_index\"], [segment_index, segment_index + 1])\n    in_seg_spikes = spikes[s0:s1]\n\n    # take only spikes in range [start_frame, end_frame]\n    # this is a slice so no copy!!\n    # the border of segment are protected by nbefore on left an nafter on the right\n    i0, i1 = np.searchsorted(\n        in_seg_spikes[\"sample_index\"], [max(start_frame, nbefore), min(end_frame, seg_size - nafter)]\n    )\n\n    # slice in absolut in spikes vector\n    l0 = i0 + s0\n    l1 = i1 + s0\n\n    if l1 > l0:\n        start = spikes[l0][\"sample_index\"] - nbefore\n        end = spikes[l1 - 1][\"sample_index\"] + nafter\n\n        # load trace in memory\n        traces = recording.get_traces(\n            start_frame=start, end_frame=end, segment_index=segment_index, return_scaled=return_scaled\n        )\n\n        for unit_ind, unit_id in enumerate(unit_ids):\n            # find pos\n            inds = inds_by_unit[unit_id]\n            (in_chunk_pos,) = np.nonzero((inds >= l0) & (inds < l1))\n            if in_chunk_pos.size == 0:\n                continue\n\n            if worker_ctx[\"mode\"] == \"memmap\":\n                # open file in demand (and also autoclose it after)\n                filename = worker_ctx[\"arrays_info\"][unit_id]\n                wfs = np.load(str(filename), mmap_mode=\"r+\")\n            elif worker_ctx[\"mode\"] == \"shared_memory\":\n                wfs = worker_ctx[\"waveforms_by_units\"][unit_id]\n\n            for pos in in_chunk_pos:\n                sample_index = spikes[inds[pos]][\"sample_index\"]\n                wf = traces[sample_index - start - nbefore : sample_index - start + nafter, :]\n\n                if sparsity_mask is None:\n                    wfs[pos, :, :] = wf\n                else:\n                    wfs[pos, :, :] = wf[:, sparsity_mask[unit_ind]]\n\n\ndef extract_waveforms_to_single_buffer(\n    recording,\n    spikes,\n    unit_ids,\n    nbefore,\n    nafter,\n    mode=\"memmap\",\n    return_scaled=False,\n    file_path=None,\n    dtype=None,\n    sparsity_mask=None,\n    copy=False,\n    job_name=None,\n    **job_kwargs,\n):\n    \"\"\"\n    Allocate a single buffer (memmap or or shared memory) and then distribute every waveform into it.\n\n    Contrary to extract_waveforms_to_buffers() all waveforms are extracted in the same buffer, so the spike vector is\n    needed to recover waveforms unit by unit. Importantly in case of sparsity, the channels are not aligned across\n    units.\n\n    Note: spikes near borders (nbefore/nafter) are not extracted and 0 are put the output buffer.\n    This ensures that spikes.shape[0] == all_waveforms.shape[0].\n\n    Important note: for the \"shared_memory\" mode wf_array_info contains reference to\n    the shared memmory buffer, this variable must be referenced as long as arrays is used.\n    This variable must also unlink() when the array is de-referenced.\n    To avoid this complicated behavior, default: (copy=True) the shared memmory buffer is copied into a standard\n    numpy array.\n\n\n    Parameters\n    ----------\n    recording: recording\n        The recording object\n    spikes: 1d numpy array with several fields\n        Spikes handled as a unique vector.\n        This vector can be obtained with: `spikes = Sorting.to_spike_vector()`\n    unit_ids: list ot numpy\n        List of unit_ids\n    nbefore: int\n        N samples before spike\n    nafter: int\n        N samples after spike\n    mode: \"memmap\" | \"shared_memory\", default: \"memmap\"\n        The mode to use for the buffer\n    return_scaled: bool, default: False\n        Scale traces before exporting to buffer or not\n    file_path: str or path or None, default: None\n        In case of memmap mode, file to save npy file\n    dtype: numpy.dtype, default: None\n        dtype for waveforms buffer\n    sparsity_mask: None or array of bool, default: None\n        If not None shape must be must be (len(unit_ids), len(channel_ids))\n    copy: bool, default: False\n        If True, the output shared memory object is copied to a numpy standard array and no reference\n        to the internal shared memory object is kept.\n        If copy=False then the shared memory object is also returned. Please keep in mind that the shared memory object\n        need to be referenced as long as all_waveforms will be used otherwise it might produce segmentation\n        faults which are hard to debug.\n        Also when copy=False the SharedMemory will need to be unlink manually if proper cleanup of resources is desired.\n\n    {}\n\n    Returns\n    -------\n    all_waveforms: numpy array\n        Single array with shape (nump_spikes, num_samples, num_channels)\n\n    wf_array_info: dict of info\n        Optionally return in case of shared_memory if copy=False.\n        Dictionary to \"construct\" array in workers process (memmap file or sharemem info)\n    \"\"\"\n    nsamples = nbefore + nafter\n\n    dtype = np.dtype(dtype)\n    if mode == \"shared_memory\":\n        assert file_path is None\n    else:\n        file_path = Path(file_path)\n\n    num_spikes = spikes.size\n    if sparsity_mask is None:\n        num_chans = recording.get_num_channels()\n    else:\n        num_chans = max(np.sum(sparsity_mask, axis=1))\n    shape = (num_spikes, nsamples, num_chans)\n\n    if mode == \"memmap\":\n        all_waveforms = np.lib.format.open_memmap(file_path, mode=\"w+\", dtype=dtype, shape=shape)\n        # wf_array_info = str(file_path)\n        wf_array_info = dict(filename=str(file_path))\n    elif mode == \"shared_memory\":\n        if num_spikes == 0 or num_chans == 0:\n            all_waveforms = np.zeros(shape, dtype=dtype)\n            shm = None\n            shm_name = None\n        else:\n            all_waveforms, shm = make_shared_array(shape, dtype)\n            shm_name = shm.name\n        # wf_array_info = (shm, shm_name, dtype.str, shape)\n        wf_array_info = dict(shm=shm, shm_name=shm_name, dtype=dtype.str, shape=shape)\n    else:\n        raise ValueError(\"allocate_waveforms_buffers bad mode\")\n\n    job_kwargs = fix_job_kwargs(job_kwargs)\n\n    if num_spikes > 0:\n        # and run\n        func = _worker_distribute_single_buffer\n        init_func = _init_worker_distribute_single_buffer\n\n        init_args = (\n            recording,\n            spikes,\n            wf_array_info,\n            nbefore,\n            nafter,\n            return_scaled,\n            mode,\n            sparsity_mask,\n        )\n        if job_name is None:\n            job_name = f\"extract waveforms {mode} mono buffer\"\n\n        processor = ChunkRecordingExecutor(recording, func, init_func, init_args, job_name=job_name, **job_kwargs)\n        processor.run()\n\n    if mode == \"memmap\":\n        return all_waveforms\n    elif mode == \"shared_memory\":\n        if copy:\n            if shm is not None:\n                # release all sharedmem buffer\n                # empty array have None\n                shm.unlink()\n            return all_waveforms.copy()\n        else:\n            return all_waveforms, wf_array_info\n\n\ndef _init_worker_distribute_single_buffer(\n    recording, spikes, wf_array_info, nbefore, nafter, return_scaled, mode, sparsity_mask\n):\n    worker_ctx = {}\n    worker_ctx[\"recording\"] = recording\n    worker_ctx[\"wf_array_info\"] = wf_array_info\n    worker_ctx[\"spikes\"] = spikes\n    worker_ctx[\"nbefore\"] = nbefore\n    worker_ctx[\"nafter\"] = nafter\n    worker_ctx[\"return_scaled\"] = return_scaled\n    worker_ctx[\"sparsity_mask\"] = sparsity_mask\n    worker_ctx[\"mode\"] = mode\n\n    if mode == \"memmap\":\n        filename = wf_array_info[\"filename\"]\n        all_waveforms = np.load(str(filename), mmap_mode=\"r+\")\n        worker_ctx[\"all_waveforms\"] = all_waveforms\n    elif mode == \"shared_memory\":\n        from multiprocessing.shared_memory import SharedMemory\n\n        shm_name, dtype, shape = wf_array_info[\"shm_name\"], wf_array_info[\"dtype\"], wf_array_info[\"shape\"]\n        shm = SharedMemory(shm_name)\n        all_waveforms = np.ndarray(shape=shape, dtype=dtype, buffer=shm.buf)\n        worker_ctx[\"shm\"] = shm\n        worker_ctx[\"all_waveforms\"] = all_waveforms\n\n    # prepare segment slices\n    segment_slices = []\n    for segment_index in range(recording.get_num_segments()):\n        s0, s1 = np.searchsorted(spikes[\"segment_index\"], [segment_index, segment_index + 1])\n        segment_slices.append((s0, s1))\n    worker_ctx[\"segment_slices\"] = segment_slices\n\n    return worker_ctx\n\n\n# used by ChunkRecordingExecutor\ndef _worker_distribute_single_buffer(segment_index, start_frame, end_frame, worker_ctx):\n    # recover variables of the worker\n    recording = worker_ctx[\"recording\"]\n    segment_slices = worker_ctx[\"segment_slices\"]\n    spikes = worker_ctx[\"spikes\"]\n    nbefore = worker_ctx[\"nbefore\"]\n    nafter = worker_ctx[\"nafter\"]\n    return_scaled = worker_ctx[\"return_scaled\"]\n    sparsity_mask = worker_ctx[\"sparsity_mask\"]\n    all_waveforms = worker_ctx[\"all_waveforms\"]\n\n    seg_size = recording.get_num_samples(segment_index=segment_index)\n\n    s0, s1 = segment_slices[segment_index]\n    in_seg_spikes = spikes[s0:s1]\n\n    # take only spikes in range [start_frame, end_frame]\n    # this is a slice so no copy!!\n    # the border of segment are protected by nbefore on left an nafter on the right\n    i0, i1 = np.searchsorted(\n        in_seg_spikes[\"sample_index\"], [max(start_frame, nbefore), min(end_frame, seg_size - nafter)]\n    )\n\n    # slice in absolut in spikes vector\n    l0 = i0 + s0\n    l1 = i1 + s0\n\n    if l1 > l0:\n        start = spikes[l0][\"sample_index\"] - nbefore\n        end = spikes[l1 - 1][\"sample_index\"] + nafter\n\n        # load trace in memory\n        traces = recording.get_traces(\n            start_frame=start, end_frame=end, segment_index=segment_index, return_scaled=return_scaled\n        )\n\n        for spike_index in range(l0, l1):\n            sample_index = spikes[spike_index][\"sample_index\"]\n            unit_index = spikes[spike_index][\"unit_index\"]\n            wf = traces[sample_index - start - nbefore : sample_index - start + nafter, :]\n\n            if sparsity_mask is None:\n                all_waveforms[spike_index, :, :] = wf\n            else:\n                mask = sparsity_mask[unit_index, :]\n                wf = wf[:, mask]\n                all_waveforms[spike_index, :, : wf.shape[1]] = wf\n\n        if worker_ctx[\"mode\"] == \"memmap\":\n            all_waveforms.flush()\n\n\ndef split_waveforms_by_units(unit_ids, spikes, all_waveforms, sparsity_mask=None, folder=None):\n    \"\"\"\n    Split a single buffer waveforms into waveforms by units (multi buffers or multi files).\n\n    Parameters\n    ----------\n    unit_ids: list or numpy array\n        List of unit ids\n    spikes: numpy array\n        The spike vector\n    all_waveforms : numpy array\n        Single buffer containing all waveforms\n    sparsity_mask : None or numpy array\n        Optionally the boolean sparsity mask\n    folder : None or str or Path\n        If a folder is given all waveforms by units are copied in a npy file using f\"waveforms_{unit_id}.npy\" naming.\n\n    Returns\n    -------\n    waveforms_by_units: dict of array\n        A dict of arrays.\n        In case of folder not None, this contain the memmap of the files.\n    \"\"\"\n    if folder is not None:\n        folder = Path(folder)\n    waveforms_by_units = {}\n    for unit_index, unit_id in enumerate(unit_ids):\n        mask = spikes[\"unit_index\"] == unit_index\n        if sparsity_mask is not None:\n            chan_mask = sparsity_mask[unit_index, :]\n            num_chans = np.sum(chan_mask)\n            wfs = all_waveforms[mask, :, :][:, :, :num_chans]\n        else:\n            wfs = all_waveforms[mask, :, :]\n\n        if folder is None:\n            waveforms_by_units[unit_id] = wfs\n        else:\n            np.save(folder / f\"waveforms_{unit_id}.npy\", wfs)\n            # this avoid keeping in memory all waveforms\n            waveforms_by_units[unit_id] = np.load(f\"waveforms_{unit_id}.npy\", mmap_mode=\"r\")\n\n    return waveforms_by_units\n\n\ndef has_exceeding_spikes(recording, sorting):\n    \"\"\"\n    Check if the sorting objects has spikes exceeding the recording number of samples, for all segments\n\n    Parameters\n    ----------\n    recording : BaseRecording\n        The recording object\n    sorting : BaseSorting\n        The sorting object\n\n    Returns\n    -------\n    bool\n        True if exceeding spikes, False otherwise\n    \"\"\"\n    spike_vector = sorting.to_spike_vector()\n    for segment_index in range(recording.get_num_segments()):\n        start_seg_ind, end_seg_ind = np.searchsorted(spike_vector[\"segment_index\"], [segment_index, segment_index + 1])\n        spike_vector_seg = spike_vector[start_seg_ind:end_seg_ind]\n        if len(spike_vector_seg) > 0:\n            if spike_vector_seg[\"sample_index\"][-1] > recording.get_num_samples(segment_index=segment_index) - 1:\n                return True\n    return False\n\n\ndef estimate_templates(\n    recording: BaseRecording,\n    spikes: np.ndarray,\n    unit_ids: list | np.ndarray,\n    nbefore: int,\n    nafter: int,\n    return_scaled: bool = True,\n    **job_kwargs,\n):\n    \"\"\"\n    This is a fast implementation to compute average templates.\n    This is useful to estimate sparsity without the need to allocate large waveform buffers.\n    The mechanism is pretty simple: it accumulates and sums spike waveforms in-place per worker and per unit.\n    Note that std, median and percentiles can't be computed with this method.\n\n    Parameters\n    ----------\n    recording: BaseRecording\n        The recording object\n    spikes: 1d numpy array with several fields\n        Spikes handled as a unique vector.\n        This vector can be obtained with: `spikes = sorting.to_spike_vector()`\n    unit_ids: list ot numpy\n        List of unit_ids\n    nbefore: int\n        Number of samples to cut out before a spike\n    nafter: int\n        Number of samples to cut out after a spike\n    return_scaled: bool, default: True\n        If True, the traces are scaled before averaging\n\n    Returns\n    -------\n    templates_array: np.array\n        The average templates with shape (num_units, nbefore + nafter, num_channels)\n    \"\"\"\n\n    assert spikes.size > 0, \"estimate_templates() need non empty sorting\"\n\n    job_kwargs = fix_job_kwargs(job_kwargs)\n    num_worker = job_kwargs[\"n_jobs\"]\n\n    num_chans = recording.get_num_channels()\n    num_units = len(unit_ids)\n\n    shape = (num_worker, num_units, nbefore + nafter, num_chans)\n    dtype = np.dtype(\"float32\")\n    waveforms_per_worker, shm = make_shared_array(shape, dtype)\n    shm_name = shm.name\n\n    # trick to get the work_index given pid arrays\n    lock = multiprocessing.Lock()\n    array_pid = multiprocessing.Array(\"i\", num_worker)\n    for i in range(num_worker):\n        array_pid[i] = -1\n\n    func = _worker_estimate_templates\n    init_func = _init_worker_estimate_templates\n\n    init_args = (\n        recording,\n        spikes,\n        shm_name,\n        shape,\n        dtype,\n        nbefore,\n        nafter,\n        return_scaled,\n        lock,\n        array_pid,\n    )\n\n    processor = ChunkRecordingExecutor(\n        recording, func, init_func, init_args, job_name=\"estimate_templates\", **job_kwargs\n    )\n    processor.run()\n\n    # average\n    templates_array = np.sum(waveforms_per_worker, axis=0)\n    unit_indices, spike_count = np.unique(spikes[\"unit_index\"], return_counts=True)\n    templates_array[unit_indices, :, :] /= spike_count[:, np.newaxis, np.newaxis]\n\n    # important : release the sharemem\n    del waveforms_per_worker\n    shm.unlink()\n    shm.close()\n\n    return templates_array\n\n\ndef _init_worker_estimate_templates(\n    recording,\n    spikes,\n    shm_name,\n    shape,\n    dtype,\n    nbefore,\n    nafter,\n    return_scaled,\n    lock,\n    array_pid,\n):\n    worker_ctx = {}\n    worker_ctx[\"recording\"] = recording\n    worker_ctx[\"spikes\"] = spikes\n    worker_ctx[\"nbefore\"] = nbefore\n    worker_ctx[\"nafter\"] = nafter\n    worker_ctx[\"return_scaled\"] = return_scaled\n\n    from multiprocessing.shared_memory import SharedMemory\n    import multiprocessing\n\n    shm = SharedMemory(shm_name)\n    waveforms_per_worker = np.ndarray(shape=shape, dtype=dtype, buffer=shm.buf)\n    worker_ctx[\"shm\"] = shm\n    worker_ctx[\"waveforms_per_worker\"] = waveforms_per_worker\n\n    # prepare segment slices\n    segment_slices = []\n    for segment_index in range(recording.get_num_segments()):\n        s0, s1 = np.searchsorted(spikes[\"segment_index\"], [segment_index, segment_index + 1])\n        segment_slices.append((s0, s1))\n    worker_ctx[\"segment_slices\"] = segment_slices\n\n    child_process = multiprocessing.current_process()\n\n    lock.acquire()\n    num_worker = None\n    for i in range(len(array_pid)):\n        if array_pid[i] == -1:\n            num_worker = i\n            array_pid[i] = child_process.ident\n            break\n    worker_ctx[\"worker_index\"] = num_worker\n    lock.release()\n\n    return worker_ctx\n\n\n# used by ChunkRecordingExecutor\ndef _worker_estimate_templates(segment_index, start_frame, end_frame, worker_ctx):\n    # recover variables of the worker\n    recording = worker_ctx[\"recording\"]\n    segment_slices = worker_ctx[\"segment_slices\"]\n    spikes = worker_ctx[\"spikes\"]\n    nbefore = worker_ctx[\"nbefore\"]\n    nafter = worker_ctx[\"nafter\"]\n    waveforms_per_worker = worker_ctx[\"waveforms_per_worker\"]\n    worker_index = worker_ctx[\"worker_index\"]\n    return_scaled = worker_ctx[\"return_scaled\"]\n\n    seg_size = recording.get_num_samples(segment_index=segment_index)\n\n    s0, s1 = segment_slices[segment_index]\n    in_seg_spikes = spikes[s0:s1]\n\n    # take only spikes in range [start_frame, end_frame]\n    # this is a slice so no copy!!\n    # the border of segment are protected by nbefore on left an nafter on the right\n    i0, i1 = np.searchsorted(\n        in_seg_spikes[\"sample_index\"], [max(start_frame, nbefore), min(end_frame, seg_size - nafter)]\n    )\n\n    # slice in absolut in spikes vector\n    l0 = i0 + s0\n    l1 = i1 + s0\n\n    if l1 > l0:\n        start = spikes[l0][\"sample_index\"] - nbefore\n        end = spikes[l1 - 1][\"sample_index\"] + nafter\n\n        # load trace in memory\n        traces = recording.get_traces(\n            start_frame=start, end_frame=end, segment_index=segment_index, return_scaled=return_scaled\n        )\n\n        for spike_index in range(l0, l1):\n            sample_index = spikes[spike_index][\"sample_index\"]\n            unit_index = spikes[spike_index][\"unit_index\"]\n            wf = traces[sample_index - start - nbefore : sample_index - start + nafter, :]\n\n            waveforms_per_worker[worker_index, unit_index, :, :] += wf\n", "import_text": ["pathlib.Path", "numpy", "multiprocessing", "spikeinterface.core.baserecording.BaseRecording"], "prompt": "\"\"\"\nDescription: This function extracts waveforms from a recording and stores them in buffers.\n\nArgs:\n    recording (object): The recording object containing the raw data.\n    spikes (array-like): The spike times.\n    unit_ids (array-like): The unit ids for which to extract waveforms.\n    nbefore (int): The number of samples to include before each spike.\n    nafter (int): The number of samples to include after each spike.\n    mode (str, optional): The mode to use for storing the waveforms. Defaults to \"memmap\".\n    return_scaled (bool, optional): Whether to return the waveforms scaled to the recording's unit. Defaults to False.\n    folder (str, optional): The folder to use for storing the waveforms. Defaults to None.\n    dtype (dtype, optional): The data type to use for storing the waveforms. Defaults to None.\n    sparsity_mask (array-like, optional): A mask for sparsely sampling the waveforms. Defaults to None.\n    copy (bool, optional): Whether to copy the waveforms. Defaults to False.\n    **job_kwargs: Additional keyword arguments for job distribution.\n\nReturns:\n    dict: A dictionary of waveforms by unit id. If mode is \"shared_memory\", returns a tuple of the waveforms and arrays info.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Allocate buffers (memmap or or shared memory) and then distribute every waveform into theses buffers.\n\n    Same as calling allocate_waveforms_buffers() and then distribute_waveforms_to_buffers().\n\n    Important note: for the \"shared_memory\" mode arrays_info contains reference to\n    the shared memmory buffer, this variable must be reference as long as arrays as used.\n    And this variable is also returned.\n    To avoid this a copy to non shared memmory can be perform at the end.\n\n    Parameters\n    ----------\n    recording: recording\n        The recording object\n    spikes: 1d numpy array with several fields\n        Spikes handled as a unique vector.\n        This vector can be obtained with: `spikes = Sorting.to_spike_vector()`\n    unit_ids: list ot numpy\n        List of unit_ids\n    nbefore: int\n        N samples before spike\n    nafter: int\n        N samples after spike\n    mode: \"memmap\" | \"shared_memory\", default: \"memmap\"\n        The mode to use for the buffer\n    return_scaled: bool, default: False\n        Scale traces before exporting to buffer or not\n    folder: str or path or None, default: None\n        In case of memmap mode, folder to save npy files\n    dtype: numpy.dtype, default: None\n        dtype for waveforms buffer\n    sparsity_mask: None or array of bool, default: None\n        If not None shape must be must be (len(unit_ids), len(channel_ids))\n    copy: bool, default: False\n        If True, the output shared memory object is copied to a numpy standard array.\n        If copy=False then arrays_info is also return. Please keep in mind that arrays_info\n        need to be referenced as long as waveforms_by_units will be used otherwise it will be very hard to debug.\n        Also when copy=False the SharedMemory will need to be unlink manually\n    {}\n\n    Returns\n    -------\n    waveforms_by_units: dict of arrays\n        Arrays for all units (memmap or shared_memmep)\n\n    arrays_info: dict of info\n        Optionally return in case of shared_memory if copy=False.\n        Dictionary to \"construct\" array in workers process (memmap file or sharemem info)\n    \"\"\"", "function_dependencies": ["numpy.dtype"], "project_create_time": "2019-07-12T13:07:46+00:00", "project_update_time": "2024-04-17T14:53:29+00:00", "file_create_time": "2023-04-05T11:11:32Z", "file_update_time": "2024-01-29T17:35:14Z", "function_update_time": "2024-01-17T08:41:44Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["numpy.dtype"], "test_function": [{"file_path": "/spikeinterface-0.100.5/spikeinterface-0.100.5/src/spikeinterface/core/tests/test_waveform_tools.py", "class_name": null, "function_name": "test_waveform_tools", "code": "\ndef test_waveform_tools():\n    # durations = [30, 40]\n    # sampling_frequency = 30000.0\n\n    # # 2 segments\n    # num_channels = 2\n    # recording = generate_recording(\n    #     num_channels=num_channels, durations=durations, sampling_frequency=sampling_frequency\n    # )\n    # recording.annotate(is_filtered=True)\n    # num_units = 15\n    # sorting = generate_sorting(num_units=num_units, sampling_frequency=sampling_frequency, durations=durations)\n\n    # test with dump !!!!\n    # recording = recording.save()\n    # sorting = sorting.save()\n\n    recording, sorting = get_dataset()\n    sampling_frequency = recording.sampling_frequency\n\n    nbefore = int(3.0 * sampling_frequency / 1000.0)\n    nafter = int(4.0 * sampling_frequency / 1000.0)\n\n    dtype = recording.get_dtype()\n    # return_scaled = False\n\n    spikes = sorting.to_spike_vector()\n\n    unit_ids = sorting.unit_ids\n\n    some_job_kwargs = [\n        {\"n_jobs\": 1, \"chunk_size\": 3000, \"progress_bar\": True},\n        {\"n_jobs\": 2, \"chunk_size\": 3000, \"progress_bar\": True},\n    ]\n    some_modes = [\n        {\"mode\": \"memmap\"},\n        {\"mode\": \"shared_memory\"},\n    ]\n    # if platform.system() != \"Windows\":\n    #     # shared memory on windows is buggy...\n    #     some_modes.append(\n    #         {\n    #             \"mode\": \"shared_memory\",\n    #         }\n    #     )\n\n    some_sparsity = [\n        dict(sparsity_mask=None),\n        dict(sparsity_mask=np.random.randint(0, 2, size=(unit_ids.size, recording.channel_ids.size), dtype=\"bool\")),\n    ]\n\n    # memmap mode\n    list_wfs_dense = []\n    list_wfs_sparse = []\n    for j, job_kwargs in enumerate(some_job_kwargs):\n        for k, mode_kwargs in enumerate(some_modes):\n            for l, sparsity_kwargs in enumerate(some_sparsity):\n                # print()\n                # print(job_kwargs, mode_kwargs, 'sparse=', sparsity_kwargs['sparsity_mask'] is None)\n\n                if mode_kwargs[\"mode\"] == \"memmap\":\n                    wf_folder = cache_folder / f\"test_waveform_tools_{j}_{k}_{l}\"\n                    if wf_folder.is_dir():\n                        shutil.rmtree(wf_folder)\n                    wf_folder.mkdir(parents=True)\n                    wf_file_path = wf_folder / \"waveforms_all_units.npy\"\n\n                mode_kwargs_ = dict(**mode_kwargs)\n                if mode_kwargs[\"mode\"] == \"memmap\":\n                    mode_kwargs_[\"folder\"] = wf_folder\n\n                wfs_arrays = extract_waveforms_to_buffers(\n                    recording,\n                    spikes,\n                    unit_ids,\n                    nbefore,\n                    nafter,\n                    return_scaled=False,\n                    dtype=dtype,\n                    copy=True,\n                    **sparsity_kwargs,\n                    **mode_kwargs_,\n                    **job_kwargs,\n                )\n                for unit_ind, unit_id in enumerate(unit_ids):\n                    wf = wfs_arrays[unit_id]\n                    assert wf.shape[0] == np.sum(spikes[\"unit_index\"] == unit_ind)\n\n                if sparsity_kwargs[\"sparsity_mask\"] is None:\n                    list_wfs_dense.append(wfs_arrays)\n                else:\n                    list_wfs_sparse.append(wfs_arrays)\n\n                mode_kwargs_ = dict(**mode_kwargs)\n                if mode_kwargs[\"mode\"] == \"memmap\":\n                    mode_kwargs_[\"file_path\"] = wf_file_path\n\n                all_waveforms = extract_waveforms_to_single_buffer(\n                    recording,\n                    spikes,\n                    unit_ids,\n                    nbefore,\n                    nafter,\n                    return_scaled=False,\n                    dtype=dtype,\n                    copy=True,\n                    **sparsity_kwargs,\n                    **mode_kwargs_,\n                    **job_kwargs,\n                )\n                wfs_arrays = split_waveforms_by_units(\n                    unit_ids, spikes, all_waveforms, sparsity_mask=sparsity_kwargs[\"sparsity_mask\"]\n                )\n                if sparsity_kwargs[\"sparsity_mask\"] is None:\n                    list_wfs_dense.append(wfs_arrays)\n                else:\n                    list_wfs_sparse.append(wfs_arrays)\n\n    _check_all_wf_equal(list_wfs_dense)\n    _check_all_wf_equal(list_wfs_sparse)"}]}, {"git_group": "royerlab", "git_name": "aydin", "version": "v0.1.15", "language": "Python", "project_name": "aydin-v0.1.15.zip", "file_path": "/aydin-v0.1.15/aydin-0.1.15/aydin/it/transforms/histogram.py", "file_name": "histogram.py", "focal_class": "HistogramEqualisationTransform", "focal_name": "preprocess", "focal_parameter": [], "solution": "\n    def preprocess(self, array: ArrayLike):\n\n        with lsection(\n            f\"Equalises histogram for array of shape: {array.shape} and dtype: {array.dtype}\"\n        ):\n\n            self._original_dtype = array.dtype\n            array = array.astype(numpy.float32, copy=False)\n\n            if self.mode == 'equalize':\n                self._cdf, self._bin_centers = cumulative_distribution(array)\n                new_array = _interpolation(array, self._bin_centers, self._cdf)\n            elif self.mode == 'clahe':\n                kernel_size = tuple(s / self.scale for s in array.shape)\n                new_array = equalize_adapthist(array, kernel_size=kernel_size)\n            else:\n                raise ValueError(\n                    f\"Unsupported mode for histogram transform: {self.mode}\"\n                )\n\n            return new_array", "function_signature": "def preprocess(self, array: ArrayLike) :", "left_context": "import numpy\n\nfrom numpy.typing import ArrayLike\nfrom skimage.exposure import equalize_adapthist, cumulative_distribution\n\nfrom aydin.it.transforms.base import ImageTransformBase\nfrom aydin.util.log.log import lsection, lprint\n\n\nclass HistogramEqualisationTransform(ImageTransformBase):\n    \"\"\"Histogram Equalisation\n\n    For some images with extremely unbalanced histograms, applying histogram\n    equalisation will improve results. Two modes are supported: 'equalize',\n    and 'clahe'.\n    \"\"\"\n\n    preprocess_description = (\n        \"Apply histogram equalisation\" + ImageTransformBase.preprocess_description\n    )\n    postprocess_description = (\n        \"Undo histogram equalisation\" + ImageTransformBase.postprocess_description\n    )\n    postprocess_supported = True\n    postprocess_recommended = True\n\n    def __init__(\n        self,\n        mode: str = 'equalize',\n        scale: float = 1.0 / 8,\n        priority: float = 0.12,\n        **kwargs,\n    ):\n\n        \"\"\"\n        Constructs a Histogram Transform\n\n        Parameters\n        ----------\n        mode : str\n            Two modes are supported: 'equalize', and 'clahe'.\n        scale : float\n            Scale of the kernel expressed relatively to the\n            size of the image, values are within [0,1].\n        priority : float\n            The priority is a value within [0,1] used to determine the order in\n            which to apply the pre- and post-processing transforms. Transforms\n            are sorted and applied in ascending order during preprocesing and in\n            the reverse, descending, order during post-processing.\n        \"\"\"\n        super().__init__(priority=priority, **kwargs)\n        self.mode = mode\n        self.scale = scale\n        self._cdf = None\n        self._bin_centers = None\n        self._original_dtype = None\n\n        lprint(f\"Instanciating: {self}\")\n\n    # We exclude certain fields from saving:\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        del state['_cdf']\n        del state['_bin_centers']\n        del state['_original_dtype']\n        return state\n\n    def __str__(self):\n        return f'{type(self).__name__}' f' (mode={self.mode},' f' scale={self.scale})'\n\n    def __repr__(self):\n        return self.__str__()\n", "right_context": "\n    def postprocess(self, array: ArrayLike):\n\n        if not self.do_postprocess:\n            return array\n\n        with lsection(\n            f\"Undoing histogram equalisation for array of shape: {array.shape} and dtype: {array.dtype}\"\n        ):\n            array = array.astype(numpy.float32, copy=False)\n\n            if self.mode == 'equalize':\n                new_array = _interpolation(array, self._cdf, self._bin_centers)\n            elif self.mode == 'clahe':\n                # Inverse not supported yet:\n                new_array = array\n            else:\n                raise ValueError(\n                    f\"Unsupported mode for histogram transform: {self.mode}\"\n                )\n\n            # cast back to original dtype:\n            new_array = new_array.astype(self._original_dtype, copy=False)\n\n            return new_array\n\n\ndef _interpolation(image, x, y):\n    out = numpy.interp(image.flat, x, y)\n    return out.reshape(image.shape)\n", "import_text": ["numpy", "numpy.typing.ArrayLike", "skimage.exposure.equalize_adapthist", "skimage.exposure.cumulative_distribution", "aydin.it.transforms.base.ImageTransformBase", "aydin.util.log.log.lsection", "aydin.util.log.log.lprint"], "prompt": "\"\"\"\nDescription: This function preprocesses an array by equalizing its histogram or applying Contrast Limited Adaptive Histogram Equalization (CLAHE).\n\nArgs:\n    array (ArrayLike): The input array to be preprocessed.\n\nReturns:\n    numpy.ndarray: The preprocessed array with equalized histogram or CLAHE applied.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["aydin.util.log.log.lsection", "skimage.exposure.cumulative_distribution", "skimage.exposure.equalize_adapthist"], "project_create_time": "2019-05-28T04:30:19+00:00", "project_update_time": "2024-03-15T22:02:23+00:00", "file_create_time": "2021-11-06T02:25:48Z", "file_update_time": "2022-01-07T17:41:41Z", "function_update_time": "2021-11-06T02:25:48Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["skimage.exposure.cumulative_distribution"], "test_function": [{"file_path": "/aydin-v0.1.15/aydin-0.1.15/aydin/it/transforms/test/test_histogram.py", "class_name": null, "function_name": "test_histogram", "code": "\ndef test_histogram():\n    image = camera()\n\n    ht = HistogramEqualisationTransform()\n\n    preprocessed = ht.preprocess(image)\n    postprocessed = ht.postprocess(preprocessed)\n\n    # import napari\n    #\n    # viewer = napari.Viewer()\n    # viewer.add_image(image, name='image')\n    # viewer.add_image(preprocessed, name='preprocessed')\n    # viewer.add_image(postprocessed, name='postprocessed')\n    # napari.run()\n\n    assert postprocessed.dtype == image.dtype\n    assert postprocessed.shape == image.shape\n    assert (\n        numpy.abs(\n            postprocessed.astype(numpy.float32) - image.astype(numpy.float32)\n        ).mean()\n        < 2\n    )"}]}, {"git_group": "holzschu", "git_name": "python3_ios", "version": "v1.0", "language": "Python", "project_name": "python3_ios-v1.0.zip", "file_path": "/python3_ios-v1.0/python3_ios-1.0/site-packages/sympy/physics/vector/fieldfunctions.py", "file_name": "fieldfunctions.py", "focal_class": null, "focal_name": "divergence", "focal_parameter": ["vect", "frame"], "solution": "def divergence(vect, frame):\n\n    _check_vector(vect)\n    if vect == 0:\n        return S(0)\n    vect = express(vect, frame, variables=True)\n    vectx = vect.dot(frame.x)\n    vecty = vect.dot(frame.y)\n    vectz = vect.dot(frame.z)\n    out = S(0)\n    out += diff(vectx, frame[0])\n    out += diff(vecty, frame[1])\n    out += diff(vectz, frame[2])\n    return out", "function_signature": "def divergence(vect, frame) :", "left_context": "from sympy import diff, integrate, S\nfrom sympy.physics.vector import Vector, express\nfrom sympy.physics.vector.frame import _check_frame\nfrom sympy.physics.vector.vector import _check_vector\n\n\n__all__ = ['curl', 'divergence', 'gradient', 'is_conservative',\n           'is_solenoidal', 'scalar_potential',\n           'scalar_potential_difference']\n\n\ndef curl(vect, frame):\n    \"\"\"\n    Returns the curl of a vector field computed wrt the coordinate\n    symbols of the given frame.\n\n    Parameters\n    ==========\n\n    vect : Vector\n        The vector operand\n\n    frame : ReferenceFrame\n        The reference frame to calculate the curl in\n\n    Examples\n    ========\n\n    >>> from sympy.physics.vector import ReferenceFrame\n    >>> from sympy.physics.vector import curl\n    >>> R = ReferenceFrame('R')\n    >>> v1 = R[1]*R[2]*R.x + R[0]*R[2]*R.y + R[0]*R[1]*R.z\n    >>> curl(v1, R)\n    0\n    >>> v2 = R[0]*R[1]*R[2]*R.x\n    >>> curl(v2, R)\n    R_x*R_y*R.y - R_x*R_z*R.z\n\n    \"\"\"\n\n    _check_vector(vect)\n    if vect == 0:\n        return Vector(0)\n    vect = express(vect, frame, variables=True)\n    #A mechanical approach to avoid looping overheads\n    vectx = vect.dot(frame.x)\n    vecty = vect.dot(frame.y)\n    vectz = vect.dot(frame.z)\n    outvec = Vector(0)\n    outvec += (diff(vectz, frame[1]) - diff(vecty, frame[2])) * frame.x\n    outvec += (diff(vectx, frame[2]) - diff(vectz, frame[0])) * frame.y\n    outvec += (diff(vecty, frame[0]) - diff(vectx, frame[1])) * frame.z\n    return outvec\n\n", "right_context": "\n\ndef gradient(scalar, frame):\n    \"\"\"\n    Returns the vector gradient of a scalar field computed wrt the\n    coordinate symbols of the given frame.\n\n    Parameters\n    ==========\n\n    scalar : sympifiable\n        The scalar field to take the gradient of\n\n    frame : ReferenceFrame\n        The frame to calculate the gradient in\n\n    Examples\n    ========\n\n    >>> from sympy.physics.vector import ReferenceFrame\n    >>> from sympy.physics.vector import gradient\n    >>> R = ReferenceFrame('R')\n    >>> s1 = R[0]*R[1]*R[2]\n    >>> gradient(s1, R)\n    R_y*R_z*R.x + R_x*R_z*R.y + R_x*R_y*R.z\n    >>> s2 = 5*R[0]**2*R[2]\n    >>> gradient(s2, R)\n    10*R_x*R_z*R.x + 5*R_x**2*R.z\n\n    \"\"\"\n\n    _check_frame(frame)\n    outvec = Vector(0)\n    scalar = express(scalar, frame, variables=True)\n    for i, x in enumerate(frame):\n        outvec += diff(scalar, frame[i]) * x\n    return outvec\n\n\ndef is_conservative(field):\n    \"\"\"\n    Checks if a field is conservative.\n\n    Parameters\n    ==========\n\n    field : Vector\n        The field to check for conservative property\n\n    Examples\n    ========\n\n    >>> from sympy.physics.vector import ReferenceFrame\n    >>> from sympy.physics.vector import is_conservative\n    >>> R = ReferenceFrame('R')\n    >>> is_conservative(R[1]*R[2]*R.x + R[0]*R[2]*R.y + R[0]*R[1]*R.z)\n    True\n    >>> is_conservative(R[2] * R.y)\n    False\n\n    \"\"\"\n\n    #Field is conservative irrespective of frame\n    #Take the first frame in the result of the\n    #separate method of Vector\n    if field == Vector(0):\n        return True\n    frame = list(field.separate())[0]\n    return curl(field, frame).simplify() == Vector(0)\n\n\ndef is_solenoidal(field):\n    \"\"\"\n    Checks if a field is solenoidal.\n\n    Parameters\n    ==========\n\n    field : Vector\n        The field to check for solenoidal property\n\n    Examples\n    ========\n\n    >>> from sympy.physics.vector import ReferenceFrame\n    >>> from sympy.physics.vector import is_solenoidal\n    >>> R = ReferenceFrame('R')\n    >>> is_solenoidal(R[1]*R[2]*R.x + R[0]*R[2]*R.y + R[0]*R[1]*R.z)\n    True\n    >>> is_solenoidal(R[1] * R.y)\n    False\n\n    \"\"\"\n\n    #Field is solenoidal irrespective of frame\n    #Take the first frame in the result of the\n    #separate method in Vector\n    if field == Vector(0):\n        return True\n    frame = list(field.separate())[0]\n    return divergence(field, frame).simplify() == S(0)\n\n\ndef scalar_potential(field, frame):\n    \"\"\"\n    Returns the scalar potential function of a field in a given frame\n    (without the added integration constant).\n\n    Parameters\n    ==========\n\n    field : Vector\n        The vector field whose scalar potential function is to be\n        calculated\n\n    frame : ReferenceFrame\n        The frame to do the calculation in\n\n    Examples\n    ========\n\n    >>> from sympy.physics.vector import ReferenceFrame\n    >>> from sympy.physics.vector import scalar_potential, gradient\n    >>> R = ReferenceFrame('R')\n    >>> scalar_potential(R.z, R) == R[2]\n    True\n    >>> scalar_field = 2*R[0]**2*R[1]*R[2]\n    >>> grad_field = gradient(scalar_field, R)\n    >>> scalar_potential(grad_field, R)\n    2*R_x**2*R_y*R_z\n\n    \"\"\"\n\n    #Check whether field is conservative\n    if not is_conservative(field):\n        raise ValueError(\"Field is not conservative\")\n    if field == Vector(0):\n        return S(0)\n    #Express the field exntirely in frame\n    #Subsitute coordinate variables also\n    _check_frame(frame)\n    field = express(field, frame, variables=True)\n    #Make a list of dimensions of the frame\n    dimensions = [x for x in frame]\n    #Calculate scalar potential function\n    temp_function = integrate(field.dot(dimensions[0]), frame[0])\n    for i, dim in enumerate(dimensions[1:]):\n        partial_diff = diff(temp_function, frame[i + 1])\n        partial_diff = field.dot(dim) - partial_diff\n        temp_function += integrate(partial_diff, frame[i + 1])\n    return temp_function\n\n\ndef scalar_potential_difference(field, frame, point1, point2, origin):\n    \"\"\"\n    Returns the scalar potential difference between two points in a\n    certain frame, wrt a given field.\n\n    If a scalar field is provided, its values at the two points are\n    considered. If a conservative vector field is provided, the values\n    of its scalar potential function at the two points are used.\n\n    Returns (potential at position 2) - (potential at position 1)\n\n    Parameters\n    ==========\n\n    field : Vector/sympyfiable\n        The field to calculate wrt\n\n    frame : ReferenceFrame\n        The frame to do the calculations in\n\n    point1 : Point\n        The initial Point in given frame\n\n    position2 : Point\n        The second Point in the given frame\n\n    origin : Point\n        The Point to use as reference point for position vector\n        calculation\n\n    Examples\n    ========\n\n    >>> from sympy.physics.vector import ReferenceFrame, Point\n    >>> from sympy.physics.vector import scalar_potential_difference\n    >>> R = ReferenceFrame('R')\n    >>> O = Point('O')\n    >>> P = O.locatenew('P', R[0]*R.x + R[1]*R.y + R[2]*R.z)\n    >>> vectfield = 4*R[0]*R[1]*R.x + 2*R[0]**2*R.y\n    >>> scalar_potential_difference(vectfield, R, O, P, O)\n    2*R_x**2*R_y\n    >>> Q = O.locatenew('O', 3*R.x + R.y + 2*R.z)\n    >>> scalar_potential_difference(vectfield, R, P, Q, O)\n    -2*R_x**2*R_y + 18\n\n    \"\"\"\n\n    _check_frame(frame)\n    if isinstance(field, Vector):\n        #Get the scalar potential function\n        scalar_fn = scalar_potential(field, frame)\n    else:\n        #Field is a scalar\n        scalar_fn = field\n    #Express positions in required frame\n    position1 = express(point1.pos_from(origin), frame, variables=True)\n    position2 = express(point2.pos_from(origin), frame, variables=True)\n    #Get the two positions as substitution dicts for coordinate variables\n    subs_dict1 = {}\n    subs_dict2 = {}\n    for i, x in enumerate(frame):\n        subs_dict1[frame[i]] = x.dot(position1)\n        subs_dict2[frame[i]] = x.dot(position2)\n    return scalar_fn.subs(subs_dict2) - scalar_fn.subs(subs_dict1)\n", "import_text": ["sympy.diff", "sympy.integrate", "sympy.S", "sympy.physics.vector.Vector", "sympy.physics.vector.express", "sympy.physics.vector.frame._check_frame", "sympy.physics.vector.vector._check_vector"], "prompt": "\"\"\"\nDescription: This function calculates the divergence of a vector in a given frame.\n\nArgs:\n    vect (type): The vector for which the divergence is to be calculated.\n    frame (type): The frame in which the vector is expressed.\n\nReturns:\n    type: The divergence of the vector in the given frame.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Returns the divergence of a vector field computed wrt the coordinate\n    symbols of the given frame.\n\n    Parameters\n    ==========\n\n    vect : Vector\n        The vector operand\n\n    frame : ReferenceFrame\n        The reference frame to calculate the divergence in\n\n    Examples\n    ========\n\n    >>> from sympy.physics.vector import ReferenceFrame\n    >>> from sympy.physics.vector import divergence\n    >>> R = ReferenceFrame('R')\n    >>> v1 = R[0]*R[1]*R[2] * (R.x+R.y+R.z)\n    >>> divergence(v1, R)\n    R_x*R_y + R_x*R_z + R_y*R_z\n    >>> v2 = 2*R[1]*R[2]*R.y\n    >>> divergence(v2, R)\n    2*R_z\n\n    \"\"\"", "function_dependencies": ["sympy.physics.vector.vector._check_vector", "sympy.S", "sympy.physics.vector.express", "sympy.physics.vector.express.dot", "sympy.diff"], "project_create_time": "2018-01-08T14:28:29+00:00", "project_update_time": "2024-04-16T22:36:42+00:00", "file_create_time": "2019-01-24T21:57:23Z", "file_update_time": "2019-01-24T21:57:23Z", "function_update_time": "2019-01-24T21:57:23Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["sympy.S"], "test_function": [{"file_path": "/python3_ios-v1.0/python3_ios-1.0/site-packages/sympy/physics/vector/tests/test_fieldfunctions.py", "class_name": null, "function_name": "test_divergence", "code": "\ndef test_divergence():\n    assert divergence(Vector(0), R) == S(0)\n    assert divergence(R.x, R) == S(0)\n    assert divergence(R[0]**2*R.x, R) == 2*R[0]\n    assert divergence(R[0]*R[1]*R[2] * (R.x+R.y+R.z), R) == \\\n           R[0]*R[1] + R[0]*R[2] + R[1]*R[2]\n    assert divergence((1/(R[0]*R[1]*R[2])) * (R.x+R.y+R.z), R) == \\\n           -1/(R[0]*R[1]*R[2]**2) - 1/(R[0]*R[1]**2*R[2]) - \\\n           1/(R[0]**2*R[1]*R[2])\n    v = P[0]*P.x + P[1]*P.y + P[2]*P.z\n    assert divergence(v, P) == 3\n    assert divergence(v, R).simplify() == 3\n    assert divergence(P[0]*R.x + R[0]*P.x, R) == 2*cos(q)"}]}, {"git_group": "openai", "git_name": "transformer-debugger", "version": "main", "language": "Python", "project_name": "transformer-debugger-main.zip", "file_path": "/transformer-debugger-main/transformer-debugger-main/neuron_explainer/models/transformer.py", "file_name": "transformer.py", "focal_class": null, "focal_name": "prep_input_and_pad", "focal_parameter": [], "solution": "\ndef prep_input_and_pad(\n    X: list[list[int]], pad_side: str, device: Device = \"cpu\"\n) -> tuple[Tensor, Tensor]:\n    # X is a list of tokenized prompts; prompts may have unequal lengths. This function will\n    # left-pad X by putting \"-1\" in all the slots where a prompt is shorter than the longest prompt.\n    # Then convert X into a tensor of int tokens. Then build the pad tensor by looking for the\n    # \"-1\"s. Then fill the \"-1\"s in X with \"0\"s so the embedding layer doesn't get upset.\n    max_len = max([len(prompt) for prompt in X])\n\n    def pad(x):\n        padding = [-1] * (max_len - len(x))\n        if pad_side == \"left\":\n            return padding + x\n        elif pad_side == \"right\":\n            return x + padding\n        else:\n            raise ValueError(f\"pad_side must be 'left' or 'right', not {pad_side}\")\n\n    X_tensor = torch.LongTensor([pad(prompt) for prompt in X]).to(device)\n    pad = X_tensor == -1\n    X_tensor = torch.where(X_tensor == -1, 0, X_tensor)\n    return X_tensor, pad", "function_signature": "def prep_input_and_pad(\n    X: list[list[int]], pad_side: str, device: Device = \"cpu\"\n) -> tuple[Tensor, Tensor] :", "left_context": "import json\nimport os.path as osp\nimport pickle\nfrom concurrent.futures import ThreadPoolExecutor\nfrom copy import deepcopy\nfrom dataclasses import asdict, dataclass\nfrom functools import cache\nfrom typing import Any, Self, Union\n\nimport numpy as np\nimport tiktoken\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nfrom torch.distributions.categorical import Categorical\nfrom torch.utils.checkpoint import checkpoint\n\nfrom neuron_explainer.file_utils import CustomFileHandler, copy_to_local_cache, file_exists\nfrom neuron_explainer.models.hooks import (\n    AttentionHooks,\n    MLPHooks,\n    NormalizationHooks,\n    TransformerHooks,\n)\n\n# for static analysis\nDevice = Union[torch.device, str]\n\n\n# NOTE: some code from this file related to attention, MLP, and layernorm operations is copy-pasted in\n# neuron_explainer/activations/derived_scalars/reconstituted.py; if those operations change here, they should correspondingly\n# be changed in that file.\n\n\nclass SerializableDataclass:\n    def to_dict(self) -> dict:\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, d) -> Self:\n        return cls(**d)\n\n    def save(self, path: str) -> None:\n        if path.endswith((\".pkl\", \".pickle\")):\n            with CustomFileHandler(path, \"wb\") as f:\n                pickle.dump(self.to_dict(), f)\n        elif path.endswith(\".json\"):\n            with CustomFileHandler(path, \"w\") as f:\n                json.dump(self.to_dict(), f)\n        else:\n            raise ValueError(f\"Unknown file extension for {path}\")\n\n    @classmethod\n    def load(cls, path: str) -> Self:\n        if path.endswith((\".pkl\", \".pickle\")):\n            with CustomFileHandler(path, \"rb\") as f:\n                return cls.from_dict(pickle.load(f))\n        elif path.endswith(\".json\"):\n            with CustomFileHandler(path, \"r\") as f:\n                return cls.from_dict(json.load(f))\n        else:\n            raise ValueError(f\"Unknown file extension for {path}\")\n\n\n@dataclass\nclass TransformerConfig(SerializableDataclass):\n    enc: str = \"gpt2\"\n    ctx_window: int = 1024\n    d_model: int = 256\n    n_layers: int = 2\n\n    # attn\n    m_attn: float = 1\n    n_heads: int = 8\n\n    # mlp\n    m_mlp: float = 4\n\n    @property\n    def d_ff(self) -> int:\n        return int(self.d_model * self.m_mlp)\n\n    @property\n    def d_attn_qk(self) -> int:\n        return int(self.d_model * self.m_attn)\n\n    @property\n    def d_attn_v(self) -> int:\n        return int(self.d_model * self.m_attn)\n\n    @property\n    def d_head_qk(self) -> int:\n        return safe_div(self.d_attn_qk, self.n_heads)\n\n    @property\n    def d_head_v(self) -> int:\n        return safe_div(self.d_attn_v, self.n_heads)\n\n\ndef default_device() -> torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef safe_div(numerator: int, denominator: int) -> int:\n    assert numerator % denominator == 0\n    return numerator // denominator\n\n\n# ====================\n# Attention utilities\n# ====================\n\n\n@cache\ndef causal_attn_mask(size: int, device: Device = \"cpu\") -> Tensor:\n    return torch.tril(torch.ones(size, size)).bool().to(device)\n\n\ndef split_heads(Z: Tensor, n_heads: int) -> Tensor:\n    batch, seq, d_attn = Z.shape\n    return Z.reshape(batch, seq, n_heads, d_attn // n_heads)\n\n\ndef merge_heads(Z: Tensor) -> Tensor:\n    batch, seq, n_heads, d_head = Z.shape\n    return Z.reshape(batch, seq, n_heads * d_head)\n\n\n# ===================================\n# MLP utilities\n# ===================================\n\n\ndef gelu(x: Tensor) -> Tensor:\n    return 0.5 * x * (1 + torch.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    # return x * torch.sigmoid(1.702 * x)\n\n\n# ========================================\n# Sampling, padding and related utilities\n# ========================================\n\n\ndef prep_input_and_right_pad_for_forward_pass(\n    X: list[list[int]], device: Device = \"cpu\"\n) -> tuple[Tensor, Tensor]:\n    # Helper function. The two tensors returned by this function are suitable to be passed to\n    # Transformer.forward.\n    return prep_input_and_pad(X, \"right\", device)\n\n", "right_context": "\n\ndef prep_pos_from_pad_and_prev_lens(pad: Tensor, prev_lens: Tensor) -> Tensor:\n    # pad has shape b x s, prev_lens has shape b x 1.\n    # For position embedding, we need a tensor of shape (b x s) whose\n    # entries are the positions of X in the sequence. When sampling with\n    # prompts of unequal length, X is left padded with pad tokens. The\n    # position tensor needs to take that into account.\n    pos = torch.logical_not(pad).long().cumsum(dim=-1) - 1\n    pos = torch.where(pos == -1, 0, pos)\n    return pos + prev_lens\n\n\ndef nucleus_sample(logits: Tensor, top_p: float) -> Tensor:\n    # top_p in [0,1] is the total probability mass of top outputs.\n    # based on https://nn.labml.ai/sampling/nucleus.html\n    # input shape: [..., n_vocab] -> output shape: [...]\n    sorted_logits, idxs = torch.sort(logits, dim=-1, descending=True)\n    sorted_probs = torch.softmax(sorted_logits, dim=-1)\n    cum_probs = torch.cumsum(sorted_probs, dim=-1)\n\n    # logic to ensure there is always at least one token with nonzero\n    # probability when selecting nucleus.\n    p0 = cum_probs[..., 0]\n    top_p = torch.where(p0 > top_p, p0, top_p)[..., None]\n\n    # sampling\n    do_not_sample = cum_probs > top_p\n    sorted_logits = sorted_logits.masked_fill(do_not_sample, float(\"-inf\"))\n    dist = Categorical(logits=sorted_logits)\n    samples = dist.sample()\n    tokens = idxs.gather(-1, samples.unsqueeze(-1)).squeeze(-1)\n    return tokens\n\n\n# ===============\n# Layer Norm\n# ===============\n\n\nclass Norm(nn.Module):\n    \"\"\"LayerNorm reimplementation with hooks.\"\"\"\n\n    def __init__(\n        self,\n        size: int,\n        eps: float = 1e-5,\n        device: Device | None = None,\n        dtype: torch.dtype | None = None,\n    ):\n        super().__init__()\n        kwargs = {\"device\": device, \"dtype\": dtype}\n        self.size = size\n        self.weight = nn.Parameter(torch.empty(size, **kwargs))  # type: ignore[arg-type]\n        self.bias = nn.Parameter(torch.empty(size, **kwargs))  # type: ignore[arg-type]\n        self.eps = eps\n\n    def forward(self, x: Tensor, hooks: NormalizationHooks = None) -> Tensor:\n        if hooks is None:\n            hooks = NormalizationHooks()\n        # always do norm in fp32\n        orig_dtype = x.dtype\n        x = x.float()\n        x = x - x.mean(axis=-1, keepdim=True)  # [batch, pos, length]\n        x = hooks.post_mean_subtraction(x)\n        scale = torch.sqrt((x**2).mean(dim=-1, keepdim=True) + self.eps)\n        scale = hooks.scale(scale)\n        x = x / scale\n        x = hooks.post_scale(x)\n        ret = x * self.weight + self.bias\n        return ret.to(orig_dtype)\n\n\ndef apply_layernorm_foldin(ln: Norm, linears: list[nn.Linear]) -> None:\n    # folds in a layernorm weight/bias into the next linear layer.\n    # ln(x) = W_ln * (x - x.mean())/(x.std()) + b_ln\n    # linear(ln(x)) = W_linear * (W_ln * (x - x.mean())/(x.std()) + b_ln) + b_linear\n\n    W_ln = ln.weight.float()\n    b_ln = ln.bias.float()\n    for linear in linears:\n        W_linear = linear.weight.float()\n        b_linear = linear.bias.float()\n\n        W_composed = W_linear * W_ln[None, :]\n\n        b_composed = None\n        b_composed = b_linear + W_linear @ b_ln\n\n        # should only copy after new weights are calculated\n        linear.weight.data.copy_(W_composed)\n        linear.bias.data.copy_(b_composed)\n\n    ln.weight.data[:] = 1\n    ln.bias.data[:] = 0\n\n\n# ===========================================\n# Attention layers and associated components\n# ===========================================\n\n\n@dataclass\nclass KeyValueCache:\n    \"\"\"KV cache to save on compute\"\"\"\n\n    K_cache: Tensor | None = None  # b x s_old x d\n    V_cache: Tensor | None = None  # b x s_old x d\n    pad_cache: Tensor | None = None  # b x s_old\n\n    def update(self, K: Tensor, V: Tensor, pad: Tensor):\n        # K, V have shape: b x (s_new - s_old) x d\n        # pad has shape: b x (s_new - s_old)\n        new = self.K_cache is None\n        self.K_cache = K if new else torch.cat([self.K_cache, K], dim=1)\n        self.V_cache = V if new else torch.cat([self.V_cache, V], dim=1)\n        self.pad_cache = pad if new else torch.cat([self.pad_cache, pad], dim=1)\n        return self.K_cache, self.V_cache, self.pad_cache\n\n\nclass MultiHeadedDotProductSelfAttention(nn.Module):\n    \"\"\"A configurable multi-headed dot product attention layer.\"\"\"\n\n    def __init__(\n        self,\n        cfg: TransformerConfig,\n        layer_idx: int,\n        device: Device | None = None,\n        dtype: torch.dtype | None = None,\n    ):\n        super().__init__()\n        self.n_heads = cfg.n_heads\n\n        # make layers\n        kwargs = {\"device\": device, \"dtype\": dtype}\n        self.q_proj = nn.Linear(cfg.d_model, cfg.d_attn_qk, **kwargs)\n        self.k_proj = nn.Linear(cfg.d_model, cfg.d_attn_qk, **kwargs)\n        self.v_proj = nn.Linear(cfg.d_model, cfg.d_attn_v, **kwargs)\n        self.out_proj = nn.Linear(cfg.d_attn_v, cfg.d_model, **kwargs)\n        self.qk_scale = 1 / np.sqrt(np.sqrt(cfg.d_head_qk))\n\n        self.cfg = cfg\n\n    def forward(\n        self,\n        X: Tensor,\n        kv_cache: KeyValueCache | None = None,\n        pad: Tensor | None = None,\n        hooks: AttentionHooks = AttentionHooks(),\n    ) -> tuple[Tensor, KeyValueCache]:\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n\n        # update KV cache\n        if kv_cache is None:\n            kv_cache = KeyValueCache()\n        K, V, pad = kv_cache.update(K, V, pad)\n\n        # split apart heads, rescale QK\n        Q = split_heads(Q, self.n_heads) * self.qk_scale\n        Q = hooks.q(Q)  # bshd\n        K = split_heads(K, self.n_heads) * self.qk_scale\n        K = hooks.k(K)  # bshd\n        V = split_heads(V, self.n_heads)\n        V = hooks.v(V)  # bshd\n\n        # useful for calculations below\n        n_queries, n_keys = Q.shape[1], K.shape[1]\n\n        # softmax multi-headed dot product attention\n        pre_softmax = torch.einsum(\"bqhd,bkhd -> bhqk\", Q, K)\n\n        # apply causal attention mask\n        M = causal_attn_mask(n_keys, device=X.device)\n        M = M[None, None, -n_queries:]  # make M broadcastable to batch, head\n        pre_softmax = pre_softmax.masked_fill(torch.logical_not(M), float(\"-inf\"))\n\n        # apply pad mask\n        if pad is not None and torch.any(pad):\n            # we only mask out pad tokens for non-pad query tokens\n            # (because masking all pad tokens => empty rows => NaNs later)\n            pad_mask = torch.bitwise_xor(pad[:, None, :], pad[:, :, None])\n\n            # make pad broadcastable on head dim, and slice for current queries only\n            pad_mask = pad_mask[:, None, -n_queries:]\n\n            # apply pad mask\n            pre_softmax = pre_softmax.masked_fill(pad_mask, float(\"-inf\"))\n\n        pre_softmax = torch.einsum(\"bhqk->bqkh\", pre_softmax)\n        pre_softmax = hooks.qk_logits(pre_softmax)\n\n        pre_softmax = pre_softmax.float()  # for numerical stability\n        if hooks.qk_softmax_denominator.is_empty():\n            attn = torch.softmax(pre_softmax, dim=-2)\n        else:\n            # factor out softmax in order to hook\n            pre_softmax_max = torch.max(pre_softmax, -2, keepdim=True)[0].detach()\n            numerator = torch.exp(pre_softmax - pre_softmax_max)\n            denominator = numerator.sum(dim=-2, keepdim=True)\n            denominator = hooks.qk_softmax_denominator(denominator)\n            attn = numerator / denominator\n        attn = attn.to(Q.dtype)\n\n        attn = hooks.qk_probs(attn)\n\n        out = torch.einsum(\"bqkh,bkhd->bqhd\", attn, V)\n        out = hooks.v_out(out)\n        out = merge_heads(out)  # concatenate results from all heads\n        # final output projection\n        return self.out_proj(out), kv_cache\n\n\n# =====================================\n# MLP layers and associated components\n# =====================================\n\n\nclass MLP(nn.Module):\n    \"\"\"An MLP for a transformer is a simple two-layer network.\"\"\"\n\n    def __init__(\n        self, cfg: TransformerConfig, device: Device | None = None, dtype: torch.dtype | None = None\n    ):\n        super().__init__()\n        kwargs = {\"device\": device, \"dtype\": dtype}\n        self.in_layer = nn.Linear(cfg.d_model, cfg.d_ff, **kwargs)\n        self.out_layer = nn.Linear(cfg.d_ff, cfg.d_model, **kwargs)\n        self.act = gelu\n\n    def forward(self, X: Tensor, hooks: MLPHooks = MLPHooks()) -> Tensor:\n        pre = self.in_layer(X)\n        pre = hooks.pre_act(pre)\n        a = self.act(pre)\n        a = hooks.post_act(a, out_layer=self.out_layer)\n        out = self.out_layer(a)\n        return out\n\n\n# =============\n# Transformers\n# =============\n\n\nclass TransformerLayer(nn.Module):\n    def __init__(\n        self,\n        cfg: TransformerConfig,\n        layer_idx: int,\n        device: Device | None = None,\n        dtype: torch.dtype | None = None,\n    ):\n        super().__init__()\n        kwargs = {\"device\": device, \"dtype\": dtype}\n        self.cfg = cfg\n        self.attn = MultiHeadedDotProductSelfAttention(cfg, layer_idx, **kwargs)\n        self.mlp = MLP(cfg, **kwargs)\n        self.ln_1 = Norm(cfg.d_model, **kwargs)\n        self.ln_2 = Norm(cfg.d_model, **kwargs)\n        self.layer_idx = layer_idx\n\n    def simplify(self) -> None:\n        ln_1_linears: list[Any] = [\n            self.attn.q_proj,\n            self.attn.k_proj,\n            self.attn.v_proj,\n        ]\n        apply_layernorm_foldin(self.ln_1, ln_1_linears)\n\n        ln_2_linears: list[Any] = [self.mlp.in_layer]\n        apply_layernorm_foldin(self.ln_2, ln_2_linears)\n\n    def attn_block(\n        self, X: Tensor, kv_cache: KeyValueCache | None, pad: Tensor | None, hooks: TransformerHooks\n    ) -> Tensor:\n        ln_X = self.ln_1(X, hooks.resid.torso.ln_attn)\n        ln_X = hooks.resid.torso.post_ln_attn(ln_X)\n        attn_delta, kv_cache = self.attn(ln_X, kv_cache, pad, hooks.attn)\n        attn_delta = hooks.resid.torso.delta_attn(attn_delta)\n        return attn_delta, kv_cache\n\n    def mlp_block(self, X: Tensor, hooks: TransformerHooks) -> Tensor:\n        ln_X = self.ln_2(X, hooks.resid.torso.ln_mlp)\n        ln_X = hooks.resid.torso.post_ln_mlp(ln_X)\n        mlp_delta = self.mlp(ln_X, hooks.mlp)\n        mlp_delta = hooks.resid.torso.delta_mlp(mlp_delta)\n        return mlp_delta\n\n    def forward(\n        self,\n        X: Tensor,\n        kv_cache: KeyValueCache | None = None,\n        pad: Tensor | None = None,\n        hooks: TransformerHooks = TransformerHooks(),\n    ) -> tuple[Tensor, KeyValueCache]:\n        attn_delta, kv_cache = self.attn_block(X, kv_cache, pad, hooks)\n        X = X + attn_delta\n        X = hooks.resid.torso.post_attn(X)\n        mlp_delta = self.mlp_block(X, hooks)\n        X = X + mlp_delta\n        X = hooks.resid.torso.post_mlp(X)\n        return X, kv_cache\n\n\nclass HiddenState:\n    \"\"\"A hidden state for a transformer. Tracks prompt lengths and KV caches.\"\"\"\n\n    def __init__(self, n_layers: int):\n        self.prev_lens = 0\n        self.kv_caches = [None for _ in range(n_layers)]\n\n    def set_prev_lens(self, prev_lens) -> None:\n        self.prev_lens = prev_lens\n\n    def __getitem__(self, idx: int):\n        return self.kv_caches[idx]\n\n    def __setitem__(self, idx: int, value: KeyValueCache | None):\n        self.kv_caches[idx] = value\n\n\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        cfg: TransformerConfig,\n        # recomputing is optional, and it trades off compute for memory.\n        recompute: bool = False,\n        device: Device | None = None,\n        dtype: torch.dtype | None = None,\n    ):\n        super().__init__()\n        self.cfg = cfg\n        self.enc = tiktoken.get_encoding(self.cfg.enc)\n        self.n_vocab = self.enc.n_vocab\n        self.recompute = recompute\n        self.dtype = dtype\n\n        # build network\n        kwargs = {\"device\": device, \"dtype\": dtype}\n        self.tok_embed = nn.Embedding(self.n_vocab, cfg.d_model, **kwargs)\n        self.pos_embed = nn.Embedding(cfg.ctx_window, cfg.d_model, **kwargs)\n        self.xf_layers = nn.ModuleList(\n            [TransformerLayer(cfg, idx, **kwargs) for idx in range(cfg.n_layers)]\n        )\n        self.final_ln = Norm(cfg.d_model, **kwargs)\n        self.unembed = nn.Linear(cfg.d_model, self.n_vocab, bias=False, **kwargs)\n\n    def simplify(self):\n        for xf_layer in self.xf_layers:\n            xf_layer.simplify()\n\n        # NOTE: we can't fold layer norm into unembedding layer\n        # because it has no bias\n        # apply_layernorm_foldin(self.final_ln, [self.unembed])\n\n    @property\n    def device(self) -> Device:\n        return next(self.parameters()).device\n\n    def set_recompute(self, recompute: bool) -> None:\n        self.recompute = recompute\n\n    def forward(\n        self,\n        tokens: Tensor,\n        H: HiddenState | None = None,\n        pad: Tensor | None = None,\n        hooks: TransformerHooks = TransformerHooks(),\n    ) -> tuple[Tensor, HiddenState]:\n        \"\"\"\n        Forward pass through the transformer!\n\n        During evaluation or first forward pass in sampling:\n            X is expected to be a [batch_size x sequence_length]-shaped LongTensor of encoded prompts.\n            H is expected to be None.\n            pad is a [batch_size x sequence_length]-shaped boolean Tensor. \"1\"s mean \"ignore this\n            token\". This parameter must be set if not all encoded prompts in X have the same length.\n            Note that activations observed by hooks will include padded values.\n\n        During sampling after first forward pass:\n            X is expected to be the new part of the sequences (eg most recently sampled tokens).\n            H is expected to have KV-caches of all Keys and Values for prior tokens.\n            pad is expected to be None (new tokens are not pad tokens).\n\n        Returns a tuple containing the resulting logits tensor and a new hidden state consisting of a KV cache.\n        \"\"\"\n        X, H, pad, hooks = self.run_embed(tokens, H, pad, hooks)\n        X, H, pad, hooks = self.run_torso(X, H, pad, hooks)\n        return self.run_unembed(X, H, hooks)\n\n    def run_embed(\n        self,\n        tokens: Tensor,\n        H: HiddenState | None = None,\n        pad: Tensor | None = None,\n        hooks: TransformerHooks = TransformerHooks(),\n    ) -> tuple[Tensor, HiddenState, Tensor | None, TransformerHooks]:\n        assert tokens.dtype == torch.long, \"tokens must be sequences of tokens.\"\n        if H is None:\n            H = HiddenState(self.cfg.n_layers)\n        if pad is None:\n            pad = torch.zeros_like(tokens, dtype=torch.bool)\n\n        # embedding\n        X = self.tok_embed(tokens)\n        # position encoding logic to support sampling with prompts of unequal length.\n        pos = prep_pos_from_pad_and_prev_lens(pad, H.prev_lens)\n        seq_lens = (pos[:, -1] + 1).unsqueeze(-1)\n        assert all(\n            seq_lens <= self.cfg.ctx_window\n        ), f\"sequences must fit in the context window {self.cfg.ctx_window}.\"\n        H.set_prev_lens(seq_lens)\n        X = X + self.pos_embed(pos)\n\n        X = hooks.resid.post_emb(X)\n        return X, H, pad, hooks\n\n    def run_torso(\n        self,\n        X: Tensor,\n        H: HiddenState | None,\n        pad: Tensor | None,\n        hooks: TransformerHooks,\n    ) -> tuple[Tensor, HiddenState, Tensor | None, TransformerHooks]:\n        # transformer torso\n        for i, xf_layer in enumerate(self.xf_layers):\n            hooks_layer_i = deepcopy(hooks).bind(layer=i)\n            if self.recompute:\n                X, H[i] = checkpoint(xf_layer, X, H[i], pad, hooks_layer_i)\n            else:\n                X, H[i] = xf_layer(X, H[i], pad, hooks_layer_i)\n        return X, H, pad, hooks\n\n    def run_ln_f(\n        self,\n        X: Tensor,\n        H: HiddenState | None,\n        hooks: TransformerHooks,\n    ) -> tuple[Tensor, HiddenState, TransformerHooks]:\n        X = self.final_ln(X, hooks.resid.ln_f)\n        X = hooks.resid.post_ln_f(X)\n        return X, H, hooks\n\n    def run_unembed(\n        self,\n        X: Tensor,\n        H: HiddenState | None,\n        hooks: TransformerHooks,\n    ) -> tuple[Tensor, HiddenState]:\n        # unembedding\n        X, H, hooks = self.run_ln_f(X, H, hooks)\n        X = self.unembed(X)\n        X = hooks.logits(X)\n        return X, H\n\n    def sample(\n        self,\n        prompts: str | list[str] | list[int] | list[list[int]],\n        num_tokens: int = 5,\n        temperature: float = 1.0,\n        top_p: float | None = None,\n        hooks: TransformerHooks = TransformerHooks(),\n    ) -> dict[str, Any]:\n        \"\"\"\n        Sampling with the transformer!\n\n        If top_p is set, then nucleus sampling is used.\n        Otherwise, the sampling will be Categorical.\n        If temperature=0, sampling is deterministic (and top_p is ignored).\n\n        (Warning: when using torch.use_deterministic_algorithms(True),\n        nucleus sampling will throw an error. It depends on torch.cumsum,\n        which unfortunately has no deterministic implementation in torch.)\n\n        Output is a dict {'tokens': list[list[int]], 'strings': list[str]}\n        \"\"\"\n        prompts = [prompts] if isinstance(prompts, str) else prompts\n        if isinstance(prompts[0], str):\n            X: list[list[int]] = [self.enc.encode(prompt) for prompt in prompts]\n        elif isinstance(prompts[0], int):\n            X = [prompts]\n        else:\n            X = prompts\n        X, pad = prep_input_and_pad(X, \"left\", self.device)\n        H = None\n        beta = 1 / max(temperature, 1e-10)\n        out = {\n            \"tokens\": [[] for _ in prompts],\n            \"strings\": [\"\" for _ in prompts],\n        }\n\n        # sampling loop\n        for _ in range(num_tokens):\n            with torch.no_grad():\n                # get logits\n                Y, H = self.forward(X, H, pad, hooks=hooks)\n                logits = Y[:, -1] * beta\n\n                # sampling only works if logits are floats\n                logits = logits.float()\n\n                # perform sampling\n                if temperature == 0:\n                    tokens = torch.argmax(logits, dim=-1)\n                elif top_p is not None:\n                    tokens = nucleus_sample(logits, top_p)\n                else:\n                    tokens = Categorical(logits=logits).sample()\n                X, pad = tokens.unsqueeze(-1), None\n\n            for batch_idx, token in enumerate(tokens):\n                out[\"tokens\"][batch_idx].append(token.item())\n                out[\"strings\"][batch_idx] += self.enc.decode([token.item()])\n\n        return out\n\n    @classmethod\n    def load(\n        cls,\n        name_or_path: str,\n        device: Device | None = None,\n        dtype: torch.dtype | None = None,\n        simplify: bool = False,\n        simplify_kwargs: dict[str, Any] | None = None,\n    ) -> \"Transformer\":\n        if name_or_path.startswith(\"https://\"):\n            path = name_or_path\n        else:\n            path = f\"https://openaipublic.blob.core.windows.net/neuron-explainer/subject-models/{name_or_path.replace('-', '/')}\"\n        xf = cls.from_checkpoint(\n            path,\n            device=device,\n            dtype=dtype,\n        )\n        if simplify:\n            if simplify_kwargs is None:\n                simplify_kwargs = {}\n            xf.simplify(**simplify_kwargs)\n        return xf\n\n    def save_checkpoint(\n        self,\n        path: str,\n    ) -> None:\n        self.cfg.save(osp.join(path, \"config.json\"))\n\n        pieces_path = osp.join(path, \"model_pieces\")\n        for k, v in self.state_dict().items():\n            with CustomFileHandler(osp.join(pieces_path, f\"{k}.pt\"), \"wb\") as f:\n                torch.save(v, f)\n\n    def load_state_from_checkpoint(\n        self, path: str, device: Device | None = None, dtype: torch.dtype | None = None\n    ):\n        pieces_path = osp.join(path, \"model_pieces\")\n        piece_names = set(self.state_dict().keys())\n        piece_files = [f\"{k}.pt\" for k in piece_names]\n\n        if dtype is not None:\n            assert isinstance(dtype, torch.dtype), \"Must provide valid dtype.\"\n        device = device or self.device\n\n        with ThreadPoolExecutor(max_workers=50) as executor:\n            k_to_future = {\n                fname[: -len(\".pt\")]: executor.submit(\n                    _load_piece, osp.join(pieces_path, fname), device, dtype\n                )\n                for fname in piece_files\n            }\n            d = {k: future.result() for k, future in k_to_future.items()}\n\n        self.load_state_dict(d)\n\n    @classmethod\n    def from_checkpoint(\n        cls, path: str, device: Device | None = None, dtype: torch.dtype | None = None\n    ) -> \"Transformer\":\n        if device is None:\n            device = default_device()\n        cfg = TransformerConfig.load(osp.join(path, \"config.json\"))\n        xf = cls(cfg, device=device, dtype=dtype)\n        xf.load_state_from_checkpoint(path, device=device, dtype=dtype)\n        return xf\n\n\ndef _load_piece(\n    file_path: str, device: Device, dtype: torch.dtype | None\n) -> tuple[str, torch.Tensor]:\n    disk_cache_path = osp.join(\n        \"/tmp/neuron-explainer-model-pieces-cache\", file_path.replace(\"https://\", \"\")\n    )\n    if not file_exists(disk_cache_path):\n        copy_to_local_cache(file_path, disk_cache_path)\n\n    with CustomFileHandler(disk_cache_path, \"rb\") as f:\n        t = torch.load(f, map_location=device)\n        if dtype is not None:\n            t = t.to(dtype)\n    return t\n", "import_text": ["json", "os.path", "pickle", "concurrent.futures.ThreadPoolExecutor", "copy.deepcopy", "dataclasses.asdict", "dataclasses.dataclass", "functools.cache", "typing.Any", "typing.Self", "typing.Union", "numpy", "tiktoken", "torch", "torch.nn", "torch.Tensor", "torch.distributions.categorical.Categorical", "torch.utils.checkpoint.checkpoint", "neuron_explainer.file_utils.CustomFileHandler", "neuron_explainer.file_utils.copy_to_local_cache", "neuron_explainer.file_utils.file_exists", "neuron_explainer.models.hooks.AttentionHooks", "neuron_explainer.models.hooks.MLPHooks", "neuron_explainer.models.hooks.NormalizationHooks", "neuron_explainer.models.hooks.TransformerHooks"], "prompt": "\"\"\"\nDescription: This function prepares input data and pads it to the same length.\n\nArgs:\n    X (list[list[int]]): A list of tokenized prompts. Each prompt is a list of integers.\n    pad_side (str): The side to pad the prompts. Can be either 'left' or 'right'.\n    device (Device, optional): The device to use for the tensors. Defaults to \"cpu\".\n\nReturns:\n    tuple[Tensor, Tensor]: A tuple containing two tensors. The first tensor is the padded input data,\n    and the second tensor is a mask tensor indicating the padding positions.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["torch.LongTensor", "torch.LongTensor.to", "torch.where"], "project_create_time": "2024-03-11T23:06:25+00:00", "project_update_time": "2024-04-18T02:55:19+00:00", "file_create_time": "2024-03-11T23:11:03Z", "file_update_time": "2024-03-15T17:50:38Z", "function_update_time": "2024-03-11T23:11:03Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["torch.LongTensor", "torch.where"], "test_function": [{"file_path": "/transformer-debugger-main/transformer-debugger-main/neuron_explainer/tests/test_transformer.py", "class_name": null, "function_name": "test_sample_utilities", "code": "def test_sample_utilities():\n\n    # ========================\n    # test prep_input_and_pad\n    # ========================\n    # make a set of test prompts of unequal lengths\n    test_inputs = [\n        [1, 100, 50, 47],  # prompt 0\n        [1298, 618, 952, 223, 4, 42],  # prompt 1\n        [31],  # prompt 2\n    ]\n    X_ref = torch.LongTensor(\n        [\n            [0, 0, 1, 100, 50, 47],\n            [1298, 618, 952, 223, 4, 42],\n            [0, 0, 0, 0, 0, 31],\n        ]\n    )\n    pad_ref = torch.BoolTensor(\n        [\n            [1, 1, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0],\n            [1, 1, 1, 1, 1, 0],\n        ]\n    )\n    X, pad = prep_input_and_pad(test_inputs, pad_side=\"left\", device=\"cpu\")\n\n    assert_equal(X_ref, X, msg=\"prep_input_and_pad produced incorrect X\")\n    assert_equal(pad_ref, pad, msg=\"prep_input_and_pad produced incorrect pad\")\n\n    # =====================================\n    # test prep_pos_from_pad_and_prev_lens\n    # =====================================\n    # based on the same example as before, compute the associated pos\n\n    pos_ref = torch.LongTensor([[0, 0, 0, 1, 2, 3], [0, 1, 2, 3, 4, 5], [0, 0, 0, 0, 0, 0]])\n    prev_lens = torch.zeros(3, 1).long()\n    pos = prep_pos_from_pad_and_prev_lens(pad_ref, prev_lens)\n\n    assert_equal(pos_ref, pos, msg=\"prep_pos_from_pad_and_prev_lens produced incorrect pos\")\n\n    # simulate one step forward of sampling\n    seq_lens = (pos[:, -1] + 1).unsqueeze(-1)\n    new_pad_ref = torch.BoolTensor(\n        [\n            [0],\n            [0],\n            [0],\n        ]\n    )\n    new_pos = prep_pos_from_pad_and_prev_lens(new_pad_ref, seq_lens)\n\n    new_pos_ref = torch.LongTensor([[4], [6], [1]])\n    assert_equal(new_pos_ref, new_pos, msg=\"prep_pos_from_pad_and_prev_lens produced incorrect pos\")"}]}, {"git_group": "landlab", "git_name": "landlab", "version": "v2.7.0", "language": "Python", "project_name": "landlab-v2.7.0.zip", "file_path": "/landlab-v2.7.0/landlab-2.7.0/landlab/graph/framed_voronoi/framed_voronoi.py", "file_name": "framed_voronoi.py", "focal_class": "HorizontalRectVoronoiGraph", "focal_name": "xy_of_node", "focal_parameter": ["shape"], "solution": "    def xy_of_node(\n        shape,\n        xy_spacing=(1.0, 1.0),\n        xy_of_lower_left=(0.0, 0.0),\n        xy_min_spacing=(0.5, 0.5),\n        seed=200,\n    ):\n\n        n_rows, n_cols = shape\n        max_move = (\n            (xy_spacing[0] - xy_min_spacing[0]) / 2,\n            (xy_spacing[1] - xy_min_spacing[1]) / 2,\n        )\n\n        if max_move[0] < 0.0 or max_move[1] < 0.0:\n            raise ValueError(\"minimum spacing must be greater than node spacing\")\n        if np.allclose(max_move, 0.0):\n            raise ValueError(\"at least one of x and y moves must be greater than zero\")\n\n        # Generation of a rectangular grid, coordinates must be float\n        x_of_node, y_of_node = np.meshgrid(\n            np.arange(n_cols, dtype=float) * xy_spacing[0] + xy_of_lower_left[0],\n            np.arange(n_rows, dtype=float) * xy_spacing[1] + xy_of_lower_left[1],\n        )\n        # Randomly move the coordinates of the core nodes of the grid. Move\n        # below +/- (spacing - min_spacing) / 2\n        xy_random_generator = np.random.default_rng(seed=seed)\n\n        x_moves = xy_random_generator.uniform(-max_move[0], max_move[0], shape)\n        y_moves = xy_random_generator.uniform(-max_move[1], max_move[1], shape)\n\n        x_of_node[1:-1, 1:-1] += x_moves[1:-1, 1:-1]\n        y_of_node[1:-1, 1:-1] += y_moves[1:-1, 1:-1]\n        # Control the node id attribution for left and right edge. For instance, for a 3x3 grid,\n        # make sure that node 3 is at the left of the 2nd row and node 5 at the right.\n        # For this, for each core row, set y of the leftmost node as the minimal y of the row\n        # and set y of the rightmost node as the maximal y of the row\n        for i in range(1, n_rows - 1):\n            y_of_node[i, 0] -= max_move[1] + 1.0e-3\n            y_of_node[i, n_cols - 1] += max_move[1] + 1.0e-3\n\n        return x_of_node.reshape(-1), y_of_node.reshape(-1)", "function_signature": "def xy_of_node(\n        shape,\n        xy_spacing=(1.0, 1.0),\n        xy_of_lower_left=(0.0, 0.0),\n        xy_min_spacing=(0.5, 0.5),\n        seed=200,\n    ) :", "left_context": "\"\"\" Implementation of the FramedVoronoiGraph and its static layout:\nHorizontalRectVoronoiGraph. This pattern is inspired from the developments of the HexModelGrid\n\n.. codeauthor:: sebastien lenard\n\"\"\"\n\nfrom functools import cached_property\n\nimport numpy as np\n\nfrom ...utils.decorators import make_return_array_immutable\nfrom ..graph import Graph\nfrom ..voronoi.voronoi import DelaunayGraph\n\n\nclass HorizontalRectVoronoiGraph:\n    \"\"\"The horizontal rectangular frame for the FramedVoronoiGraph.\"\"\"\n\n    @staticmethod\n    def number_of_nodes(shape):\n        \"\"\"\n        Parameters\n        ----------\n        shape: tuple of int\n            Number of rows and number of columns\n\n        Returns\n        -------\n        int\n            Number of nodes\n\n        Examples\n        --------\n        >>> from landlab.graph.framed_voronoi.framed_voronoi import (\n        ...     HorizontalRectVoronoiGraph,\n        ... )\n        >>> HorizontalRectVoronoiGraph.number_of_nodes((3, 2))\n        6\n        \"\"\"\n        n_rows, n_cols = shape\n        return n_rows * n_cols\n\n    @staticmethod", "right_context": "\n    @staticmethod\n    def corner_nodes(shape):\n        \"\"\"\n        Parameters\n        ----------\n        shape: tuple of int\n            Number of rows and number of columns\n\n        Returns\n        -------\n        ndarray of int\n            Ids of the corner nodes\n\n        Examples\n        --------\n        >>> from landlab.graph.framed_voronoi.framed_voronoi import (\n        ...     HorizontalRectVoronoiGraph,\n        ... )\n        >>> HorizontalRectVoronoiGraph.corner_nodes((3, 4))\n        (11, 8, 0, 3)\n        \"\"\"\n        n_rows, n_cols = shape\n        return (n_rows * n_cols - 1, n_cols * (n_rows - 1), 0, n_cols - 1)\n\n    @staticmethod\n    def number_of_perimeter_nodes(shape):\n        \"\"\"\n        Parameters\n        ----------\n        shape: tuple of int\n            Number of rows and number of columns\n\n        Returns\n        -------\n        int\n            Number of perimeter nodes\n\n        Examples\n        --------\n        >>> from landlab.graph.framed_voronoi.framed_voronoi import (\n        ...     HorizontalRectVoronoiGraph,\n        ... )\n        >>> HorizontalRectVoronoiGraph.number_of_perimeter_nodes((3, 4))\n        10\n        \"\"\"\n        if 1 in shape:\n            return np.prod(shape)\n        return 2 * shape[0] + 2 * (shape[1] - 2)\n\n    @staticmethod\n    def perimeter_nodes(shape):\n        \"\"\"\n        Parameters\n        ----------\n        shape: tuple of int\n            Number of rows and number of columns\n\n        Returns\n        -------\n        ndarray of int\n            Ids of the perimeter nodes\n\n        Examples\n        --------\n        >>> from landlab.graph.framed_voronoi.framed_voronoi import (\n        ...     HorizontalRectVoronoiGraph,\n        ... )\n        >>> HorizontalRectVoronoiGraph.perimeter_nodes((3, 3))\n        array([2, 5, 8, 7, 6, 3, 0, 1])\n        \"\"\"\n        return np.concatenate(HorizontalRectVoronoiGraph.nodes_at_edge(shape))\n\n    @staticmethod\n    def nodes_at_edge(shape):\n        \"\"\"\n        Parameters\n        ----------\n        shape: tuple of int\n            Number of rows and number of columns\n\n        Returns\n        -------\n        right, top, left, bottom : ndarray of int\n            For each edge give the ids of the nodes present at the edge\n\n        Examples\n        --------\n        >>> from landlab.graph.framed_voronoi.framed_voronoi import (\n        ...     HorizontalRectVoronoiGraph,\n        ... )\n        >>> HorizontalRectVoronoiGraph.nodes_at_edge((3, 3))\n        (array([2, 5]), array([8, 7]), array([6, 3]), array([0, 1]))\n        \"\"\"\n        n_rows, n_cols = shape\n        if n_rows == n_cols == 1:\n            return (np.array([0]),) + (np.array([], dtype=int),) * 3\n        (\n            northeast,\n            northwest,\n            southwest,\n            southeast,\n        ) = HorizontalRectVoronoiGraph.corner_nodes(shape)\n\n        if n_rows > 1:\n            south = np.arange(southwest, southeast)\n        else:\n            south = np.array([southwest], dtype=int)\n\n        if n_cols > 1:\n            west = np.arange(northwest, southwest, -n_cols)\n        else:\n            west = np.array([northwest], dtype=int)\n\n        return (\n            np.arange(southeast, northeast, n_cols),\n            np.arange(northeast, northwest, -1),\n            west,\n            south,\n        )\n\n\nclass FramedVoronoiGraph(DelaunayGraph):\n    \"\"\"VoronoiDelaunay graph based on a fixed lattice.\n\n    Graph of an unstructured grid of Voronoi Delaunay cells and\n    irregular patches. It is a special type of :class`~.VoronoiDelaunayGraph` in which\n    the initial set of points is arranged in a fixed lattice (e.g. like a\n    :class:`~.RasterModelGrid`) named here \"layout\" and the core points are\n    then moved from their initial position by a random distance, lower than a\n    certain threshold.\n\n    Examples\n    --------\n    >>> from landlab.graph import FramedVoronoiGraph\n\n    >>> graph = FramedVoronoiGraph((3, 3), seed=200)\n    >>> graph.number_of_nodes\n    9\n\n    >>> graph.x_of_node[2:4]\n    array([ 2.,  0.])\n    >>> graph.y_of_node[2:4]\n    array([ 0.   ,  0.749])\n    >>> graph.y_of_node[5]\n    1.2509999999999999\n\n    >>> graph.number_of_links\n    16\n    >>> graph.number_of_patches\n    8\n    \"\"\"\n\n    def __init__(\n        self,\n        shape,\n        xy_spacing=(1.0, 1.0),\n        xy_of_lower_left=(0.0, 0.0),\n        sort=False,\n        xy_min_spacing=(0.5, 0.5),\n        seed=200,\n    ):\n        \"\"\"Create the graph.\n\n        Parameters\n        ----------\n        shape : tuple of int\n            Number of rows and columns of nodes.\n        xy_spacing : float or tuple of float, optional\n            Node spacing along *x* and *y* coordinates. If ``float``, same spacing *x* and *y*\n            spacing.\n        xy_of_lower_left : tuple, optional\n            Minimum *x*-of-node and *y*-of-node values. Depending on the grid,\n            a node may not be present at this location.\n        sort: bool\n            If ``True``, nodes, links and patches are re-numbered according\n            certain their positions.  Currently not used.\n        xy_min_spacing: float or tuple of float, optional\n            Final minimal spacing between nodes. Random moves of the core nodes\n            around their position cannot be above this threshold:\n            ``(xy_spacing - xy_min_spacing) / 2``\n            If ``float``, same minimal spacing for *x* and *y*.\n        seed: int, optional\n            Seed used to generate the random *x* and *y* moves.\n            When set, controls a pseudo-randomness of moves to ensure\n            reproducibility. When ``None``, seed is random and the moves of coordinates are\n            completely random.\n\n        Returns\n        -------\n        FramedVoronoiGraph\n            A newly-created graph.\n\n        Examples\n        --------\n        Create a grid with 3 rows and 2 columns of nodes.\n\n        >>> from landlab.graph import FramedVoronoiGraph\n        >>> graph = FramedVoronoiGraph((3, 2))\n        >>> graph.number_of_nodes\n        6\n        \"\"\"\n        # 1. Check and format input arguments\n        #####################################\n        self._shape = shape\n        self._seed = seed\n\n        try:\n            xy_spacing = np.asfarray(np.broadcast_to(xy_spacing, 2))\n        except TypeError as exc:\n            raise TypeError(\"spacing must be a float or a tuple of floats\") from exc\n        else:\n            self._xy_spacing = xy_spacing[0], xy_spacing[1]\n\n        try:\n            xy_of_lower_left = np.asfarray(np.broadcast_to(xy_of_lower_left, 2))\n        except TypeError as exc:\n            raise TypeError(\n                \"xy of lower left must be a float or a tuple of floats\"\n            ) from exc\n        else:\n            self._xy_of_lower_left = xy_of_lower_left[0], xy_of_lower_left[1]\n\n        node_layout = self._node_layout = \"rect\"\n        orientation = self._orientation = \"horizontal\"\n\n        layouts = {\n            \"horizontal_rect\": HorizontalRectVoronoiGraph,\n        }\n        layout = layouts[\"_\".join([orientation, node_layout])]\n\n        try:\n            xy_min_spacing = np.asfarray(np.broadcast_to(xy_min_spacing, 2))\n        except TypeError as exc:\n            raise TypeError(\n                \"minimal spacing must be a float or a tuple of floats\"\n            ) from exc\n        else:\n            self._xy_min_spacing = xy_min_spacing[0], xy_min_spacing[1]\n\n        # 2. Construction of the layout and the x-y coordinates of nodes\n        ################################################################\n        x_of_node, y_of_node = layout.xy_of_node(\n            self._shape,\n            xy_spacing=self._xy_spacing,\n            xy_of_lower_left=self._xy_of_lower_left,\n            xy_min_spacing=self._xy_min_spacing,\n            seed=self._seed,\n        )\n\n        # 3. Determination of the perimeter and edge nodes\n        #########################################\n        self._perimeter_nodes = layout.perimeter_nodes(self._shape)\n\n        (right, top, left, bottom) = layout.nodes_at_edge(self._shape)\n        self._nodes_at_right_edge = np.sort(np.append(right, top[0]))\n        self._nodes_at_top_edge = np.sort(np.append(top, left[0]))\n        self._nodes_at_left_edge = np.sort(np.append(left, bottom[0]))\n        self._nodes_at_bottom_edge = np.sort(np.append(bottom, right[0]))\n\n        perimeter_links = np.empty((len(self._perimeter_nodes), 2), dtype=int)\n        perimeter_links[:, 0] = self._perimeter_nodes\n        perimeter_links[:-1, 1] = self._perimeter_nodes[1:]\n        perimeter_links[-1, 1] = self._perimeter_nodes[0]\n\n        self._x_of_node = x_of_node\n        self._y_of_node = y_of_node\n        self._perimeter_links = perimeter_links\n\n        # 3. Instantiation of the parent class\n        ######################################\n        if 1 in shape:\n            Graph.__init__(\n                self,\n                (y_of_node, x_of_node),\n                links=list(\n                    zip(np.arange(len(y_of_node) - 1), np.arange(1, len(y_of_node)))\n                ),\n                sort=False,\n            )\n        else:\n            DelaunayGraph.__init__(\n                self,\n                (y_of_node, x_of_node),\n                perimeter_links=perimeter_links,\n                sort=False,\n            )\n\n    @property\n    def shape(self):\n        return self._shape\n\n    @property\n    def xy_spacing(self):\n        return self._xy_spacing\n\n    @property\n    def orientation(self):\n        return self._orientation\n\n    @property\n    def node_layout(self):\n        return self._node_layout\n\n    @cached_property\n    @make_return_array_immutable\n    def perimeter_nodes(self):\n        return self._perimeter_nodes\n\n    @property\n    @make_return_array_immutable\n    def nodes_at_right_edge(self):\n        return self._nodes_at_right_edge\n\n    @property\n    @make_return_array_immutable\n    def nodes_at_top_edge(self):\n        return self._nodes_at_top_edge\n\n    @property\n    @make_return_array_immutable\n    def nodes_at_left_edge(self):\n        return self._nodes_at_left_edge\n\n    @property\n    @make_return_array_immutable\n    def nodes_at_bottom_edge(self):\n        return self._nodes_at_bottom_edge\n", "import_text": ["functools.cached_property", "numpy"], "prompt": "\"\"\"\nDescription: This function generates a grid of nodes with randomized coordinates.\n\nArgs:\n    shape (tuple): The shape of the grid (number of rows, number of columns).\n    xy_spacing (tuple, optional): The spacing between nodes in the x and y directions. Defaults to (1.0, 1.0).\n    xy_of_lower_left (tuple, optional): The coordinates of the lower left corner of the grid. Defaults to (0.0, 0.0).\n    xy_min_spacing (tuple, optional): The minimum spacing between nodes in the x and y directions. Defaults to (0.5, 0.5).\n    seed (int, optional): The seed for the random number generator. Defaults to 200.\n\nReturns:\n    tuple: The x and y coordinates of the nodes, reshaped into a single array.\n\nRaises:\n    ValueError: If the minimum spacing is greater than the node spacing or if at least one of x and y moves is not greater than zero.\n\nNotes:\n    This function uses the numpy.allclose, numpy.random.default_rng, and numpy.meshgrid APIs.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"The x and y coordinates of the graph's nodes.\n\n        Calculation of the x-y coordinates is done following these steps:\n\n        1. Generate a rectangular, regular meshgrid.\n        2. Move the coordinates of the core nodes over a random distance around their\n           initial position, within a threshold calculated from *xy_spacing* and\n           *xy_min_spacing*.\n        3. Rectify the y-coordinates of the nodes of the left and right to ensure\n           that the leftmost node of a row has a lower y than the rightmost node.\n           This ensures that the ids of these nodes are not modified by subsequent\n           sorting operations on the graph and make it possible to get the\n           perimeter nodes in simple way.\n\n        Parameters\n        ----------\n        shape : tuple of int\n            Number of rows and columns of nodes.\n        xy_spacing : float or tuple of float, optional\n            Node spacing along x and y coordinates. If ``float``, same spacing at *x* and *y*.\n        xy_of_lower_left : tuple, optional\n            Minimum *x*-of-node and *y*-of-node values. Depending on the grid,\n            a node may not be present at this location.\n        xy_min_spacing: float or tuple of float, optional\n            Final minimal spacing between nodes. Random moves of the core nodes\n            around their initial positions cannot be above this threshold:\n            ``(xy_spacing - xy_min_spacing) / 2``.  If ``float``, same minimal\n            spacing for *x* and *y*.\n        seed: int, optional\n            Seed used to generate the random *x* and *y* moves. When set, controls a\n            pseudo-randomness of moves to ensure reproducibility.\n            When ``None``, seed is random and the moves of coordinates are\n            completely random.\n\n        Returns\n        -------\n        x_of_node, y_of_node : ndarray of float\n            The arrays of *x* and *y* coordinates.\n\n        Examples\n        --------\n        >>> from landlab.graph.framed_voronoi.framed_voronoi import (\n        ...     HorizontalRectVoronoiGraph,\n        ... )\n\n        >>> x_of_node, y_of_node = HorizontalRectVoronoiGraph.xy_of_node(\n        ...     (3, 3), seed=200\n        ... )\n\n        Coordinates of the lower left node,\n\n        >>> x_of_node[0], y_of_node[0]\n        (0.0, 0.0)\n\n        *x* coordinates of the left and right edges,\n\n        >>> x_of_node[3], x_of_node[5]\n        (0.0, 2.0)\n\n        *y* coordinate of the middle row of the left edge,\n\n        >>> y_of_node[3]\n        0.749\n        \"\"\"", "function_dependencies": ["numpy.allclose", "numpy.meshgrid", "numpy.arange", "numpy.random.default_rng", "numpy.random.default_rng.uniform"], "project_create_time": "2014-05-09T04:52:50+00:00", "project_update_time": "2024-04-16T13:37:47+00:00", "file_create_time": "2022-08-06T17:22:35Z", "file_update_time": "2023-10-29T17:17:11Z", "function_update_time": "2022-08-06T17:22:35Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["numpy.allclose", "numpy.random.default_rng", "numpy.meshgrid"], "test_function": [{"file_path": "/landlab-v2.7.0/landlab-2.7.0/tests/graph/framed_voronoi/test_framed_voronoi.py", "class_name": null, "function_name": "test_xy_of_node_rect_horizontal", "code": "\ndef test_xy_of_node_rect_horizontal(n_rows, n_cols):\n    expected = {\n        (1, 3): ([0.0, 1.0, 2.0], [0.0, 0.0, 0.0]),\n        (3, 3): (\n            [0.0, 1.0, 2.0],\n            [0.0, 0.0, 0.0],\n        ),\n    }\n    x_of_node, y_of_node = HorizontalRectVoronoiGraph.xy_of_node((n_rows, n_cols))\n\n    assert np.all(x_of_node[0:3] == approx(expected[(n_rows, n_cols)][0]))\n    assert np.all(y_of_node[0:3] == approx(expected[(n_rows, n_cols)][1]))"}]}, {"git_group": "SFDO-Tooling", "git_name": "CumulusCI", "version": "v3.86.0", "language": "Python", "project_name": "CumulusCI-v3.86.0.zip", "file_path": "/CumulusCI-v3.86.0/CumulusCI-3.86.0/cumulusci/core/metadeploy/api.py", "file_name": "api.py", "focal_class": null, "focal_name": "make_api_session", "focal_parameter": [], "solution": "def make_api_session(metadeploy_service: ServiceConfig) -> Session:\n    base_url: str = metadeploy_service.url\n\n    patched_session = Session()\n    patched_session.headers[\"Authorization\"] = f\"token {metadeploy_service.token}\"\n\n    def patched_request(base, func, method, url, *args, **kwargs):\n        \"\"\"Call requests.request with base_url prepended.\"\"\"\n        base_url = base + url.replace(base, \"\", 1)\n        return func(method, base_url, *args, **kwargs)\n\n    patched_session.request = functools.partial(\n        patched_request, base_url, patched_session.request\n    )\n\n    return patched_session", "function_signature": "def make_api_session(metadeploy_service: ServiceConfig) -> Session :", "left_context": "\"\"\"Calls to the MetaDeploy REST API.\"\"\"\n\nimport functools\nfrom typing import Optional, Union\n\nfrom requests.models import Response\nfrom requests.sessions import Session\n\nfrom cumulusci.core.config import ServiceConfig\nfrom cumulusci.core.exceptions import CumulusCIException\nfrom cumulusci.utils.http.requests_utils import safe_json_from_response\n\n", "right_context": "\n\nclass MetaDeployAPI:\n    def __init__(self, metadeploy_service: ServiceConfig) -> None:\n        self.session: Session = make_api_session(metadeploy_service=metadeploy_service)\n\n    def _create(self, obj: str, json: dict) -> dict:\n        response: Response = self.session.post(obj, json=json)\n        return safe_json_from_response(response)\n\n    def _find(self, obj: str, query: Union[dict, str]) -> Optional[dict]:\n        response: Response = self.session.get(obj, params=query)\n        result: dict = safe_json_from_response(response)\n\n        try:\n            return result[\"data\"][0]\n        except IndexError:\n            return None\n        except KeyError:\n            raise CumulusCIException(\n                \"CumulusCI received an unexpected response from MetaDeploy. \"\n                \"Ensure that your MetaDeploy service is configured with the Admin API URL, which \"\n                \"ends in /rest, and that your authentication token is valid.\"\n            ) from None\n\n    def create_plan(self, plan: dict) -> dict:\n        return self._create(\"/plans\", plan)\n\n    def create_plan_template(self, template: dict) -> dict:\n        return self._create(\"/plantemplates\", json=template)\n\n    def create_plan_slug(self, slug: dict) -> dict:\n        return self._create(\"/planslug\", json=slug)\n\n    def create_version(self, version: dict) -> dict:\n        return self._create(\"/versions\", json=version)\n\n    def find_product(self, repo_url: str) -> Optional[dict]:\n        return self._find(\"/products\", {\"repo_url\": repo_url})\n\n    def find_plan_template(self, query: dict) -> Optional[dict]:\n        return self._find(\"/plantemplates\", query)\n\n    def find_version(self, query: dict) -> Optional[dict]:\n        return self._find(\"/versions\", query)\n\n    def update_version(self, pk: Union[int, str]) -> dict:\n        response: Response = self.session.patch(\n            f\"/versions/{pk}\", json={\"is_listed\": True}\n        )\n        return safe_json_from_response(response)\n\n    def update_lang_translation(self, lang: str, labels: dict) -> dict:\n        response: Response = self.session.patch(f\"/translations/{lang}\", json=labels)\n        return safe_json_from_response(response)\n", "import_text": ["functools", "typing.Optional", "typing.Union", "requests.models.Response", "requests.sessions.Session", "cumulusci.core.config.ServiceConfig", "cumulusci.core.exceptions.CumulusCIException", "cumulusci.utils.http.requests_utils.safe_json_from_response"], "prompt": "\"\"\"\nDescription: This function creates a session object for making API calls.\n\nArgs:\n    metadeploy_service (ServiceConfig): An instance of ServiceConfig containing the URL and token for the API.\n\nReturns:\n    Session: A Session object with the base URL and token prepended to the request headers. The request method of the Session object is patched to prepend the base URL to the URL of the request.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Given a MetaDeploy ServiceConfig, returns a requests.Session with\n    authorization headers and service.base_url set.\n    \"\"\"", "function_dependencies": ["requests.sessions.Session", "functools.partial"], "project_create_time": "2014-01-02T20:01:31+00:00", "project_update_time": "2024-04-11T18:23:31+00:00", "file_create_time": "2022-06-24T03:21:41Z", "file_update_time": "2022-06-24T03:21:41Z", "function_update_time": "2022-06-24T03:21:41Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["requests.sessions.Session"], "test_function": [{"file_path": "/CumulusCI-v3.86.0/CumulusCI-3.86.0/cumulusci/core/metadeploy/tests/test_api.py", "class_name": null, "function_name": "test_make_api", "code": "\ndef test_make_api(auth_header_matcher):\n    expected_result: dict = {\"data\": [1]}\n    responses.add(\n        \"GET\",\n        f\"{DEFAULT_REST_URL}/resource\",\n        json=expected_result,\n        match=[auth_header_matcher],\n    )\n    responses.add(\n        \"POST\",\n        f\"{DEFAULT_REST_URL}/resource\",\n        json=expected_result,\n        match=[auth_header_matcher],\n    )\n    service: ServiceConfig = ServiceConfig(\n        {\"url\": f\"{DEFAULT_REST_URL}\", \"token\": DEFAULT_TOKEN}\n    )\n    api_session: Session = make_api_session(service)\n    resp: Response = api_session.get(\"/resource\")\n    result: dict = resp.json()\n    assert result == expected_result\n    _ = api_session.get(f\"{DEFAULT_REST_URL}/resource\")\n\n    assert responses.assert_call_count(f\"{DEFAULT_REST_URL}/resource\", 2) is True"}]}, {"git_group": "Project-MONAI", "git_name": "MONAILabel", "version": "pretrained", "language": "Python", "project_name": "MONAILabel-pretrained.zip", "file_path": "/MONAILabel-pretrained/MONAILabel-pretrained/monailabel/datastore/utils/convert.py", "file_name": "convert.py", "focal_class": null, "focal_name": "binary_to_image", "focal_parameter": ["reference_image", "label"], "solution": "\ndef binary_to_image(reference_image, label, dtype=np.uint16, file_ext=\".nii.gz\", use_itk=True):\n    start = time.time()\n\n    image_np, meta_dict = LoadImage()(reference_image)\n    label_np = np.fromfile(label, dtype=dtype)\n\n    logger.info(f\"Image: {image_np.shape}\")\n    logger.info(f\"Label: {label_np.shape}\")\n\n    label_np = label_np.reshape(image_np.shape, order=\"F\")\n    logger.info(f\"Label (reshape): {label_np.shape}\")\n\n    output_file = tempfile.NamedTemporaryFile(suffix=file_ext).name\n    affine = meta_dict.get(\"affine\")\n    if use_itk:\n        write_itk(label_np, output_file, affine=affine, dtype=None, compress=True)\n    else:\n        write_nifti(label_np, output_file, affine=affine)\n\n    logger.info(f\"binary_to_image latency : {time.time() - start} (sec)\")\n    return output_file", "function_signature": "def binary_to_image(reference_image, label, dtype=np.uint16, file_ext=\".nii.gz\", use_itk=True) :", "left_context": "# Copyright (c) MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport os\nimport pathlib\nimport tempfile\nimport time\n\nimport numpy as np\nimport pydicom\nimport pydicom_seg\nimport SimpleITK\nfrom monai.data import write_nifti\nfrom monai.transforms import LoadImage\nfrom pydicom.filereader import dcmread\n\nfrom monailabel.datastore.utils.colors import GENERIC_ANATOMY_COLORS\nfrom monailabel.transform.writer import write_itk\nfrom monailabel.utils.others.generic import run_command\n\nlogger = logging.getLogger(__name__)\n\n\ndef dicom_to_nifti(series_dir, is_seg=False):\n    start = time.time()\n\n    if is_seg:\n        output_file = dicom_seg_to_itk_image(series_dir)\n    else:\n        # https://simpleitk.readthedocs.io/en/master/link_DicomConvert_docs.html\n        if os.path.isdir(series_dir) and len(os.listdir(series_dir)) > 1:\n            reader = SimpleITK.ImageSeriesReader()\n            dicom_names = reader.GetGDCMSeriesFileNames(series_dir)\n            reader.SetFileNames(dicom_names)\n            image = reader.Execute()\n        else:\n            filename = (\n                series_dir if not os.path.isdir(series_dir) else os.path.join(series_dir, os.listdir(series_dir)[0])\n            )\n\n            file_reader = SimpleITK.ImageFileReader()\n            file_reader.SetImageIO(\"GDCMImageIO\")\n            file_reader.SetFileName(filename)\n            image = file_reader.Execute()\n\n        logger.info(f\"Image size: {image.GetSize()}\")\n        output_file = tempfile.NamedTemporaryFile(suffix=\".nii.gz\").name\n        SimpleITK.WriteImage(image, output_file)\n\n    logger.info(f\"dicom_to_nifti latency : {time.time() - start} (sec)\")\n    return output_file\n\n", "right_context": "\n\ndef nifti_to_dicom_seg(series_dir, label, label_info, file_ext=\"*\", use_itk=True):\n    start = time.time()\n\n    label_np, meta_dict = LoadImage()(label)\n    unique_labels = np.unique(label_np.flatten()).astype(np.int)\n    unique_labels = unique_labels[unique_labels != 0]\n\n    segment_attributes = []\n    for i, idx in enumerate(unique_labels):\n        info = label_info[i] if label_info and i < len(label_info) else {}\n        name = info.get(\"name\", \"unknown\")\n        description = info.get(\"description\", \"Unknown\")\n        rgb = list(info.get(\"color\", GENERIC_ANATOMY_COLORS.get(name, (255, 0, 0))))[0:3]\n        rgb = [int(x) for x in rgb]\n\n        logger.info(f\"{i} => {idx} => {name}\")\n\n        segment_attribute = info.get(\n            \"segmentAttribute\",\n            {\n                \"labelID\": int(idx),\n                \"SegmentLabel\": name,\n                \"SegmentDescription\": description,\n                \"SegmentAlgorithmType\": \"AUTOMATIC\",\n                \"SegmentAlgorithmName\": \"MONAILABEL\",\n                \"SegmentedPropertyCategoryCodeSequence\": {\n                    \"CodeValue\": \"123037004\",\n                    \"CodingSchemeDesignator\": \"SCT\",\n                    \"CodeMeaning\": \"Anatomical Structure\",\n                },\n                \"SegmentedPropertyTypeCodeSequence\": {\n                    \"CodeValue\": \"78961009\",\n                    \"CodingSchemeDesignator\": \"SCT\",\n                    \"CodeMeaning\": name,\n                },\n                \"recommendedDisplayRGBValue\": rgb,\n            },\n        )\n        segment_attributes.append(segment_attribute)\n\n    template = {\n        \"ContentCreatorName\": \"Reader1\",\n        \"ClinicalTrialSeriesID\": \"Session1\",\n        \"ClinicalTrialTimePointID\": \"1\",\n        \"SeriesDescription\": \"Segmentation\",\n        \"SeriesNumber\": \"300\",\n        \"InstanceNumber\": \"1\",\n        \"segmentAttributes\": [segment_attributes],\n        \"ContentLabel\": \"SEGMENTATION\",\n        \"ContentDescription\": \"MONAI Label - Image segmentation\",\n        \"ClinicalTrialCoordinatingCenterName\": \"MONAI\",\n        \"BodyPartExamined\": \"\",\n    }\n\n    logger.info(json.dumps(template, indent=2))\n    if not segment_attributes:\n        logger.error(\"Missing Attributes/Empty Label provided\")\n        return None\n\n    if use_itk:\n        output_file = itk_image_to_dicom_seg(label, series_dir, template)\n    else:\n        template = pydicom_seg.template.from_dcmqi_metainfo(template)\n        writer = pydicom_seg.MultiClassWriter(\n            template=template,\n            inplane_cropping=False,\n            skip_empty_slices=False,\n            skip_missing_segment=False,\n        )\n\n        # Read source Images\n        series_dir = pathlib.Path(series_dir)\n        image_files = series_dir.glob(file_ext)\n        image_datasets = [dcmread(str(f), stop_before_pixels=True) for f in image_files]\n        logger.info(f\"Total Source Images: {len(image_datasets)}\")\n\n        mask = SimpleITK.ReadImage(label)\n        mask = SimpleITK.Cast(mask, SimpleITK.sitkUInt16)\n\n        output_file = tempfile.NamedTemporaryFile(suffix=\".dcm\").name\n        dcm = writer.write(mask, image_datasets)\n        dcm.save_as(output_file)\n\n    logger.info(f\"nifti_to_dicom_seg latency : {time.time() - start} (sec)\")\n    return output_file\n\n\ndef itk_image_to_dicom_seg(label, series_dir, template):\n    output_file = tempfile.NamedTemporaryFile(suffix=\".dcm\").name\n    meta_data = tempfile.NamedTemporaryFile(suffix=\".json\").name\n    with open(meta_data, \"w\") as fp:\n        json.dump(template, fp)\n\n    command = \"itkimage2segimage\"\n    args = [\n        \"--inputImageList\",\n        label,\n        \"--inputDICOMDirectory\",\n        series_dir,\n        \"--outputDICOM\",\n        output_file,\n        \"--inputMetadata\",\n        meta_data,\n    ]\n    run_command(command, args)\n    os.unlink(meta_data)\n    return output_file\n\n\ndef dicom_seg_to_itk_image(label, output_ext=\".seg.nrrd\"):\n    filename = label if not os.path.isdir(label) else os.path.join(label, os.listdir(label)[0])\n\n    dcm = pydicom.dcmread(filename)\n    reader = pydicom_seg.MultiClassReader()\n    result = reader.read(dcm)\n    image = result.image\n\n    output_file = tempfile.NamedTemporaryFile(suffix=output_ext).name\n\n    SimpleITK.WriteImage(image, output_file, True)\n\n    if not os.path.exists(output_file):\n        logger.warning(f\"Failed to convert DICOM-SEG {label} to ITK image\")\n        return None\n\n    logger.info(f\"Result/Output File: {output_file}\")\n    return output_file\n", "import_text": ["json", "logging", "os", "pathlib", "tempfile", "time", "numpy", "pydicom", "pydicom_seg", "SimpleITK", "monai.data.write_nifti", "monai.transforms.LoadImage", "pydicom.filereader.dcmread", "monailabel.datastore.utils.colors.GENERIC_ANATOMY_COLORS", "monailabel.transform.writer.write_itk", "monailabel.utils.others.generic.run_command"], "prompt": "\"\"\"\nDescription: This function converts a binary file into an image format.\n\nArgs:\n    reference_image (str): The path to the reference image file.\n    label (str): The path to the binary label file.\n    dtype (numpy.dtype, optional): The data type of the binary file. Defaults to np.uint16.\n    file_ext (str, optional): The file extension for the output image. Defaults to \".nii.gz\".\n    use_itk (bool, optional): Whether to use the ITK library for writing the image. Defaults to True.\n\nReturns:\n    str: The path to the output image file.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["time.time", "monai.transforms.LoadImage", "numpy.fromfile", "numpy.fromfile.reshape", "tempfile.NamedTemporaryFile", "monailabel.transform.writer.write_itk", "monai.data.write_nifti"], "project_create_time": "2021-03-26T15:25:10+00:00", "project_update_time": "2024-04-15T13:01:02+00:00", "file_create_time": "2021-09-10T19:12:45Z", "file_update_time": "2022-09-28T08:44:06Z", "function_update_time": "2021-09-10T19:12:45Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["numpy.fromfile"], "test_function": [{"file_path": "/MONAILabel-pretrained/MONAILabel-pretrained/tests/unit/datastore/test_convert.py", "class_name": "TestConvert", "function_name": "test_binary_to_image", "code": "\n    def test_binary_to_image(self):\n        reference_image = os.path.join(self.local_dataset, \"labels\", \"final\", \"spleen_3.nii.gz\")\n        label, _ = LoadImage()(reference_image)\n        label = label.astype(np.uint16)\n        label = label.flatten(order=\"F\")\n\n        label_bin = tempfile.NamedTemporaryFile(suffix=\".bin\").name\n        label.tofile(label_bin)\n\n        result = binary_to_image(reference_image, label_bin)\n        os.unlink(label_bin)\n\n        assert os.path.exists(result)\n        assert result.endswith(\".nii.gz\")\n        os.unlink(result)"}]}, {"git_group": "obspy", "git_name": "obspy", "version": "sudelfeld14", "language": "Python", "project_name": "obspy-sudelfeld14.zip", "file_path": "/obspy-sudelfeld14/obspy-sudelfeld14/obspy/imaging/beachball.py", "file_name": "beachball.py", "focal_class": null, "focal_name": "Beachball", "focal_parameter": ["fm"], "solution": "def Beachball(fm, linewidth=2, facecolor='b', bgcolor='w', edgecolor='k',\n              alpha=1.0, xy=(0, 0), width=200, size=100, nofill=False,\n              zorder=100, outfile=None, format=None, fig=None):\n    plot_width = width * 0.95\n\n    # plot the figure\n    if not fig:\n        fig = plt.figure(figsize=(3, 3), dpi=100)\n        fig.subplots_adjust(left=0, bottom=0, right=1, top=1)\n        fig.set_figheight(width // 100)\n        fig.set_figwidth(width // 100)\n    ax = fig.add_subplot(111, aspect='equal')\n\n    # hide axes + ticks\n    ax.axison = False\n\n    # plot the collection\n    collection = Beach(fm, linewidth=linewidth, facecolor=facecolor,\n                       edgecolor=edgecolor, bgcolor=bgcolor,\n                       alpha=alpha, nofill=nofill, xy=xy,\n                       width=plot_width, size=size, zorder=zorder)\n    ax.add_collection(collection)\n\n    ax.autoscale_view(tight=False, scalex=True, scaley=True)\n    # export\n    if outfile:\n        if format:\n            fig.savefig(outfile, dpi=100, transparent=True, format=format)\n        else:\n            fig.savefig(outfile, dpi=100, transparent=True)\n    elif format and not outfile:\n        imgdata = StringIO.StringIO()\n        fig.savefig(imgdata, format=format, dpi=100, transparent=True)\n        imgdata.seek(0)\n        return imgdata.read()\n    else:\n        plt.show()\n        return fig", "function_signature": "def Beachball(fm, linewidth=2, facecolor='b', bgcolor='w', edgecolor='k',\n              alpha=1.0, xy=(0, 0), width=200, size=100, nofill=False,\n              zorder=100, outfile=None, format=None, fig=None) :", "left_context": "# -*- coding: utf-8 -*-\n#-------------------------------------------------------------------\n# Filename: beachball.py\n#  Purpose: Draws a beach ball diagram of an earthquake focal mechanism.\n#   Author: Robert Barsch\n#    Email: barsch@geophysik.uni-muenchen.de\n#\n# Copyright (C) 2008-2012 Robert Barsch\n#---------------------------------------------------------------------\n\n\"\"\"\nDraws a beachball diagram of an earthquake focal mechanism\n\nMost source code provided here are adopted from\n\n1. MatLab script `bb.m`_ written by Andy Michael and Oliver Boyd.\n2. ps_meca program from the `Generic Mapping Tools (GMT)`_.\n\n:copyright:\n    The ObsPy Development Team (devs@obspy.org)\n:license:\n    GNU General Public License (GPL)\n    (http://www.gnu.org/licenses/gpl.txt)\n\n.. _`Generic Mapping Tools (GMT)`: http://gmt.soest.hawaii.edu\n.. _`bb.m`: http://www.ceri.memphis.edu/people/olboyd/Software/Software.html\n\"\"\"\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches, collections, path as mplpath\nimport StringIO\nimport numpy as np\n\n\nD2R = np.pi / 180\nR2D = 180 / np.pi\nEPSILON = 0.00001\n\n\ndef Beach(fm, linewidth=2, facecolor='b', bgcolor='w', edgecolor='k',\n          alpha=1.0, xy=(0, 0), width=200, size=100, nofill=False,\n          zorder=100):\n    \"\"\"\n    Return a beach ball as a collection which can be connected to an\n    current matplotlib axes instance (ax.add_collection).\n\n    S1, D1, and R1, the strike, dip and rake of one of the focal planes, can\n    be vectors of multiple focal mechanisms.\n\n    :param fm: Focal mechanism that is either number of mechanisms (NM) by 3\n        (strike, dip, and rake) or NM x 6 (M11, M22, M33, M12, M13, M23 - the\n        six independent components of the moment tensor, where the coordinate\n        system is 1,2,3 = Up,South,East which equals r,theta,phi). The strike\n        is of the first plane, clockwise relative to north.\n        The dip is of the first plane, defined clockwise and perpendicular to\n        strike, relative to horizontal such that 0 is horizontal and 90 is\n        vertical. The rake is of the first focal plane solution. 90 moves the\n        hanging wall up-dip (thrust), 0 moves it in the strike direction\n        (left-lateral), -90 moves it down-dip (normal), and 180 moves it\n        opposite to strike (right-lateral).\n    :param facecolor: Color to use for quadrants of tension; can be a string,\n        e.g. ``'r'``, ``'b'`` or three component color vector, [R G B].\n        Defaults to ``'b'`` (blue).\n    :param bgcolor: The background color. Defaults to ``'w'`` (white).\n    :param edgecolor: Color of the edges. Defaults to ``'k'`` (black).\n    :param alpha: The alpha level of the beach ball. Defaults to ``1.0``\n        (opaque).\n    :param xy: Origin position of the beach ball as tuple. Defaults to\n        ``(0, 0)``.\n    :type width: int or tuple\n    :param width: Symbol size of beach ball, or tuple for elliptically\n        shaped patches. Defaults to size ``200``.\n    :param size: Controls the number of interpolation points for the\n        curves. Minimum is automatically set to ``100``.\n    :param nofill: Do not fill the beach ball, but only plot the planes.\n    :param zorder: Set zorder. Artists with lower zorder values are drawn\n        first.\n    \"\"\"\n    # check if one or two widths are specified (Circle or Ellipse)\n    try:\n        assert(len(width) == 2)\n    except TypeError:\n        width = (width, width)\n    mt = None\n    np1 = None\n    if isinstance(fm, MomentTensor):\n        mt = fm\n        np1 = MT2Plane(mt)\n    elif isinstance(fm, NodalPlane):\n        np1 = fm\n    elif len(fm) == 6:\n        mt = MomentTensor(fm[0], fm[1], fm[2], fm[3], fm[4], fm[5], 0)\n        np1 = MT2Plane(mt)\n    elif len(fm) == 3:\n        np1 = NodalPlane(fm[0], fm[1], fm[2])\n    else:\n        raise TypeError(\"Wrong input value for 'fm'.\")\n\n    # Only at least size 100, i.e. 100 points in the matrix are allowed\n    if size < 100:\n        size = 100\n\n    # Return as collection\n    if mt:\n        (T, N, P) = MT2Axes(mt)\n        if np.fabs(N.val) < EPSILON and np.fabs(T.val + P.val) < EPSILON:\n            colors, p = plotDC(np1, size, xy=xy, width=width)\n        else:\n            colors, p = plotMT(T, N, P, size,\n                               plot_zerotrace=True, xy=xy, width=width)\n    else:\n        colors, p = plotDC(np1, size=size, xy=xy, width=width)\n\n    if nofill:\n        # XXX: not tested with plotMT\n        col = collections.PatchCollection([p[1]], match_original=False)\n        col.set_facecolor('none')\n    else:\n        col = collections.PatchCollection(p, match_original=False)\n        # Replace color dummies 'b' and 'w' by face and bgcolor\n        fc = [facecolor if c == 'b' else bgcolor for c in colors]\n        col.set_facecolors(fc)\n\n    col.set_edgecolor(edgecolor)\n    col.set_alpha(alpha)\n    col.set_linewidth(linewidth)\n    col.set_zorder(zorder)\n    return col\n\n", "right_context": "\n\ndef plotMT(T, N, P, size=200, plot_zerotrace=True,\n           x0=0, y0=0, xy=(0, 0), width=200):\n    \"\"\"\n    Uses a principal axis T, N and P to draw a beach ball plot.\n\n    :param ax: axis object of a matplotlib figure\n    :param T: :class:`~PrincipalAxis`\n    :param N: :class:`~PrincipalAxis`\n    :param P: :class:`~PrincipalAxis`\n\n    Adapted from ps_tensor / utilmeca.c / `Generic Mapping Tools (GMT)`_.\n\n    .. _`Generic Mapping Tools (GMT)`: http://gmt.soest.hawaii.edu\n    \"\"\"\n    # check if one or two widths are specified (Circle or Ellipse)\n    try:\n        assert(len(width) == 2)\n    except TypeError:\n        width = (width, width)\n    collect = []\n    colors = []\n    res = [value / float(size) for value in width]\n    b = 1\n    big_iso = 0\n    j = 1\n    j2 = 0\n    j3 = 0\n    n = 0\n    azi = np.zeros((3, 2))\n    x = np.zeros(400)\n    y = np.zeros(400)\n    x2 = np.zeros(400)\n    y2 = np.zeros(400)\n    x3 = np.zeros(400)\n    y3 = np.zeros(400)\n    xp1 = np.zeros(800)\n    yp1 = np.zeros(800)\n    xp2 = np.zeros(400)\n    yp2 = np.zeros(400)\n\n    a = np.zeros(3)\n    p = np.zeros(3)\n    v = np.zeros(3)\n    a[0] = T.strike\n    a[1] = N.strike\n    a[2] = P.strike\n    p[0] = T.dip\n    p[1] = N.dip\n    p[2] = P.dip\n    v[0] = T.val\n    v[1] = N.val\n    v[2] = P.val\n\n    vi = (v[0] + v[1] + v[2]) / 3.\n    for i in range(0, 3):\n        v[i] = v[i] - vi\n\n    radius_size = size * 0.5\n\n    if np.fabs(v[0] * v[0] + v[1] * v[1] + v[2] * v[2]) < EPSILON:\n        # pure implosion-explosion\n        if vi > 0.:\n            cir = patches.Ellipse(xy, width=width[0], height=width[1])\n            collect.append(cir)\n            colors.append('b')\n        if vi < 0.:\n            cir = patches.Ellipse(xy, width=width[0], height=width[1])\n            collect.append(cir)\n            colors.append('w')\n        return colors, collect\n\n    if np.fabs(v[0]) >= np.fabs(v[2]):\n        d = 0\n        m = 2\n    else:\n        d = 2\n        m = 0\n\n    if (plot_zerotrace):\n        vi = 0.\n\n    f = -v[1] / float(v[d])\n    iso = vi / float(v[d])\n\n    # Cliff Frohlich, Seismological Research letters,\n    # Vol 7, Number 1, January-February, 1996\n    # Unless the isotropic parameter lies in the range\n    # between -1 and 1 - f there will be no nodes whatsoever\n\n    if iso < -1:\n        cir = patches.Ellipse(xy, width=width[0], height=width[1])\n        collect.append(cir)\n        colors.append('w')\n        return colors, collect\n    elif iso > 1 - f:\n        cir = patches.Ellipse(xy, width=width[0], height=width[1])\n        collect.append(cir)\n        colors.append('b')\n        return colors, collect\n\n    spd = np.sin(p[d] * D2R)\n    cpd = np.cos(p[d] * D2R)\n    spb = np.sin(p[b] * D2R)\n    cpb = np.cos(p[b] * D2R)\n    spm = np.sin(p[m] * D2R)\n    cpm = np.cos(p[m] * D2R)\n    sad = np.sin(a[d] * D2R)\n    cad = np.cos(a[d] * D2R)\n    sab = np.sin(a[b] * D2R)\n    cab = np.cos(a[b] * D2R)\n    sam = np.sin(a[m] * D2R)\n    cam = np.cos(a[m] * D2R)\n\n    for i in range(0, 360):\n        fir = i * D2R\n        s2alphan = (2. + 2. * iso) / \\\n            float(3. + (1. - 2. * f) * np.cos(2. * fir))\n        if s2alphan > 1.:\n            big_iso += 1\n        else:\n            alphan = np.arcsin(np.sqrt(s2alphan))\n            sfi = np.sin(fir)\n            cfi = np.cos(fir)\n            san = np.sin(alphan)\n            can = np.cos(alphan)\n\n            xz = can * spd + san * sfi * spb + san * cfi * spm\n            xn = can * cpd * cad + san * sfi * cpb * cab + \\\n                san * cfi * cpm * cam\n            xe = can * cpd * sad + san * sfi * cpb * sab + \\\n                san * cfi * cpm * sam\n\n            if np.fabs(xn) < EPSILON and np.fabs(xe) < EPSILON:\n                takeoff = 0.\n                az = 0.\n            else:\n                az = np.arctan2(xe, xn)\n                if az < 0.:\n                    az += np.pi * 2.\n                takeoff = np.arccos(xz / float(np.sqrt(xz * xz + xn * xn +\n                                                       xe * xe)))\n            if takeoff > np.pi / 2.:\n                takeoff = np.pi - takeoff\n                az += np.pi\n                if az > np.pi * 2.:\n                    az -= np.pi * 2.\n            r = np.sqrt(2) * np.sin(takeoff / 2.)\n            si = np.sin(az)\n            co = np.cos(az)\n            if i == 0:\n                azi[i][0] = az\n                x[i] = x0 + radius_size * r * si\n                y[i] = y0 + radius_size * r * co\n                azp = az\n            else:\n                if np.fabs(np.fabs(az - azp) - np.pi) < D2R * 10.:\n                        azi[n][1] = azp\n                        n += 1\n                        azi[n][0] = az\n                if np.fabs(np.fabs(az - azp) - np.pi * 2.) < D2R * 2.:\n                        if azp < az:\n                            azi[n][0] += np.pi * 2.\n                        else:\n                            azi[n][0] -= np.pi * 2.\n                if n == 0:\n                    x[j] = x0 + radius_size * r * si\n                    y[j] = y0 + radius_size * r * co\n                    j += 1\n                elif n == 1:\n                    x2[j2] = x0 + radius_size * r * si\n                    y2[j2] = y0 + radius_size * r * co\n                    j2 += 1\n                elif n == 2:\n                    x3[j3] = x0 + radius_size * r * si\n                    y3[j3] = y0 + radius_size * r * co\n                    j3 += 1\n                azp = az\n    azi[n][1] = az\n\n    if v[1] < 0.:\n        rgb1 = 'b'\n        rgb2 = 'w'\n    else:\n        rgb1 = 'w'\n        rgb2 = 'b'\n\n    cir = patches.Ellipse(xy, width=width[0], height=width[1])\n    collect.append(cir)\n    colors.append(rgb2)\n    if n == 0:\n        collect.append(xy2patch(x[0:360], y[0:360], res, xy))\n        colors.append(rgb1)\n        return colors, collect\n    elif n == 1:\n        for i in range(0, j):\n            xp1[i] = x[i]\n            yp1[i] = y[i]\n        if azi[0][0] - azi[0][1] > np.pi:\n            azi[0][0] -= np.pi * 2.\n        elif azi[0][1] - azi[0][0] > np.pi:\n            azi[0][0] += np.pi * 2.\n        if azi[0][0] < azi[0][1]:\n            az = azi[0][1] - D2R\n            while az > azi[0][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp1[i] = x0 + radius_size * si\n                yp1[i] = y0 + radius_size * co\n                i += 1\n                az -= D2R\n        else:\n            az = azi[0][1] + D2R\n            while az < azi[0][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp1[i] = x0 + radius_size * si\n                yp1[i] = y0 + radius_size * co\n                i += 1\n                az += D2R\n        collect.append(xy2patch(xp1[0:i], yp1[0:i], res, xy))\n        colors.append(rgb1)\n        for i in range(0, j2):\n            xp2[i] = x2[i]\n            yp2[i] = y2[i]\n        if azi[1][0] - azi[1][1] > np.pi:\n            azi[1][0] -= np.pi * 2.\n        elif azi[1][1] - azi[1][0] > np.pi:\n            azi[1][0] += np.pi * 2.\n        if azi[1][0] < azi[1][1]:\n            az = azi[1][1] - D2R\n            while az > azi[1][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp2[i] = x0 + radius_size * si\n                i += 1\n                yp2[i] = y0 + radius_size * co\n                az -= D2R\n        else:\n            az = azi[1][1] + D2R\n            while az < azi[1][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp2[i] = x0 + radius_size * si\n                i += 1\n                yp2[i] = y0 + radius_size * co\n                az += D2R\n        collect.append(xy2patch(xp2[0:i], yp2[0:i], res, xy))\n        colors.append(rgb1)\n        return colors, collect\n    elif n == 2:\n        for i in range(0, j3):\n            xp1[i] = x3[i]\n            yp1[i] = y3[i]\n        for ii in range(0, j):\n            xp1[i] = x[ii]\n            i += 1\n            yp1[i] = y[ii]\n        if big_iso:\n            ii = j2 - 1\n            while ii >= 0:\n                xp1[i] = x2[ii]\n                i += 1\n                yp1[i] = y2[ii]\n                ii -= 1\n            collect.append(xy2patch(xp1[0:i], yp1[0:i], res, xy))\n            colors.append(rgb1)\n            return colors, collect\n\n        if azi[2][0] - azi[0][1] > np.pi:\n            azi[2][0] -= np.pi * 2.\n        elif azi[0][1] - azi[2][0] > np.pi:\n            azi[2][0] += np.pi * 2.\n        if azi[2][0] < azi[0][1]:\n            az = azi[0][1] - D2R\n            while az > azi[2][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp1[i] = x0 + radius_size * si\n                i += 1\n                yp1[i] = y0 + radius_size * co\n                az -= D2R\n        else:\n            az = azi[0][1] + D2R\n            while az < azi[2][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp1[i] = x0 + radius_size * si\n                i += 1\n                yp1[i] = y0 + radius_size * co\n                az += D2R\n        collect.append(xy2patch(xp1[0:i], yp1[0:i], res, xy))\n        colors.append(rgb1)\n\n        for i in range(0, j2):\n            xp2[i] = x2[i]\n            yp2[i] = y2[i]\n        if azi[1][0] - azi[1][1] > np.pi:\n            azi[1][0] -= np.pi * 2.\n        elif azi[1][1] - azi[1][0] > np.pi:\n            azi[1][0] += np.pi * 2.\n        if azi[1][0] < azi[1][1]:\n            az = azi[1][1] - D2R\n            while az > azi[1][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp2[i] = x0 + radius_size * si\n                i += 1\n                yp2[i] = y0 + radius_size * co\n                az -= D2R\n        else:\n            az = azi[1][1] + D2R\n            while az < azi[1][0]:\n                si = np.sin(az)\n                co = np.cos(az)\n                xp2[i] = x0 + radius_size * si\n                i += 1\n                yp2[i] = y0 + radius_size * co\n                az += D2R\n        collect.append(xy2patch(xp2[0:i], yp2[0:i], res, xy))\n        colors.append(rgb1)\n        return colors, collect\n\n\ndef plotDC(np1, size=200, xy=(0, 0), width=200):\n    \"\"\"\n    Uses one nodal plane of a double couple to draw a beach ball plot.\n\n    :param ax: axis object of a matplotlib figure\n    :param np1: :class:`~NodalPlane`\n\n    Adapted from MATLAB script\n    `bb.m <http://www.ceri.memphis.edu/people/olboyd/Software/Software.html>`_\n    written by Andy Michael and Oliver Boyd.\n    \"\"\"\n    # check if one or two widths are specified (Circle or Ellipse)\n    try:\n        assert(len(width) == 2)\n    except TypeError:\n        width = (width, width)\n    S1 = np1.strike\n    D1 = np1.dip\n    R1 = np1.rake\n\n    M = 0\n    if R1 > 180:\n        R1 -= 180\n        M = 1\n    if R1 < 0:\n        R1 += 180\n        M = 1\n\n    # Get azimuth and dip of second plane\n    (S2, D2, _R2) = AuxPlane(S1, D1, R1)\n\n    D = size / 2\n\n    if D1 >= 90:\n        D1 = 89.9999\n    if D2 >= 90:\n        D2 = 89.9999\n\n    # arange checked for numerical stablility, np.pi is not multiple of 0.1\n    phi = np.arange(0, np.pi, .01)\n    l1 = np.sqrt(\n        np.power(90 - D1, 2) / (\n            np.power(np.sin(phi), 2) +\n            np.power(np.cos(phi), 2) * np.power(90 - D1, 2) / np.power(90, 2)))\n    l2 = np.sqrt(\n        np.power(90 - D2, 2) / (\n            np.power(np.sin(phi), 2) + np.power(np.cos(phi), 2) *\n            np.power(90 - D2, 2) / np.power(90, 2)))\n\n    inc = 1\n    (X1, Y1) = Pol2Cart(phi + S1 * D2R, l1)\n\n    if M == 1:\n        lo = S1 - 180\n        hi = S2\n        if lo > hi:\n            inc = -1\n        th1 = np.arange(S1 - 180, S2, inc)\n        (Xs1, Ys1) = Pol2Cart(th1 * D2R, 90 * np.ones((1, len(th1))))\n        (X2, Y2) = Pol2Cart(phi + S2 * D2R, l2)\n        th2 = np.arange(S2 + 180, S1, -inc)\n    else:\n        hi = S1 - 180\n        lo = S2 - 180\n        if lo > hi:\n            inc = -1\n        th1 = np.arange(hi, lo, -inc)\n        (Xs1, Ys1) = Pol2Cart(th1 * D2R, 90 * np.ones((1, len(th1))))\n        (X2, Y2) = Pol2Cart(phi + S2 * D2R, l2)\n        X2 = X2[::-1]\n        Y2 = Y2[::-1]\n        th2 = np.arange(S2, S1, inc)\n    (Xs2, Ys2) = Pol2Cart(th2 * D2R, 90 * np.ones((1, len(th2))))\n    X = np.concatenate((X1, Xs1[0], X2, Xs2[0]))\n    Y = np.concatenate((Y1, Ys1[0], Y2, Ys2[0]))\n\n    X = X * D / 90\n    Y = Y * D / 90\n\n    # calculate resolution\n    res = [value / float(size) for value in width]\n\n    # construct the patches\n    collect = [patches.Ellipse(xy, width=width[0], height=width[1])]\n    collect.append(xy2patch(Y, X, res, xy))\n    return ['b', 'w'], collect\n\n\ndef xy2patch(x, y, res, xy):\n    # check if one or two resolutions are specified (Circle or Ellipse)\n    try:\n        assert(len(res) == 2)\n    except TypeError:\n        res = (res, res)\n    # transform into the Path coordinate system\n    x = x * res[0] + xy[0]\n    y = y * res[1] + xy[1]\n    verts = zip(x.tolist(), y.tolist())\n    codes = [mplpath.Path.MOVETO]\n    codes.extend([mplpath.Path.LINETO] * (len(x) - 2))\n    codes.append(mplpath.Path.CLOSEPOLY)\n    path = mplpath.Path(verts, codes)\n    return patches.PathPatch(path)\n\n\ndef Pol2Cart(th, r):\n    \"\"\"\n    \"\"\"\n    x = r * np.cos(th)\n    y = r * np.sin(th)\n    return (x, y)\n\n\ndef StrikeDip(n, e, u):\n    \"\"\"\n    Finds strike and dip of plane given normal vector having components n, e,\n    and u.\n\n    Adapted from MATLAB script\n    `bb.m <http://www.ceri.memphis.edu/people/olboyd/Software/Software.html>`_\n    written by Andy Michael and Oliver Boyd.\n    \"\"\"\n    r2d = 180 / np.pi\n    if u < 0:\n        n = -n\n        e = -e\n        u = -u\n\n    strike = np.arctan2(e, n) * r2d\n    strike = strike - 90\n    while strike >= 360:\n            strike = strike - 360\n    while strike < 0:\n            strike = strike + 360\n    x = np.sqrt(np.power(n, 2) + np.power(e, 2))\n    dip = np.arctan2(x, u) * r2d\n    return (strike, dip)\n\n\ndef AuxPlane(s1, d1, r1):\n    \"\"\"\n    Get Strike and dip of second plane.\n\n    Adapted from MATLAB script\n    `bb.m <http://www.ceri.memphis.edu/people/olboyd/Software/Software.html>`_\n    written by Andy Michael and Oliver Boyd.\n    \"\"\"\n    r2d = 180 / np.pi\n\n    z = (s1 + 90) / r2d\n    z2 = d1 / r2d\n    z3 = r1 / r2d\n    # slick vector in plane 1\n    sl1 = -np.cos(z3) * np.cos(z) - np.sin(z3) * np.sin(z) * np.cos(z2)\n    sl2 = np.cos(z3) * np.sin(z) - np.sin(z3) * np.cos(z) * np.cos(z2)\n    sl3 = np.sin(z3) * np.sin(z2)\n    (strike, dip) = StrikeDip(sl2, sl1, sl3)\n\n    n1 = np.sin(z) * np.sin(z2)  # normal vector to plane 1\n    n2 = np.cos(z) * np.sin(z2)\n    h1 = -sl2  # strike vector of plane 2\n    h2 = sl1\n    # note h3=0 always so we leave it out\n    # n3 = np.cos(z2)\n\n    z = h1 * n1 + h2 * n2\n    z = z / np.sqrt(h1 * h1 + h2 * h2)\n    z = np.arccos(z)\n    rake = 0\n    if sl3 > 0:\n        rake = z * r2d\n    if sl3 <= 0:\n        rake = -z * r2d\n    return (strike, dip, rake)\n\n\ndef MT2Plane(mt):\n    \"\"\"\n    Calculates a nodal plane of a given moment tensor.\n\n    :param mt: :class:`~MomentTensor`\n    :return: :class:`~NodalPlane`\n\n    Adapted from MATLAB script\n    `bb.m <http://www.ceri.memphis.edu/people/olboyd/Software/Software.html>`_\n    written by Andy Michael and Oliver Boyd.\n    \"\"\"\n    (d, v) = np.linalg.eig(mt.mt)\n    D = np.array([d[1], d[0], d[2]])\n    V = np.array([[v[1, 1], -v[1, 0], -v[1, 2]],\n                 [v[2, 1], -v[2, 0], -v[2, 2]],\n                 [-v[0, 1], v[0, 0], v[0, 2]]])\n    IMAX = D.argmax()\n    IMIN = D.argmin()\n    AE = (V[:, IMAX] + V[:, IMIN]) / np.sqrt(2.0)\n    AN = (V[:, IMAX] - V[:, IMIN]) / np.sqrt(2.0)\n    AER = np.sqrt(np.power(AE[0], 2) + np.power(AE[1], 2) + np.power(AE[2], 2))\n    ANR = np.sqrt(np.power(AN[0], 2) + np.power(AN[1], 2) + np.power(AN[2], 2))\n    AE = AE / AER\n    if not ANR:\n        AN = np.array([np.nan, np.nan, np.nan])\n    else:\n        AN = AN / ANR\n    if AN[2] <= 0.:\n        AN1 = AN\n        AE1 = AE\n    else:\n        AN1 = -AN\n        AE1 = -AE\n    (ft, fd, fl) = TDL(AN1, AE1)\n    return NodalPlane(360 - ft, fd, 180 - fl)\n\n\ndef TDL(AN, BN):\n    \"\"\"\n    Helper function for MT2Plane.\n\n    Adapted from MATLAB script\n    `bb.m <http://www.ceri.memphis.edu/people/olboyd/Software/Software.html>`_\n    written by Andy Michael and Oliver Boyd.\n    \"\"\"\n    XN = AN[0]\n    YN = AN[1]\n    ZN = AN[2]\n    XE = BN[0]\n    YE = BN[1]\n    ZE = BN[2]\n    AAA = 1.0 / (1000000)\n    CON = 57.2957795\n    if np.fabs(ZN) < AAA:\n        FD = 90.\n        AXN = np.fabs(XN)\n        if AXN > 1.0:\n            AXN = 1.0\n        FT = np.arcsin(AXN) * CON\n        ST = -XN\n        CT = YN\n        if ST >= 0. and CT < 0:\n            FT = 180. - FT\n        if ST < 0. and CT <= 0:\n            FT = 180. + FT\n        if ST < 0. and CT > 0:\n            FT = 360. - FT\n        FL = np.arcsin(abs(ZE)) * CON\n        SL = -ZE\n        if np.fabs(XN) < AAA:\n            CL = XE / YN\n        else:\n            CL = -YE / XN\n        if SL >= 0. and CL < 0:\n            FL = 180. - FL\n        if SL < 0. and CL <= 0:\n            FL = FL - 180.\n        if SL < 0. and CL > 0:\n            FL = -FL\n    else:\n        if - ZN > 1.0:\n            ZN = -1.0\n        FDH = np.arccos(-ZN)\n        FD = FDH * CON\n        SD = np.sin(FDH)\n        if SD == 0:\n            return\n        ST = -XN / SD\n        CT = YN / SD\n        SX = np.fabs(ST)\n        if SX > 1.0:\n            SX = 1.0\n        FT = np.arcsin(SX) * CON\n        if ST >= 0. and CT < 0:\n            FT = 180. - FT\n        if ST < 0. and CT <= 0:\n            FT = 180. + FT\n        if ST < 0. and CT > 0:\n            FT = 360. - FT\n        SL = -ZE / SD\n        SX = np.fabs(SL)\n        if SX > 1.0:\n            SX = 1.0\n        FL = np.arcsin(SX) * CON\n        if ST == 0:\n            CL = XE / CT\n        else:\n            XXX = YN * ZN * ZE / SD / SD + YE\n            CL = -SD * XXX / XN\n            if CT == 0:\n                CL = YE / ST\n        if SL >= 0. and CL < 0:\n            FL = 180. - FL\n        if SL < 0. and CL <= 0:\n            FL = FL - 180.\n        if SL < 0. and CL > 0:\n            FL = -FL\n    return (FT, FD, FL)\n\n\ndef MT2Axes(mt):\n    \"\"\"\n    Calculates the principal axes of a given moment tensor.\n\n    :param mt: :class:`~MomentTensor`\n    :return: tuple of :class:`~PrincipalAxis` T, N and P\n\n    Adapted from ps_tensor / utilmeca.c /\n    `Generic Mapping Tools (GMT) <http://gmt.soest.hawaii.edu>`_.\n    \"\"\"\n    (D, V) = np.linalg.eigh(mt.mt)\n    pl = np.arcsin(-V[0])\n    az = np.arctan2(V[2], -V[1])\n    for i in range(0, 3):\n        if pl[i] <= 0:\n            pl[i] = -pl[i]\n            az[i] += np.pi\n        if az[i] < 0:\n            az[i] += 2 * np.pi\n        if az[i] > 2 * np.pi:\n            az[i] -= 2 * np.pi\n    pl *= R2D\n    az *= R2D\n\n    T = PrincipalAxis(D[2], az[2], pl[2])\n    N = PrincipalAxis(D[1], az[1], pl[1])\n    P = PrincipalAxis(D[0], az[0], pl[0])\n    return (T, N, P)\n\n\nclass PrincipalAxis(object):\n    \"\"\"\n    A principal axis.\n\n    Strike and dip values are in degrees.\n\n    >>> a = PrincipalAxis(1.3, 20, 50)\n    >>> a.dip\n    50\n    >>> a.strike\n    20\n    >>> a.val\n    1.3\n    \"\"\"\n    def __init__(self, val=0, strike=0, dip=0):\n        self.val = val\n        self.strike = strike\n        self.dip = dip\n\n\nclass NodalPlane(object):\n    \"\"\"\n    A nodal plane.\n\n    All values are in degrees.\n\n    >>> a = NodalPlane(13, 20, 50)\n    >>> a.strike\n    13\n    >>> a.dip\n    20\n    >>> a.rake\n    50\n    \"\"\"\n    def __init__(self, strike=0, dip=0, rake=0):\n        self.strike = strike\n        self.dip = dip\n        self.rake = rake\n\n\nclass MomentTensor(object):\n    \"\"\"\n    A moment tensor.\n\n    >>> a = MomentTensor(1, 1, 0, 0, 0, -1, 26)\n    >>> b = MomentTensor(np.array([1, 1, 0, 0, 0, -1]), 26)\n    >>> c = MomentTensor(np.array([[1, 0, 0], [0, 1, -1], [0, -1, 0]]), 26)\n    >>> a.mt\n    array([[ 1,  0,  0],\n           [ 0,  1, -1],\n           [ 0, -1,  0]])\n    >>> b.yz\n    -1\n    >>> a.expo\n    26\n    \"\"\"\n    def __init__(self, *args):\n        if len(args) == 2:\n            A = args[0]\n            self.expo = args[1]\n            if len(A) == 6:\n                # six independent components\n                self.mt = np.array([[A[0], A[3], A[4]],\n                                    [A[3], A[1], A[5]],\n                                    [A[4], A[5], A[2]]])\n            elif isinstance(A, np.ndarray) and A.shape == (3, 3):\n                # full matrix\n                self.mt = A\n            else:\n                raise TypeError(\"Wrong size of input parameter.\")\n        elif len(args) == 7:\n            # six independent components\n            self.mt = np.array([[args[0], args[3], args[4]],\n                                [args[3], args[1], args[5]],\n                                [args[4], args[5], args[2]]])\n            self.expo = args[6]\n        else:\n            raise TypeError(\"Wrong size of input parameter.\")\n\n    @property\n    def xx(self):\n        return self.mt[0][0]\n\n    @property\n    def xy(self):\n        return self.mt[0][1]\n\n    @property\n    def xz(self):\n        return self.mt[0][2]\n\n    @property\n    def yz(self):\n        return self.mt[1][2]\n\n    @property\n    def yy(self):\n        return self.mt[1][1]\n\n    @property\n    def zz(self):\n        return self.mt[2][2]\n\n\nif __name__ == '__main__':\n    import doctest\n    doctest.testmod()\n", "import_text": ["matplotlib.pyplot", "matplotlib.patches", "matplotlib.collections", "matplotlib.path", "StringIO", "numpy"], "prompt": "\"\"\"\nDescription: This function generates a beachball plot using matplotlib.\n\nArgs:\n    fm (float): The focal mechanism parameter.\n    linewidth (int, optional): The width of the beachball lines. Defaults to 2.\n    facecolor (str, optional): The color of the beachball face. Defaults to 'b'.\n    bgcolor (str, optional): The background color of the plot. Defaults to 'w'.\n    edgecolor (str, optional): The color of the beachball edges. Defaults to 'k'.\n    alpha (float, optional): The transparency of the beachball. Defaults to 1.0.\n    xy (tuple, optional): The coordinates of the beachball. Defaults to (0, 0).\n    width (int, optional): The width of the plot. Defaults to 200.\n    size (int, optional): The size of the beachball. Defaults to 100.\n    nofill (bool, optional): Whether to fill the beachball. Defaults to False.\n    zorder (int, optional): The z-order of the beachball. Defaults to 100.\n    outfile (str, optional): The output file for the plot. Defaults to None.\n    format (str, optional): The format of the output file. Defaults to None.\n    fig (matplotlib.figure.Figure, optional): The figure to plot on. Defaults to None.\n\nReturns:\n    matplotlib.figure.Figure or str: The generated figure or the image data as a string.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Draws a beach ball diagram of an earthquake focal mechanism.\n\n    S1, D1, and R1, the strike, dip and rake of one of the focal planes, can\n    be vectors of multiple focal mechanisms.\n\n    :param fm: Focal mechanism that is either number of mechanisms (NM) by 3\n        (strike, dip, and rake) or NM x 6 (M11, M22, M33, M12, M13, M23 - the\n        six independent components of the moment tensor, where the coordinate\n        system is 1,2,3 = Up,South,East which equals r,theta,phi). The strike\n        is of the first plane, clockwise relative to north.\n        The dip is of the first plane, defined clockwise and perpendicular to\n        strike, relative to horizontal such that 0 is horizontal and 90 is\n        vertical. The rake is of the first focal plane solution. 90 moves the\n        hanging wall up-dip (thrust), 0 moves it in the strike direction\n        (left-lateral), -90 moves it down-dip (normal), and 180 moves it\n        opposite to strike (right-lateral).\n    :param facecolor: Color to use for quadrants of tension; can be a string,\n        e.g. ``'r'``, ``'b'`` or three component color vector, [R G B].\n        Defaults to ``'b'`` (blue).\n    :param bgcolor: The background color. Defaults to ``'w'`` (white).\n    :param edgecolor: Color of the edges. Defaults to ``'k'`` (black).\n    :param alpha: The alpha level of the beach ball. Defaults to ``1.0``\n        (opaque).\n    :param xy: Origin position of the beach ball as tuple. Defaults to\n        ``(0, 0)``.\n    :type width: int\n    :param width: Symbol size of beach ball. Defaults to ``200``.\n    :param size: Controls the number of interpolation points for the\n        curves. Minimum is automatically set to ``100``.\n    :param nofill: Do not fill the beach ball, but only plot the planes.\n    :param zorder: Set zorder. Artists with lower zorder values are drawn\n        first.\n    :param outfile: Output file string. Also used to automatically\n        determine the output format. Supported file formats depend on your\n        matplotlib backend. Most backends support png, pdf, ps, eps and\n        svg. Defaults to ``None``.\n    :param format: Format of the graph picture. If no format is given the\n        outfile parameter will be used to try to automatically determine\n        the output format. If no format is found it defaults to png output.\n        If no outfile is specified but a format is, than a binary\n        imagestring will be returned.\n        Defaults to ``None``.\n    :param fig: Give an existing figure instance to plot into. New Figure if\n        set to ``None``.\n    \"\"\"", "function_dependencies": ["matplotlib.pyplot.figure", "matplotlib.pyplot.figure.subplots_adjust", "matplotlib.pyplot.figure.set_figheight", "matplotlib.pyplot.figure.set_figwidth", "matplotlib.pyplot.figure.add_subplot", "matplotlib.pyplot.figure.add_subplot.add_collection", "matplotlib.pyplot.figure.add_subplot.autoscale_view", "matplotlib.pyplot.figure.savefig", "StringIO.StringIO", "StringIO.StringIO.seek", "StringIO.StringIO.read", "matplotlib.pyplot.show"], "project_create_time": "2012-09-08T19:17:46+00:00", "project_update_time": "2024-04-16T17:18:13+00:00", "file_create_time": "2012-10-14T11:52:35Z", "file_update_time": "2014-02-16T12:22:52Z", "function_update_time": "2012-10-14T11:52:35Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["matplotlib.pyplot.show", "matplotlib.pyplot.figure"], "test_function": [{"file_path": "/obspy-sudelfeld14/obspy-sudelfeld14/obspy/imaging/tests/test_beachball.py", "class_name": "BeachballTestCase", "function_name": "test_Beachball", "code": "    def test_Beachball(self):\n        reltol = 1\n        if MATPLOTLIB_VERSION < [1, 3, 0]:\n            reltol = 60\n        # http://en.wikipedia.org/wiki/File:USGS_sumatra_mts.gif\n        data = [[0.91, -0.89, -0.02, 1.78, -1.55, 0.47],\n                [274, 13, 55],\n                [130, 79, 98],\n                [264.98, 45.00, -159.99],\n                [160.55, 76.00, -46.78],\n                [1.45, -6.60, 5.14, -2.67, -3.16, 1.36],\n                [235, 80, 35],\n                [138, 56, 168],\n                # Explosion\n                [1, 1, 1, 0, 0, 0],\n                # Implosion\n                [-1, -1, -1, 0, 0, 0],\n                # CLVD - Compensate Linear Vector Dipole\n                [1, -2, 1, 0, 0, 0],\n                # Double Couple\n                [1, -1, 0, 0, 0, 0],\n                # Lars\n                [1, -1, 0, 0, 0, -1],\n                # http://wwweic.eri.u-tokyo.ac.jp/yuji/Aki-nada/\n                [179, 55, -78],\n                [10, 42.5, 90],\n                [10, 42.5, 92],\n                # http://wwweic.eri.u-tokyo.ac.jp/yuji/tottori/\n                [150, 87, 1],\n                # http://iisee.kenken.go.jp/staff/thara/2004/09/20040905_1/\n                # 2nd.html\n                [0.99, -2.00, 1.01, 0.92, 0.48, 0.15],\n                # http://iisee.kenken.go.jp/staff/thara/2004/09/20040905_0/\n                # 1st.html\n                [5.24, -6.77, 1.53, 0.81, 1.49, -0.05],\n                # http://iisee.kenken.go.jp/staff/thara/miyagi.htm\n                [16.578, -7.987, -8.592, -5.515, -29.732, 7.517],\n                # http://iisee.kenken.go.jp/staff/thara/20050613/chile.html\n                [-2.39, 1.04, 1.35, 0.57, -2.94, -0.94],\n                ]\n        filenames = ['bb_sumatra_mt.png', 'bb_sumatra_np1.png',\n                     'bb_sumatra_np2.png', 'bb_19950128_np1.png',\n                     'bb_19950128_np2.png', 'bb_20090102_mt.png',\n                     'bb_20090102_np1.png', 'bb-20090102-np2.png',\n                     'bb_explosion.png', 'bb_implosion.png', 'bb_clvd.png',\n                     'bb_double_couple.png', 'bb_lars.png', 'bb_geiyo_np1.png',\n                     'bb_honshu_np1.png', 'bb_honshu_np2.png',\n                     'bb_tottori_np1.png', 'bb_20040905_1_mt.png',\n                     'bb_20040905_0_mt.png', 'bb_miyagi_mt.png',\n                     'bb_chile_mt.png',\n                     ]\n        for data_, filename in zip(data, filenames):\n            with ImageComparison(self.path, filename, reltol=reltol) as ic:\n                Beachball(data_, outfile=ic.name)"}, {"file_path": "/obspy-sudelfeld14/obspy-sudelfeld14/obspy/imaging/tests/test_beachball.py", "class_name": "BeachballTestCase", "function_name": "test_BeachBallOutputFormats", "code": "    def test_BeachBallOutputFormats(self):\n        fm = [115, 35, 50]\n        # PDF\n        data = Beachball(fm, format='pdf')\n        self.assertEqual(data[0:4], \"%PDF\")\n        # as file\n        # create and compare image\n        with NamedTemporaryFile(suffix='.pdf') as tf:\n            Beachball(fm, format='pdf', outfile=tf.name)\n        # PS\n        data = Beachball(fm, format='ps')\n        self.assertEqual(data[0:4], \"%!PS\")\n        # as file\n        with NamedTemporaryFile(suffix='.ps') as tf:\n            Beachball(fm, format='ps', outfile=tf.name)\n        # PNG\n        data = Beachball(fm, format='png')\n        self.assertEqual(data[1:4], \"PNG\")\n        # as file\n        with NamedTemporaryFile(suffix='.png') as tf:\n            Beachball(fm, format='png', outfile=tf.name)\n        # SVG\n        data = Beachball(fm, format='svg')\n        self.assertEqual(data[0:5], \"<?xml\")\n        # as file\n        with NamedTemporaryFile(suffix='.svg') as tf:\n            Beachball(fm, format='svg', outfile=tf.name)"}]}, {"git_group": "DeepPSP", "git_name": "torch_ecg", "version": "v0.0.28", "language": "Python", "project_name": "torch_ecg-v0.0.28.zip", "file_path": "/torch_ecg-v0.0.28/torch_ecg-0.0.28/torch_ecg/utils/utils_signal.py", "file_name": "utils_signal.py", "focal_class": null, "focal_name": "detect_peaks", "focal_parameter": [], "solution": "def detect_peaks(\n    x: Sequence,\n    mph: Optional[Real] = None,\n    mpd: int = 1,\n    threshold: Real = 0,\n    left_threshold: Real = 0,\n    right_threshold: Real = 0,\n    prominence: Optional[Real] = None,\n    prominence_wlen: Optional[int] = None,\n    edge: Union[str, None] = \"rising\",\n    kpsh: bool = False,\n    valley: bool = False,\n    show: bool = False,\n    ax=None,\n    verbose: int = 0,\n) -> np.ndarray:\n    data = deepcopy(x)\n    data = np.atleast_1d(data).astype(\"float64\")\n    if data.size < 3:\n        return np.array([], dtype=int)\n\n    if valley:\n        data = -data\n        if mph is not None:\n            mph = -mph\n\n    # find indices of all peaks\n    dx = data[1:] - data[:-1]  # equiv to np.diff()\n\n    # handle NaN's\n    indnan = np.where(np.isnan(data))[0]\n    if indnan.size:\n        data[indnan] = np.inf\n        dx[np.where(np.isnan(dx))[0]] = np.inf\n\n    ine, ire, ife = np.array([[], [], []], dtype=int)\n    if not edge:\n        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n    else:\n        if edge.lower() in [\"rising\", \"both\"]:\n            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n        if edge.lower() in [\"falling\", \"both\"]:\n            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n    ind = np.unique(np.hstack((ine, ire, ife)))\n\n    if verbose >= 1:\n        print(f\"before filtering by mpd = {mpd}, and threshold = {threshold}, ind = {ind.tolist()}\")\n        print(\n            f\"additionally, left_threshold = {left_threshold}, \"\n            f\"right_threshold = {right_threshold}, length of data = {len(data)}\"\n        )\n\n    # handle NaN's\n    if ind.size and indnan.size:\n        # NaN's and values close to NaN's cannot be peaks\n        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan - 1, indnan + 1))), invert=True)]\n\n    if verbose >= 1:\n        print(f\"after handling nan values, ind = {ind.tolist()}\")\n\n    # peaks are only valid within [mpb, len(data)-mpb[\n    ind = np.array([pos for pos in ind if mpd <= pos < len(data) - mpd])\n\n    if verbose >= 1:\n        print(f\"after fitering out elements too close to border by mpd = {mpd}, ind = {ind.tolist()}\")\n\n    # first and last values of data cannot be peaks\n    # if ind.size and ind[0] == 0:\n    #     ind = ind[1:]\n    # if ind.size and ind[-1] == data.size-1:\n    #     ind = ind[:-1]\n    # remove peaks < minimum peak height\n    if ind.size and mph is not None:\n        ind = ind[data[ind] >= mph]\n\n    if verbose >= 1:\n        print(f\"after filtering by mph = {mph}, ind = {ind.tolist()}\")\n\n    # remove peaks - neighbors < threshold\n    _left_threshold = left_threshold if left_threshold > 0 else threshold\n    _right_threshold = right_threshold if right_threshold > 0 else threshold\n    if ind.size and (_left_threshold > 0 and _right_threshold > 0):\n        # dx = np.min(np.vstack([data[ind]-data[ind-1], data[ind]-data[ind+1]]), axis=0)\n        dx = np.max(np.vstack([data[ind] - data[ind + idx] for idx in range(-mpd, 0)]), axis=0)\n        ind = np.delete(ind, np.where(dx < _left_threshold)[0])\n        if verbose >= 2:\n            print(f\"from left, dx = {dx.tolist()}\")\n            print(f\"after deleting those dx < _left_threshold = {_left_threshold}, ind = {ind.tolist()}\")\n        dx = np.max(\n            np.vstack([data[ind] - data[ind + idx] for idx in range(1, mpd + 1)]),\n            axis=0,\n        )\n        ind = np.delete(ind, np.where(dx < _right_threshold)[0])\n        if verbose >= 2:\n            print(f\"from right, dx = {dx.tolist()}\")\n            print(f\"after deleting those dx < _right_threshold = {_right_threshold}, ind = {ind.tolist()}\")\n    if verbose >= 1:\n        print(f\"after filtering by threshold, ind = {ind.tolist()}\")\n    # detect small peaks closer than minimum peak distance\n    if ind.size and mpd > 1:\n        ind = ind[np.argsort(data[ind])][::-1]  # sort ind by peak height\n        idel = np.zeros(ind.size, dtype=bool)\n        for i in range(ind.size):\n            if not idel[i]:\n                # keep peaks with the same height if kpsh is True\n                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) & (data[ind[i]] > data[ind] if kpsh else True)\n                idel[i] = 0  # Keep current peak\n        # remove the small peaks and sort back the indices by their occurrence\n        ind = np.sort(ind[~idel])\n\n    ind = np.array([item for item in ind if data[item] == np.max(data[item - mpd : item + mpd + 1])])\n\n    if verbose >= 1:\n        print(f\"after filtering by mpd, ind = {ind.tolist()}\")\n\n    if prominence:\n        _p = peak_prominences(data, ind, prominence_wlen)[0]\n        ind = ind[np.where(_p >= prominence)[0]]\n        if verbose >= 1:\n            print(f\"after filtering by prominence, ind = {ind.tolist()}\")\n            if verbose >= 2:\n                print(f\"with detailed prominence = {_p.tolist()}\")\n\n    return ind", "function_signature": "def detect_peaks(\n    x: Sequence,\n    mph: Optional[Real] = None,\n    mpd: int = 1,\n    threshold: Real = 0,\n    left_threshold: Real = 0,\n    right_threshold: Real = 0,\n    prominence: Optional[Real] = None,\n    prominence_wlen: Optional[int] = None,\n    edge: Union[str, None] = \"rising\",\n    kpsh: bool = False,\n    valley: bool = False,\n    show: bool = False,\n    ax=None,\n    verbose: int = 0,\n) -> np.ndarray :", "left_context": "\"\"\"\nUtilities for signal processing,\nincluding spatial, temporal, spatio-temporal domains.\n\"\"\"\n\nimport warnings\nfrom copy import deepcopy\nfrom numbers import Real\nfrom typing import Iterable, Optional, Sequence, Tuple, Union\n\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy.signal import butter, filtfilt, peak_prominences\n\nfrom .utils_data import ensure_siglen\n\n__all__ = [\n    \"smooth\",\n    \"resample_irregular_timeseries\",\n    \"detect_peaks\",\n    \"remove_spikes_naive\",\n    \"butter_bandpass_filter\",\n    \"get_ampl\",\n    \"normalize\",\n]\n\n\ndef smooth(\n    x: np.ndarray,\n    window_len: int = 11,\n    window: str = \"hanning\",\n    mode: str = \"valid\",\n    keep_dtype: bool = True,\n) -> np.ndarray:\n    \"\"\"Smooth the 1d data using a window with requested size.\n\n    This method is originally from [#smooth]_,\n    based on the convolution of a scaled window with the signal.\n    The signal is prepared by introducing reflected copies of the signal\n    (with the window size) in both ends so that transient parts are minimized\n    in the begining and end part of the output signal.\n\n    Parameters\n    ----------\n    x : numpy.ndarray\n        The input signal.\n    window_len : int, default 11\n        Length of the smoothing window,\n        (previously should be an odd integer,\n        currently can be any (positive) integer).\n    window : {\"flat\", \"hanning\", \"hamming\", \"bartlett\", \"blackman\"}, optional\n        Type of window from, by default \"hanning\".\n        See also :func:`numpy.hanning`, :func:`numpy.hamming`, etc.\n        Flat type window will produce a moving average smoothing.\n    mode : str, default \"valid\"\n        Mode of convolution, see :func:`numpy.convolve` for details.\n    keep_dtype : bool, default True\n        Whether `dtype` of the returned value keeps\n        the same with that of `x` or not.\n\n    Returns\n    -------\n    y : numpy.ndarray\n        The smoothed signal.\n\n    Examples\n    --------\n    .. code-block:: python\n\n        t = np.linspace(-2, 2, 50)\n        x = np.sin(t) + np.random.randn(len(t)) * 0.1\n        y = smooth(x)\n\n    See also\n    --------\n    :func:`numpy.hanning`, :func:`numpy.hamming`,\n    :func:`numpy.bartlett`, :func:`numpy.blackman`, :func:`numpy.convolve`,\n    :func:`scipy.signal.lfilter`.\n\n    TODO\n    ----\n    The window parameter could be the window itself\n    if an array instead of a string.\n\n    NOTE\n    ----\n    length(output) != length(input), to correct this, using\n\n    .. code-block:: python\n\n        return y[(window_len/2-1):-(window_len/2)]\n\n    instead of just returning `y`.\n\n    References\n    ----------\n    .. [#smooth] https://scipy-cookbook.readthedocs.io/items/SignalSmooth.html\n\n    \"\"\"\n    radius = min(len(x), window_len)\n    radius = radius if radius % 2 == 1 else radius - 1\n\n    if x.ndim != 1:\n        raise ValueError(\"function `smooth` only accepts 1 dimension arrays.\")\n\n    # if x.size < radius:\n    #     raise ValueError(\"Input vector needs to be bigger than window size.\")\n\n    if radius < 3:\n        return x\n\n    if window not in [\"flat\", \"hanning\", \"hamming\", \"bartlett\", \"blackman\"]:\n        raise ValueError(\"\"\" `window` should be of \"flat\", \"hanning\", \"hamming\", \"bartlett\", \"blackman\" \"\"\")\n\n    s = np.r_[x[radius - 1 : 0 : -1], x, x[-2 : -radius - 1 : -1]]\n    # print(len(s))\n    if window == \"flat\":  # moving average\n        w = np.ones(radius, \"d\")\n    else:\n        w = eval(\"np.\" + window + \"(radius)\")\n\n    y = np.convolve(w / w.sum(), s, mode=mode)\n    y = y[(radius // 2 - 1) : -(radius // 2) - 1]\n    assert len(x) == len(y)\n\n    if keep_dtype:\n        y = y.astype(x.dtype)\n\n    return y\n\n\ndef resample_irregular_timeseries(\n    sig: np.ndarray,\n    output_fs: Optional[Real] = None,\n    method: str = \"spline\",\n    return_with_time: bool = False,\n    tnew: Optional[np.ndarray] = None,\n    interp_kw: dict = {},\n    verbose: int = 0,\n) -> np.ndarray:\n    \"\"\"\n    Resample the 2d irregular timeseries `sig` into a 1d or 2d\n    regular time series with frequency `output_fs`,\n    elements of `sig` are in the form ``[time, value]``,\n    where the unit of `time` is ms.\n\n    Parameters\n    ----------\n    sig : numpy.ndarray\n        The 2d irregular timeseries.\n        Each row is ``[time, value]``.\n    output_fs : numbers.Real, optional\n        the frequency of the output 1d regular timeseries,\n        one and only one of `output_fs` and `tnew` should be specified\n    method : str, default \"spline\"\n        interpolation method, can be \"spline\" or \"interp1d\"\n    return_with_time : bool, default False\n        return a 2d array, with the 0-th coordinate being time\n    tnew : array_like, optional\n        the array of time of the output array,\n        one and only one of `output_fs` and `tnew` should be specified\n    interp_kw : dict, optional\n        additional options for the corresponding methods in scipy.interpolate\n\n    Returns\n    -------\n    numpy.ndarray\n        A 1d or 2d regular time series with frequency `output_freq`.\n\n    Examples\n    --------\n    .. code-block:: python\n\n        fs = 100\n        t_irr = np.sort(np.random.rand(fs)) * 1000\n        vals = np.random.randn(fs)\n        sig = np.stack([t_irr, vals], axis=1)\n        sig_reg = resample_irregular_timeseries(sig, output_fs=fs * 2, return_with_time=True)\n        sig_reg = resample_irregular_timeseries(sig, output_fs=fs, method=\"interp1d\")\n        t_irr_2 = np.sort(np.random.rand(2 * fs)) * 1000\n        sig_reg = resample_irregular_timeseries(sig, tnew=t_irr_2, return_with_time=True)\n\n    NOTE\n    ----\n    ``pandas`` also has the function to regularly resample irregular timeseries.\n\n    \"\"\"\n    assert sig.ndim == 2, \"`sig` should be a 2D array\"\n    assert method.lower() in [\n        \"spline\",\n        \"interp1d\",\n    ], f\"method `{method}` not supported\"\n    assert sum([output_fs is None, tnew is None]) == 1, \"one and only one of `output_fs` and `tnew` should be specified\"\n\n    _interp_kw = deepcopy(interp_kw)\n\n    if verbose >= 1:\n        print(f\"len(sig) = {len(sig)}\")\n\n    if len(sig) == 0:\n        return np.array([])\n\n    dtype = sig.dtype\n    time_series = np.atleast_2d(sig).astype(dtype)\n    if tnew is None:\n        step_ts = 1000 / output_fs\n        tot_len = int((time_series[-1][0] - time_series[0][0]) / step_ts) + 1\n        xnew = time_series[0][0] + np.arange(0, tot_len * step_ts, step_ts)\n    else:\n        assert tnew.ndim == 1, \"`tnew` should be a 1D array\"\n        xnew = np.array(tnew)\n        tot_len = len(xnew)\n\n    if verbose >= 1:\n        print(f\"time_series start ts = {time_series[0][0]}, end ts = {time_series[-1][0]}\")\n        print(f\"tot_len = {tot_len}\")\n        print(f\"xnew start = {xnew[0]}, end = {xnew[-1]}\")\n\n    if method.lower() == \"spline\":\n        m = len(time_series)\n        w = interp_kw.get(\"w\", np.ones(shape=(m,)))\n        # s = interp_kw.get(\"s\", np.random.uniform(m-np.sqrt(2*m),m+np.sqrt(2*m)))\n        s = interp_kw.get(\"s\", m - np.sqrt(2 * m))\n        _interp_kw.update(w=w, s=s)\n\n        tck = interpolate.splrep(time_series[:, 0], time_series[:, 1], **_interp_kw)\n\n        regular_timeseries = interpolate.splev(xnew, tck)\n    elif method.lower() == \"interp1d\":\n        f = interpolate.interp1d(time_series[:, 0], time_series[:, 1], **_interp_kw)\n\n        regular_timeseries = f(xnew)\n\n    if return_with_time:\n        return np.column_stack((xnew, regular_timeseries)).astype(dtype)\n    else:\n        return regular_timeseries.astype(dtype)\n\n", "right_context": "\n\ndef remove_spikes_naive(sig: np.ndarray, threshold: Real = 20, inplace: bool = True) -> np.ndarray:\n    \"\"\"Remove signal spikes using a naive method.\n\n    This is a method proposed in entry 0416 of CPSC2019.\n    `spikes` here refers to abrupt large bumps with (abs) value\n    larger than the given threshold,\n    or nan values (read by `wfdb`).\n    Do **NOT** confuse with `spikes` in paced rhythm.\n\n    Parameters\n    ----------\n    sig : numpy.ndarray\n        1D signal with potential spikes.\n    threshold : numbers.Real, optional\n        Values of `sig` that are larger than `threshold` will be removed.\n    inplace : bool, optional\n        Whether to modify `sig` in place or not.\n\n    Returns\n    -------\n    numpy.ndarray\n        Signal with `spikes` removed.\n\n    Examples\n    --------\n    .. code-block:: python\n\n        sig = np.random.randn(1000)\n        pos = np.random.randint(0, 1000, 10)\n        sig[pos] = 100\n        sig = remove_spikes_naive(sig)\n        pos = np.random.randint(0, 1000, 1)\n        sig[pos] = np.nan\n        sig = remove_spikes_naive(sig)\n\n    \"\"\"\n    dtype = sig.dtype\n    b = list(\n        filter(\n            lambda k: k > 0,\n            np.argwhere(np.logical_or(np.abs(sig) > threshold, np.isnan(sig))).squeeze(-1),\n        )\n    )\n    if not inplace:\n        sig = sig.copy()\n    if abs(sig[0]) > threshold or np.isnan(sig[0]):\n        sig[0] = 0\n    for k in b:\n        sig[k] = sig[k - 1]\n    return sig.astype(dtype)\n\n\ndef butter_bandpass(lowcut: Real, highcut: Real, fs: Real, order: int, verbose: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Butterworth Bandpass Filter Design.\n\n    Parameters\n    ----------\n    lowcut : numbers.Real\n        Low cutoff frequency.\n    highcut : numbers.Real\n        High cutoff frequency.\n    fs : numbers.Real\n        Sampling frequency of `data`.\n    order : int,\n        Order of the filter.\n    verbose : int, default 0\n        Verbosity level for debugging.\n\n    Returns\n    -------\n    b, a : numpy.ndarray\n        Coefficients of numerator and denominator of the filter.\n\n    NOTE\n    ----\n    According to `lowcut` and `highcut`,\n    the filter type might degenerate to lowpass or highpass filter.\n\n    References\n    ----------\n    1. :func:`scipy.signal.butter`\n    2. https://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html\n\n    \"\"\"\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    if low >= 1:\n        raise ValueError(\"frequency out of range!\")\n    high = highcut / nyq\n\n    if low <= 0 and high >= 1:\n        raise ValueError(\"frequency out of range!\")\n\n    if low <= 0:\n        Wn = high\n        btype = \"low\"\n    elif high >= 1:\n        Wn = low\n        btype = \"high\"\n    elif lowcut == highcut:\n        Wn = high\n        btype = \"low\"\n    else:\n        Wn = [low, high]\n        btype = \"band\"\n\n    if verbose >= 1:\n        print(f\"by the setup of lowcut and highcut, the filter type falls to {btype}, with Wn = {Wn}\")\n\n    b, a = butter(order, Wn, btype=btype)\n    return b, a\n\n\ndef butter_bandpass_filter(\n    data: np.ndarray,\n    lowcut: Real,\n    highcut: Real,\n    fs: Real,\n    order: int,\n    btype: Optional[str] = None,\n    verbose: int = 0,\n) -> np.ndarray:\n    \"\"\"Butterworth bandpass filtering the signals.\n\n    Apply a Butterworth bandpass filter to the signal.\n    For references, see [#bp1]_ and [#bp2]_.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        Signal to be filtered.\n    lowcut : numbers.real\n        Low cutoff frequency.\n    highcut : numbers.real\n        High cutoff frequency.\n    fs : numbers.real\n        Frequency of the signal.\n    order : int\n        Order of the filter.\n    btype : {\"lohi\", \"hilo\"}, optional\n        (special) type of the filter.\n        Ignored for lowpass and highpass filters\n        (as defined by `lowcut` and `highcut`).\n    verbose : int, default 0\n        Verbosity level for printing.\n\n    Returns\n    -------\n    y : numpy.ndarray\n        The filtered signal.\n\n    References\n    ----------\n    .. [#bp1] https://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html\n    .. [#bp2] https://dsp.stackexchange.com/q/19084\n\n    \"\"\"\n    dtype = data.dtype\n    if btype is None:\n        b, a = butter_bandpass(lowcut, highcut, fs, order=order, verbose=verbose)\n        y = filtfilt(b, a, data)\n        return y.astype(dtype)\n    if btype.lower() == \"lohi\":\n        b, a = butter_bandpass(0, highcut, fs, order=order, verbose=verbose)\n        y = filtfilt(b, a, data)\n        b, a = butter_bandpass(lowcut, fs, fs, order=order, verbose=verbose)\n        y = filtfilt(b, a, y)\n    elif btype.lower() == \"hilo\":\n        b, a = butter_bandpass(lowcut, fs, fs, order=order, verbose=verbose)\n        y = filtfilt(b, a, data)\n        b, a = butter_bandpass(0, highcut, fs, order=order, verbose=verbose)\n        y = filtfilt(b, a, y)\n    else:\n        raise ValueError(f\"special btype `{btype}` is not supported\")\n    return y.astype(dtype)\n\n\ndef get_ampl(\n    sig: np.ndarray,\n    fs: Real,\n    fmt: str = \"lead_first\",\n    window: Real = 0.2,\n    critical_points: Optional[Sequence] = None,\n) -> Union[float, np.ndarray]:\n    \"\"\"Get amplitude of a signal (near critical points if given).\n\n    Parameters\n    ----------\n    sig : numpy.ndarray\n        (ECG) signal.\n    fs : numbers.Real\n        Sampling frequency of the signal\n    fmt : str, default \"lead_first\"\n        Format of the signal, can be\n        \"channel_last\" (alias \"lead_last\"), or\n        \"channel_first\" (alias \"lead_first\").\n        Ignored if sig is 1d array (single-lead).\n    window : int, default 0.2\n        Window length of a window for computing amplitude, with units in seconds.\n    critical_points : numpy.ndarray, optional\n        Positions of critical points near which to compute amplitude,\n        e.g. can be rpeaks, t peaks, etc.\n\n    Returns\n    -------\n    ampl : float or numpy.ndarray\n        Amplitude of the signal.\n\n    \"\"\"\n    dtype = sig.dtype\n    if fmt.lower() in [\"channel_last\", \"lead_last\"]:\n        _sig = sig.T\n    elif fmt.lower() in [\"channel_first\", \"lead_first\"]:\n        _sig = sig\n    else:\n        raise ValueError(f\"unknown format `{fmt}`\")\n    _window = int(round(window * fs))\n    half_window = _window // 2\n    _window = half_window * 2\n    if critical_points is not None:\n        s = np.stack(\n            [\n                ensure_siglen(\n                    _sig[\n                        ...,\n                        max(0, p - half_window) : min(_sig.shape[-1], p + half_window),\n                    ],\n                    siglen=_window,\n                    fmt=\"lead_first\",\n                )\n                for p in critical_points\n            ],\n            axis=-1,\n        ).astype(dtype)\n        # the following is much slower\n        # for p in critical_points:\n        #     s = _sig[...,max(0,p-half_window):min(_sig.shape[-1],p+half_window)]\n        #     ampl = np.max(np.array([ampl, np.max(s,axis=-1) - np.min(s,axis=-1)]), axis=0)\n    else:\n        s = np.stack(\n            [_sig[..., idx * half_window : idx * half_window + _window] for idx in range(_sig.shape[-1] // half_window - 1)],\n            axis=-1,\n        ).astype(dtype)\n        # the following is much slower\n        # for idx in range(_sig.shape[-1]//half_window-1):\n        #     s = _sig[..., idx*half_window: idx*half_window+_window]\n        #     ampl = np.max(np.array([ampl, np.max(s,axis=-1) - np.min(s,axis=-1)]), axis=0)\n    ampl = np.max(np.max(s, axis=-2) - np.min(s, axis=-2), axis=-1)\n    return ampl\n\n\ndef normalize(\n    sig: np.ndarray,\n    method: str,\n    mean: Union[Real, Iterable[Real]] = 0.0,\n    std: Union[Real, Iterable[Real]] = 1.0,\n    sig_fmt: str = \"channel_first\",\n    per_channel: bool = False,\n) -> np.ndarray:\n    \"\"\"Normalize a signal.\n\n    Perform z-score normalization on `sig`,\n    to make it has fixed mean and standard deviation,\n    or perform min-max normalization on `sig`,\n    or normalize `sig` using `mean` and `std` via (sig - mean) / std.\n    More precisely,\n\n    .. math::\n\n        \\\\begin{align*}\n        \\\\text{Min-Max normalization:}\\\\quad & \\\\frac{sig - \\\\min(sig)}{\\\\max(sig) - \\\\min(sig)} \\\\\\\\\n        \\\\text{Naive normalization:}\\\\quad & \\\\frac{sig - m}{s} \\\\\\\\\n        \\\\text{Z-score normalization:}\\\\quad & \\\\left(\\\\frac{sig - mean(sig)}{std(sig)}\\\\right) \\\\cdot s + m\n        \\\\end{align*}\n\n    Parameters\n    ----------\n    sig : numpy.ndarray\n        The signal to be normalized.\n    method : {\"naive\", \"min-max\", \"z-score\"}\n        Normalization method, case insensitive.\n    mean : numbers.Real or array_like, default 0.0\n        Mean value of the normalized signal,\n        or mean values for each lead of the normalized signal.\n        Useless if `method` is \"min-max\".\n    std : numbers.Real or array_like, default 1.0\n        Standard deviation of the normalized signal,\n        or standard deviations for each lead of the normalized signal.\n        Useless if `method` is \"min-max\".\n    sig_fmt : str, default \"channel_first\"\n        Format of the signal, can be of one of\n        \"channel_last\" (alias \"lead_last\"), or\n        \"channel_first\" (alias \"lead_first\"),\n        ignored if sig is 1d array (single-lead).\n    per_channel : bool, default False\n        If True, normalization will be done per channel.\n        Ignored if `sig` is 1d array (single-lead).\n\n    Returns\n    -------\n    nm_sig : numpy.ndarray\n        The normalized signal.\n\n    NOTE\n    ----\n    In cases where normalization is infeasible (``std = 0``),\n    only the mean value will be shifted.\n\n    \"\"\"\n    assert sig.ndim in [1, 2, 3], \"signal `sig` should be 1d or 2d or 3d array\"\n    if sig.ndim == 1 and per_channel:\n        warnings.warn(\n            \"per-channel normalization is not supported for 1d signal, \" \"`per_channel` will be set to False\",\n            RuntimeWarning,\n        )\n        per_channel = False\n    dtype = sig.dtype\n    _method = method.lower()\n    assert _method in [\n        \"z-score\",\n        \"naive\",\n        \"min-max\",\n    ], f\"unknown normalization method `{method}`\"\n    if not per_channel:\n        if sig.ndim == 2:\n            assert isinstance(mean, Real) and isinstance(\n                std, Real\n            ), \"`mean` and `std` should be real numbers in the non per-channel setting for 2d signal\"\n        else:  # sig.ndim == 3\n            assert (isinstance(mean, Real) or np.shape(mean) == (sig.shape[0],)) and (\n                isinstance(std, Real) or np.shape(std) == (sig.shape[0],)\n            ), (\n                f\"`mean` and `std` should be real numbers or have shape ({sig.shape[0]},) \"\n                \"in the non per-channel setting for 3d signal\"\n            )\n    if isinstance(std, Real):\n        assert std > 0, \"standard deviation should be positive\"\n    else:\n        assert (np.array(std) > 0).all(), \"standard deviations should all be positive\"\n    assert sig_fmt.lower() in [\n        \"channel_first\",\n        \"lead_first\",\n        \"channel_last\",\n        \"lead_last\",\n    ], f\"format `{sig_fmt}` of the signal not supported!\"\n\n    if isinstance(mean, Iterable):\n        assert sig.ndim in [2, 3], \"`mean` should be a real number for 1d signal\"\n        if sig.ndim == 2:\n            assert np.shape(mean) in [\n                (sig.shape[0],),\n                (sig.shape[-1],),\n            ], f\"shape of `mean` = {np.shape(mean)} not compatible with the `sig` = {np.shape(sig)}\"\n            if sig_fmt.lower() in [\n                \"channel_first\",\n                \"lead_first\",\n            ]:\n                _mean = np.array(mean, dtype=dtype)[..., np.newaxis]\n            else:\n                _mean = np.array(mean, dtype=dtype)[np.newaxis, ...]\n        else:  # sig.ndim == 3\n            if sig_fmt.lower() in [\n                \"channel_first\",\n                \"lead_first\",\n            ]:\n                if np.shape(mean) == (sig.shape[0],):\n                    _mean = np.array(mean, dtype=dtype)[..., np.newaxis, np.newaxis]\n                elif np.shape(mean) == (sig.shape[1],):\n                    _mean = np.repeat(\n                        np.array(mean, dtype=dtype)[np.newaxis, ..., np.newaxis],\n                        sig.shape[0],\n                        axis=0,\n                    )\n                elif np.shape(mean) == sig.shape[:2]:\n                    _mean = np.array(mean, dtype=dtype)[..., np.newaxis]\n                else:\n                    raise AssertionError(f\"shape of `mean` = {np.shape(mean)} not compatible with the `sig` = {np.shape(sig)}\")\n            else:  # \"channel_last\" or \"lead_last\"\n                if np.shape(mean) == (sig.shape[0],):\n                    _mean = np.array(mean, dtype=dtype)[..., np.newaxis, np.newaxis]\n                elif np.shape(mean) == (sig.shape[-1],):\n                    _mean = np.repeat(\n                        np.array(mean, dtype=dtype)[np.newaxis, np.newaxis, ...],\n                        sig.shape[0],\n                        axis=0,\n                    )\n                elif np.shape(mean) == (sig.shape[0], sig.shape[-1]):\n                    _mean = np.expand_dims(np.array(mean, dtype=dtype), axis=1)\n                else:\n                    raise AssertionError(f\"shape of `mean` = {np.shape(mean)} not compatible with the `sig` = {np.shape(sig)}\")\n    else:\n        _mean = mean\n    if isinstance(std, Iterable):\n        assert sig.ndim in [2, 3], \"`std` should be a real number for 1d signal\"\n        if sig.ndim == 2:\n            assert np.shape(std) in [\n                (sig.shape[0],),\n                (sig.shape[-1],),\n            ], f\"shape of `std` = {np.shape(std)} not compatible with the `sig` = {np.shape(sig)}\"\n            if sig_fmt.lower() in [\n                \"channel_first\",\n                \"lead_first\",\n            ]:\n                _std = np.array(std, dtype=dtype)[..., np.newaxis]\n            else:\n                _std = np.array(std, dtype=dtype)[np.newaxis, ...]\n        else:  # sig.ndim == 3\n            if sig_fmt.lower() in [\n                \"channel_first\",\n                \"lead_first\",\n            ]:\n                if np.shape(std) == (sig.shape[0],):\n                    _std = np.array(std, dtype=dtype)[..., np.newaxis, np.newaxis]\n                elif np.shape(std) == (sig.shape[1],):\n                    _std = np.repeat(\n                        np.array(std, dtype=dtype)[np.newaxis, ..., np.newaxis],\n                        sig.shape[0],\n                        axis=0,\n                    )\n                elif np.shape(std) == sig.shape[:2]:\n                    _std = np.array(std, dtype=dtype)[..., np.newaxis]\n                else:\n                    raise AssertionError(f\"shape of `std` = {np.shape(std)} not compatible with the `sig` = {np.shape(sig)}\")\n            else:  # \"channel_last\" or \"lead_last\"\n                if np.shape(std) == (sig.shape[0],):\n                    _std = np.array(std, dtype=dtype)[..., np.newaxis, np.newaxis]\n                elif np.shape(std) == (sig.shape[-1],):\n                    _std = np.repeat(\n                        np.array(std, dtype=dtype)[np.newaxis, np.newaxis, ...],\n                        sig.shape[0],\n                        axis=0,\n                    )\n                elif np.shape(std) == (sig.shape[0], sig.shape[-1]):\n                    _std = np.expand_dims(np.array(std, dtype=dtype), axis=1)\n                else:\n                    raise AssertionError(f\"shape of `std` = {np.shape(std)} not compatible with the `sig` = {np.shape(sig)}\")\n    else:\n        _std = std\n\n    if _method == \"naive\":\n        nm_sig = (sig - _mean) / _std\n        return nm_sig.astype(dtype)\n\n    eps = 1e-7  # to avoid dividing by zero\n    if sig.ndim == 3:  # the first dimension is the batch dimension\n        if not per_channel:\n            options = dict(axis=(1, 2), keepdims=True)\n        elif sig_fmt.lower() in [\n            \"channel_first\",\n            \"lead_first\",\n        ]:\n            options = dict(axis=2, keepdims=True)\n        else:\n            options = dict(axis=1, keepdims=True)\n    else:\n        if not per_channel:\n            options = dict(axis=None)\n        elif sig_fmt.lower() in [\n            \"channel_first\",\n            \"lead_first\",\n        ]:\n            options = dict(axis=1, keepdims=True)\n        else:\n            options = dict(axis=0, keepdims=True)\n\n    if _method == \"z-score\":\n        nm_sig = ((sig - np.mean(sig, dtype=dtype, **options)) / (np.std(sig, dtype=dtype, **options) + eps)) * _std + _mean\n    elif _method == \"min-max\":\n        nm_sig = (sig - np.amin(sig, **options)) / (np.amax(sig, **options) - np.amin(sig, **options) + eps)\n    return nm_sig.astype(dtype)\n", "import_text": ["warnings", "copy.deepcopy", "numbers.Real", "typing.Iterable", "typing.Optional", "typing.Sequence", "typing.Tuple", "typing.Union", "numpy", "scipy.interpolate", "scipy.signal.butter", "scipy.signal.filtfilt", "scipy.signal.peak_prominences"], "prompt": "\"\"\"\nDescription: This function detects peaks in a sequence of data.\n\nArgs:\n    x (Sequence): The sequence of data to detect peaks in.\n    mph (Optional[Real]): The minimum peak height. Defaults to None.\n    mpd (int): The minimum peak distance. Defaults to 1.\n    threshold (Real): The threshold for peak detection. Defaults to 0.\n    left_threshold (Real): The threshold for peak detection from the left. Defaults to 0.\n    right_threshold (Real): The threshold for peak detection from the right. Defaults to 0.\n    prominence (Optional[Real]): The minimum peak prominence. Defaults to None.\n    prominence_wlen (Optional[int]): The window length for peak prominence calculation. Defaults to None.\n    edge (Union[str, None]): The edge for peak detection. Defaults to \"rising\".\n    kpsh (bool): Keep peaks with same height. Defaults to False.\n    valley (bool): Detect valleys instead of peaks. Defaults to False.\n    show (bool): Show the plot. Defaults to False.\n    ax (Optional[matplotlib.axes.Axes]): The axes to plot on. Defaults to None.\n    verbose (int): The verbosity level. Defaults to 0.\n\nReturns:\n    np.ndarray: The indices of the detected peaks.\n\"\"\"\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Detect peaks in data based on their amplitude and other features.\n\n    Parameters\n    ----------\n    x : array_like\n        1D array of data.\n    mph : positive number, optional\n        abbr. for maximum (minimum) peak height,\n        detect peaks that are greater than minimum peak height (if parameter `valley` is False),\n        or peaks that are smaller than maximum peak height (if parameter `valley` is True)\n    mpd : positive integer, default 1\n        abbr. for minimum peak distance,\n        detect peaks that are at least separated by minimum peak distance (in number of samples)\n    threshold : positive number, default 0\n        detect peaks (valleys) that are greater (smaller) than `threshold`,\n        in relation to their neighbors within the range of `mpd`\n    left_threshold : positive number, default 0\n        `threshold` that is restricted to the left\n    right_threshold : positive number, default 0\n        `threshold` that is restricted to the left\n    prominence: positive number, optional\n        threshold of prominence of the detected peaks (valleys)\n    prominence_wlen : positive int, optional\n        the `wlen` parameter of the function `scipy.signal.peak_prominences`\n    edge : str or None, default \"rising\"\n        can also be \"falling\", \"both\",\n        for a flat peak, keep only the rising edge (\"rising\"), only the falling edge (\"falling\"),\n        both edges (\"both\"), or don't detect a flat peak (None)\n    kpsh : bool, default False\n        keep peaks with same height even if they are closer than `mpd`\n    valley : bool, default False\n        if True (1), detect valleys (local minima) instead of peaks\n    show : bool, default False\n        if True (1), plot data in matplotlib figure\n    ax : a matplotlib.axes.Axes instance, optional\n\n    Returns\n    -------\n    ind : array_like\n        Indeces of the peaks in `x`.\n\n    NOTE\n    ----\n    The detection of valleys instead of peaks is performed internally by simply\n    negating the data: ``ind_valleys = detect_peaks(-x)``.\n\n    The function can handle NaN's.\n\n    See this IPython Notebook [#peak]_.\n\n    References\n    ----------\n    .. [#peak] https://nbviewer.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\n\n    Examples\n    --------\n    .. code-block:: python\n\n        x = np.random.randn(100)\n        x[60:81] = np.nan\n        # detect all peaks and plot data\n        ind = detect_peaks(x, show=True)\n        print(ind)\n\n        x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5\n        # set minimum peak height = 0 and minimum peak distance = 20\n        detect_peaks(x, mph=0, mpd=20, show=True)\n\n        x = [0, 1, 0, 2, 0, 3, 0, 2, 0, 1, 0]\n        # set minimum peak distance = 2\n        detect_peaks(x, mpd=2, show=True)\n\n        x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5\n        # detection of valleys instead of peaks\n        detect_peaks(x, mph=-1.2, mpd=20, valley=True, show=True)\n\n        x = [0, 1, 1, 0, 1, 1, 0]\n        # detect both edges\n        detect_peaks(x, edge=\"both\", show=True)\n\n        x = [-2, 1, -2, 2, 1, 1, 3, 0]\n        # set threshold = 2\n        detect_peaks(x, threshold = 2, show=True)\n\n    Version history\n    ---------------\n    \"1.0.5\":\n        The sign of `mph` is inverted if parameter `valley` is True\n\n    \"\"\"", "function_dependencies": ["copy.deepcopy", "numpy.atleast_1d", "numpy.atleast_1d.astype", "numpy.array", "numpy.where", "numpy.isnan", "numpy.hstack", "numpy.unique", "numpy.unique.tolist", "numpy.in1d", "numpy.array.tolist", "numpy.max", "numpy.vstack", "numpy.delete", "numpy.max.tolist", "numpy.delete.tolist", "numpy.argsort", "numpy.zeros", "numpy.sort", "scipy.signal.peak_prominences"], "project_create_time": "2020-09-25T06:03:17+00:00", "project_update_time": "2024-04-16T13:51:10+00:00", "file_create_time": "2020-09-25T06:01:23Z", "file_update_time": "2024-03-12T03:42:06Z", "function_update_time": "2020-09-25T06:01:23Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["numpy.atleast_1d"], "test_function": [{"file_path": "/torch_ecg-v0.0.28/torch_ecg-0.0.28/test/test_utils/test_utils_signal.py", "class_name": null, "function_name": "test_detect_peaks", "code": "\ndef test_detect_peaks():\n    # TODO: it's quite weird that\n    # very rarely, the detected peaks are empty\n    x = DEFAULTS.RNG.normal(size=(100,))\n    x[60:81] = np.nan\n    ind = detect_peaks(x, verbose=2)\n    # assert ind.ndim == 1 and len(ind) > 0\n    assert ind.ndim == 1 and \"int\" in str(ind.dtype)\n\n    x = np.sin(2 * np.pi * 5 * np.linspace(0, 1, 200)) + DEFAULTS.RNG.normal(size=(200,)) / 5\n    # set minimum peak height = 0 and minimum peak distance = 20\n    ind = detect_peaks(x, mph=0, mpd=20, verbose=2)\n    # assert ind.ndim == 1 and len(ind) > 0\n    assert ind.ndim == 1 and \"int\" in str(ind.dtype)\n\n    x = [0, 1, 0, 2, 0, 3, 0, 2, 0, 1, 0]\n    # set minimum peak distance = 2\n    ind = detect_peaks(x, mpd=2, verbose=2)\n    # assert ind.ndim == 1 and len(ind) > 0\n    assert ind.ndim == 1 and \"int\" in str(ind.dtype)\n\n    x = np.sin(2 * np.pi * 5 * np.linspace(0, 1, 200)) + DEFAULTS.RNG.normal(size=(200,)) / 5\n    # detection of valleys instead of peaks\n    ind = detect_peaks(x, mph=-1.2, mpd=20, valley=True, verbose=2)\n    assert ind.ndim == 1 and len(ind) > 0\n\n    x = [0, 1, 3, 0, 1, -1, 0]\n    # detect both edges\n    ind = detect_peaks(x, edge=\"both\", verbose=2)\n    # assert ind.ndim == 1 and len(ind) > 0\n    assert ind.ndim == 1 and \"int\" in str(ind.dtype)\n    ind = detect_peaks(x, edge=None, prominence=0.4, verbose=2)\n    assert ind.ndim == 1 and \"int\" in str(ind.dtype)\n\n    x = [-2, 1, -2, 2, 1, 1, 3, 0]\n    # set threshold = 2\n    ind = detect_peaks(x, threshold=2, verbose=2)\n    # assert ind.ndim == 1 and len(ind) > 0\n    assert ind.ndim == 1 and \"int\" in str(ind.dtype)\n\n    assert len(detect_peaks([0, 1], verbose=2)) == 0"}]}, {"git_group": "hmmlearn", "git_name": "hmmlearn", "version": "0.3.2", "language": "Python", "project_name": "hmmlearn-0.3.2.zip", "file_path": "/hmmlearn-0.3.2/hmmlearn-0.3.2/lib/hmmlearn/utils.py", "file_name": "utils.py", "focal_class": null, "focal_name": "fill_covars", "focal_parameter": ["covars"], "solution": "\ndef fill_covars(covars, covariance_type='full', n_components=1, n_features=1):\n    if covariance_type == 'full':\n        return covars\n    elif covariance_type == 'diag':\n        return np.array(list(map(np.diag, covars)))\n    elif covariance_type == 'tied':\n        return np.tile(covars, (n_components, 1, 1))\n    elif covariance_type == 'spherical':\n        # Regardless of what is passed in, we flatten in\n        # and then expand it to the correct shape\n        covars = np.ravel(covars)\n        eye = np.eye(n_features)[np.newaxis, :, :]\n        covars = covars[:, np.newaxis, np.newaxis]\n        return eye * covars", "function_signature": "def fill_covars(covars, covariance_type='full', n_components=1, n_features=1) :", "left_context": "import numpy as np\nfrom scipy import special\n\n\ndef normalize(a, axis=None):\n    \"\"\"\n    Normalize the input array so that it sums to 1.\n\n    Parameters\n    ----------\n    a : array\n        Non-normalized input data.\n\n    axis : int\n        Dimension along which normalization is performed.\n\n    Notes\n    -----\n    Modifies the input **inplace**.\n    \"\"\"\n    a_sum = a.sum(axis)\n    if axis and a.ndim > 1:\n        # Make sure we don't divide by zero.\n        a_sum[a_sum == 0] = 1\n        shape = list(a.shape)\n        shape[axis] = 1\n        a_sum.shape = shape\n\n    a /= a_sum\n\n\ndef log_normalize(a, axis=None):\n    \"\"\"\n    Normalize the input array so that ``sum(exp(a)) == 1``.\n\n    Parameters\n    ----------\n    a : array\n        Non-normalized input data.\n\n    axis : int\n        Dimension along which normalization is performed.\n\n    Notes\n    -----\n    Modifies the input **inplace**.\n    \"\"\"\n    if axis is not None and a.shape[axis] == 1:\n        # Handle single-state GMMHMM in the degenerate case normalizing a\n        # single -inf to zero.\n        a[:] = 0\n    else:\n        with np.errstate(under=\"ignore\"):\n            a_lse = special.logsumexp(a, axis, keepdims=True)\n        a -= a_lse\n\n", "right_context": "", "import_text": ["numpy", "scipy.special"], "prompt": "\"\"\"\nDescription: This function fills the covariance matrix based on the specified type.\n\nArgs:\n    covars (list): A list of covariance matrices.\n    covariance_type (str): The type of covariance matrix. Can be 'full', 'diag', 'tied', or 'spherical'.\n    n_components (int): The number of components.\n    n_features (int): The number of features.\n\nReturns:\n    numpy.ndarray: The filled covariance matrix.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["numpy.array", "numpy.tile", "numpy.ravel", "numpy.eye"], "project_create_time": "2014-03-23T10:33:09+00:00", "project_update_time": "2024-04-17T09:42:42+00:00", "file_create_time": "2018-10-17T15:40:05Z", "file_update_time": "2022-12-21T10:53:55Z", "function_update_time": "2018-10-17T15:40:05Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["numpy.eye", "numpy.tile"], "test_function": [{"file_path": "/hmmlearn-0.3.2/hmmlearn-0.3.2/lib/hmmlearn/tests/test_utils.py", "class_name": null, "function_name": "test_fill_covars", "code": "\ndef test_fill_covars():\n    full = np.arange(12).reshape(3, 2, 2) + 1\n    np.testing.assert_equal(fill_covars(full, 'full', 3, 2), full)\n\n    diag = np.arange(6).reshape(3, 2) + 1\n    expected = np.array([[[1, 0], [0, 2]],\n                         [[3, 0], [0, 4]],\n                         [[5, 0], [0, 6]]])\n    np.testing.assert_equal(fill_covars(diag, 'diag', 3, 2), expected)\n\n    tied = np.arange(4).reshape(2, 2) + 1\n    expected = np.array([[[1, 2], [3, 4]],\n                         [[1, 2], [3, 4]],\n                         [[1, 2], [3, 4]]])\n    np.testing.assert_equal(fill_covars(tied, 'tied', 3, 2), expected)\n\n    spherical = np.array([1, 2, 3])\n    expected = np.array([[[1, 0], [0, 1]],\n                         [[2, 0], [0, 2]],\n                         [[3, 0], [0, 3]]])\n    np.testing.assert_equal(\n        fill_covars(spherical, 'spherical', 3, 2), expected)"}]}, {"git_group": "SqueezeAILab", "git_name": "KVQuant", "version": "main", "language": "Python", "project_name": "KVQuant-main.zip", "file_path": "/KVQuant-main/KVQuant-main/quant/dbrx/src/transformers/audio_utils.py", "file_name": "audio_utils.py", "focal_class": null, "focal_name": "spectrogram", "focal_parameter": [], "solution": "def spectrogram(\n    waveform: np.ndarray,\n    window: np.ndarray,\n    frame_length: int,\n    hop_length: int,\n    fft_length: Optional[int] = None,\n    power: Optional[float] = 1.0,\n    center: bool = True,\n    pad_mode: str = \"reflect\",\n    onesided: bool = True,\n    preemphasis: Optional[float] = None,\n    mel_filters: Optional[np.ndarray] = None,\n    mel_floor: float = 1e-10,\n    log_mel: Optional[str] = None,\n    reference: float = 1.0,\n    min_value: float = 1e-10,\n    db_range: Optional[float] = None,\n    remove_dc_offset: Optional[bool] = None,\n    dtype: np.dtype = np.float32,\n) -> np.ndarray:\n    window_length = len(window)\n\n    if fft_length is None:\n        fft_length = frame_length\n\n    if frame_length > fft_length:\n        raise ValueError(f\"frame_length ({frame_length}) may not be larger than fft_length ({fft_length})\")\n\n    if window_length != frame_length:\n        raise ValueError(f\"Length of the window ({window_length}) must equal frame_length ({frame_length})\")\n\n    if hop_length <= 0:\n        raise ValueError(\"hop_length must be greater than zero\")\n\n    if waveform.ndim != 1:\n        raise ValueError(f\"Input waveform must have only one dimension, shape is {waveform.shape}\")\n\n    if np.iscomplexobj(waveform):\n        raise ValueError(\"Complex-valued input waveforms are not currently supported\")\n\n    if power is None and mel_filters is not None:\n        raise ValueError(\n            \"You have provided `mel_filters` but `power` is `None`. Mel spectrogram computation is not yet supported for complex-valued spectrogram.\"\n            \"Specify `power` to fix this issue.\"\n        )\n\n    # center pad the waveform\n    if center:\n        padding = [(int(frame_length // 2), int(frame_length // 2))]\n        waveform = np.pad(waveform, padding, mode=pad_mode)\n\n    # promote to float64, since np.fft uses float64 internally\n    waveform = waveform.astype(np.float64)\n    window = window.astype(np.float64)\n\n    # split waveform into frames of frame_length size\n    num_frames = int(1 + np.floor((waveform.size - frame_length) / hop_length))\n\n    num_frequency_bins = (fft_length // 2) + 1 if onesided else fft_length\n    spectrogram = np.empty((num_frames, num_frequency_bins), dtype=np.complex64)\n\n    # rfft is faster than fft\n    fft_func = np.fft.rfft if onesided else np.fft.fft\n    buffer = np.zeros(fft_length)\n\n    timestep = 0\n    for frame_idx in range(num_frames):\n        buffer[:frame_length] = waveform[timestep : timestep + frame_length]\n\n        if remove_dc_offset:\n            buffer[:frame_length] = buffer[:frame_length] - buffer[:frame_length].mean()\n\n        if preemphasis is not None:\n            buffer[1:frame_length] -= preemphasis * buffer[: frame_length - 1]\n            buffer[0] *= 1 - preemphasis\n\n        buffer[:frame_length] *= window\n\n        spectrogram[frame_idx] = fft_func(buffer)\n        timestep += hop_length\n\n    # note: ** is much faster than np.power\n    if power is not None:\n        spectrogram = np.abs(spectrogram, dtype=np.float64) ** power\n\n    spectrogram = spectrogram.T\n\n    if mel_filters is not None:\n        spectrogram = np.maximum(mel_floor, np.dot(mel_filters.T, spectrogram))\n\n    if power is not None and log_mel is not None:\n        if log_mel == \"log\":\n            spectrogram = np.log(spectrogram)\n        elif log_mel == \"log10\":\n            spectrogram = np.log10(spectrogram)\n        elif log_mel == \"dB\":\n            if power == 1.0:\n                spectrogram = amplitude_to_db(spectrogram, reference, min_value, db_range)\n            elif power == 2.0:\n                spectrogram = power_to_db(spectrogram, reference, min_value, db_range)\n            else:\n                raise ValueError(f\"Cannot use log_mel option '{log_mel}' with power {power}\")\n        else:\n            raise ValueError(f\"Unknown log_mel option: {log_mel}\")\n\n        spectrogram = np.asarray(spectrogram, dtype)\n\n    return spectrogram", "function_signature": "def spectrogram(\n    waveform: np.ndarray,\n    window: np.ndarray,\n    frame_length: int,\n    hop_length: int,\n    fft_length: Optional[int] = None,\n    power: Optional[float] = 1.0,\n    center: bool = True,\n    pad_mode: str = \"reflect\",\n    onesided: bool = True,\n    preemphasis: Optional[float] = None,\n    mel_filters: Optional[np.ndarray] = None,\n    mel_floor: float = 1e-10,\n    log_mel: Optional[str] = None,\n    reference: float = 1.0,\n    min_value: float = 1e-10,\n    db_range: Optional[float] = None,\n    remove_dc_offset: Optional[bool] = None,\n    dtype: np.dtype = np.float32,\n) -> np.ndarray :", "left_context": "# coding=utf-8\n# Copyright 2023 The HuggingFace Inc. team and the librosa & torchaudio authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nAudio processing functions to extract features from audio waveforms. This code is pure numpy to support all frameworks\nand remove unnecessary dependencies.\n\"\"\"\nimport warnings\nfrom typing import Optional, Tuple, Union\n\nimport numpy as np\n\n\ndef hertz_to_mel(freq: Union[float, np.ndarray], mel_scale: str = \"htk\") -> Union[float, np.ndarray]:\n    \"\"\"\n    Convert frequency from hertz to mels.\n\n    Args:\n        freq (`float` or `np.ndarray`):\n            The frequency, or multiple frequencies, in hertz (Hz).\n        mel_scale (`str`, *optional*, defaults to `\"htk\"`):\n            The mel frequency scale to use, `\"htk\"`, `\"kaldi\"` or `\"slaney\"`.\n\n    Returns:\n        `float` or `np.ndarray`: The frequencies on the mel scale.\n    \"\"\"\n\n    if mel_scale not in [\"slaney\", \"htk\", \"kaldi\"]:\n        raise ValueError('mel_scale should be one of \"htk\", \"slaney\" or \"kaldi\".')\n\n    if mel_scale == \"htk\":\n        return 2595.0 * np.log10(1.0 + (freq / 700.0))\n    elif mel_scale == \"kaldi\":\n        return 1127.0 * np.log(1.0 + (freq / 700.0))\n\n    min_log_hertz = 1000.0\n    min_log_mel = 15.0\n    logstep = 27.0 / np.log(6.4)\n    mels = 3.0 * freq / 200.0\n\n    if isinstance(freq, np.ndarray):\n        log_region = freq >= min_log_hertz\n        mels[log_region] = min_log_mel + np.log(freq[log_region] / min_log_hertz) * logstep\n    elif freq >= min_log_hertz:\n        mels = min_log_mel + np.log(freq / min_log_hertz) * logstep\n\n    return mels\n\n\ndef mel_to_hertz(mels: Union[float, np.ndarray], mel_scale: str = \"htk\") -> Union[float, np.ndarray]:\n    \"\"\"\n    Convert frequency from mels to hertz.\n\n    Args:\n        mels (`float` or `np.ndarray`):\n            The frequency, or multiple frequencies, in mels.\n        mel_scale (`str`, *optional*, `\"htk\"`):\n            The mel frequency scale to use, `\"htk\"`, `\"kaldi\"` or `\"slaney\"`.\n\n    Returns:\n        `float` or `np.ndarray`: The frequencies in hertz.\n    \"\"\"\n\n    if mel_scale not in [\"slaney\", \"htk\", \"kaldi\"]:\n        raise ValueError('mel_scale should be one of \"htk\", \"slaney\" or \"kaldi\".')\n\n    if mel_scale == \"htk\":\n        return 700.0 * (np.power(10, mels / 2595.0) - 1.0)\n    elif mel_scale == \"kaldi\":\n        return 700.0 * (np.exp(mels / 1127.0) - 1.0)\n\n    min_log_hertz = 1000.0\n    min_log_mel = 15.0\n    logstep = np.log(6.4) / 27.0\n    freq = 200.0 * mels / 3.0\n\n    if isinstance(mels, np.ndarray):\n        log_region = mels >= min_log_mel\n        freq[log_region] = min_log_hertz * np.exp(logstep * (mels[log_region] - min_log_mel))\n    elif mels >= min_log_mel:\n        freq = min_log_hertz * np.exp(logstep * (mels - min_log_mel))\n\n    return freq\n\n\ndef hertz_to_octave(\n    freq: Union[float, np.ndarray], tuning: Optional[float] = 0.0, bins_per_octave: Optional[int] = 12\n):\n    \"\"\"\n    Convert frequency from hertz to fractional octave numbers.\n    Adapted from *librosa*.\n\n    Args:\n        freq (`float` or `np.ndarray`):\n            The frequency, or multiple frequencies, in hertz (Hz).\n        tuning (`float`, defaults to `0.`):\n            Tuning deviation from the Stuttgart pitch (A440) in (fractional) bins per octave.\n        bins_per_octave (`int`, defaults to `12`):\n            Number of bins per octave.\n\n    Returns:\n        `float` or `np.ndarray`: The frequencies on the octave scale.\n    \"\"\"\n    stuttgart_pitch = 440.0 * 2.0 ** (tuning / bins_per_octave)\n    octave = np.log2(freq / (float(stuttgart_pitch) / 16))\n    return octave\n\n\ndef _create_triangular_filter_bank(fft_freqs: np.ndarray, filter_freqs: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Creates a triangular filter bank.\n\n    Adapted from *torchaudio* and *librosa*.\n\n    Args:\n        fft_freqs (`np.ndarray` of shape `(num_frequency_bins,)`):\n            Discrete frequencies of the FFT bins in Hz.\n        filter_freqs (`np.ndarray` of shape `(num_mel_filters,)`):\n            Center frequencies of the triangular filters to create, in Hz.\n\n    Returns:\n        `np.ndarray` of shape `(num_frequency_bins, num_mel_filters)`\n    \"\"\"\n    filter_diff = np.diff(filter_freqs)\n    slopes = np.expand_dims(filter_freqs, 0) - np.expand_dims(fft_freqs, 1)\n    down_slopes = -slopes[:, :-2] / filter_diff[:-1]\n    up_slopes = slopes[:, 2:] / filter_diff[1:]\n    return np.maximum(np.zeros(1), np.minimum(down_slopes, up_slopes))\n\n\ndef chroma_filter_bank(\n    num_frequency_bins: int,\n    num_chroma: int,\n    sampling_rate: int,\n    tuning: float = 0.0,\n    power: Optional[float] = 2.0,\n    weighting_parameters: Optional[Tuple[float]] = (5.0, 2),\n    start_at_c_chroma: Optional[bool] = True,\n):\n    \"\"\"\n    Creates a chroma filter bank, i.e a linear transformation to project spectrogram bins onto chroma bins.\n\n    Adapted from *librosa*.\n\n    Args:\n        num_frequency_bins (`int`):\n            Number of frequencies used to compute the spectrogram (should be the same as in `stft`).\n        num_chroma (`int`):\n            Number of chroma bins (i.e pitch classes).\n        sampling_rate (`float`):\n            Sample rate of the audio waveform.\n        tuning (`float`):\n            Tuning deviation from A440 in fractions of a chroma bin.\n        power (`float`, *optional*, defaults to 2.0):\n            If 12.0, normalizes each column with their L2 norm. If 1.0, normalizes each column with their L1 norm.\n        weighting_parameters (`Tuple[float]`, *optional*, defaults to `(5., 2.)`):\n            If specified, apply a Gaussian weighting parameterized by the first element of the tuple being the center and\n            the second element being the Gaussian half-width.\n        start_at_c_chroma (`float`, *optional*, defaults to `True`):\n            If True, the filter bank will start at the 'C' pitch class. Otherwise, it will start at 'A'.\n    Returns:\n        `np.ndarray` of shape `(num_frequency_bins, num_chroma)`\n    \"\"\"\n    # Get the FFT bins, not counting the DC component\n    frequencies = np.linspace(0, sampling_rate, num_frequency_bins, endpoint=False)[1:]\n\n    freq_bins = num_chroma * hertz_to_octave(frequencies, tuning=tuning, bins_per_octave=num_chroma)\n\n    # make up a value for the 0 Hz bin = 1.5 octaves below bin 1\n    # (so chroma is 50% rotated from bin 1, and bin width is broad)\n    freq_bins = np.concatenate(([freq_bins[0] - 1.5 * num_chroma], freq_bins))\n\n    bins_width = np.concatenate((np.maximum(freq_bins[1:] - freq_bins[:-1], 1.0), [1]))\n\n    chroma_filters = np.subtract.outer(freq_bins, np.arange(0, num_chroma, dtype=\"d\")).T\n\n    num_chroma2 = np.round(float(num_chroma) / 2)\n\n    # Project into range -num_chroma/2 .. num_chroma/2\n    # add on fixed offset of 10*num_chroma to ensure all values passed to\n    # rem are positive\n    chroma_filters = np.remainder(chroma_filters + num_chroma2 + 10 * num_chroma, num_chroma) - num_chroma2\n\n    # Gaussian bumps - 2*D to make them narrower\n    chroma_filters = np.exp(-0.5 * (2 * chroma_filters / np.tile(bins_width, (num_chroma, 1))) ** 2)\n\n    # normalize each column\n    if power is not None:\n        chroma_filters = chroma_filters / np.sum(chroma_filters**power, axis=0, keepdims=True) ** (1.0 / power)\n\n    # Maybe apply scaling for fft bins\n    if weighting_parameters is not None:\n        center, half_width = weighting_parameters\n        chroma_filters *= np.tile(\n            np.exp(-0.5 * (((freq_bins / num_chroma - center) / half_width) ** 2)),\n            (num_chroma, 1),\n        )\n\n    if start_at_c_chroma:\n        chroma_filters = np.roll(chroma_filters, -3 * (num_chroma // 12), axis=0)\n\n    # remove aliasing columns, copy to ensure row-contiguity\n    return np.ascontiguousarray(chroma_filters[:, : int(1 + num_frequency_bins / 2)])\n\n\ndef mel_filter_bank(\n    num_frequency_bins: int,\n    num_mel_filters: int,\n    min_frequency: float,\n    max_frequency: float,\n    sampling_rate: int,\n    norm: Optional[str] = None,\n    mel_scale: str = \"htk\",\n    triangularize_in_mel_space: bool = False,\n) -> np.ndarray:\n    \"\"\"\n    Creates a frequency bin conversion matrix used to obtain a mel spectrogram. This is called a *mel filter bank*, and\n    various implementation exist, which differ in the number of filters, the shape of the filters, the way the filters\n    are spaced, the bandwidth of the filters, and the manner in which the spectrum is warped. The goal of these\n    features is to approximate the non-linear human perception of the variation in pitch with respect to the frequency.\n\n    Different banks of mel filters were introduced in the literature. The following variations are supported:\n\n    - MFCC FB-20: introduced in 1980 by Davis and Mermelstein, it assumes a sampling frequency of 10 kHz and a speech\n      bandwidth of `[0, 4600]` Hz.\n    - MFCC FB-24 HTK: from the Cambridge HMM Toolkit (HTK) (1995) uses a filter bank of 24 filters for a speech\n      bandwidth of `[0, 8000]` Hz. This assumes sampling rate \u2265 16 kHz.\n    - MFCC FB-40: from the Auditory Toolbox for MATLAB written by Slaney in 1998, assumes a sampling rate of 16 kHz and\n      speech bandwidth of `[133, 6854]` Hz. This version also includes area normalization.\n    - HFCC-E FB-29 (Human Factor Cepstral Coefficients) of Skowronski and Harris (2004), assumes a sampling rate of\n      12.5 kHz and speech bandwidth of `[0, 6250]` Hz.\n\n    This code is adapted from *torchaudio* and *librosa*. Note that the default parameters of torchaudio's\n    `melscale_fbanks` implement the `\"htk\"` filters while librosa uses the `\"slaney\"` implementation.\n\n    Args:\n        num_frequency_bins (`int`):\n            Number of frequencies used to compute the spectrogram (should be the same as in `stft`).\n        num_mel_filters (`int`):\n            Number of mel filters to generate.\n        min_frequency (`float`):\n            Lowest frequency of interest in Hz.\n        max_frequency (`float`):\n            Highest frequency of interest in Hz. This should not exceed `sampling_rate / 2`.\n        sampling_rate (`int`):\n            Sample rate of the audio waveform.\n        norm (`str`, *optional*):\n            If `\"slaney\"`, divide the triangular mel weights by the width of the mel band (area normalization).\n        mel_scale (`str`, *optional*, defaults to `\"htk\"`):\n            The mel frequency scale to use, `\"htk\"`, `\"kaldi\"` or `\"slaney\"`.\n        triangularize_in_mel_space (`bool`, *optional*, defaults to `False`):\n            If this option is enabled, the triangular filter is applied in mel space rather than frequency space. This\n            should be set to `true` in order to get the same results as `torchaudio` when computing mel filters.\n\n    Returns:\n        `np.ndarray` of shape (`num_frequency_bins`, `num_mel_filters`): Triangular filter bank matrix. This is a\n        projection matrix to go from a spectrogram to a mel spectrogram.\n    \"\"\"\n    if norm is not None and norm != \"slaney\":\n        raise ValueError('norm must be one of None or \"slaney\"')\n\n    # center points of the triangular mel filters\n    mel_min = hertz_to_mel(min_frequency, mel_scale=mel_scale)\n    mel_max = hertz_to_mel(max_frequency, mel_scale=mel_scale)\n    mel_freqs = np.linspace(mel_min, mel_max, num_mel_filters + 2)\n    filter_freqs = mel_to_hertz(mel_freqs, mel_scale=mel_scale)\n\n    if triangularize_in_mel_space:\n        # frequencies of FFT bins in Hz, but filters triangularized in mel space\n        fft_bin_width = sampling_rate / (num_frequency_bins * 2)\n        fft_freqs = hertz_to_mel(fft_bin_width * np.arange(num_frequency_bins), mel_scale=mel_scale)\n        filter_freqs = mel_freqs\n    else:\n        # frequencies of FFT bins in Hz\n        fft_freqs = np.linspace(0, sampling_rate // 2, num_frequency_bins)\n\n    mel_filters = _create_triangular_filter_bank(fft_freqs, filter_freqs)\n\n    if norm is not None and norm == \"slaney\":\n        # Slaney-style mel is scaled to be approx constant energy per channel\n        enorm = 2.0 / (filter_freqs[2 : num_mel_filters + 2] - filter_freqs[:num_mel_filters])\n        mel_filters *= np.expand_dims(enorm, 0)\n\n    if (mel_filters.max(axis=0) == 0.0).any():\n        warnings.warn(\n            \"At least one mel filter has all zero values. \"\n            f\"The value for `num_mel_filters` ({num_mel_filters}) may be set too high. \"\n            f\"Or, the value for `num_frequency_bins` ({num_frequency_bins}) may be set too low.\"\n        )\n\n    return mel_filters\n\n\ndef optimal_fft_length(window_length: int) -> int:\n    \"\"\"\n    Finds the best FFT input size for a given `window_length`. This function takes a given window length and, if not\n    already a power of two, rounds it up to the next power or two.\n\n    The FFT algorithm works fastest when the length of the input is a power of two, which may be larger than the size\n    of the window or analysis frame. For example, if the window is 400 samples, using an FFT input size of 512 samples\n    is more optimal than an FFT size of 400 samples. Using a larger FFT size does not affect the detected frequencies,\n    it simply gives a higher frequency resolution (i.e. the frequency bins are smaller).\n    \"\"\"\n    return 2 ** int(np.ceil(np.log2(window_length)))\n\n\ndef window_function(\n    window_length: int,\n    name: str = \"hann\",\n    periodic: bool = True,\n    frame_length: Optional[int] = None,\n    center: bool = True,\n) -> np.ndarray:\n    \"\"\"\n    Returns an array containing the specified window. This window is intended to be used with `stft`.\n\n    The following window types are supported:\n\n        - `\"boxcar\"`: a rectangular window\n        - `\"hamming\"`: the Hamming window\n        - `\"hann\"`: the Hann window\n        - `\"povey\"`: the Povey window\n\n    Args:\n        window_length (`int`):\n            The length of the window in samples.\n        name (`str`, *optional*, defaults to `\"hann\"`):\n            The name of the window function.\n        periodic (`bool`, *optional*, defaults to `True`):\n            Whether the window is periodic or symmetric.\n        frame_length (`int`, *optional*):\n            The length of the analysis frames in samples. Provide a value for `frame_length` if the window is smaller\n            than the frame length, so that it will be zero-padded.\n        center (`bool`, *optional*, defaults to `True`):\n            Whether to center the window inside the FFT buffer. Only used when `frame_length` is provided.\n\n    Returns:\n        `np.ndarray` of shape `(window_length,)` or `(frame_length,)` containing the window.\n    \"\"\"\n    length = window_length + 1 if periodic else window_length\n\n    if name == \"boxcar\":\n        window = np.ones(length)\n    elif name in [\"hamming\", \"hamming_window\"]:\n        window = np.hamming(length)\n    elif name in [\"hann\", \"hann_window\"]:\n        window = np.hanning(length)\n    elif name in [\"povey\"]:\n        window = np.power(np.hanning(length), 0.85)\n    else:\n        raise ValueError(f\"Unknown window function '{name}'\")\n\n    if periodic:\n        window = window[:-1]\n\n    if frame_length is None:\n        return window\n\n    if window_length > frame_length:\n        raise ValueError(\n            f\"Length of the window ({window_length}) may not be larger than frame_length ({frame_length})\"\n        )\n\n    padded_window = np.zeros(frame_length)\n    offset = (frame_length - window_length) // 2 if center else 0\n    padded_window[offset : offset + window_length] = window\n    return padded_window\n\n\n# TODO This method does not support batching yet as we are mainly focused on inference.", "right_context": "\n\ndef power_to_db(\n    spectrogram: np.ndarray,\n    reference: float = 1.0,\n    min_value: float = 1e-10,\n    db_range: Optional[float] = None,\n) -> np.ndarray:\n    \"\"\"\n    Converts a power spectrogram to the decibel scale. This computes `10 * log10(spectrogram / reference)`, using basic\n    logarithm properties for numerical stability.\n\n    The motivation behind applying the log function on the (mel) spectrogram is that humans do not hear loudness on a\n    linear scale. Generally to double the perceived volume of a sound we need to put 8 times as much energy into it.\n    This means that large variations in energy may not sound all that different if the sound is loud to begin with.\n    This compression operation makes the (mel) spectrogram features match more closely what humans actually hear.\n\n    Based on the implementation of `librosa.power_to_db`.\n\n    Args:\n        spectrogram (`np.ndarray`):\n            The input power (mel) spectrogram. Note that a power spectrogram has the amplitudes squared!\n        reference (`float`, *optional*, defaults to 1.0):\n            Sets the input spectrogram value that corresponds to 0 dB. For example, use `np.max(spectrogram)` to set\n            the loudest part to 0 dB. Must be greater than zero.\n        min_value (`float`, *optional*, defaults to `1e-10`):\n            The spectrogram will be clipped to this minimum value before conversion to decibels, to avoid taking\n            `log(0)`. The default of `1e-10` corresponds to a minimum of -100 dB. Must be greater than zero.\n        db_range (`float`, *optional*):\n            Sets the maximum dynamic range in decibels. For example, if `db_range = 80`, the difference between the\n            peak value and the smallest value will never be more than 80 dB. Must be greater than zero.\n\n    Returns:\n        `np.ndarray`: the spectrogram in decibels\n    \"\"\"\n    if reference <= 0.0:\n        raise ValueError(\"reference must be greater than zero\")\n    if min_value <= 0.0:\n        raise ValueError(\"min_value must be greater than zero\")\n\n    reference = max(min_value, reference)\n\n    spectrogram = np.clip(spectrogram, a_min=min_value, a_max=None)\n    spectrogram = 10.0 * (np.log10(spectrogram) - np.log10(reference))\n\n    if db_range is not None:\n        if db_range <= 0.0:\n            raise ValueError(\"db_range must be greater than zero\")\n        spectrogram = np.clip(spectrogram, a_min=spectrogram.max() - db_range, a_max=None)\n\n    return spectrogram\n\n\ndef amplitude_to_db(\n    spectrogram: np.ndarray,\n    reference: float = 1.0,\n    min_value: float = 1e-5,\n    db_range: Optional[float] = None,\n) -> np.ndarray:\n    \"\"\"\n    Converts an amplitude spectrogram to the decibel scale. This computes `20 * log10(spectrogram / reference)`, using\n    basic logarithm properties for numerical stability.\n\n    The motivation behind applying the log function on the (mel) spectrogram is that humans do not hear loudness on a\n    linear scale. Generally to double the perceived volume of a sound we need to put 8 times as much energy into it.\n    This means that large variations in energy may not sound all that different if the sound is loud to begin with.\n    This compression operation makes the (mel) spectrogram features match more closely what humans actually hear.\n\n    Args:\n        spectrogram (`np.ndarray`):\n            The input amplitude (mel) spectrogram.\n        reference (`float`, *optional*, defaults to 1.0):\n            Sets the input spectrogram value that corresponds to 0 dB. For example, use `np.max(spectrogram)` to set\n            the loudest part to 0 dB. Must be greater than zero.\n        min_value (`float`, *optional*, defaults to `1e-5`):\n            The spectrogram will be clipped to this minimum value before conversion to decibels, to avoid taking\n            `log(0)`. The default of `1e-5` corresponds to a minimum of -100 dB. Must be greater than zero.\n        db_range (`float`, *optional*):\n            Sets the maximum dynamic range in decibels. For example, if `db_range = 80`, the difference between the\n            peak value and the smallest value will never be more than 80 dB. Must be greater than zero.\n\n    Returns:\n        `np.ndarray`: the spectrogram in decibels\n    \"\"\"\n    if reference <= 0.0:\n        raise ValueError(\"reference must be greater than zero\")\n    if min_value <= 0.0:\n        raise ValueError(\"min_value must be greater than zero\")\n\n    reference = max(min_value, reference)\n\n    spectrogram = np.clip(spectrogram, a_min=min_value, a_max=None)\n    spectrogram = 20.0 * (np.log10(spectrogram) - np.log10(reference))\n\n    if db_range is not None:\n        if db_range <= 0.0:\n            raise ValueError(\"db_range must be greater than zero\")\n        spectrogram = np.clip(spectrogram, a_min=spectrogram.max() - db_range, a_max=None)\n\n    return spectrogram\n\n\n### deprecated functions below this line ###\n\n\ndef get_mel_filter_banks(\n    nb_frequency_bins: int,\n    nb_mel_filters: int,\n    frequency_min: float,\n    frequency_max: float,\n    sample_rate: int,\n    norm: Optional[str] = None,\n    mel_scale: str = \"htk\",\n) -> np.array:\n    warnings.warn(\n        \"The function `get_mel_filter_banks` is deprecated and will be removed in version 4.31.0 of Transformers\",\n        FutureWarning,\n    )\n    return mel_filter_bank(\n        num_frequency_bins=nb_frequency_bins,\n        num_mel_filters=nb_mel_filters,\n        min_frequency=frequency_min,\n        max_frequency=frequency_max,\n        sampling_rate=sample_rate,\n        norm=norm,\n        mel_scale=mel_scale,\n    )\n\n\ndef fram_wave(waveform: np.array, hop_length: int = 160, fft_window_size: int = 400, center: bool = True):\n    \"\"\"\n    In order to compute the short time fourier transform, the waveform needs to be split in overlapping windowed\n    segments called `frames`.\n\n    The window length (window_length) defines how much of the signal is contained in each frame, while the hop length\n    defines the step between the beginning of each new frame.\n\n\n    Args:\n        waveform (`np.array` of shape `(sample_length,)`):\n            The raw waveform which will be split into smaller chunks.\n        hop_length (`int`, *optional*, defaults to 160):\n            Step between each window of the waveform.\n        fft_window_size (`int`, *optional*, defaults to 400):\n            Defines the size of the window.\n        center (`bool`, defaults to `True`):\n            Whether or not to center each frame around the middle of the frame. Centering is done by reflecting the\n            waveform on the left and on the right.\n\n    Return:\n        framed_waveform (`np.array` of shape `(waveform.shape // hop_length , fft_window_size)`):\n            The framed waveforms that can be fed to `np.fft`.\n    \"\"\"\n    warnings.warn(\n        \"The function `fram_wave` is deprecated and will be removed in version 4.31.0 of Transformers\",\n        FutureWarning,\n    )\n    frames = []\n    for i in range(0, waveform.shape[0] + 1, hop_length):\n        if center:\n            half_window = (fft_window_size - 1) // 2 + 1\n            start = i - half_window if i > half_window else 0\n            end = i + half_window if i < waveform.shape[0] - half_window else waveform.shape[0]\n            frame = waveform[start:end]\n            if start == 0:\n                padd_width = (-i + half_window, 0)\n                frame = np.pad(frame, pad_width=padd_width, mode=\"reflect\")\n\n            elif end == waveform.shape[0]:\n                padd_width = (0, (i - waveform.shape[0] + half_window))\n                frame = np.pad(frame, pad_width=padd_width, mode=\"reflect\")\n\n        else:\n            frame = waveform[i : i + fft_window_size]\n            frame_width = frame.shape[0]\n            if frame_width < waveform.shape[0]:\n                frame = np.lib.pad(\n                    frame, pad_width=(0, fft_window_size - frame_width), mode=\"constant\", constant_values=0\n                )\n        frames.append(frame)\n\n    frames = np.stack(frames, 0)\n    return frames\n\n\ndef stft(frames: np.array, windowing_function: np.array, fft_window_size: int = None):\n    \"\"\"\n    Calculates the complex Short-Time Fourier Transform (STFT) of the given framed signal. Should give the same results\n    as `torch.stft`.\n\n    Args:\n        frames (`np.array` of dimension `(num_frames, fft_window_size)`):\n            A framed audio signal obtained using `audio_utils.fram_wav`.\n        windowing_function (`np.array` of dimension `(nb_frequency_bins, nb_mel_filters)`:\n            A array reprensenting the function that will be used to reduces the amplitude of the discontinuities at the\n            boundaries of each frame when computing the STFT. Each frame will be multiplied by the windowing_function.\n            For more information on the discontinuities, called *Spectral leakage*, refer to [this\n            tutorial]https://download.ni.com/evaluation/pxi/Understanding%20FFTs%20and%20Windowing.pdf\n        fft_window_size (`int`, *optional*):\n            Size of the window om which the Fourier transform is applied. This controls the frequency resolution of the\n            spectrogram. 400 means that the fourrier transform is computed on windows of 400 samples. The number of\n            frequency bins (`nb_frequency_bins`) used to divide the window into equal strips is equal to\n            `(1+fft_window_size)//2`. An increase of the fft_window_size slows the calculus time proportionnally.\n\n    Example:\n\n    ```python\n    >>> from transformers.audio_utils import stft, fram_wave\n    >>> import numpy as np\n\n    >>> audio = np.random.rand(50)\n    >>> fft_window_size = 10\n    >>> hop_length = 2\n    >>> framed_audio = fram_wave(audio, hop_length, fft_window_size)\n    >>> spectrogram = stft(framed_audio, np.hanning(fft_window_size + 1))\n    ```\n\n    Returns:\n        spectrogram (`np.ndarray`):\n            A spectrogram of shape `(num_frames, nb_frequency_bins)` obtained using the STFT algorithm\n    \"\"\"\n    warnings.warn(\n        \"The function `stft` is deprecated and will be removed in version 4.31.0 of Transformers\",\n        FutureWarning,\n    )\n    frame_size = frames.shape[1]\n\n    if fft_window_size is None:\n        fft_window_size = frame_size\n\n    if fft_window_size < frame_size:\n        raise ValueError(\"FFT size must greater or equal the frame size\")\n    # number of FFT bins to store\n    nb_frequency_bins = (fft_window_size >> 1) + 1\n\n    spectrogram = np.empty((len(frames), nb_frequency_bins), dtype=np.complex64)\n    fft_signal = np.zeros(fft_window_size)\n\n    for f, frame in enumerate(frames):\n        if windowing_function is not None:\n            np.multiply(frame, windowing_function, out=fft_signal[:frame_size])\n        else:\n            fft_signal[:frame_size] = frame\n        spectrogram[f] = np.fft.fft(fft_signal, axis=0)[:nb_frequency_bins]\n    return spectrogram.T\n", "import_text": ["warnings", "typing.Optional", "typing.Tuple", "typing.Union", "numpy"], "prompt": "\"\"\"\nDescription: This function computes a spectrogram from a waveform using a sliding window and FFT.\n\nArgs:\n    waveform (np.ndarray): The input waveform.\n    window (np.ndarray): The window function to apply to each frame.\n    frame_length (int): The length of each frame.\n    hop_length (int): The number of samples to advance between frames.\n    fft_length (Optional[int]): The length of the FFT. If not provided, it defaults to frame_length.\n    power (Optional[float]): The exponent for the magnitude spectrogram. Defaults to 1.0.\n    center (bool): Whether to center pad the waveform. Defaults to True.\n    pad_mode (str): The mode for padding. Defaults to \"reflect\".\n    onesided (bool): Whether to return a one-sided spectrum for real data. Defaults to True.\n    preemphasis (Optional[float]): The preemphasis coefficient. Defaults to None.\n    mel_filters (Optional[np.ndarray]): The Mel filters to apply. Defaults to None.\n    mel_floor (float): The floor value for the Mel spectrogram. Defaults to 1e-10.\n    log_mel (Optional[str]): The type of logarithmic scaling to apply to the Mel spectrogram. Defaults to None.\n    reference (float): The reference value for dB conversion. Defaults to 1.0.\n    min_value (float): The minimum value for dB conversion. Defaults to 1e-10.\n    db_range (Optional[float]): The range for dB conversion. Defaults to None.\n    remove_dc_offset (Optional[bool]): Whether to remove the DC offset. Defaults to None.\n    dtype (np.dtype): The data type for the output spectrogram. Defaults to np.float32.\n\nReturns:\n    np.ndarray: The computed spectrogram.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Calculates a spectrogram over one waveform using the Short-Time Fourier Transform.\n\n    This function can create the following kinds of spectrograms:\n\n      - amplitude spectrogram (`power = 1.0`)\n      - power spectrogram (`power = 2.0`)\n      - complex-valued spectrogram (`power = None`)\n      - log spectrogram (use `log_mel` argument)\n      - mel spectrogram (provide `mel_filters`)\n      - log-mel spectrogram (provide `mel_filters` and `log_mel`)\n\n    How this works:\n\n      1. The input waveform is split into frames of size `frame_length` that are partially overlapping by `frame_length\n         - hop_length` samples.\n      2. Each frame is multiplied by the window and placed into a buffer of size `fft_length`.\n      3. The DFT is taken of each windowed frame.\n      4. The results are stacked into a spectrogram.\n\n    We make a distinction between the following \"blocks\" of sample data, each of which may have a different lengths:\n\n      - The analysis frame. This is the size of the time slices that the input waveform is split into.\n      - The window. Each analysis frame is multiplied by the window to avoid spectral leakage.\n      - The FFT input buffer. The length of this determines how many frequency bins are in the spectrogram.\n\n    In this implementation, the window is assumed to be zero-padded to have the same size as the analysis frame. A\n    padded window can be obtained from `window_function()`. The FFT input buffer may be larger than the analysis frame,\n    typically the next power of two.\n\n    Note: This function is not optimized for speed yet. It should be mostly compatible with `librosa.stft` and\n    `torchaudio.functional.transforms.Spectrogram`, although it is more flexible due to the different ways spectrograms\n    can be constructed.\n\n    Args:\n        waveform (`np.ndarray` of shape `(length,)`):\n            The input waveform. This must be a single real-valued, mono waveform.\n        window (`np.ndarray` of shape `(frame_length,)`):\n            The windowing function to apply, including zero-padding if necessary. The actual window length may be\n            shorter than `frame_length`, but we're assuming the array has already been zero-padded.\n        frame_length (`int`):\n            The length of the analysis frames in samples. With librosa this is always equal to `fft_length` but we also\n            allow smaller sizes.\n        hop_length (`int`):\n            The stride between successive analysis frames in samples.\n        fft_length (`int`, *optional*):\n            The size of the FFT buffer in samples. This determines how many frequency bins the spectrogram will have.\n            For optimal speed, this should be a power of two. If `None`, uses `frame_length`.\n        power (`float`, *optional*, defaults to 1.0):\n            If 1.0, returns the amplitude spectrogram. If 2.0, returns the power spectrogram. If `None`, returns\n            complex numbers.\n        center (`bool`, *optional*, defaults to `True`):\n            Whether to pad the waveform so that frame `t` is centered around time `t * hop_length`. If `False`, frame\n            `t` will start at time `t * hop_length`.\n        pad_mode (`str`, *optional*, defaults to `\"reflect\"`):\n            Padding mode used when `center` is `True`. Possible values are: `\"constant\"` (pad with zeros), `\"edge\"`\n            (pad with edge values), `\"reflect\"` (pads with mirrored values).\n        onesided (`bool`, *optional*, defaults to `True`):\n            If True, only computes the positive frequencies and returns a spectrogram containing `fft_length // 2 + 1`\n            frequency bins. If False, also computes the negative frequencies and returns `fft_length` frequency bins.\n        preemphasis (`float`, *optional*)\n            Coefficient for a low-pass filter that applies pre-emphasis before the DFT.\n        mel_filters (`np.ndarray` of shape `(num_freq_bins, num_mel_filters)`, *optional*):\n            The mel filter bank. If supplied, applies a this filter bank to create a mel spectrogram.\n        mel_floor (`float`, *optional*, defaults to 1e-10):\n            Minimum value of mel frequency banks.\n        log_mel (`str`, *optional*):\n            How to convert the spectrogram to log scale. Possible options are: `None` (don't convert), `\"log\"` (take\n            the natural logarithm) `\"log10\"` (take the base-10 logarithm), `\"dB\"` (convert to decibels). Can only be\n            used when `power` is not `None`.\n        reference (`float`, *optional*, defaults to 1.0):\n            Sets the input spectrogram value that corresponds to 0 dB. For example, use `np.max(spectrogram)` to set\n            the loudest part to 0 dB. Must be greater than zero.\n        min_value (`float`, *optional*, defaults to `1e-10`):\n            The spectrogram will be clipped to this minimum value before conversion to decibels, to avoid taking\n            `log(0)`. For a power spectrogram, the default of `1e-10` corresponds to a minimum of -100 dB. For an\n            amplitude spectrogram, the value `1e-5` corresponds to -100 dB. Must be greater than zero.\n        db_range (`float`, *optional*):\n            Sets the maximum dynamic range in decibels. For example, if `db_range = 80`, the difference between the\n            peak value and the smallest value will never be more than 80 dB. Must be greater than zero.\n        remove_dc_offset (`bool`, *optional*):\n            Subtract mean from waveform on each frame, applied before pre-emphasis. This should be set to `true` in\n            order to get the same results as `torchaudio.compliance.kaldi.fbank` when computing mel filters.\n        dtype (`np.dtype`, *optional*, defaults to `np.float32`):\n            Data type of the spectrogram tensor. If `power` is None, this argument is ignored and the dtype will be\n            `np.complex64`.\n\n    Returns:\n        `nd.array` containing a spectrogram of shape `(num_frequency_bins, length)` for a regular spectrogram or shape\n        `(num_mel_filters, length)` for a mel spectrogram.\n    \"\"\"", "function_dependencies": ["numpy.iscomplexobj", "numpy.pad", "numpy.pad.astype", "numpy.floor", "numpy.empty", "numpy.zeros", "numpy.abs", "numpy.maximum", "numpy.dot", "numpy.log", "numpy.log10", "numpy.asarray"], "project_create_time": "2024-01-31T17:30:10+00:00", "project_update_time": "2024-04-17T08:33:26+00:00", "file_create_time": "2024-04-19T21:49:07Z", "file_update_time": "2024-04-19T21:49:07Z", "function_update_time": "2024-04-19T21:49:07Z", "license": null, "reference_api": ["numpy.asarray", "numpy.abs", "numpy.dot", "numpy.empty", "numpy.maximum", "numpy.floor", "numpy.pad"], "test_function": [{"file_path": "/KVQuant-main/KVQuant-main/quant/dbrx/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_impulse", "code": "\n    def test_spectrogram_impulse(self):\n        waveform = np.zeros(40)\n        waveform[9] = 1.0  # impulse shifted in time\n\n        spec = spectrogram(\n            waveform,\n            window_function(12, \"hann\", frame_length=16),\n            frame_length=16,\n            hop_length=4,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (9, 11))\n\n        expected = np.array([[0.0, 0.0669873, 0.9330127, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])\n        self.assertTrue(np.allclose(spec, expected))"}, {"file_path": "/KVQuant-main/KVQuant-main/quant/dbrx/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_integration_test", "code": "\n    def test_spectrogram_integration_test(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=128,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        # fmt: off\n        expected = np.array([\n            0.02464888, 0.04648664, 0.05872392, 0.02311783, 0.0327175 ,\n            0.02433643, 0.01198814, 0.02055709, 0.01559287, 0.01394357,\n            0.01299037, 0.01728045, 0.0254554 , 0.02486533, 0.02011792,\n            0.01755333, 0.02100457, 0.02337024, 0.01436963, 0.01464558,\n            0.0211017 , 0.0193489 , 0.01272165, 0.01858462, 0.03722598,\n            0.0456542 , 0.03281558, 0.00620586, 0.02226466, 0.03618042,\n            0.03508182, 0.02271432, 0.01051649, 0.01225771, 0.02315293,\n            0.02331886, 0.01417785, 0.0106844 , 0.01791214, 0.017177  ,\n            0.02125114, 0.05028201, 0.06830665, 0.05216664, 0.01963666,\n            0.06941418, 0.11513043, 0.12257859, 0.10948435, 0.08568069,\n            0.05509328, 0.05047818, 0.047112  , 0.05060737, 0.02982424,\n            0.02803827, 0.02933729, 0.01760491, 0.00587815, 0.02117637,\n            0.0293578 , 0.03452379, 0.02194803, 0.01676056,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 400], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            fft_length=512,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n        self.assertTrue(np.allclose(spec[:64, 400], expected))\n\n        mel_filters = mel_filter_bank(\n            num_frequency_bins=256,\n            num_mel_filters=400,\n            min_frequency=20,\n            max_frequency=8000,\n            sampling_rate=16000,\n            norm=None,\n            mel_scale=\"kaldi\",\n            triangularize_in_mel_space=True,\n        )\n\n        mel_filters = np.pad(mel_filters, ((0, 1), (0, 0)))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"povey\", periodic=False),\n            frame_length=400,\n            hop_length=160,\n            fft_length=512,\n            power=2.0,\n            center=False,\n            pad_mode=\"reflect\",\n            onesided=True,\n            preemphasis=0.97,\n            mel_filters=mel_filters,\n            log_mel=\"log\",\n            mel_floor=1.1920928955078125e-07,\n            remove_dc_offset=True,\n        )\n        self.assertEqual(spec.shape, (400, 584))\n\n        # fmt: off\n        expected = np.array([-15.94238515,  -8.20712299,  -8.22704352, -15.94238515,\n       -15.94238515, -15.94238515, -15.94238515, -15.94238515,\n        -6.52463769,  -7.73677889, -15.94238515, -15.94238515,\n       -15.94238515, -15.94238515,  -4.18650018,  -3.37195286,\n       -15.94238515, -15.94238515, -15.94238515, -15.94238515,\n        -4.70190154,  -2.4217066 , -15.94238515, -15.94238515,\n       -15.94238515, -15.94238515,  -5.62755239,  -3.53385194,\n       -15.94238515, -15.94238515, -15.94238515, -15.94238515,\n        -9.43303023,  -8.77480925, -15.94238515, -15.94238515,\n       -15.94238515, -15.94238515,  -4.2951092 ,  -5.51585994,\n       -15.94238515, -15.94238515, -15.94238515,  -4.40151721,\n        -3.95228878, -15.94238515, -15.94238515, -15.94238515,\n        -6.10365415,  -4.59494697, -15.94238515, -15.94238515,\n       -15.94238515,  -8.10727767,  -6.2585298 , -15.94238515,\n       -15.94238515, -15.94238515,  -5.60161702,  -4.47217004,\n       -15.94238515, -15.94238515, -15.94238515,  -5.91641988]\n        )\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 400], expected, atol=1e-5))"}, {"file_path": "/KVQuant-main/KVQuant-main/quant/dbrx/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_center_padding", "code": "\n    def test_spectrogram_center_padding(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=128,\n            center=True,\n            pad_mode=\"reflect\",\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        # fmt: off\n        expected = np.array([\n            0.1287945 , 0.12792738, 0.08311573, 0.03155122, 0.02470202,\n            0.00727857, 0.00910694, 0.00686163, 0.01238981, 0.01473668,\n            0.00336144, 0.00370314, 0.00600871, 0.01120164, 0.01942998,\n            0.03132008, 0.0232842 , 0.01124642, 0.02754783, 0.02423725,\n            0.00147893, 0.00038027, 0.00112299, 0.00596233, 0.00571529,\n            0.02084235, 0.0231855 , 0.00810006, 0.01837943, 0.00651339,\n            0.00093931, 0.00067426, 0.01058399, 0.01270507, 0.00151734,\n            0.00331913, 0.00302416, 0.01081792, 0.00754549, 0.00148963,\n            0.00111943, 0.00152573, 0.00608017, 0.01749986, 0.01205949,\n            0.0143082 , 0.01910573, 0.00413786, 0.03916619, 0.09873404,\n            0.08302026, 0.02673891, 0.00401255, 0.01397392, 0.00751862,\n            0.01024884, 0.01544606, 0.00638907, 0.00623633, 0.0085103 ,\n            0.00217659, 0.00276204, 0.00260835, 0.00299299,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 0], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=128,\n            center=True,\n            pad_mode=\"constant\",\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        # fmt: off\n        expected = np.array([\n            0.06558744, 0.06889656, 0.06263352, 0.04264418, 0.03404115,\n            0.03244197, 0.02279134, 0.01646339, 0.01452216, 0.00826055,\n            0.00062093, 0.0031821 , 0.00419456, 0.00689327, 0.01106367,\n            0.01712119, 0.01721762, 0.00977533, 0.01606626, 0.02275621,\n            0.01727687, 0.00992739, 0.01217688, 0.01049927, 0.01022947,\n            0.01302475, 0.01166873, 0.01081812, 0.01057327, 0.00767912,\n            0.00429567, 0.00089625, 0.00654583, 0.00912084, 0.00700984,\n            0.00225026, 0.00290545, 0.00667712, 0.00730663, 0.00410813,\n            0.00073102, 0.00219296, 0.00527618, 0.00996585, 0.01123781,\n            0.00872816, 0.01165121, 0.02047945, 0.03681747, 0.0514379 ,\n            0.05137928, 0.03960042, 0.02821562, 0.01813349, 0.01201322,\n            0.01260964, 0.00900654, 0.00207905, 0.00456714, 0.00850599,\n            0.00788239, 0.00664407, 0.00824227, 0.00628301,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 0], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=128,\n            center=False,\n        )\n        self.assertEqual(spec.shape, (257, 728))\n\n        # fmt: off\n        expected = np.array([\n            0.00250445, 0.02161521, 0.06232229, 0.04339567, 0.00937727,\n            0.01080616, 0.00248685, 0.0095264 , 0.00727476, 0.0079152 ,\n            0.00839946, 0.00254932, 0.00716622, 0.005559  , 0.00272623,\n            0.00581774, 0.01896395, 0.01829788, 0.01020514, 0.01632692,\n            0.00870888, 0.02065827, 0.0136022 , 0.0132382 , 0.011827  ,\n            0.00194505, 0.0189979 , 0.026874  , 0.02194014, 0.01923883,\n            0.01621437, 0.00661967, 0.00289517, 0.00470257, 0.00957801,\n            0.00191455, 0.00431664, 0.00544359, 0.01126213, 0.00785778,\n            0.00423469, 0.01322504, 0.02226548, 0.02318576, 0.03428908,\n            0.03648811, 0.0202938 , 0.011902  , 0.03226198, 0.06347476,\n            0.01306318, 0.05308729, 0.05474771, 0.03127991, 0.00998512,\n            0.01449977, 0.01272741, 0.00868176, 0.00850386, 0.00313876,\n            0.00811857, 0.00538216, 0.00685749, 0.00535275,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 0], expected))"}, {"file_path": "/KVQuant-main/KVQuant-main/quant/dbrx/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_shapes", "code": "\n    def test_spectrogram_shapes(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (201, 732))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            power=1.0,\n            center=False,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (201, 729))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            fft_length=512,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=64,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=False,\n        )\n        self.assertEqual(spec.shape, (512, 1464))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=64,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=False,\n        )\n        self.assertEqual(spec.shape, (512, 1464))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=512,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=False,\n        )\n        self.assertEqual(spec.shape, (512, 183))"}, {"file_path": "/KVQuant-main/KVQuant-main/quant/dbrx/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_mel_spectrogram", "code": "\n    def test_mel_spectrogram(self):\n        waveform = self._load_datasamples(1)[0]\n\n        mel_filters = mel_filter_bank(\n            num_frequency_bins=513,\n            num_mel_filters=13,\n            min_frequency=100,\n            max_frequency=4000,\n            sampling_rate=16000,\n            norm=None,\n            mel_scale=\"htk\",\n        )\n        self.assertEqual(mel_filters.shape, (513, 13))\n\n        spec = spectrogram(\n            waveform,\n            window_function(800, \"hann\", frame_length=1024),\n            frame_length=1024,\n            hop_length=128,\n            power=2.0,\n        )\n        self.assertEqual(spec.shape, (513, 732))\n\n        spec = spectrogram(\n            waveform,\n            window_function(800, \"hann\", frame_length=1024),\n            frame_length=1024,\n            hop_length=128,\n            power=2.0,\n            mel_filters=mel_filters,\n        )\n        self.assertEqual(spec.shape, (13, 732))\n\n        # fmt: off\n        expected = np.array([\n            1.08027889e+02, 1.48080673e+01, 7.70758213e+00, 9.57676639e-01,\n            8.81639061e-02, 5.26073833e-02, 1.52736155e-02, 9.95350117e-03,\n            7.95364356e-03, 1.01148004e-02, 4.29241020e-03, 9.90708797e-03,\n            9.44153646e-04\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:, 300], expected))"}, {"file_path": "/KVQuant-main/KVQuant-main/quant/dbrx/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_power", "code": "\n    def test_spectrogram_power(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=128,\n            power=None,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n        self.assertEqual(spec.dtype, np.complex64)\n\n        # fmt: off\n        expected = np.array([\n             0.01452305+0.01820039j, -0.01737362-0.01641946j,\n             0.0121028 +0.01565081j, -0.02794554-0.03021514j,\n             0.04719803+0.04086519j, -0.04391563-0.02779365j,\n             0.05682834+0.01571325j, -0.08604821-0.02023657j,\n             0.07497991+0.0186641j , -0.06366091-0.00922475j,\n             0.11003416+0.0114788j , -0.13677941-0.01523552j,\n             0.10934535-0.00117226j, -0.11635598+0.02551187j,\n             0.14708674-0.03469823j, -0.1328196 +0.06034218j,\n             0.12667368-0.13973421j, -0.14764774+0.18912019j,\n             0.10235471-0.12181523j, -0.00773012+0.04730498j,\n            -0.01487191-0.07312611j, -0.02739162+0.09619419j,\n             0.02895459-0.05398273j,  0.01198589+0.05276592j,\n            -0.02117299-0.10123465j,  0.00666388+0.09526499j,\n            -0.01672773-0.05649684j,  0.02723125+0.05939891j,\n            -0.01879361-0.062954j  ,  0.03686557+0.04568823j,\n            -0.07394181-0.07949649j,  0.06238583+0.13905765j,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[64:96, 321], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=128,\n            power=1.0,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n        self.assertEqual(spec.dtype, np.float64)\n\n        # fmt: off\n        expected = np.array([\n            0.02328461, 0.02390484, 0.01978448, 0.04115711, 0.0624309 ,\n            0.05197181, 0.05896072, 0.08839577, 0.07726794, 0.06432579,\n            0.11063128, 0.13762532, 0.10935163, 0.11911998, 0.15112405,\n            0.14588428, 0.18860507, 0.23992978, 0.15910825, 0.04793241,\n            0.07462307, 0.10001811, 0.06125769, 0.05411011, 0.10342509,\n            0.09549777, 0.05892122, 0.06534349, 0.06569936, 0.05870678,\n            0.10856833, 0.1524107 , 0.11463385, 0.05766969, 0.12385171,\n            0.14472842, 0.11978184, 0.10353675, 0.07244056, 0.03461861,\n            0.02624896, 0.02227475, 0.01238363, 0.00885281, 0.0110049 ,\n            0.00807005, 0.01033663, 0.01703181, 0.01445856, 0.00585615,\n            0.0132431 , 0.02754132, 0.01524478, 0.0204908 , 0.07453328,\n            0.10716327, 0.07195779, 0.08816078, 0.18340898, 0.16449876,\n            0.12322842, 0.1621659 , 0.12334293, 0.06033659,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[64:128, 321], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=128,\n            power=2.0,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n        self.assertEqual(spec.dtype, np.float64)\n\n        # fmt: off\n        expected = np.array([\n            5.42173162e-04, 5.71441371e-04, 3.91425507e-04, 1.69390778e-03,\n            3.89761780e-03, 2.70106923e-03, 3.47636663e-03, 7.81381316e-03,\n            5.97033510e-03, 4.13780799e-03, 1.22392802e-02, 1.89407300e-02,\n            1.19577805e-02, 1.41895693e-02, 2.28384770e-02, 2.12822221e-02,\n            3.55718732e-02, 5.75663000e-02, 2.53154356e-02, 2.29751552e-03,\n            5.56860259e-03, 1.00036217e-02, 3.75250424e-03, 2.92790355e-03,\n            1.06967501e-02, 9.11982451e-03, 3.47171025e-03, 4.26977174e-03,\n            4.31640586e-03, 3.44648538e-03, 1.17870830e-02, 2.32290216e-02,\n            1.31409196e-02, 3.32579296e-03, 1.53392460e-02, 2.09463164e-02,\n            1.43476883e-02, 1.07198600e-02, 5.24763530e-03, 1.19844836e-03,\n            6.89007982e-04, 4.96164430e-04, 1.53354369e-04, 7.83722571e-05,\n            1.21107812e-04, 6.51257360e-05, 1.06845939e-04, 2.90082477e-04,\n            2.09049831e-04, 3.42945241e-05, 1.75379610e-04, 7.58524227e-04,\n            2.32403356e-04, 4.19872697e-04, 5.55520924e-03, 1.14839673e-02,\n            5.17792348e-03, 7.77232368e-03, 3.36388536e-02, 2.70598419e-02,\n            1.51852425e-02, 2.62977779e-02, 1.52134784e-02, 3.64050455e-03,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[64:128, 321], expected))"}, {"file_path": "/KVQuant-main/KVQuant-main/gradients/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_impulse", "code": "\n    def test_spectrogram_impulse(self):\n        waveform = np.zeros(40)\n        waveform[9] = 1.0  # impulse shifted in time\n\n        spec = spectrogram(\n            waveform,\n            window_function(12, \"hann\", frame_length=16),\n            frame_length=16,\n            hop_length=4,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (9, 11))\n\n        expected = np.array([[0.0, 0.0669873, 0.9330127, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])\n        self.assertTrue(np.allclose(spec, expected))"}, {"file_path": "/KVQuant-main/KVQuant-main/gradients/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_integration_test", "code": "\n    def test_spectrogram_integration_test(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=128,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        # fmt: off\n        expected = np.array([\n            0.02464888, 0.04648664, 0.05872392, 0.02311783, 0.0327175 ,\n            0.02433643, 0.01198814, 0.02055709, 0.01559287, 0.01394357,\n            0.01299037, 0.01728045, 0.0254554 , 0.02486533, 0.02011792,\n            0.01755333, 0.02100457, 0.02337024, 0.01436963, 0.01464558,\n            0.0211017 , 0.0193489 , 0.01272165, 0.01858462, 0.03722598,\n            0.0456542 , 0.03281558, 0.00620586, 0.02226466, 0.03618042,\n            0.03508182, 0.02271432, 0.01051649, 0.01225771, 0.02315293,\n            0.02331886, 0.01417785, 0.0106844 , 0.01791214, 0.017177  ,\n            0.02125114, 0.05028201, 0.06830665, 0.05216664, 0.01963666,\n            0.06941418, 0.11513043, 0.12257859, 0.10948435, 0.08568069,\n            0.05509328, 0.05047818, 0.047112  , 0.05060737, 0.02982424,\n            0.02803827, 0.02933729, 0.01760491, 0.00587815, 0.02117637,\n            0.0293578 , 0.03452379, 0.02194803, 0.01676056,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 400], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            fft_length=512,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n        self.assertTrue(np.allclose(spec[:64, 400], expected))\n\n        mel_filters = mel_filter_bank(\n            num_frequency_bins=256,\n            num_mel_filters=400,\n            min_frequency=20,\n            max_frequency=8000,\n            sampling_rate=16000,\n            norm=None,\n            mel_scale=\"kaldi\",\n            triangularize_in_mel_space=True,\n        )\n\n        mel_filters = np.pad(mel_filters, ((0, 1), (0, 0)))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"povey\", periodic=False),\n            frame_length=400,\n            hop_length=160,\n            fft_length=512,\n            power=2.0,\n            center=False,\n            pad_mode=\"reflect\",\n            onesided=True,\n            preemphasis=0.97,\n            mel_filters=mel_filters,\n            log_mel=\"log\",\n            mel_floor=1.1920928955078125e-07,\n            remove_dc_offset=True,\n        )\n        self.assertEqual(spec.shape, (400, 584))\n\n        # fmt: off\n        expected = np.array([-15.94238515,  -8.20712299,  -8.22704352, -15.94238515,\n       -15.94238515, -15.94238515, -15.94238515, -15.94238515,\n        -6.52463769,  -7.73677889, -15.94238515, -15.94238515,\n       -15.94238515, -15.94238515,  -4.18650018,  -3.37195286,\n       -15.94238515, -15.94238515, -15.94238515, -15.94238515,\n        -4.70190154,  -2.4217066 , -15.94238515, -15.94238515,\n       -15.94238515, -15.94238515,  -5.62755239,  -3.53385194,\n       -15.94238515, -15.94238515, -15.94238515, -15.94238515,\n        -9.43303023,  -8.77480925, -15.94238515, -15.94238515,\n       -15.94238515, -15.94238515,  -4.2951092 ,  -5.51585994,\n       -15.94238515, -15.94238515, -15.94238515,  -4.40151721,\n        -3.95228878, -15.94238515, -15.94238515, -15.94238515,\n        -6.10365415,  -4.59494697, -15.94238515, -15.94238515,\n       -15.94238515,  -8.10727767,  -6.2585298 , -15.94238515,\n       -15.94238515, -15.94238515,  -5.60161702,  -4.47217004,\n       -15.94238515, -15.94238515, -15.94238515,  -5.91641988]\n        )\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 400], expected, atol=1e-5))"}, {"file_path": "/KVQuant-main/KVQuant-main/gradients/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_center_padding", "code": "\n    def test_spectrogram_center_padding(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=128,\n            center=True,\n            pad_mode=\"reflect\",\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        # fmt: off\n        expected = np.array([\n            0.1287945 , 0.12792738, 0.08311573, 0.03155122, 0.02470202,\n            0.00727857, 0.00910694, 0.00686163, 0.01238981, 0.01473668,\n            0.00336144, 0.00370314, 0.00600871, 0.01120164, 0.01942998,\n            0.03132008, 0.0232842 , 0.01124642, 0.02754783, 0.02423725,\n            0.00147893, 0.00038027, 0.00112299, 0.00596233, 0.00571529,\n            0.02084235, 0.0231855 , 0.00810006, 0.01837943, 0.00651339,\n            0.00093931, 0.00067426, 0.01058399, 0.01270507, 0.00151734,\n            0.00331913, 0.00302416, 0.01081792, 0.00754549, 0.00148963,\n            0.00111943, 0.00152573, 0.00608017, 0.01749986, 0.01205949,\n            0.0143082 , 0.01910573, 0.00413786, 0.03916619, 0.09873404,\n            0.08302026, 0.02673891, 0.00401255, 0.01397392, 0.00751862,\n            0.01024884, 0.01544606, 0.00638907, 0.00623633, 0.0085103 ,\n            0.00217659, 0.00276204, 0.00260835, 0.00299299,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 0], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=128,\n            center=True,\n            pad_mode=\"constant\",\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        # fmt: off\n        expected = np.array([\n            0.06558744, 0.06889656, 0.06263352, 0.04264418, 0.03404115,\n            0.03244197, 0.02279134, 0.01646339, 0.01452216, 0.00826055,\n            0.00062093, 0.0031821 , 0.00419456, 0.00689327, 0.01106367,\n            0.01712119, 0.01721762, 0.00977533, 0.01606626, 0.02275621,\n            0.01727687, 0.00992739, 0.01217688, 0.01049927, 0.01022947,\n            0.01302475, 0.01166873, 0.01081812, 0.01057327, 0.00767912,\n            0.00429567, 0.00089625, 0.00654583, 0.00912084, 0.00700984,\n            0.00225026, 0.00290545, 0.00667712, 0.00730663, 0.00410813,\n            0.00073102, 0.00219296, 0.00527618, 0.00996585, 0.01123781,\n            0.00872816, 0.01165121, 0.02047945, 0.03681747, 0.0514379 ,\n            0.05137928, 0.03960042, 0.02821562, 0.01813349, 0.01201322,\n            0.01260964, 0.00900654, 0.00207905, 0.00456714, 0.00850599,\n            0.00788239, 0.00664407, 0.00824227, 0.00628301,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 0], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=128,\n            center=False,\n        )\n        self.assertEqual(spec.shape, (257, 728))\n\n        # fmt: off\n        expected = np.array([\n            0.00250445, 0.02161521, 0.06232229, 0.04339567, 0.00937727,\n            0.01080616, 0.00248685, 0.0095264 , 0.00727476, 0.0079152 ,\n            0.00839946, 0.00254932, 0.00716622, 0.005559  , 0.00272623,\n            0.00581774, 0.01896395, 0.01829788, 0.01020514, 0.01632692,\n            0.00870888, 0.02065827, 0.0136022 , 0.0132382 , 0.011827  ,\n            0.00194505, 0.0189979 , 0.026874  , 0.02194014, 0.01923883,\n            0.01621437, 0.00661967, 0.00289517, 0.00470257, 0.00957801,\n            0.00191455, 0.00431664, 0.00544359, 0.01126213, 0.00785778,\n            0.00423469, 0.01322504, 0.02226548, 0.02318576, 0.03428908,\n            0.03648811, 0.0202938 , 0.011902  , 0.03226198, 0.06347476,\n            0.01306318, 0.05308729, 0.05474771, 0.03127991, 0.00998512,\n            0.01449977, 0.01272741, 0.00868176, 0.00850386, 0.00313876,\n            0.00811857, 0.00538216, 0.00685749, 0.00535275,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 0], expected))"}, {"file_path": "/KVQuant-main/KVQuant-main/gradients/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_shapes", "code": "\n    def test_spectrogram_shapes(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (201, 732))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            power=1.0,\n            center=False,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (201, 729))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            fft_length=512,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=64,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=False,\n        )\n        self.assertEqual(spec.shape, (512, 1464))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=64,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=False,\n        )\n        self.assertEqual(spec.shape, (512, 1464))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=512,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=False,\n        )\n        self.assertEqual(spec.shape, (512, 183))"}, {"file_path": "/KVQuant-main/KVQuant-main/gradients/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_mel_spectrogram", "code": "\n    def test_mel_spectrogram(self):\n        waveform = self._load_datasamples(1)[0]\n\n        mel_filters = mel_filter_bank(\n            num_frequency_bins=513,\n            num_mel_filters=13,\n            min_frequency=100,\n            max_frequency=4000,\n            sampling_rate=16000,\n            norm=None,\n            mel_scale=\"htk\",\n        )\n        self.assertEqual(mel_filters.shape, (513, 13))\n\n        spec = spectrogram(\n            waveform,\n            window_function(800, \"hann\", frame_length=1024),\n            frame_length=1024,\n            hop_length=128,\n            power=2.0,\n        )\n        self.assertEqual(spec.shape, (513, 732))\n\n        spec = spectrogram(\n            waveform,\n            window_function(800, \"hann\", frame_length=1024),\n            frame_length=1024,\n            hop_length=128,\n            power=2.0,\n            mel_filters=mel_filters,\n        )\n        self.assertEqual(spec.shape, (13, 732))\n\n        # fmt: off\n        expected = np.array([\n            1.08027889e+02, 1.48080673e+01, 7.70758213e+00, 9.57676639e-01,\n            8.81639061e-02, 5.26073833e-02, 1.52736155e-02, 9.95350117e-03,\n            7.95364356e-03, 1.01148004e-02, 4.29241020e-03, 9.90708797e-03,\n            9.44153646e-04\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:, 300], expected))"}, {"file_path": "/KVQuant-main/KVQuant-main/gradients/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_power", "code": "\n    def test_spectrogram_power(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=128,\n            power=None,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n        self.assertEqual(spec.dtype, np.complex64)\n\n        # fmt: off\n        expected = np.array([\n             0.01452305+0.01820039j, -0.01737362-0.01641946j,\n             0.0121028 +0.01565081j, -0.02794554-0.03021514j,\n             0.04719803+0.04086519j, -0.04391563-0.02779365j,\n             0.05682834+0.01571325j, -0.08604821-0.02023657j,\n             0.07497991+0.0186641j , -0.06366091-0.00922475j,\n             0.11003416+0.0114788j , -0.13677941-0.01523552j,\n             0.10934535-0.00117226j, -0.11635598+0.02551187j,\n             0.14708674-0.03469823j, -0.1328196 +0.06034218j,\n             0.12667368-0.13973421j, -0.14764774+0.18912019j,\n             0.10235471-0.12181523j, -0.00773012+0.04730498j,\n            -0.01487191-0.07312611j, -0.02739162+0.09619419j,\n             0.02895459-0.05398273j,  0.01198589+0.05276592j,\n            -0.02117299-0.10123465j,  0.00666388+0.09526499j,\n            -0.01672773-0.05649684j,  0.02723125+0.05939891j,\n            -0.01879361-0.062954j  ,  0.03686557+0.04568823j,\n            -0.07394181-0.07949649j,  0.06238583+0.13905765j,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[64:96, 321], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=128,\n            power=1.0,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n        self.assertEqual(spec.dtype, np.float64)\n\n        # fmt: off\n        expected = np.array([\n            0.02328461, 0.02390484, 0.01978448, 0.04115711, 0.0624309 ,\n            0.05197181, 0.05896072, 0.08839577, 0.07726794, 0.06432579,\n            0.11063128, 0.13762532, 0.10935163, 0.11911998, 0.15112405,\n            0.14588428, 0.18860507, 0.23992978, 0.15910825, 0.04793241,\n            0.07462307, 0.10001811, 0.06125769, 0.05411011, 0.10342509,\n            0.09549777, 0.05892122, 0.06534349, 0.06569936, 0.05870678,\n            0.10856833, 0.1524107 , 0.11463385, 0.05766969, 0.12385171,\n            0.14472842, 0.11978184, 0.10353675, 0.07244056, 0.03461861,\n            0.02624896, 0.02227475, 0.01238363, 0.00885281, 0.0110049 ,\n            0.00807005, 0.01033663, 0.01703181, 0.01445856, 0.00585615,\n            0.0132431 , 0.02754132, 0.01524478, 0.0204908 , 0.07453328,\n            0.10716327, 0.07195779, 0.08816078, 0.18340898, 0.16449876,\n            0.12322842, 0.1621659 , 0.12334293, 0.06033659,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[64:128, 321], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=128,\n            power=2.0,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n        self.assertEqual(spec.dtype, np.float64)\n\n        # fmt: off\n        expected = np.array([\n            5.42173162e-04, 5.71441371e-04, 3.91425507e-04, 1.69390778e-03,\n            3.89761780e-03, 2.70106923e-03, 3.47636663e-03, 7.81381316e-03,\n            5.97033510e-03, 4.13780799e-03, 1.22392802e-02, 1.89407300e-02,\n            1.19577805e-02, 1.41895693e-02, 2.28384770e-02, 2.12822221e-02,\n            3.55718732e-02, 5.75663000e-02, 2.53154356e-02, 2.29751552e-03,\n            5.56860259e-03, 1.00036217e-02, 3.75250424e-03, 2.92790355e-03,\n            1.06967501e-02, 9.11982451e-03, 3.47171025e-03, 4.26977174e-03,\n            4.31640586e-03, 3.44648538e-03, 1.17870830e-02, 2.32290216e-02,\n            1.31409196e-02, 3.32579296e-03, 1.53392460e-02, 2.09463164e-02,\n            1.43476883e-02, 1.07198600e-02, 5.24763530e-03, 1.19844836e-03,\n            6.89007982e-04, 4.96164430e-04, 1.53354369e-04, 7.83722571e-05,\n            1.21107812e-04, 6.51257360e-05, 1.06845939e-04, 2.90082477e-04,\n            2.09049831e-04, 3.42945241e-05, 1.75379610e-04, 7.58524227e-04,\n            2.32403356e-04, 4.19872697e-04, 5.55520924e-03, 1.14839673e-02,\n            5.17792348e-03, 7.77232368e-03, 3.36388536e-02, 2.70598419e-02,\n            1.51852425e-02, 2.62977779e-02, 1.52134784e-02, 3.64050455e-03,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[64:128, 321], expected))"}, {"file_path": "/KVQuant-main/KVQuant-main/deployment/transformers/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_impulse", "code": "\n    def test_spectrogram_impulse(self):\n        waveform = np.zeros(40)\n        waveform[9] = 1.0  # impulse shifted in time\n\n        spec = spectrogram(\n            waveform,\n            window_function(12, \"hann\", frame_length=16),\n            frame_length=16,\n            hop_length=4,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (9, 11))\n\n        expected = np.array([[0.0, 0.0669873, 0.9330127, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])\n        self.assertTrue(np.allclose(spec, expected))"}, {"file_path": "/KVQuant-main/KVQuant-main/deployment/transformers/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_integration_test", "code": "\n    def test_spectrogram_integration_test(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=128,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        # fmt: off\n        expected = np.array([\n            0.02464888, 0.04648664, 0.05872392, 0.02311783, 0.0327175 ,\n            0.02433643, 0.01198814, 0.02055709, 0.01559287, 0.01394357,\n            0.01299037, 0.01728045, 0.0254554 , 0.02486533, 0.02011792,\n            0.01755333, 0.02100457, 0.02337024, 0.01436963, 0.01464558,\n            0.0211017 , 0.0193489 , 0.01272165, 0.01858462, 0.03722598,\n            0.0456542 , 0.03281558, 0.00620586, 0.02226466, 0.03618042,\n            0.03508182, 0.02271432, 0.01051649, 0.01225771, 0.02315293,\n            0.02331886, 0.01417785, 0.0106844 , 0.01791214, 0.017177  ,\n            0.02125114, 0.05028201, 0.06830665, 0.05216664, 0.01963666,\n            0.06941418, 0.11513043, 0.12257859, 0.10948435, 0.08568069,\n            0.05509328, 0.05047818, 0.047112  , 0.05060737, 0.02982424,\n            0.02803827, 0.02933729, 0.01760491, 0.00587815, 0.02117637,\n            0.0293578 , 0.03452379, 0.02194803, 0.01676056,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 400], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            fft_length=512,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n        self.assertTrue(np.allclose(spec[:64, 400], expected))\n\n        mel_filters = mel_filter_bank(\n            num_frequency_bins=256,\n            num_mel_filters=400,\n            min_frequency=20,\n            max_frequency=8000,\n            sampling_rate=16000,\n            norm=None,\n            mel_scale=\"kaldi\",\n            triangularize_in_mel_space=True,\n        )\n\n        mel_filters = np.pad(mel_filters, ((0, 1), (0, 0)))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"povey\", periodic=False),\n            frame_length=400,\n            hop_length=160,\n            fft_length=512,\n            power=2.0,\n            center=False,\n            pad_mode=\"reflect\",\n            onesided=True,\n            preemphasis=0.97,\n            mel_filters=mel_filters,\n            log_mel=\"log\",\n            mel_floor=1.1920928955078125e-07,\n            remove_dc_offset=True,\n        )\n        self.assertEqual(spec.shape, (400, 584))\n\n        # fmt: off\n        expected = np.array([-15.94238515,  -8.20712299,  -8.22704352, -15.94238515,\n       -15.94238515, -15.94238515, -15.94238515, -15.94238515,\n        -6.52463769,  -7.73677889, -15.94238515, -15.94238515,\n       -15.94238515, -15.94238515,  -4.18650018,  -3.37195286,\n       -15.94238515, -15.94238515, -15.94238515, -15.94238515,\n        -4.70190154,  -2.4217066 , -15.94238515, -15.94238515,\n       -15.94238515, -15.94238515,  -5.62755239,  -3.53385194,\n       -15.94238515, -15.94238515, -15.94238515, -15.94238515,\n        -9.43303023,  -8.77480925, -15.94238515, -15.94238515,\n       -15.94238515, -15.94238515,  -4.2951092 ,  -5.51585994,\n       -15.94238515, -15.94238515, -15.94238515,  -4.40151721,\n        -3.95228878, -15.94238515, -15.94238515, -15.94238515,\n        -6.10365415,  -4.59494697, -15.94238515, -15.94238515,\n       -15.94238515,  -8.10727767,  -6.2585298 , -15.94238515,\n       -15.94238515, -15.94238515,  -5.60161702,  -4.47217004,\n       -15.94238515, -15.94238515, -15.94238515,  -5.91641988]\n        )\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 400], expected, atol=1e-5))"}, {"file_path": "/KVQuant-main/KVQuant-main/deployment/transformers/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_center_padding", "code": "\n    def test_spectrogram_center_padding(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=128,\n            center=True,\n            pad_mode=\"reflect\",\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        # fmt: off\n        expected = np.array([\n            0.1287945 , 0.12792738, 0.08311573, 0.03155122, 0.02470202,\n            0.00727857, 0.00910694, 0.00686163, 0.01238981, 0.01473668,\n            0.00336144, 0.00370314, 0.00600871, 0.01120164, 0.01942998,\n            0.03132008, 0.0232842 , 0.01124642, 0.02754783, 0.02423725,\n            0.00147893, 0.00038027, 0.00112299, 0.00596233, 0.00571529,\n            0.02084235, 0.0231855 , 0.00810006, 0.01837943, 0.00651339,\n            0.00093931, 0.00067426, 0.01058399, 0.01270507, 0.00151734,\n            0.00331913, 0.00302416, 0.01081792, 0.00754549, 0.00148963,\n            0.00111943, 0.00152573, 0.00608017, 0.01749986, 0.01205949,\n            0.0143082 , 0.01910573, 0.00413786, 0.03916619, 0.09873404,\n            0.08302026, 0.02673891, 0.00401255, 0.01397392, 0.00751862,\n            0.01024884, 0.01544606, 0.00638907, 0.00623633, 0.0085103 ,\n            0.00217659, 0.00276204, 0.00260835, 0.00299299,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 0], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=128,\n            center=True,\n            pad_mode=\"constant\",\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        # fmt: off\n        expected = np.array([\n            0.06558744, 0.06889656, 0.06263352, 0.04264418, 0.03404115,\n            0.03244197, 0.02279134, 0.01646339, 0.01452216, 0.00826055,\n            0.00062093, 0.0031821 , 0.00419456, 0.00689327, 0.01106367,\n            0.01712119, 0.01721762, 0.00977533, 0.01606626, 0.02275621,\n            0.01727687, 0.00992739, 0.01217688, 0.01049927, 0.01022947,\n            0.01302475, 0.01166873, 0.01081812, 0.01057327, 0.00767912,\n            0.00429567, 0.00089625, 0.00654583, 0.00912084, 0.00700984,\n            0.00225026, 0.00290545, 0.00667712, 0.00730663, 0.00410813,\n            0.00073102, 0.00219296, 0.00527618, 0.00996585, 0.01123781,\n            0.00872816, 0.01165121, 0.02047945, 0.03681747, 0.0514379 ,\n            0.05137928, 0.03960042, 0.02821562, 0.01813349, 0.01201322,\n            0.01260964, 0.00900654, 0.00207905, 0.00456714, 0.00850599,\n            0.00788239, 0.00664407, 0.00824227, 0.00628301,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 0], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=128,\n            center=False,\n        )\n        self.assertEqual(spec.shape, (257, 728))\n\n        # fmt: off\n        expected = np.array([\n            0.00250445, 0.02161521, 0.06232229, 0.04339567, 0.00937727,\n            0.01080616, 0.00248685, 0.0095264 , 0.00727476, 0.0079152 ,\n            0.00839946, 0.00254932, 0.00716622, 0.005559  , 0.00272623,\n            0.00581774, 0.01896395, 0.01829788, 0.01020514, 0.01632692,\n            0.00870888, 0.02065827, 0.0136022 , 0.0132382 , 0.011827  ,\n            0.00194505, 0.0189979 , 0.026874  , 0.02194014, 0.01923883,\n            0.01621437, 0.00661967, 0.00289517, 0.00470257, 0.00957801,\n            0.00191455, 0.00431664, 0.00544359, 0.01126213, 0.00785778,\n            0.00423469, 0.01322504, 0.02226548, 0.02318576, 0.03428908,\n            0.03648811, 0.0202938 , 0.011902  , 0.03226198, 0.06347476,\n            0.01306318, 0.05308729, 0.05474771, 0.03127991, 0.00998512,\n            0.01449977, 0.01272741, 0.00868176, 0.00850386, 0.00313876,\n            0.00811857, 0.00538216, 0.00685749, 0.00535275,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:64, 0], expected))"}, {"file_path": "/KVQuant-main/KVQuant-main/deployment/transformers/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_shapes", "code": "\n    def test_spectrogram_shapes(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (201, 732))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            power=1.0,\n            center=False,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (201, 729))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\"),\n            frame_length=400,\n            hop_length=128,\n            fft_length=512,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=True,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=64,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=False,\n        )\n        self.assertEqual(spec.shape, (512, 1464))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=64,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=False,\n        )\n        self.assertEqual(spec.shape, (512, 1464))\n\n        spec = spectrogram(\n            waveform,\n            window_function(512, \"hann\"),\n            frame_length=512,\n            hop_length=512,\n            power=1.0,\n            center=True,\n            pad_mode=\"reflect\",\n            onesided=False,\n        )\n        self.assertEqual(spec.shape, (512, 183))"}, {"file_path": "/KVQuant-main/KVQuant-main/deployment/transformers/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_mel_spectrogram", "code": "\n    def test_mel_spectrogram(self):\n        waveform = self._load_datasamples(1)[0]\n\n        mel_filters = mel_filter_bank(\n            num_frequency_bins=513,\n            num_mel_filters=13,\n            min_frequency=100,\n            max_frequency=4000,\n            sampling_rate=16000,\n            norm=None,\n            mel_scale=\"htk\",\n        )\n        self.assertEqual(mel_filters.shape, (513, 13))\n\n        spec = spectrogram(\n            waveform,\n            window_function(800, \"hann\", frame_length=1024),\n            frame_length=1024,\n            hop_length=128,\n            power=2.0,\n        )\n        self.assertEqual(spec.shape, (513, 732))\n\n        spec = spectrogram(\n            waveform,\n            window_function(800, \"hann\", frame_length=1024),\n            frame_length=1024,\n            hop_length=128,\n            power=2.0,\n            mel_filters=mel_filters,\n        )\n        self.assertEqual(spec.shape, (13, 732))\n\n        # fmt: off\n        expected = np.array([\n            1.08027889e+02, 1.48080673e+01, 7.70758213e+00, 9.57676639e-01,\n            8.81639061e-02, 5.26073833e-02, 1.52736155e-02, 9.95350117e-03,\n            7.95364356e-03, 1.01148004e-02, 4.29241020e-03, 9.90708797e-03,\n            9.44153646e-04\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[:, 300], expected))"}, {"file_path": "/KVQuant-main/KVQuant-main/deployment/transformers/tests/utils/test_audio_utils.py", "class_name": "AudioUtilsFunctionTester", "function_name": "test_spectrogram_power", "code": "\n    def test_spectrogram_power(self):\n        waveform = self._load_datasamples(1)[0]\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=128,\n            power=None,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n        self.assertEqual(spec.dtype, np.complex64)\n\n        # fmt: off\n        expected = np.array([\n             0.01452305+0.01820039j, -0.01737362-0.01641946j,\n             0.0121028 +0.01565081j, -0.02794554-0.03021514j,\n             0.04719803+0.04086519j, -0.04391563-0.02779365j,\n             0.05682834+0.01571325j, -0.08604821-0.02023657j,\n             0.07497991+0.0186641j , -0.06366091-0.00922475j,\n             0.11003416+0.0114788j , -0.13677941-0.01523552j,\n             0.10934535-0.00117226j, -0.11635598+0.02551187j,\n             0.14708674-0.03469823j, -0.1328196 +0.06034218j,\n             0.12667368-0.13973421j, -0.14764774+0.18912019j,\n             0.10235471-0.12181523j, -0.00773012+0.04730498j,\n            -0.01487191-0.07312611j, -0.02739162+0.09619419j,\n             0.02895459-0.05398273j,  0.01198589+0.05276592j,\n            -0.02117299-0.10123465j,  0.00666388+0.09526499j,\n            -0.01672773-0.05649684j,  0.02723125+0.05939891j,\n            -0.01879361-0.062954j  ,  0.03686557+0.04568823j,\n            -0.07394181-0.07949649j,  0.06238583+0.13905765j,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[64:96, 321], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=128,\n            power=1.0,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n        self.assertEqual(spec.dtype, np.float64)\n\n        # fmt: off\n        expected = np.array([\n            0.02328461, 0.02390484, 0.01978448, 0.04115711, 0.0624309 ,\n            0.05197181, 0.05896072, 0.08839577, 0.07726794, 0.06432579,\n            0.11063128, 0.13762532, 0.10935163, 0.11911998, 0.15112405,\n            0.14588428, 0.18860507, 0.23992978, 0.15910825, 0.04793241,\n            0.07462307, 0.10001811, 0.06125769, 0.05411011, 0.10342509,\n            0.09549777, 0.05892122, 0.06534349, 0.06569936, 0.05870678,\n            0.10856833, 0.1524107 , 0.11463385, 0.05766969, 0.12385171,\n            0.14472842, 0.11978184, 0.10353675, 0.07244056, 0.03461861,\n            0.02624896, 0.02227475, 0.01238363, 0.00885281, 0.0110049 ,\n            0.00807005, 0.01033663, 0.01703181, 0.01445856, 0.00585615,\n            0.0132431 , 0.02754132, 0.01524478, 0.0204908 , 0.07453328,\n            0.10716327, 0.07195779, 0.08816078, 0.18340898, 0.16449876,\n            0.12322842, 0.1621659 , 0.12334293, 0.06033659,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[64:128, 321], expected))\n\n        spec = spectrogram(\n            waveform,\n            window_function(400, \"hann\", frame_length=512),\n            frame_length=512,\n            hop_length=128,\n            power=2.0,\n        )\n        self.assertEqual(spec.shape, (257, 732))\n        self.assertEqual(spec.dtype, np.float64)\n\n        # fmt: off\n        expected = np.array([\n            5.42173162e-04, 5.71441371e-04, 3.91425507e-04, 1.69390778e-03,\n            3.89761780e-03, 2.70106923e-03, 3.47636663e-03, 7.81381316e-03,\n            5.97033510e-03, 4.13780799e-03, 1.22392802e-02, 1.89407300e-02,\n            1.19577805e-02, 1.41895693e-02, 2.28384770e-02, 2.12822221e-02,\n            3.55718732e-02, 5.75663000e-02, 2.53154356e-02, 2.29751552e-03,\n            5.56860259e-03, 1.00036217e-02, 3.75250424e-03, 2.92790355e-03,\n            1.06967501e-02, 9.11982451e-03, 3.47171025e-03, 4.26977174e-03,\n            4.31640586e-03, 3.44648538e-03, 1.17870830e-02, 2.32290216e-02,\n            1.31409196e-02, 3.32579296e-03, 1.53392460e-02, 2.09463164e-02,\n            1.43476883e-02, 1.07198600e-02, 5.24763530e-03, 1.19844836e-03,\n            6.89007982e-04, 4.96164430e-04, 1.53354369e-04, 7.83722571e-05,\n            1.21107812e-04, 6.51257360e-05, 1.06845939e-04, 2.90082477e-04,\n            2.09049831e-04, 3.42945241e-05, 1.75379610e-04, 7.58524227e-04,\n            2.32403356e-04, 4.19872697e-04, 5.55520924e-03, 1.14839673e-02,\n            5.17792348e-03, 7.77232368e-03, 3.36388536e-02, 2.70598419e-02,\n            1.51852425e-02, 2.62977779e-02, 1.52134784e-02, 3.64050455e-03,\n        ])\n        # fmt: on\n        self.assertTrue(np.allclose(spec[64:128, 321], expected))"}]}, {"git_group": "canonical", "git_name": "operator", "version": "2.12.0", "language": "Python", "project_name": "operator-2.12.0.zip", "file_path": "/operator-2.12.0/operator-2.12.0/ops/storage.py", "file_name": "storage.py", "focal_class": "_JujuStorageBackend", "focal_name": "get", "focal_parameter": [], "solution": "    def get(self, key: str) -> Any:\n        # We don't capture stderr here so it can end up in debug logs.\n        p = _run([\"state-get\", key], stdout=subprocess.PIPE, check=True)\n        if p.stdout == '' or p.stdout == '\\n':\n            raise KeyError(key)\n        return yaml.load(p.stdout, Loader=_SimpleLoader)  # type: ignore  # noqa: S506", "function_signature": "def get(self, key: str) -> Any :", "left_context": "# Copyright 2019-2020 Canonical Ltd.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Structures to offer storage to the charm (through Juju or locally).\"\"\"\nimport logging\nimport os\nimport pickle\nimport shutil\nimport sqlite3\nimport stat\nimport subprocess\nfrom datetime import timedelta\nfrom pathlib import Path\nfrom typing import Any, Callable, Generator, List, Optional, Tuple, Union, cast\n\nimport yaml  # pyright: ignore[reportMissingModuleSource]\n\nlogger = logging.getLogger()\n\n\n# _Notice = Tuple[event_path, observer_path, method_name]\n_Notice = Tuple[str, str, str]\n_Notices = List[_Notice]\n\n# This is a function that takes a Tuple and returns a yaml node.\n# it replaces a method, so the first argument passed to the function\n# (Any) is 'self'.\n_TupleRepresenterType = Callable[[Any, Tuple[Any, ...]], yaml.Node]\n_NoticeGenerator = Generator['_Notice', None, None]\n\n\ndef _run(args: List[str], **kw: Any):\n    cmd: Optional[str] = shutil.which(args[0])\n    if cmd is None:\n        raise FileNotFoundError(args[0])\n    return subprocess.run([cmd, *args[1:]], encoding='utf-8', **kw)\n\n\nclass SQLiteStorage:\n    \"\"\"Storage using SQLite backend.\"\"\"\n\n    DB_LOCK_TIMEOUT = timedelta(hours=1)\n\n    def __init__(self, filename: Union['Path', str]):\n        # The isolation_level argument is set to None such that the implicit\n        # transaction management behavior of the sqlite3 module is disabled.\n\n        if not os.path.exists(str(filename)):\n            # sqlite3.connect creates the file silently if it does not exist\n            logger.debug(f\"Initializing SQLite local storage: {filename}.\")\n\n        if filename != \":memory:\":\n            self._ensure_db_permissions(str(filename))\n        self._db = sqlite3.connect(str(filename),\n                                   isolation_level=None,\n                                   timeout=self.DB_LOCK_TIMEOUT.total_seconds())\n        self._setup()\n\n    def _ensure_db_permissions(self, filename: str):\n        \"\"\"Make sure that the DB file has appropriately secure permissions.\"\"\"\n        mode = stat.S_IRUSR | stat.S_IWUSR\n        if os.path.exists(filename):\n            try:\n                os.chmod(filename, mode)\n            except OSError as e:\n                raise RuntimeError(f\"Unable to adjust access permission of {filename!r}\") from e\n            return\n\n        try:\n            fd = os.open(filename, os.O_CREAT | os.O_EXCL, mode=mode)\n        except OSError as e:\n            raise RuntimeError(f\"Unable to adjust access permission of {filename!r}\") from e\n        os.close(fd)\n\n    def _setup(self):\n        \"\"\"Make the database ready to be used as storage.\"\"\"\n        # Make sure that the database is locked until the connection is closed,\n        # not until the transaction ends.\n        self._db.execute(\"PRAGMA locking_mode=EXCLUSIVE\")\n        c = self._db.execute(\"BEGIN\")\n        c.execute(\"SELECT count(name) FROM sqlite_master WHERE type='table' AND name='snapshot'\")\n        if c.fetchone()[0] == 0:\n            # Keep in mind what might happen if the process dies somewhere below.\n            # The system must not be rendered permanently broken by that.\n            self._db.execute(\"CREATE TABLE snapshot (handle TEXT PRIMARY KEY, data BLOB)\")\n            self._db.execute('''\n                CREATE TABLE notice (\n                  sequence INTEGER PRIMARY KEY AUTOINCREMENT,\n                  event_path TEXT,\n                  observer_path TEXT,\n                  method_name TEXT)\n                ''')\n            self._db.commit()\n\n    def close(self) -> None:\n        \"\"\"Part of the Storage API, close the storage backend.\"\"\"\n        self._db.close()\n\n    def commit(self) -> None:\n        \"\"\"Part of the Storage API, commit latest changes in the storage backend.\"\"\"\n        self._db.commit()\n\n    # There's commit but no rollback. For abort to be supported, we'll need logic that\n    # can rollback decisions made by third-party code in terms of the internal state\n    # of objects that have been snapshotted, and hooks to let them know about it and\n    # take the needed actions to undo their logic until the last snapshot.\n    # This is doable but will increase significantly the chances for mistakes.\n\n    def save_snapshot(self, handle_path: str, snapshot_data: Any) -> None:\n        \"\"\"Part of the Storage API, persist a snapshot data under the given handle.\n\n        Args:\n            handle_path: The string identifying the snapshot.\n            snapshot_data: The data to be persisted. (as returned by Object.snapshot()). This\n            might be a dict/tuple/int, but must only contain 'simple' Python types.\n        \"\"\"\n        # Use pickle for serialization, so the value remains portable.\n        raw_data = pickle.dumps(snapshot_data)\n        self._db.execute(\"REPLACE INTO snapshot VALUES (?, ?)\", (handle_path, raw_data))\n\n    def load_snapshot(self, handle_path: str) -> Any:\n        \"\"\"Part of the Storage API, retrieve a snapshot that was previously saved.\n\n        Args:\n            handle_path: The string identifying the snapshot.\n\n        Raises:\n            NoSnapshotError: if there is no snapshot for the given handle_path.\n        \"\"\"\n        c = self._db.cursor()\n        c.execute(\"SELECT data FROM snapshot WHERE handle=?\", (handle_path,))\n        row = c.fetchone()\n        if row:\n            return pickle.loads(row[0])  # noqa: S301\n        raise NoSnapshotError(handle_path)\n\n    def drop_snapshot(self, handle_path: str):\n        \"\"\"Part of the Storage API, remove a snapshot that was previously saved.\n\n        Dropping a snapshot that doesn't exist is treated as a no-op.\n        \"\"\"\n        self._db.execute(\"DELETE FROM snapshot WHERE handle=?\", (handle_path,))\n\n    def list_snapshots(self) -> Generator[str, None, None]:\n        \"\"\"Return the name of all snapshots that are currently saved.\"\"\"\n        c = self._db.cursor()\n        c.execute(\"SELECT handle FROM snapshot\")\n        while True:\n            rows = c.fetchmany()\n            if not rows:\n                break\n            for row in rows:\n                yield row[0]\n\n    def save_notice(self, event_path: str, observer_path: str, method_name: str) -> None:\n        \"\"\"Part of the Storage API, record an notice (event and observer).\"\"\"\n        self._db.execute('INSERT INTO notice VALUES (NULL, ?, ?, ?)',\n                         (event_path, observer_path, method_name))\n\n    def drop_notice(self, event_path: str, observer_path: str, method_name: str) -> None:\n        \"\"\"Part of the Storage API, remove a notice that was previously recorded.\"\"\"\n        self._db.execute('''\n            DELETE FROM notice\n             WHERE event_path=?\n               AND observer_path=?\n               AND method_name=?\n            ''', (event_path, observer_path, method_name))\n\n    def notices(self, event_path: Optional[str] = None) -> '_NoticeGenerator':\n        \"\"\"Part of the Storage API, return all notices that begin with event_path.\n\n        Args:\n            event_path: If supplied, will only yield events that match event_path. If not\n                supplied (or None/'') will return all events.\n\n        Returns:\n            Iterable of (event_path, observer_path, method_name) tuples\n        \"\"\"\n        if event_path:\n            c = self._db.execute('''\n                SELECT event_path, observer_path, method_name\n                  FROM notice\n                 WHERE event_path=?\n                 ORDER BY sequence\n                ''', (event_path,))\n        else:\n            c = self._db.execute('''\n                SELECT event_path, observer_path, method_name\n                  FROM notice\n                 ORDER BY sequence\n                ''')\n        while True:\n            rows = c.fetchmany()\n            if not rows:\n                break\n            for row in rows:\n                yield cast(_Notice, tuple(row))\n\n\nclass JujuStorage:\n    \"\"\"Storing the content tracked by the Framework in Juju.\n\n    This uses :class:`_JujuStorageBackend` to interact with state-get/state-set\n    as the way to store state for the framework and for components.\n    \"\"\"\n\n    NOTICE_KEY = \"#notices#\"\n\n    def __init__(self, backend: Optional['_JujuStorageBackend'] = None):\n        self._backend: _JujuStorageBackend = backend or _JujuStorageBackend()\n\n    def close(self) -> None:\n        \"\"\"Part of the Storage API, close the storage backend.\n\n        Nothing to be done for Juju backend, as it's transactional.\n        \"\"\"\n\n    def commit(self) -> None:\n        \"\"\"Part of the Storage API, commit latest changes in the storage backend.\n\n        Nothing to be done for Juju backend, as it's transactional.\n        \"\"\"\n\n    def save_snapshot(self, handle_path: str, snapshot_data: Any) -> None:\n        \"\"\"Part of the Storage API, persist a snapshot data under the given handle.\n\n        Args:\n            handle_path: The string identifying the snapshot.\n            snapshot_data: The data to be persisted. (as returned by Object.snapshot()). This\n                might be a dict/tuple/int, but must only contain 'simple' python types.\n        \"\"\"\n        self._backend.set(handle_path, snapshot_data)\n\n    def load_snapshot(self, handle_path: str):\n        \"\"\"Part of the Storage API, retrieve a snapshot that was previously saved.\n\n        Args:\n            handle_path: The string identifying the snapshot.\n\n        Raises:\n            NoSnapshotError: if there is no snapshot for the given handle_path.\n        \"\"\"\n        try:\n            content = self._backend.get(handle_path)\n        except KeyError:\n            raise NoSnapshotError(handle_path) from None\n        return content\n\n    def drop_snapshot(self, handle_path: str):\n        \"\"\"Part of the Storage API, remove a snapshot that was previously saved.\n\n        Dropping a snapshot that doesn't exist is treated as a no-op.\n        \"\"\"\n        self._backend.delete(handle_path)\n\n    def save_notice(self, event_path: str, observer_path: str, method_name: str):\n        \"\"\"Part of the Storage API, record a notice (event and observer).\"\"\"\n        notice_list = self._load_notice_list()\n        notice_list.append((event_path, observer_path, method_name))\n        self._save_notice_list(notice_list)\n\n    def drop_notice(self, event_path: str, observer_path: str, method_name: str):\n        \"\"\"Part of the Storage API, remove a notice that was previously recorded.\"\"\"\n        notice_list = self._load_notice_list()\n        notice_list.remove((event_path, observer_path, method_name))\n        self._save_notice_list(notice_list)\n\n    def notices(self, event_path: Optional[str] = None):\n        \"\"\"Part of the Storage API, return all notices that begin with event_path.\n\n        Args:\n            event_path: If supplied, will only yield events that match event_path. If not\n                supplied (or None/'') will return all events.\n\n        Returns:\n            Iterable of (event_path, observer_path, method_name) tuples\n        \"\"\"\n        notice_list = self._load_notice_list()\n        for row in notice_list:\n            if event_path and row[0] != event_path:\n                continue\n            yield tuple(row)\n\n    def _load_notice_list(self) -> '_Notices':\n        \"\"\"Load a notice list from current key.\n\n        Returns:\n            List of (event_path, observer_path, method_name) tuples; empty if no key or is None.\n        \"\"\"\n        try:\n            notice_list = self._backend.get(self.NOTICE_KEY)\n        except KeyError:\n            return []\n        if notice_list is None:\n            return []\n        return notice_list\n\n    def _save_notice_list(self, notices: '_Notices') -> None:\n        \"\"\"Save a notice list under current key.\n\n        Args:\n            notices: List of (event_path, observer_path, method_name) tuples.\n        \"\"\"\n        self._backend.set(self.NOTICE_KEY, notices)\n\n\n# we load yaml.CSafeX if available, falling back to slower yaml.SafeX.\n_BaseDumper = getattr(yaml, 'CSafeDumper', yaml.SafeDumper)\n_BaseLoader = getattr(yaml, 'CSafeLoader', yaml.SafeLoader)\n\n\nclass _SimpleLoader(_BaseLoader):  # type: ignore\n    \"\"\"Handle a couple basic python types.\n\n    yaml.SafeLoader can handle all the basic int/float/dict/set/etc that we want. The only one\n    that it *doesn't* handle is tuples. We don't want to support arbitrary types, so we just\n    subclass SafeLoader and add tuples back in.\n    \"\"\"\n    # Taken from the example at:\n    # https://stackoverflow.com/questions/9169025/how-can-i-add-a-python-tuple-to-a-yaml-file-using-pyyaml\n\n    construct_python_tuple = yaml.Loader.construct_python_tuple  # type: ignore\n\n\n_SimpleLoader.add_constructor(  # type: ignore\n    'tag:yaml.org,2002:python/tuple',\n    _SimpleLoader.construct_python_tuple)  # type: ignore\n\n\nclass _SimpleDumper(_BaseDumper):  # type: ignore\n    \"\"\"Add types supported by 'marshal'.\n\n    YAML can support arbitrary types, but that is generally considered unsafe (like pickle). So\n    we want to only support dumping out types that are safe to load.\n    \"\"\"\n    represent_tuple: '_TupleRepresenterType' = yaml.Dumper.represent_tuple\n\n\n_SimpleDumper.add_representer(tuple, _SimpleDumper.represent_tuple)  # type: ignore\n\n\ndef juju_backend_available() -> bool:\n    \"\"\"Check if Juju state storage is available.\"\"\"\n    p = shutil.which('state-get')\n    return p is not None\n\n\nclass _JujuStorageBackend:\n    \"\"\"Implements the interface from the ops library to Juju's state-get/set/etc.\"\"\"\n\n    def set(self, key: str, value: Any) -> None:\n        \"\"\"Set a key to a given value.\n\n        Args:\n            key: The string key that will be used to find the value later\n            value: Arbitrary content that will be returned by get().\n\n        Raises:\n            CalledProcessError: if 'state-set' returns an error code.\n        \"\"\"\n        # default_flow_style=None means that it can use Block for\n        # complex types (types that have nested types) but use flow\n        # for simple types (like an array). Not all versions of PyYAML\n        # have the same default style.\n        encoded_value = yaml.dump(value, Dumper=_SimpleDumper, default_flow_style=None)\n        content = yaml.dump(\n            {key: encoded_value},\n            default_style='|',\n            default_flow_style=False,\n            Dumper=_SimpleDumper)\n        _run([\"state-set\", \"--file\", \"-\"], input=content, check=True)\n", "right_context": "\n    def delete(self, key: str) -> None:\n        \"\"\"Remove a key from being tracked.\n\n        Args:\n            key: The key to stop storing\n        Raises:\n            CalledProcessError: if 'state-delete' returns an error code.\n        \"\"\"\n        _run([\"state-delete\", key], check=True)\n\n\nclass NoSnapshotError(Exception):\n    \"\"\"Exception to flag that there is no snapshot for the given handle_path.\"\"\"\n\n    def __init__(self, handle_path: str):\n        self.handle_path = handle_path\n\n    def __str__(self):\n        return f'no snapshot data found for {self.handle_path} object'\n", "import_text": ["logging", "os", "pickle", "shutil", "sqlite3", "stat", "subprocess", "datetime.timedelta", "pathlib.Path", "typing.Any", "typing.Callable", "typing.Generator", "typing.List", "typing.Optional", "typing.Tuple", "typing.Union", "typing.cast", "yaml"], "prompt": "\"\"\"\nDescription: This function retrieves a value from a state using a provided key.\n\nArgs:\n    key (str): The key to retrieve the value for.\n\nReturns:\n    Any: The value associated with the provided key.\n\nRaises:\n    KeyError: If the key does not exist in the state.\n    subprocess.CalledProcessError: If the subprocess call to 'state-get' fails.\n\nNotes:\n    - The function uses the 'state-get' command to retrieve the value associated with the provided key.\n    - The function uses the 'yaml.load' function to parse the output of the 'state-get' command.\n    - If the output of the 'state-get' command is an empty string or a newline character, a KeyError is raised.\n    - The function uses the '_SimpleLoader' class from the 'yaml' module to load the YAML data.\n\"\"\"", "comment": "        \"\"\"Get the bytes value associated with a given key.\n\n        Args:\n            key: The string key that will be used to find the value\n        Raises:\n            CalledProcessError: if 'state-get' returns an error code.\n        \"\"\"", "prompt_is_gen_from_api": true, "function_dependencies": ["yaml.load"], "project_create_time": "2019-10-01T13:06:11+00:00", "project_update_time": "2024-03-19T04:32:14+00:00", "file_create_time": "2020-06-25T17:35:06Z", "file_update_time": "2024-02-15T19:35:48Z", "function_update_time": "2020-06-25T17:35:06Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["yaml.load"], "test_function": [{"file_path": "/operator-2.12.0/operator-2.12.0/test/test_storage.py", "class_name": "TestJujuStateBackend", "function_name": "test_set_and_get_complex_value", "code": "\n    def test_set_and_get_complex_value(self):\n        t = tempfile.NamedTemporaryFile()\n        try:\n            fake_script(self, 'state-set', dedent(\"\"\"\n                cat >> {}\n                \"\"\").format(pathlib.Path(t.name).as_posix()))\n            backend = ops.storage._JujuStorageBackend()\n            complex_val = {\n                'foo': 2,\n                3: [1, 2, '3'],\n                'four': {2, 3},\n                'five': {'a': 2, 'b': 3.0},\n                'six': ('a', 'b'),\n                'seven': b'1234',\n            }\n            backend.set('Class[foo]/_stored', complex_val)\n            self.assertEqual(fake_script_calls(self, clear=True), [\n                ['state-set', '--file', '-'],\n            ])\n            t.seek(0)\n            content = t.read()\n        finally:\n            t.close()\n        outer = yaml.safe_load(content)\n        key = 'Class[foo]/_stored'\n        self.assertEqual(list(outer.keys()), [key])\n        inner = yaml.load(outer[key], Loader=ops.storage._SimpleLoader)  # noqa: S506\n        self.assertEqual(complex_val, inner)\n        self.assertEqual(content.decode('utf-8'), dedent(\"\"\"\\\n            \"Class[foo]/_stored\": |\n              foo: 2\n              3: [1, 2, '3']\n              four: !!set {2: null, 3: null}\n              five: {a: 2, b: 3.0}\n              six: !!python/tuple [a, b]\n              seven: !!binary |\n                MTIzNA==\n            \"\"\"))\n        # Note that the content is yaml in a string, embedded inside YAML to declare the Key:\n        # Value of where to store the entry.\n        fake_script(self, 'state-get', dedent(\"\"\"\n            echo \"foo: 2\n            3: [1, 2, '3']\n            four: !!set {2: null, 3: null}\n            five: {a: 2, b: 3.0}\n            six: !!python/tuple [a, b]\n            seven: !!binary |\n              MTIzNA==\n            \"\n        \"\"\"))\n        out = backend.get('Class[foo]/_stored')\n        self.assertEqual(out, complex_val)"}]}, {"git_group": "hudson-and-thames", "git_name": "arbitragelab", "version": "0.9.1", "language": "Python", "project_name": "arbitragelab-0.9.1.zip", "file_path": "/arbitragelab-0.9.1/arbitragelab-0.9.1/arbitragelab/stochastic_control_approach/optimal_convergence.py", "file_name": "optimal_convergence.py", "focal_class": "OptimalConvergence", "focal_name": "fit", "focal_parameter": [], "solution": "    def fit(self, prices: pd.DataFrame, mu_m: float, sigma_m: float, r: float, delta_t: float = 1 / 252):\n\n        # Setting instance attributes\n        self.delta_t = delta_t\n        self.ticker_A, self.ticker_B = prices.columns[0], prices.columns[1]\n\n        self.mu_m = mu_m\n        self.sigma_m = sigma_m\n        self.r = r\n\n        # Using Equations (1) and (2) to calculate lambda's and beta term\n\n        x, _ = self._x_tau_calc(prices)\n        prices = self._data_preprocessing(prices)\n\n        returns_df = prices.pct_change()\n        returns_df = returns_df.replace([np.inf, -np.inf], np.nan).ffill().dropna()\n\n        lr = LinearRegression(fit_intercept=True)\n\n        y_1 = returns_df.iloc[:, 0].to_numpy()\n        y_2 = returns_df.iloc[:, 1].to_numpy()\n\n        lr.fit(x[:-1].reshape(-1, 1), y_1)\n\n        self.lambda_1 = -lr.coef_[0]\n        beta_1 = (lr.intercept_ - self.r) / self.mu_m\n\n        lr.fit(x[:-1].reshape(-1, 1), y_2)\n        self.lambda_2 = lr.coef_[0]\n        beta_2 = (lr.intercept_ - self.r) / self.mu_m\n\n        self.beta = (beta_1 + beta_2) / 2\n        # Equation (5) in the paper models x as a mean reverting OU process with 0 drift.\n        # The parameter estimators are taken from Appendix in Jurek paper.\n\n        mu = 0\n        # Estimator for rate of mean reversion\n        k = (-1 / self.delta_t) * np.log(np.multiply(x[1:] - mu, x[:-1] - mu).sum()\n                                              / np.power(x[1:] - mu, 2).sum())\n\n        # Part of sigma estimation formula\n        sigma_calc_sum = np.power((x[1:] - mu - np.exp(-k * self.delta_t) * (x[:-1] - mu))\n                                  / np.exp(-k * self.delta_t), 2).sum()\n\n        # Estimator for standard deviation\n        b_x = np.sqrt(2 * k * sigma_calc_sum / ((np.exp(2 * k * self.delta_t) - 1) * (len(x) - 2)))\n\n        self.b_squared = (b_x ** 2) / 2\n\n        sigma_squared_1 = (np.var(y_1, ddof=1) / self.delta_t) - self.b_squared - (self.beta ** 2) * (self.sigma_m ** 2)\n        sigma_squared_2 = (np.var(y_2, ddof=1) / self.delta_t) - self.b_squared - (self.beta ** 2) * (self.sigma_m ** 2)\n\n        if sigma_squared_1 < 0 or sigma_squared_2 < 0:\n            sigma_squared_1 = max(0, sigma_squared_1)\n            sigma_squared_2 = max(0, sigma_squared_2)\n\n            warnings.warn(\"The value of sigma estimated from the inputted data is poor. \"\n                          \"This pricing data might be not be well-suited for this particular model. \"\n                          \"Possible solution would be to use a longer time period, and use pairs suited for exploiting arbitrage mis-pricings.\")\n\n        self.sigma_squared = (sigma_squared_1 + sigma_squared_2) / 2", "function_signature": "def fit(self, prices: pd.DataFrame, mu_m: float, sigma_m: float, r: float, delta_t: float = 1 / 252) :", "left_context": "\"\"\"\nThis module is a realization of the methodology in the following paper:\n`Liu, J. and Timmermann, A., 2013. Optimal convergence trade strategies. The Review of Financial Studies, 26(4), pp.1048-1086.\n<https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.905.236&rep=rep1&type=pdf>`__\n\"\"\"\n# pylint: disable=invalid-name, too-many-instance-attributes\n\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n\nclass OptimalConvergence:\n    \"\"\"\n    This module models the optimal convergence trades under both recurring and nonrecurring arbitrage opportunities\n    represented by continuing and \u201cstopped\u201d co-integrated price processes.\n\n    Along with delta neutral portfolios, this module also considers unconstrained optimal portfolios where the portfolio weights\n    of both the stocks in the spread are calculated dynamically. Conventional long-short delta neutral strategies\n    are generally suboptimal and it can be optimal to simultaneously go long (or short) in two mis-priced assets.\n    Standard arbitrage strategies and/or delta neutral convergence trades are designed to explore long-term arbitrage\n    opportunities but do typically not optimally exploit the short-run risk return trade-off. By placing arbitrage\n    opportunities in the context of a portfolio maximization problem, this optimal convergence strategy accounts\n    for both arbitrage opportunities and diversification benefits.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the parameters of the module.\n        \"\"\"\n\n        # Estimated from error-correction model\n        self.ticker_A = None\n        self.ticker_B = None\n        self.delta_t = 1 / 252\n        self.lambda_1 = 0\n        self.lambda_2 = None\n\n        # Moment based estimates\n        self.b_squared = None\n        self.sigma_squared = 0\n        self.beta = None\n\n        # Parameters inputted by user\n        self.gamma = None  # gamma should be positive\n        self.r = None\n        self.mu_m = None\n        self.sigma_m = None\n", "right_context": "\n\n    def describe(self) -> pd.Series:\n        \"\"\"\n        Method returns values of instance attributes calculated from training data.\n\n        :return: (pd.Series) Series describing parameter values.\n        \"\"\"\n\n        if self.beta is None:\n            raise Exception(\"Please run the fit method before calling describe.\")\n\n        # List defines the indexes of the final pandas object\n        index = ['Ticker of first stock', 'Ticker of second stock',\n                 'lambda_1', 'lambda_2', 'b_squared', 'sigma_squared',\n                 'beta']\n\n        # List defines the values of the final pandas object\n        data = [self.ticker_A, self.ticker_B,\n                self.lambda_1, self.lambda_2, self.b_squared, self.sigma_squared,\n                self.beta]\n\n        # Combine data and indexes into the pandas Series\n        output = pd.Series(data=data, index=index)\n\n        return output\n\n\n    def unconstrained_portfolio_weights_continuous(self, prices: pd.DataFrame, gamma: float = 4) -> tuple:\n        \"\"\"\n        Implementation of Proposition 1.\n\n        This method calculates the portfolio weights for the market asset and for both the stocks in the spread\n        when there are no constraints put on values of lambda.\n        We also assume a continuing cointegrated price process (recurring arbitrage opportunities),\n        which gives closed-form solutions for the optimal portfolio weights.\n\n        If lambda_1 = lambda_2, from the portfolio weights outputted from this method phi_2 + phi_1 = 0,\n        which implies delta neutrality. This follows Proposition 3 in the paper.\n\n        :param prices: (pd.DataFrame) Contains price series of both stocks in spread.\n        :param gamma: (float) Signifies investor's attitude towards risk (positive float value).\n        :return: (tuple) Consists of three numpy arrays: weights for asset 1, asset 2, and market portfolio.\n        \"\"\"\n\n        if self.beta is None:\n            raise Exception(\"Please run fit before calling this method.\")\n\n        if gamma <= 0:\n            raise Exception(\"The value of gamma should be positive.\")\n\n        self.gamma = gamma\n\n        x, tau = self._x_tau_calc(prices)\n\n        C_t = self._C_calc(tau)\n\n        matrix = np.zeros((2, 2))\n        matrix[0, 0] = self.sigma_squared + self.b_squared\n        matrix[0, 1] = - self.sigma_squared\n        matrix[1, 0] = matrix[0, 1]\n        matrix[1, 1] = matrix[0, 0]\n\n        C_matrix = np.zeros((2, len(tau)))\n        C_matrix[0, :] = - self.lambda_1 + self.b_squared * C_t\n        C_matrix[1, :] =   self.lambda_2 - self.b_squared * C_t\n\n        phi = (1 / (self.gamma * (2 * self.sigma_squared + self.b_squared) * self.b_squared)) \\\n              * (matrix @ C_matrix) * x\n\n        phi_1 = phi[0, :]\n        phi_2 = phi[1, :]\n\n        phi_m = (self.mu_m / (self.gamma * self.sigma_m ** 2)) - (phi_1 + phi_2) * self.beta\n\n        return phi_1, phi_2, phi_m\n\n\n    def delta_neutral_portfolio_weights_continuous(self, prices: pd.DataFrame, gamma: float = 4) -> tuple:\n        \"\"\"\n        Implementation of Proposition 2.\n\n        This method calculates the portfolio weights for the market asset and for both the stocks in the spread\n        when the portfolio is constrained to be delta-neutral, where sum of portfolio weights of both the assets\n        in the spread is zero. We also assume a continuing cointegrated price process (recurring arbitrage opportunities),\n        which gives closed-form solutions for the optimal portfolio weights.\n\n        :param prices: (pd.DataFrame) Contains price series of both stocks in spread.\n        :param gamma: (float) Signifies investor's attitude towards risk (positive float value).\n        :return: (tuple) Consists of three numpy arrays: weights for asset 1, asset 2, and market portfolio.\n        \"\"\"\n\n        if self.beta is None:\n            raise Exception(\"Please run fit before calling this method.\")\n\n        if gamma <= 0:\n            raise Exception(\"The value of gamma should be positive.\")\n\n        self.gamma = gamma\n\n        x, tau = self._x_tau_calc(prices)\n\n        D_t = self._D_calc(tau)\n\n        phi_1 = (-(self.lambda_1 + self.lambda_2) * x + 2 * self.b_squared * D_t * x) / (2 * self.gamma * self.b_squared)\n\n        phi_2 = -phi_1\n\n        phi_m = self.mu_m / (self.gamma * self.sigma_m ** 2) + np.zeros(phi_1.shape)\n\n        return phi_1, phi_2, phi_m\n\n\n    def wealth_gain_continuous(self, gamma: float = 4, sigma: float = 0.15, b: float = 0.30, mu_m: float = 0.05,\n                               sigma_m: float = 0.35, r: float = 0.02, lambda_1: float = 0.52, lambda_2: float = -0.35 ) -> np.array:\n        \"\"\"\n        Implementation of Proposition 4.\n\n        This method calculates the expected wealth gain of the unconstrained optimal strategy relative to the\n        delta neutral strategy assuming a mis-pricing of the spread.\n\n        We take fixed values of spread between [0, 0.2]. The time to maturity is assumed to be 1 year for each value of spread.\n\n        The default values of the input parameters are taken according to the paper.\n\n        :param gamma: (float) Signifies investor's attitude towards risk (positive float value).\n        :param b: (float) Model Parameter found in Equation (2-3) in the paper.\n        :param mu_m: (float) Market Risk Premium.\n        :param sigma_m: (float) Market Volatility.\n        :param r: (float) Interest Rate.\n        :param lambda_1: (float) Parameter signifies relative liquidity of asset 1.\n        :param lambda_2: (float) Parameter signifies relative liquidity of asset 2.\n        :param sigma: (float) Model Parameter found in Equation (2-3) in the paper.\n        :return: (np.array) Wealth gain numpy array.\n        \"\"\"\n\n        if gamma <= 0:\n            raise Exception(\"The value of gamma should be positive.\")\n\n        self.gamma = gamma\n\n        x = np.linspace(0, 0.2, 252)\n        tau = np.ones(len(x))\n\n        self.sigma_squared = sigma ** 2\n        self.b_squared = b ** 2\n        self.mu_m = mu_m\n        self.sigma_m = sigma_m\n        self.r = r\n        self.lambda_1 = lambda_1\n        self.lambda_2 = lambda_2\n\n        u_x_t = self._u_func_continuous_calc(x, tau)\n        v_x_t = self._v_func_continuous_calc(x, tau)\n\n        R = np.exp((u_x_t - v_x_t) / (1 - self.gamma))\n        return R\n\n\n    @staticmethod\n    def plot_wealth_process(prices: pd.DataFrame, phi_1: np.array, phi_2: np.array, r: float, delta_t: float = 1/252):\n        \"\"\"\n        Function for plotting the wealth process.\n\n        :param prices: (pd.DataFrame) Contains price series of both stocks in spread.\n        :param phi_1: (np.array) Weights for asset 1.\n        :param phi_2: (np.array) Weights for asset 2.\n        :param r: (float) Interest Rate.\n        :param delta_t: (float) Time difference between each index of data, calculated in years.\n        \"\"\"\n\n        returns_df = prices.ffill().pct_change()\n        returns_df = returns_df.replace([np.inf, -np.inf], np.nan).ffill().dropna()\n        phi_1, phi_2 = phi_1[1:], phi_2[1:]\n\n        phi_1_ = phi_1 / (abs(phi_1) + abs(phi_2))\n        phi_2_ = phi_2 / (abs(phi_1) + abs(phi_2))\n        phi_1, phi_2 = phi_1_, phi_2_\n\n        V = np.ones(len(prices) - 1)\n\n        for i in range(len(prices) - 2):\n            # Calculating the wealth process from optimal weights.\n            # Follows Section 2 in the paper.\n\n            V[i + 1] = V[i] + V[i] * (r * delta_t + phi_1[i] * (returns_df.iloc[i, 0] - r * delta_t)\n                                      + phi_2[i] * (returns_df.iloc[i, 1] - r * delta_t))\n\n        # Plotting\n        plt.figure(figsize=(10, 6))\n        plt.plot(prices.index[1:], V, 'c-')\n        plt.title(\"Wealth process with initial wealth normalized to 1\")\n        plt.ylabel(\"Wealth\")\n        plt.xlabel(\"Date\")\n        plt.show()\n\n\n    def _x_tau_calc(self, prices: pd.DataFrame) -> tuple:\n        \"\"\"\n        Calculates the error correction term x given in equation (4) and the time remaining in years.\n\n        :param prices: (pd.DataFrame) Contains price series of both stocks in spread.\n        :return: (tuple) Consists of two numpy arrays: error correction term x, time remaining in years.\n        \"\"\"\n\n        prices = self._data_preprocessing(prices).to_numpy()\n\n        t = np.arange(0, len(prices)) * self.delta_t\n        tau = t[-1] - t  # Stores time remaining till closure (in years)\n\n        x = np.log(prices[:, 0] / prices[:, 1])\n\n        return x, tau\n\n\n    def _lambda_x_calc(self) -> float:\n        \"\"\"\n        Helper function calculates lambda_x.\n\n        :return: (float) Final value of lambda_x.\n        \"\"\"\n\n        lambda_x = self.lambda_1 + self.lambda_2  # This should be always positive\n\n        return lambda_x\n\n\n    def _xi_calc(self) -> tuple:\n        \"\"\"\n        Helper function which calculates xi, present in Appendix A.1.\n        Xi is used in the calculations of A and C functions.\n\n        :return: (tuple) Consists of two floats, xi and lambda_x.\n        \"\"\"\n\n        lambda_x = self._lambda_x_calc()\n\n        inner_term = ((self.lambda_1 ** 2 + self.lambda_2 ** 2) * (self.sigma_squared + self.b_squared)\n                      + 2 * self.lambda_1 * self.lambda_2 * self.sigma_squared) / (\n                                 self.b_squared + 2 * self.sigma_squared)\n\n        xi = np.sqrt(lambda_x ** 2 - 2 * inner_term * (1 - self.gamma))\n\n        return xi, lambda_x\n\n\n    def _C_calc(self, tau: np.array) -> np.array:\n        \"\"\"\n        Implementation of function C given in Appendix A.1.\n\n        :param tau: (np.array) Time remaining in years.\n        :return: (np.array) Final C array.\n        \"\"\"\n\n        xi, lambda_x = self._xi_calc()\n\n        C_plus = (lambda_x + xi) / (2 * self.b_squared)\n        C_minus = (lambda_x - xi) / (2 * self.b_squared)\n\n        exp_term = np.exp((2 * self.b_squared / self.gamma) * (C_plus - C_minus) * tau)\n\n        C = C_minus * (exp_term - 1) / (exp_term - (C_minus / C_plus))\n\n        return C\n\n\n    def _D_calc(self, tau: np.array) -> np.array:\n        \"\"\"\n        Implementation of function D given in Appendix A.2.\n\n        :param tau: (np.array) Time remaining in years.\n        :return: (np.array) Final D array.\n        \"\"\"\n\n        lambda_x = self._lambda_x_calc()\n        sqrt_term = np.sqrt(self.gamma)\n\n        D_plus = (lambda_x / (2 * self.b_squared)) * (1 + sqrt_term)\n        D_minus = (lambda_x / (2 * self.b_squared)) * (1 - sqrt_term)\n\n        exp_term = np.exp(2 * lambda_x * tau / sqrt_term)\n\n        D = (1 - exp_term) / ((1 / D_plus) - (exp_term / D_minus))\n\n        return D\n\n\n    def _A_calc(self, tau: np.array) -> np.array:\n        \"\"\"\n        Implementation of function A given in Appendix A.1.\n\n        :param tau: (np.array) Time remaining in years.\n        :return: (np.array) Final A array.\n        \"\"\"\n\n        xi, lambda_x = self._xi_calc()\n\n        A = self._A_B_helper(lambda_x, tau, xi)\n\n        return A\n\n\n    def _B_calc(self, tau: np.array) -> np.array:\n        \"\"\"\n        Implementation of function B given in Appendix A.2.\n\n        :param tau: (np.array) Time remaining in years.\n        :return: (np.array) Final B array.\n        \"\"\"\n\n        lambda_x = self._lambda_x_calc()\n        eta = lambda_x * np.sqrt(self.gamma)\n\n        B = self._A_B_helper(lambda_x, tau, eta)\n\n        return B\n\n\n    def _A_B_helper(self, lambda_x: float, tau: np.array, rep_term: float) -> np.array:\n        \"\"\"\n        Helper function implements the common formulae present in A and B function calculations.\n        Returns either the A or B array depending on whether xi or eta is inputted to the argument rep_term.\n\n        :param lambda_x: (float) Sum of lambda's.\n        :param tau: (np.array) Time remaining in years.\n        :param rep_term: (float) Either the xi or eta value.\n        :return: (np.array) Final result array.\n        \"\"\"\n\n        inner_exp_term = (rep_term / self.gamma) * tau\n        exp_term_1 = np.exp(inner_exp_term)\n        exp_term_2 = np.exp(-inner_exp_term)\n\n        first_term = self.r + (1 / (2 * self.gamma)) * (self.mu_m ** 2 / self.sigma_m ** 2)\n        log_term = np.log((lambda_x / 2) * ((exp_term_1 - exp_term_2) / rep_term) + 0.5 * (exp_term_1 + exp_term_2))\n\n        result_array = first_term * (1 - self.gamma) * tau + (lambda_x / 2) * tau - (self.gamma / 2) * log_term\n\n        return result_array\n\n\n    def _u_func_continuous_calc(self, x: np.array, tau: np.array) -> np.array:\n        \"\"\"\n        Implementation of the u function given in Lemma 1.\n\n        :param x: (np.array) Error correction term.\n        :param tau: (np.array) Time remaining in years.\n        :return: (np.array) Final output of u function.\n        \"\"\"\n\n        C_t = self._C_calc(tau)\n        A_t = self._A_calc(tau)\n\n        u = A_t + 0.5 * C_t * np.power(x, 2)\n\n        return u\n\n\n    def _v_func_continuous_calc(self, x: np.array, tau: np.array) -> np.array:\n        \"\"\"\n        Implementation of the v function given in Lemma 2.\n\n        :param x: (np.array) Error correction term.\n        :param tau: (np.array) Time remaining in years.\n        :return: (np.array) Final output of u function.\n        \"\"\"\n\n        D_t = self._D_calc(tau)\n        B_t = self._B_calc(tau)\n\n        v = B_t + 0.5 * D_t * np.power(x, 2)\n\n        return v\n\n\n    @staticmethod\n    def _data_preprocessing(prices: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Helper function for input data preprocessing.\n\n        :param prices: (pd.DataFrame) Pricing data of both stocks in spread.\n        :return: (pd.DataFrame) Processed dataframe.\n        \"\"\"\n\n        return prices.ffill()\n", "import_text": ["warnings", "numpy", "pandas", "matplotlib.pyplot", "sklearn.linear_model.LinearRegression"], "prompt": "\"\"\"\nDescription: This function fits a model to the given price data using the Ornstein-Uhlenbeck process.\n\nArgs:\n    prices (pd.DataFrame): A DataFrame containing the price data of two assets.\n    mu_m (float): The mean of the market portfolio.\n    sigma_m (float): The standard deviation of the market portfolio.\n    r (float): The risk-free rate.\n    delta_t (float): The time step size. Defaults to 1/252.\n\nReturns:\n    None\n\nRaises:\n    ValueError: If the input DataFrame does not contain exactly two columns.\n\nNotes:\n    This function uses the numpy.power function to calculate powers.\n    It also uses sklearn.linear_model.LinearRegression to perform linear regression.\n    The model is fitted to the price data using the Ornstein-Uhlenbeck process.\n    The fitted parameters are stored as instance attributes.\n    If the estimated value of sigma is negative, it is set to zero.\n    A warning is issued if the estimated value of sigma is poor.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"\n        This method estimates the error-correction terms (lambda) using the inputted pricing data.\n\n        :param prices: (pd.DataFrame) Contains price series of both stocks in spread.\n        :param mu_m: (float) Market Risk Premium.\n        :param sigma_m: (float) Market Volatility.\n        :param r: (float) Interest Rate.\n        :param delta_t: (float) Time difference between each index of data, calculated in years.\n        \"\"\"", "function_dependencies": ["sklearn.linear_model.LinearRegression", "sklearn.linear_model.LinearRegression.fit", "numpy.log", "numpy.multiply", "numpy.multiply.sum", "numpy.power", "numpy.power.sum", "numpy.exp", "numpy.sqrt", "numpy.var", "warnings.warn"], "project_create_time": "2020-11-06T10:14:16+00:00", "project_update_time": "2024-04-18T03:29:41+00:00", "file_create_time": "2021-04-14T13:15:54Z", "file_update_time": "2024-04-14T10:44:17Z", "function_update_time": "2021-04-14T13:15:54Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["numpy.power", "sklearn.linear_model.LinearRegression"], "test_function": [{"file_path": "/arbitragelab-0.9.1/arbitragelab-0.9.1/tests/test_optimal_convergence.py", "class_name": "TestOptimalConvergence", "function_name": "test_fit", "code": "    def test_fit(self):\n\n        # Creating an object of the class\n        sc_liu = OptimalConvergence()\n\n        # Testing negative values of sigma warning\n        with self.assertWarns(UserWarning):\n            sc_liu.fit(self.dataframe, mu_m=0.05, sigma_m=0.35, r=0.02)\n\n        sc_liu.fit(self.shell_rdp_data, mu_m=0.05, sigma_m=0.35, r=0.02)\n\n        # Checking parameter values\n        self.assertAlmostEqual(sc_liu.lambda_1, -0.000662, delta=1e-4)\n        self.assertAlmostEqual(sc_liu.lambda_2, 0.004175, delta=1e-4)\n        self.assertAlmostEqual(sc_liu.b_squared, 0.014012, delta=1e-3)\n        self.assertAlmostEqual(sc_liu.sigma_squared, 0.036801, delta=1e-4)\n        self.assertAlmostEqual(sc_liu.beta, -0.498947, delta=1e-4)"}, {"file_path": "/arbitragelab-0.9.1/arbitragelab-0.9.1/tests/test_optimal_convergence.py", "class_name": "TestOptimalConvergence", "function_name": "test_describe", "code": "    def test_describe(self):\n\n        # Creating an object of the class\n        sc_liu = OptimalConvergence()\n\n        # Testing for the run fit before this method exception\n        with self.assertRaises(Exception):\n            sc_liu.describe()\n\n        sc_liu.fit(self.shell_rdp_data, mu_m=0.05, sigma_m=0.35, r=0.02)\n\n        index = ['Ticker of first stock', 'Ticker of second stock',\n                 'lambda_1', 'lambda_2', 'b_squared', 'sigma_squared',\n                 'beta']\n\n        data = ['Shell', 'RDP', -0.000662, 0.004175, 0.014012, 0.036801, -0.498947]\n\n        # Testing the output of describe method\n        pd.testing.assert_series_equal(pd.Series(index=index, data=data), sc_liu.describe(), check_exact=False, atol=1e-3)"}, {"file_path": "/arbitragelab-0.9.1/arbitragelab-0.9.1/tests/test_optimal_convergence.py", "class_name": "TestOptimalConvergence", "function_name": "test_unconstrained_continuous", "code": "    def test_unconstrained_continuous(self):\n\n        # Creating an object of the class\n        sc_liu = OptimalConvergence()\n\n        # Testing for the run fit before this method exception\n        with self.assertRaises(Exception):\n            sc_liu.unconstrained_portfolio_weights_continuous(self.shell_rdp_data, gamma=4)\n\n        sc_liu.fit(self.shell_rdp_data, mu_m=0.05, sigma_m=0.35, r=0.02)\n\n        # Testing for the positive gamma exception\n        with self.assertRaises(Exception):\n            sc_liu.unconstrained_portfolio_weights_continuous(self.shell_rdp_data, gamma=-4)\n\n        phi_1, phi_2, phi_m = sc_liu.unconstrained_portfolio_weights_continuous(self.shell_rdp_data, gamma=4)\n\n        # Checking the values of phi_1 weights\n        self.assertAlmostEqual(np.mean(phi_1), -0.054245, delta=1e-5)\n        self.assertAlmostEqual(phi_1[7],  -0.051831, delta=1e-4)\n        self.assertAlmostEqual(phi_1[28], -0.052293, delta=1e-4)\n        self.assertAlmostEqual(phi_1[-1], -0.057334, delta=1e-4)\n\n        # Checking the values of phi_2 weights\n        self.assertAlmostEqual(np.mean(phi_2), 0.083759, delta=1e-5)\n        self.assertAlmostEqual(phi_2[7],  0.078997, delta=1e-4)\n        self.assertAlmostEqual(phi_2[28], 0.079711, delta=1e-4)\n        self.assertAlmostEqual(phi_2[-1], 0.089716, delta=1e-4)\n\n        # Checking the values of phi_m weights\n        self.assertAlmostEqual(np.mean(phi_m), 0.116766, delta=1e-5)\n        self.assertAlmostEqual(phi_m[7],  0.115595, delta=1e-4)\n        self.assertAlmostEqual(phi_m[28], 0.115720, delta=1e-4)\n        self.assertAlmostEqual(phi_m[-1], 0.118197, delta=1e-4)"}, {"file_path": "/arbitragelab-0.9.1/arbitragelab-0.9.1/tests/test_optimal_convergence.py", "class_name": "TestOptimalConvergence", "function_name": "test_delta_neutral_continuous", "code": "    def test_delta_neutral_continuous(self):\n\n        # Creating an object of the class\n        sc_liu = OptimalConvergence()\n\n        # Testing for the run fit before this method exception\n        with self.assertRaises(Exception):\n            sc_liu.delta_neutral_portfolio_weights_continuous(self.shell_rdp_data, gamma=4)\n\n        sc_liu.fit(self.shell_rdp_data, mu_m=0.05, sigma_m=0.35, r=0.02)\n\n        # Testing for the positive gamma exception\n        with self.assertRaises(Exception):\n            sc_liu.delta_neutral_portfolio_weights_continuous(self.shell_rdp_data, gamma=-4)\n\n        phi_1, phi_2, phi_m = sc_liu.delta_neutral_portfolio_weights_continuous(self.shell_rdp_data, gamma=4)\n\n        # Checking the values of phi_1 weights\n        self.assertAlmostEqual(np.mean(phi_1), -0.068540, delta=1e-5)\n        self.assertAlmostEqual(phi_1[7], -0.064546, delta=1e-4)\n        self.assertAlmostEqual(phi_1[28], -0.065130, delta=1e-4)\n        self.assertAlmostEqual(phi_1[-1], -0.073525, delta=1e-4)\n\n        # Checking the values of phi_2 weights\n        self.assertAlmostEqual(np.mean(phi_2), 0.068540, delta=1e-5)\n        self.assertAlmostEqual(phi_2[7], 0.064546, delta=1e-4)\n        self.assertAlmostEqual(phi_2[28], 0.065130, delta=1e-4)\n        self.assertAlmostEqual(phi_2[-1], 0.073525, delta=1e-4)\n\n        # Checking the values of phi_m weights\n        self.assertAlmostEqual(np.mean(phi_m), 0.102041, delta=1e-5)\n        self.assertAlmostEqual(phi_m[7], 0.102041, delta=1e-4)\n        self.assertAlmostEqual(phi_m[28], 0.102041, delta=1e-4)\n        self.assertAlmostEqual(phi_m[-1], 0.102041, delta=1e-4)"}, {"file_path": "/arbitragelab-0.9.1/arbitragelab-0.9.1/tests/test_optimal_convergence.py", "class_name": "TestOptimalConvergence", "function_name": "test_plotting", "code": "    def test_plotting(self, mock_plt):\n        # pylint: disable=invalid-name\n\n        data_train_dataframe = self.shell_rdp_data['1994':'2002']\n        data_test_dataframe = self.shell_rdp_data['2003':'2004']\n\n        oc = OptimalConvergence()\n\n        oc.fit(data_train_dataframe, r=0.05, mu_m=0.20, sigma_m=0.3)\n\n        phi_1, phi_2, _ = oc.unconstrained_portfolio_weights_continuous(data_test_dataframe, gamma=4)\n\n        oc.plot_wealth_process(data_test_dataframe, phi_1, phi_2, 0.05)\n\n        # Assert plt.figure got called\n        assert mock_plt.show.called"}]}, {"git_group": "automl", "git_name": "SMAC3", "version": "v2.0.2", "language": "Python", "project_name": "SMAC3-v2.0.2.zip", "file_path": "/SMAC3-v2.0.2/SMAC3-2.0.2/smac/model/abstract_model.py", "file_name": "abstract_model.py", "focal_class": "AbstractModel", "focal_name": "predict_marginalized", "focal_parameter": [], "solution": "    def predict_marginalized(self, X: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n        if len(X.shape) != 2:\n            raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n        if X.shape[1] != self._n_hps:\n            raise ValueError(\n                f\"Feature mismatch: X should have {self._n_hps} hyperparameters (and no features) for this method, \"\n                f\"but has {X.shape[1]} in total.\"\n            )\n\n        if self._instance_features is None:\n            mean, var = self.predict(X)\n            assert var is not None\n\n            var[var < self._var_threshold] = self._var_threshold\n            var[np.isnan(var)] = self._var_threshold\n\n            return mean, var\n        else:\n            n_instances = len(self._instance_features)\n\n            mean = np.zeros(X.shape[0])\n            var = np.zeros(X.shape[0])\n            for i, x in enumerate(X):\n                features = np.array(list(self._instance_features.values()))\n                x_tiled = np.tile(x, (n_instances, 1))\n                X_ = np.hstack((x_tiled, features))\n\n                means, vars = self.predict(X_)\n                assert vars is not None\n\n                # VAR[1/n (X_1 + ... + X_n)] =\n                # 1/n^2 * ( VAR(X_1) + ... + VAR(X_n))\n                # for independent X_1 ... X_n\n                var_x = np.sum(vars) / (len(vars) ** 2)\n                if var_x < self._var_threshold:\n                    var_x = self._var_threshold\n\n                var[i] = var_x\n                mean[i] = np.mean(means)\n\n            if len(mean.shape) == 1:\n                mean = mean.reshape((-1, 1))\n\n            if len(var.shape) == 1:\n                var = var.reshape((-1, 1))\n\n            return mean, var", "function_signature": "def predict_marginalized(self, X: np.ndarray) -> tuple[np.ndarray, np.ndarray] :", "left_context": "from __future__ import annotations\n\nfrom abc import abstractmethod\nfrom typing import Any, TypeVar\n\nimport copy\nimport warnings\n\nimport numpy as np\nfrom ConfigSpace import ConfigurationSpace\nfrom sklearn.decomposition import PCA\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom smac.constants import VERY_SMALL_NUMBER\nfrom smac.utils.configspace import get_types\nfrom smac.utils.logging import get_logger\n\n__copyright__ = \"Copyright 2022, automl.org\"\n__license__ = \"3-clause BSD\"\n\n\nlogger = get_logger(__name__)\n\n\nSelf = TypeVar(\"Self\", bound=\"AbstractModel\")\n\n\nclass AbstractModel:\n    \"\"\"Abstract implementation of the surrogate model.\n\n    Note\n    ----\n    The input dimensionality of Y for training and the output dimensions of all predictions depend on the concrete\n    implementation of this abstract class.\n\n    Parameters\n    ----------\n    configspace : ConfigurationSpace\n    instance_features : dict[str, list[int | float]] | None, defaults to None\n        Features (list of int or floats) of the instances (str). The features are incorporated into the X data,\n        on which the model is trained on.\n    pca_components : float, defaults to 7\n        Number of components to keep when using PCA to reduce dimensionality of instance features.\n    seed : int\n    \"\"\"\n\n    def __init__(\n        self,\n        configspace: ConfigurationSpace,\n        instance_features: dict[str, list[int | float]] | None = None,\n        pca_components: int | None = 7,\n        seed: int = 0,\n    ) -> None:\n        self._configspace = configspace\n        self._seed = seed\n        self._rng = np.random.RandomState(self._seed)\n        self._instance_features = instance_features\n        self._pca_components = pca_components\n\n        n_features = 0\n        if self._instance_features is not None:\n            for v in self._instance_features.values():\n                if n_features == 0:\n                    n_features = len(v)\n                else:\n                    if len(v) != n_features:\n                        raise RuntimeError(\"Instances must have the same number of features.\")\n\n        self._n_features = n_features\n        self._n_hps = len(self._configspace.get_hyperparameters())\n\n        self._pca = PCA(n_components=self._pca_components)\n        self._scaler = MinMaxScaler()\n        self._apply_pca = False\n\n        # Never use a lower variance than this.\n        # If estimated variance < var_threshold, set to var_threshold\n        self._var_threshold = VERY_SMALL_NUMBER\n        self._types, self._bounds = get_types(configspace, instance_features)\n\n        # Initial types array which is used to reset the type array at every call to `self.train()`\n        self._initial_types = copy.deepcopy(self._types)\n\n    @property\n    def meta(self) -> dict[str, Any]:\n        \"\"\"Returns the meta data of the created object.\"\"\"\n        return {\n            \"name\": self.__class__.__name__,\n            \"types\": self._types,\n            \"bounds\": self._bounds,\n            \"pca_components\": self._pca_components,\n        }\n\n    def train(self: Self, X: np.ndarray, Y: np.ndarray) -> Self:\n        \"\"\"Trains the random forest on X and Y. Internally, calls the method `_train`.\n\n        Parameters\n        ----------\n        X : np.ndarray [#samples, #hyperparameters + #features]\n            Input data points.\n        Y : np.ndarray [#samples, #objectives]\n            The corresponding target values.\n\n        Returns\n        -------\n        self : AbstractModel\n        \"\"\"\n        if len(X.shape) != 2:\n            raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n        if X.shape[1] != self._n_hps + self._n_features:\n            raise ValueError(\n                f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n                f\"but has {X.shape[1]} in total.\"\n            )\n\n        if X.shape[0] != Y.shape[0]:\n            raise ValueError(\"X.shape[0] ({}) != y.shape[0] ({})\".format(X.shape[0], Y.shape[0]))\n\n        # Reduce dimensionality of features if larger than PCA_DIM\n        if (\n            self._pca_components is not None\n            and X.shape[0] > self._pca.n_components\n            and self._n_features >= self._pca_components\n        ):\n            X_feats = X[:, -self._n_features :]\n\n            # Scale features\n            X_feats = self._scaler.fit_transform(X_feats)\n            X_feats = np.nan_to_num(X_feats)  # if features with max == min\n\n            # PCA\n            X_feats = self._pca.fit_transform(X_feats)\n            X = np.hstack((X[:, : self._n_hps], X_feats))\n\n            if hasattr(self, \"_types\"):\n                # For RF, adapt types list\n                # if X_feats.shape[0] < self._pca, X_feats.shape[1] == X_feats.shape[0]\n                self._types = np.array(\n                    np.hstack((self._types[: self._n_hps], np.zeros(X_feats.shape[1]))),\n                    dtype=np.uint,\n                )  # type: ignore\n\n            self._apply_pca = True\n        else:\n            self._apply_pca = False\n\n            if hasattr(self, \"_types\"):\n                self._types = copy.deepcopy(self._initial_types)\n\n        return self._train(X, Y)\n\n    @abstractmethod\n    def _train(self: Self, X: np.ndarray, Y: np.ndarray) -> Self:\n        \"\"\"Trains the random forest on X and Y.\n\n        Parameters\n        ----------\n        X : np.ndarray [#samples, #hyperparameters + #features]\n            Input data points.\n        Y : np.ndarray [#samples, #objectives]\n            The corresponding target values.\n\n        Returns\n        -------\n        self : AbstractModel\n        \"\"\"\n        raise NotImplementedError()\n\n    def predict(\n        self,\n        X: np.ndarray,\n        covariance_type: str | None = \"diagonal\",\n    ) -> tuple[np.ndarray, np.ndarray | None]:\n        \"\"\"Predicts mean and variance for a given X. Internally, calls the method `_predict`.\n\n        Parameters\n        ----------\n        X : np.ndarray [#samples, #hyperparameters + #features]\n            Input data points.\n        covariance_type: str | None, defaults to \"diagonal\"\n            Specifies what to return along with the mean. Applied only to Gaussian Processes.\n            Takes four valid inputs:\n            * None: Only the mean is returned.\n            * \"std\": Standard deviation at test points is returned.\n            * \"diagonal\": Diagonal of the covariance matrix is returned.\n            * \"full\": Whole covariance matrix between the test points is returned.\n\n        Returns\n        -------\n        means : np.ndarray [#samples, #objectives]\n            The predictive mean.\n        vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None\n            Predictive variance or standard deviation.\n        \"\"\"\n        if len(X.shape) != 2:\n            raise ValueError(\"Expected 2d array, got %dd array!\" % len(X.shape))\n\n        if X.shape[1] != self._n_hps + self._n_features:\n            raise ValueError(\n                f\"Feature mismatch: X should have {self._n_hps} hyperparameters + {self._n_features} features, \"\n                f\"but has {X.shape[1]} in total.\"\n            )\n\n        if self._apply_pca:\n            try:\n                X_feats = X[:, -self._n_features :]\n                X_feats = self._scaler.transform(X_feats)\n                X_feats = self._pca.transform(X_feats)\n                X = np.hstack((X[:, : self._n_hps], X_feats))\n            except NotFittedError:\n                # PCA not fitted if only one training sample\n                pass\n\n        if X.shape[1] != len(self._types):\n            raise ValueError(\"Rows in X should have %d entries but have %d!\" % (len(self._types), X.shape[1]))\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"Predicted variances smaller than 0. Setting those variances to 0.\")\n            mean, var = self._predict(X, covariance_type)\n\n        if len(mean.shape) == 1:\n            mean = mean.reshape((-1, 1))\n\n        if var is not None and len(var.shape) == 1:\n            var = var.reshape((-1, 1))\n\n        return mean, var\n\n    def _predict(\n        self,\n        X: np.ndarray,\n        covariance_type: str | None = \"diagonal\",\n    ) -> tuple[np.ndarray, np.ndarray | None]:\n        \"\"\"Predicts mean and variance for a given X.\n\n        Parameters\n        ----------\n        X : np.ndarray [#samples, #hyperparameters + #features]\n            Input data points.\n        covariance_type : str | None, defaults to \"diagonal\"\n            Specifies what to return along with the mean. Applied only to Gaussian Processes.\n            Takes four valid inputs:\n            * None: Only the mean is returned.\n            * \"std\": Standard deviation at test points is returned.\n            * \"diagonal\": Diagonal of the covariance matrix is returned.\n            * \"full\": Whole covariance matrix between the test points is returned.\n\n        Returns\n        -------\n        means : np.ndarray [#samples, #objectives]\n            The predictive mean.\n        vars : np.ndarray [#samples, #objectives] or [#samples, #samples] | None\n            Predictive variance or standard deviation.\n        \"\"\"\n        raise NotImplementedError()\n", "right_context": "", "import_text": ["abc.abstractmethod", "typing.Any", "typing.TypeVar", "copy", "warnings", "numpy", "ConfigSpace.ConfigurationSpace", "sklearn.decomposition.PCA", "sklearn.exceptions.NotFittedError", "sklearn.preprocessing.MinMaxScaler", "smac.constants.VERY_SMALL_NUMBER", "smac.utils.configspace.get_types", "smac.utils.logging.get_logger"], "prompt": "\"\"\"\nDescription: This function predicts the marginalized values of a given dataset.\n\nArgs:\n    X (np.ndarray): A 2D array of hyperparameters.\n\nReturns:\n    tuple[np.ndarray, np.ndarray]: A tuple containing the mean and variance of the predicted values.\n\nRaises:\n    ValueError: If the input array is not 2D or if the number of hyperparameters does not match the expected value.\n\nNotes:\n    - The function uses numpy.isnan to check for NaN values in the variance array.\n    - The function uses numpy.tile to replicate the hyperparameters for each instance in the dataset.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"Predicts mean and variance marginalized over all instances.\n\n        Warning\n        -------\n        The input data must not include any features.\n\n        Parameters\n        ----------\n        X : np.ndarray [#samples, #hyperparameters]\n            Input data points.\n\n        Returns\n        -------\n        means : np.ndarray [#samples, 1]\n            The predictive mean.\n        vars : np.ndarray [#samples, 1]\n            The predictive variance.\n        \"\"\"", "function_dependencies": ["numpy.isnan", "numpy.zeros", "numpy.array", "numpy.tile", "numpy.hstack", "numpy.sum", "numpy.mean", "numpy.zeros.reshape"], "project_create_time": "2016-08-17T10:58:05+00:00", "project_update_time": "2024-04-17T22:42:21+00:00", "file_create_time": "2022-10-12T10:30:30Z", "file_update_time": "2023-01-07T10:42:35Z", "function_update_time": "2022-10-12T10:30:30Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["numpy.isnan", "numpy.tile"], "test_function": [{"file_path": "/SMAC3-v2.0.2/SMAC3-2.0.2/tests/test_model/test_abstract_model.py", "class_name": null, "function_name": "test_no_pca", "code": "\ndef test_no_pca(configspace_small, make_scenario):\n    n_instances = 100\n    n_instance_features = 10\n    n_samples = 5\n\n    scenario = make_scenario(\n        configspace_small,\n        use_instances=True,\n        n_instances=n_instances,\n        n_instance_features=n_instance_features,\n    )\n    model = AbstractModel(configspace_small, scenario.instance_features, pca_components=7)\n    # We just overwrite the function as mock here\n    model._train = _train\n\n    # No PCA\n    X, y = get_X_y(configspace_small, n_samples, n_instance_features)\n    model.train(X, y)\n    assert not model._apply_pca\n\n    X, y = get_X_y(configspace_small, n_samples, n_instance_features + 1)\n    with pytest.raises(ValueError, match=\"Feature mismatch.*\"):\n        model.train(X, y)\n\n    X_test, _ = get_X_y(configspace_small, n_samples, None)\n    with pytest.raises(NotImplementedError):\n        model.predict_marginalized(X_test)\n\n    X_test, _ = get_X_y(configspace_small, n_samples, 10)\n    with pytest.raises(ValueError, match=\"Feature mismatch.*\"):\n        model.predict_marginalized(X_test)"}, {"file_path": "/SMAC3-v2.0.2/SMAC3-2.0.2/tests/test_model/test_abstract_model.py", "class_name": null, "function_name": "test_pca", "code": "\ndef test_pca(configspace_small, make_scenario):\n    n_instances = 100\n    n_instance_features = 10\n    n_samples = 155\n\n    scenario = make_scenario(\n        configspace_small,\n        use_instances=True,\n        n_instances=n_instances,\n        n_instance_features=n_instance_features,\n    )\n    model = AbstractModel(configspace_small, scenario.instance_features, pca_components=7)\n    # We just overwrite the function as mock here\n    model._train = _train\n\n    # PCA\n    X, y = get_X_y(configspace_small, n_samples, n_instance_features)\n    model.train(X, y)\n    assert model._apply_pca\n\n    X, y = get_X_y(configspace_small, n_samples, n_instance_features + 1)\n    with pytest.raises(ValueError, match=\"Feature mismatch.*\"):\n        model.train(X, y)\n\n    X_test, _ = get_X_y(configspace_small, n_samples, None)\n    with pytest.raises(NotImplementedError):\n        model.predict_marginalized(X_test)\n\n    X_test, _ = get_X_y(configspace_small, n_samples, 10)\n    with pytest.raises(ValueError, match=\"Feature mismatch.*\"):\n        model.predict_marginalized(X_test)"}]}, {"git_group": "gugarosa", "git_name": "opytimizer", "version": "v3.1.3", "language": "Python", "project_name": "opytimizer-v3.1.3.zip", "file_path": "/opytimizer-v3.1.3/opytimizer-3.1.3/opytimizer/visualization/convergence.py", "file_name": "convergence.py", "focal_class": null, "focal_name": "plot", "focal_parameter": [], "solution": "def plot(\n    *args,\n    labels: Optional[List[str]] = None,\n    title: str = \"\",\n    subtitle: str = \"\",\n    xlabel: str = \"iteration\",\n    ylabel: str = \"value\",\n    grid: bool = True,\n    legend: bool = True,\n) -> None:\n\n    _, ax = plt.subplots(figsize=(7, 5))\n\n    ax.set(xlabel=xlabel, ylabel=ylabel)\n    ax.set_title(title, loc=\"left\", fontsize=14)\n    ax.set_title(subtitle, loc=\"right\", fontsize=8, color=\"grey\")\n\n    if grid:\n        ax.grid()\n\n    if labels:\n        if not isinstance(labels, list):\n            raise e.TypeError(\"`labels` should be a list\")\n\n        if len(labels) != len(args):\n            raise e.SizeError(\"`args` and `labels` should have the same size\")\n    else:\n        labels = [f\"variable_{i}\" for i in range(len(args))]\n\n    for (arg, label) in zip(args, labels):\n        ax.plot(arg, label=label)\n\n    if legend:\n        ax.legend()\n\n    plt.show()", "function_signature": "def plot(\n    *args,\n    labels: Optional[List[str]] = None,\n    title: str = \"\",\n    subtitle: str = \"\",\n    xlabel: str = \"iteration\",\n    ylabel: str = \"value\",\n    grid: bool = True,\n    legend: bool = True,\n) -> None :", "left_context": "\"\"\"Convergence plots.\n\"\"\"\n\nfrom typing import List, Optional\n\nimport matplotlib.pyplot as plt\n\nimport opytimizer.utils.exception as e\n\n", "right_context": "", "import_text": ["typing.List", "typing.Optional", "matplotlib.pyplot", "opytimizer.utils.exception"], "prompt": "\"\"\"\nDescription: This function is used to plot multiple lines on a single plot.\n\nArgs:\n    *args: Variable length argument list. Each argument should be a list of y-values for a line.\n    labels (Optional[List[str]]): A list of labels for the lines. If not provided, default labels will be used.\n    title (str): The title of the plot. Default is an empty string.\n    subtitle (str): The subtitle of the plot. Default is an empty string.\n    xlabel (str): The label for the x-axis. Default is 'iteration'.\n    ylabel (str): The label for the y-axis. Default is 'value'.\n    grid (bool): Whether to display a grid on the plot. Default is True.\n    legend (bool): Whether to display a legend on the plot. Default is True.\n\nReturns:\n    None\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Plots the convergence graph of desired variables.\n\n    Essentially, each variable is a list or numpy array\n    with size equals to `n_iterations`.\n\n    Args:\n        labels: Labels to be applied for each plot in legend.\n        title: Title of the plot.\n        subtitle: Subtitle of the plot.\n        xlabel: Axis `x` label.\n        ylabel: Axis `y` label.\n        grid: If grid should be used or not.\n        legend: If legend should be displayed or not.\n\n    \"\"\"", "function_dependencies": ["matplotlib.pyplot.subplots", "opytimizer.utils.exception.TypeError", "opytimizer.utils.exception.SizeError", "matplotlib.pyplot.show"], "project_create_time": "2017-11-01T16:04:01+00:00", "project_update_time": "2024-04-13T19:09:53+00:00", "file_create_time": "2019-10-03T19:51:49Z", "file_update_time": "2023-09-22T14:51:43Z", "function_update_time": "2022-05-04T14:07:19Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["matplotlib.pyplot.show", "matplotlib.pyplot.subplots"], "test_function": [{"file_path": "/opytimizer-v3.1.3/opytimizer-3.1.3/tests/opytimizer/visualization/test_convergence.py", "class_name": null, "function_name": "test_convergence_plot", "code": "\ndef test_convergence_plot():\n    agent_pos = [[0.5, 0.4, 0.3], [0.5, 0.4, 0.3]]\n\n    try:\n        convergence.plot(agent_pos[0], agent_pos[1], labels=1)\n    except:\n        convergence.plot(agent_pos[0], agent_pos[1], labels=[\"agent[0]\", \"agent[1]\"])\n\n    try:\n        convergence.plot(agent_pos[0], agent_pos[1], labels=[\"agent[0]\"])\n    except:\n        convergence.plot(agent_pos[0], agent_pos[1])"}]}, {"git_group": "catboost", "git_name": "catboost", "version": "v1.2.5", "language": "Python", "project_name": "catboost-v1.2.5.zip", "file_path": "/catboost-v1.2.5/catboost-1.2.5/contrib/python/scipy/py2/scipy/optimize/_lsq/common.py", "file_name": "common.py", "focal_class": null, "focal_name": "step_size_to_bound", "focal_parameter": ["x", "s", "lb", "ub"], "solution": "def step_size_to_bound(x, s, lb, ub):\n    non_zero = np.nonzero(s)\n    s_non_zero = s[non_zero]\n    steps = np.empty_like(x)\n    steps.fill(np.inf)\n    with np.errstate(over='ignore'):\n        steps[non_zero] = np.maximum((lb - x)[non_zero] / s_non_zero,\n                                     (ub - x)[non_zero] / s_non_zero)\n    min_step = np.min(steps)\n    return min_step, np.equal(steps, min_step) * np.sign(s).astype(int)", "function_signature": "def step_size_to_bound(x, s, lb, ub) :", "left_context": "\"\"\"Functions used by least-squares algorithms.\"\"\"\nfrom __future__ import division, print_function, absolute_import\n\nfrom math import copysign\n\nimport numpy as np\nfrom numpy.linalg import norm\n\nfrom scipy.linalg import cho_factor, cho_solve, LinAlgError\nfrom scipy.sparse import issparse\nfrom scipy.sparse.linalg import LinearOperator, aslinearoperator\n\n\nEPS = np.finfo(float).eps\n\n\n# Functions related to a trust-region problem.\n\n\ndef intersect_trust_region(x, s, Delta):\n    \"\"\"Find the intersection of a line with the boundary of a trust region.\n    \n    This function solves the quadratic equation with respect to t\n    ||(x + s*t)||**2 = Delta**2.\n    \n    Returns\n    -------\n    t_neg, t_pos : tuple of float\n        Negative and positive roots.\n    \n    Raises\n    ------\n    ValueError\n        If `s` is zero or `x` is not within the trust region.\n    \"\"\"\n    a = np.dot(s, s)\n    if a == 0:\n        raise ValueError(\"`s` is zero.\")\n\n    b = np.dot(x, s)\n\n    c = np.dot(x, x) - Delta**2\n    if c > 0:\n        raise ValueError(\"`x` is not within the trust region.\")\n\n    d = np.sqrt(b*b - a*c)  # Root from one fourth of the discriminant.\n\n    # Computations below avoid loss of significance, see \"Numerical Recipes\".\n    q = -(b + copysign(d, b))\n    t1 = q / a\n    t2 = c / q\n\n    if t1 < t2:\n        return t1, t2\n    else:\n        return t2, t1\n\n\ndef solve_lsq_trust_region(n, m, uf, s, V, Delta, initial_alpha=None,\n                           rtol=0.01, max_iter=10):\n    \"\"\"Solve a trust-region problem arising in least-squares minimization.\n    \n    This function implements a method described by J. J. More [1]_ and used\n    in MINPACK, but it relies on a single SVD of Jacobian instead of series\n    of Cholesky decompositions. Before running this function, compute:\n    ``U, s, VT = svd(J, full_matrices=False)``.\n    \n    Parameters\n    ----------\n    n : int\n        Number of variables.\n    m : int\n        Number of residuals.\n    uf : ndarray\n        Computed as U.T.dot(f).\n    s : ndarray\n        Singular values of J.\n    V : ndarray\n        Transpose of VT.\n    Delta : float\n        Radius of a trust region.\n    initial_alpha : float, optional\n        Initial guess for alpha, which might be available from a previous\n        iteration. If None, determined automatically.\n    rtol : float, optional\n        Stopping tolerance for the root-finding procedure. Namely, the\n        solution ``p`` will satisfy ``abs(norm(p) - Delta) < rtol * Delta``.\n    max_iter : int, optional\n        Maximum allowed number of iterations for the root-finding procedure.\n    \n    Returns\n    -------\n    p : ndarray, shape (n,)\n        Found solution of a trust-region problem.\n    alpha : float\n        Positive value such that (J.T*J + alpha*I)*p = -J.T*f.\n        Sometimes called Levenberg-Marquardt parameter.\n    n_iter : int\n        Number of iterations made by root-finding procedure. Zero means\n        that Gauss-Newton step was selected as the solution.\n    \n    References\n    ----------\n    .. [1] More, J. J., \"The Levenberg-Marquardt Algorithm: Implementation\n           and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture Notes\n           in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\n    \"\"\"\n    def phi_and_derivative(alpha, suf, s, Delta):\n        \"\"\"Function of which to find zero.\n        \n        It is defined as \"norm of regularized (by alpha) least-squares\n        solution minus `Delta`\". Refer to [1]_.\n        \"\"\"\n        denom = s**2 + alpha\n        p_norm = norm(suf / denom)\n        phi = p_norm - Delta\n        phi_prime = -np.sum(suf ** 2 / denom**3) / p_norm\n        return phi, phi_prime\n\n    suf = s * uf\n\n    # Check if J has full rank and try Gauss-Newton step.\n    if m >= n:\n        threshold = EPS * m * s[0]\n        full_rank = s[-1] > threshold\n    else:\n        full_rank = False\n\n    if full_rank:\n        p = -V.dot(uf / s)\n        if norm(p) <= Delta:\n            return p, 0.0, 0\n\n    alpha_upper = norm(suf) / Delta\n\n    if full_rank:\n        phi, phi_prime = phi_and_derivative(0.0, suf, s, Delta)\n        alpha_lower = -phi / phi_prime\n    else:\n        alpha_lower = 0.0\n\n    if initial_alpha is None or not full_rank and initial_alpha == 0:\n        alpha = max(0.001 * alpha_upper, (alpha_lower * alpha_upper)**0.5)\n    else:\n        alpha = initial_alpha\n\n    for it in range(max_iter):\n        if alpha < alpha_lower or alpha > alpha_upper:\n            alpha = max(0.001 * alpha_upper, (alpha_lower * alpha_upper)**0.5)\n\n        phi, phi_prime = phi_and_derivative(alpha, suf, s, Delta)\n\n        if phi < 0:\n            alpha_upper = alpha\n\n        ratio = phi / phi_prime\n        alpha_lower = max(alpha_lower, alpha - ratio)\n        alpha -= (phi + Delta) * ratio / Delta\n\n        if np.abs(phi) < rtol * Delta:\n            break\n\n    p = -V.dot(suf / (s**2 + alpha))\n\n    # Make the norm of p equal to Delta, p is changed only slightly during\n    # this. It is done to prevent p lie outside the trust region (which can\n    # cause problems later).\n    p *= Delta / norm(p)\n\n    return p, alpha, it + 1\n\n\ndef solve_trust_region_2d(B, g, Delta):\n    \"\"\"Solve a general trust-region problem in 2 dimensions.\n    \n    The problem is reformulated as a 4-th order algebraic equation,\n    the solution of which is found by numpy.roots.\n    \n    Parameters\n    ----------\n    B : ndarray, shape (2, 2)\n        Symmetric matrix, defines a quadratic term of the function.\n    g : ndarray, shape (2,)\n        Defines a linear term of the function.\n    Delta : float\n        Radius of a trust region.\n    \n    Returns\n    -------\n    p : ndarray, shape (2,)\n        Found solution.\n    newton_step : bool\n        Whether the returned solution is the Newton step which lies within\n        the trust region.\n    \"\"\"\n    try:\n        R, lower = cho_factor(B)\n        p = -cho_solve((R, lower), g)\n        if np.dot(p, p) <= Delta**2:\n            return p, True\n    except LinAlgError:\n        pass\n\n    a = B[0, 0] * Delta**2\n    b = B[0, 1] * Delta**2\n    c = B[1, 1] * Delta**2\n\n    d = g[0] * Delta\n    f = g[1] * Delta\n\n    coeffs = np.array(\n        [-b + d, 2 * (a - c + f), 6 * b, 2 * (-a + c + f), -b - d])\n    t = np.roots(coeffs)  # Can handle leading zeros.\n    t = np.real(t[np.isreal(t)])\n\n    p = Delta * np.vstack((2 * t / (1 + t**2), (1 - t**2) / (1 + t**2)))\n    value = 0.5 * np.sum(p * B.dot(p), axis=0) + np.dot(g, p)\n    i = np.argmin(value)\n    p = p[:, i]\n\n    return p, False\n\n\ndef update_tr_radius(Delta, actual_reduction, predicted_reduction,\n                     step_norm, bound_hit):\n    \"\"\"Update the radius of a trust region based on the cost reduction.\n\n    Returns\n    -------\n    Delta : float\n        New radius.\n    ratio : float\n        Ratio between actual and predicted reductions. Zero if predicted\n        reduction is zero.\n    \"\"\"\n    if predicted_reduction > 0:\n        ratio = actual_reduction / predicted_reduction\n    else:\n        ratio = 0\n\n    if ratio < 0.25:\n        Delta = 0.25 * step_norm\n    elif ratio > 0.75 and bound_hit:\n        Delta *= 2.0\n\n    return Delta, ratio\n\n\n# Construction and minimization of quadratic functions.\n\n\ndef build_quadratic_1d(J, g, s, diag=None, s0=None):\n    \"\"\"Parameterize a multivariate quadratic function along a line.\n    \n    The resulting univariate quadratic function is given as follows:\n    ::\n        f(t) = 0.5 * (s0 + s*t).T * (J.T*J + diag) * (s0 + s*t) +\n               g.T * (s0 + s*t)\n    \n    Parameters\n    ----------\n    J : ndarray, sparse matrix or LinearOperator shape (m, n)\n        Jacobian matrix, affects the quadratic term.\n    g : ndarray, shape (n,)\n        Gradient, defines the linear term.\n    s : ndarray, shape (n,)\n        Direction vector of a line.\n    diag : None or ndarray with shape (n,), optional\n        Addition diagonal part, affects the quadratic term.\n        If None, assumed to be 0.\n    s0 : None or ndarray with shape (n,), optional\n        Initial point. If None, assumed to be 0.\n    \n    Returns\n    -------\n    a : float\n        Coefficient for t**2.\n    b : float\n        Coefficient for t.\n    c : float\n        Free term. Returned only if `s0` is provided.\n    \"\"\"\n    v = J.dot(s)\n    a = np.dot(v, v)\n    if diag is not None:\n        a += np.dot(s * diag, s)\n    a *= 0.5\n\n    b = np.dot(g, s)\n\n    if s0 is not None:\n        u = J.dot(s0)\n        b += np.dot(u, v)\n        c = 0.5 * np.dot(u, u) + np.dot(g, s0)\n        if diag is not None:\n            b += np.dot(s0 * diag, s)\n            c += 0.5 * np.dot(s0 * diag, s0)\n        return a, b, c\n    else:\n        return a, b\n\n\ndef minimize_quadratic_1d(a, b, lb, ub, c=0):\n    \"\"\"Minimize a 1-d quadratic function subject to bounds.\n    \n    The free term `c` is 0 by default. Bounds must be finite.\n    \n    Returns\n    -------\n    t : float\n        Minimum point.\n    y : float\n        Minimum value.\n    \"\"\"\n    t = [lb, ub]\n    if a != 0:\n        extremum = -0.5 * b / a\n        if lb < extremum < ub:\n            t.append(extremum)\n    t = np.asarray(t)\n    y = a * t**2 + b * t + c\n    min_index = np.argmin(y)\n    return t[min_index], y[min_index]\n\n\ndef evaluate_quadratic(J, g, s, diag=None):\n    \"\"\"Compute values of a quadratic function arising in least squares.\n    \n    The function is 0.5 * s.T * (J.T * J + diag) * s + g.T * s.\n    \n    Parameters\n    ----------\n    J : ndarray, sparse matrix or LinearOperator, shape (m, n)\n        Jacobian matrix, affects the quadratic term.\n    g : ndarray, shape (n,)\n        Gradient, defines the linear term.\n    s : ndarray, shape (k, n) or (n,)\n        Array containing steps as rows.\n    diag : ndarray, shape (n,), optional\n        Addition diagonal part, affects the quadratic term.\n        If None, assumed to be 0.\n    \n    Returns\n    -------\n    values : ndarray with shape (k,) or float\n        Values of the function. If `s` was 2-dimensional then ndarray is\n        returned, otherwise float is returned.\n    \"\"\"\n    if s.ndim == 1:\n        Js = J.dot(s)\n        q = np.dot(Js, Js)\n        if diag is not None:\n            q += np.dot(s * diag, s)\n    else:\n        Js = J.dot(s.T)\n        q = np.sum(Js**2, axis=0)\n        if diag is not None:\n            q += np.sum(diag * s**2, axis=1)\n\n    l = np.dot(s, g)\n\n    return 0.5 * q + l\n\n\n# Utility functions to work with bound constraints.\n\n\ndef in_bounds(x, lb, ub):\n    \"\"\"Check if a point lies within bounds.\"\"\"\n    return np.all((x >= lb) & (x <= ub))\n\n", "right_context": "\n\ndef find_active_constraints(x, lb, ub, rtol=1e-10):\n    \"\"\"Determine which constraints are active in a given point.\n    \n    The threshold is computed using `rtol` and the absolute value of the\n    closest bound.\n    \n    Returns\n    -------\n    active : ndarray of int with shape of x\n        Each component shows whether the corresponding constraint is active:\n             \n             *  0 - a constraint is not active.\n             * -1 - a lower bound is active.\n             *  1 - a upper bound is active.\n    \"\"\"\n    active = np.zeros_like(x, dtype=int)\n\n    if rtol == 0:\n        active[x <= lb] = -1\n        active[x >= ub] = 1\n        return active\n\n    lower_dist = x - lb\n    upper_dist = ub - x\n\n    lower_threshold = rtol * np.maximum(1, np.abs(lb))\n    upper_threshold = rtol * np.maximum(1, np.abs(ub))\n\n    lower_active = (np.isfinite(lb) &\n                    (lower_dist <= np.minimum(upper_dist, lower_threshold)))\n    active[lower_active] = -1\n\n    upper_active = (np.isfinite(ub) &\n                    (upper_dist <= np.minimum(lower_dist, upper_threshold)))\n    active[upper_active] = 1\n\n    return active\n\n\ndef make_strictly_feasible(x, lb, ub, rstep=1e-10):\n    \"\"\"Shift a point to the interior of a feasible region.\n    \n    Each element of the returned vector is at least at a relative distance\n    `rstep` from the closest bound. If ``rstep=0`` then `np.nextafter` is used.\n    \"\"\"\n    x_new = x.copy()\n\n    active = find_active_constraints(x, lb, ub, rstep)\n    lower_mask = np.equal(active, -1)\n    upper_mask = np.equal(active, 1)\n\n    if rstep == 0:\n        x_new[lower_mask] = np.nextafter(lb[lower_mask], ub[lower_mask])\n        x_new[upper_mask] = np.nextafter(ub[upper_mask], lb[upper_mask])\n    else:\n        x_new[lower_mask] = (lb[lower_mask] +\n                             rstep * np.maximum(1, np.abs(lb[lower_mask])))\n        x_new[upper_mask] = (ub[upper_mask] -\n                             rstep * np.maximum(1, np.abs(ub[upper_mask])))\n\n    tight_bounds = (x_new < lb) | (x_new > ub)\n    x_new[tight_bounds] = 0.5 * (lb[tight_bounds] + ub[tight_bounds])\n\n    return x_new\n\n \ndef CL_scaling_vector(x, g, lb, ub):\n    \"\"\"Compute Coleman-Li scaling vector and its derivatives.\n    \n    Components of a vector v are defined as follows:\n    ::\n               | ub[i] - x[i], if g[i] < 0 and ub[i] < np.inf\n        v[i] = | x[i] - lb[i], if g[i] > 0 and lb[i] > -np.inf\n               | 1,           otherwise\n    \n    According to this definition v[i] >= 0 for all i. It differs from the\n    definition in paper [1]_ (eq. (2.2)), where the absolute value of v is\n    used. Both definitions are equivalent down the line.\n    Derivatives of v with respect to x take value 1, -1 or 0 depending on a\n    case.\n    \n    Returns\n    -------\n    v : ndarray with shape of x\n        Scaling vector.\n    dv : ndarray with shape of x\n        Derivatives of v[i] with respect to x[i], diagonal elements of v's\n        Jacobian.\n    \n    References\n    ----------\n    .. [1] M.A. Branch, T.F. Coleman, and Y. Li, \"A Subspace, Interior,\n           and Conjugate Gradient Method for Large-Scale Bound-Constrained\n           Minimization Problems,\" SIAM Journal on Scientific Computing,\n           Vol. 21, Number 1, pp 1-23, 1999.\n    \"\"\"\n    v = np.ones_like(x)\n    dv = np.zeros_like(x)\n\n    mask = (g < 0) & np.isfinite(ub)\n    v[mask] = ub[mask] - x[mask]\n    dv[mask] = -1\n\n    mask = (g > 0) & np.isfinite(lb)\n    v[mask] = x[mask] - lb[mask]\n    dv[mask] = 1\n\n    return v, dv\n\n\ndef reflective_transformation(y, lb, ub):\n    \"\"\"Compute reflective transformation and its gradient.\"\"\"\n    if in_bounds(y, lb, ub):\n        return y, np.ones_like(y)\n\n    lb_finite = np.isfinite(lb)\n    ub_finite = np.isfinite(ub)\n\n    x = y.copy()\n    g_negative = np.zeros_like(y, dtype=bool)\n\n    mask = lb_finite & ~ub_finite\n    x[mask] = np.maximum(y[mask], 2 * lb[mask] - y[mask])\n    g_negative[mask] = y[mask] < lb[mask]\n\n    mask = ~lb_finite & ub_finite\n    x[mask] = np.minimum(y[mask], 2 * ub[mask] - y[mask])\n    g_negative[mask] = y[mask] > ub[mask]\n\n    mask = lb_finite & ub_finite\n    d = ub - lb\n    t = np.remainder(y[mask] - lb[mask], 2 * d[mask])\n    x[mask] = lb[mask] + np.minimum(t, 2 * d[mask] - t)\n    g_negative[mask] = t > d[mask]\n\n    g = np.ones_like(y)\n    g[g_negative] = -1\n\n    return x, g\n\n\n# Functions to display algorithm's progress.\n\n\ndef print_header_nonlinear():\n    print(\"{0:^15}{1:^15}{2:^15}{3:^15}{4:^15}{5:^15}\"\n          .format(\"Iteration\", \"Total nfev\", \"Cost\", \"Cost reduction\",\n                  \"Step norm\", \"Optimality\"))\n\n\ndef print_iteration_nonlinear(iteration, nfev, cost, cost_reduction,\n                              step_norm, optimality):\n    if cost_reduction is None:\n        cost_reduction = \" \" * 15\n    else:\n        cost_reduction = \"{0:^15.2e}\".format(cost_reduction)\n\n    if step_norm is None:\n        step_norm = \" \" * 15\n    else:\n        step_norm = \"{0:^15.2e}\".format(step_norm)\n\n    print(\"{0:^15}{1:^15}{2:^15.4e}{3}{4}{5:^15.2e}\"\n          .format(iteration, nfev, cost, cost_reduction,\n                  step_norm, optimality))\n\n\ndef print_header_linear():\n    print(\"{0:^15}{1:^15}{2:^15}{3:^15}{4:^15}\"\n          .format(\"Iteration\", \"Cost\", \"Cost reduction\", \"Step norm\",\n                  \"Optimality\"))\n\n\ndef print_iteration_linear(iteration, cost, cost_reduction, step_norm,\n                           optimality):\n    if cost_reduction is None:\n        cost_reduction = \" \" * 15\n    else:\n        cost_reduction = \"{0:^15.2e}\".format(cost_reduction)\n\n    if step_norm is None:\n        step_norm = \" \" * 15\n    else:\n        step_norm = \"{0:^15.2e}\".format(step_norm)\n\n    print(\"{0:^15}{1:^15.4e}{2}{3}{4:^15.2e}\".format(\n        iteration, cost, cost_reduction, step_norm, optimality))\n\n\n# Simple helper functions.\n\n\ndef compute_grad(J, f):\n    \"\"\"Compute gradient of the least-squares cost function.\"\"\"\n    if isinstance(J, LinearOperator):\n        return J.rmatvec(f)\n    else:\n        return J.T.dot(f)\n\n\ndef compute_jac_scale(J, scale_inv_old=None):\n    \"\"\"Compute variables scale based on the Jacobian matrix.\"\"\"\n    if issparse(J):\n        scale_inv = np.asarray(J.power(2).sum(axis=0)).ravel()**0.5\n    else:\n        scale_inv = np.sum(J**2, axis=0)**0.5\n\n    if scale_inv_old is None:\n        scale_inv[scale_inv == 0] = 1\n    else:\n        scale_inv = np.maximum(scale_inv, scale_inv_old)\n\n    return 1 / scale_inv, scale_inv\n\n\ndef left_multiplied_operator(J, d):\n    \"\"\"Return diag(d) J as LinearOperator.\"\"\"\n    J = aslinearoperator(J)\n\n    def matvec(x):\n        return d * J.matvec(x)\n\n    def matmat(X):\n        return d[:, np.newaxis] * J.matmat(X)\n\n    def rmatvec(x):\n        return J.rmatvec(x.ravel() * d)\n\n    return LinearOperator(J.shape, matvec=matvec, matmat=matmat,\n                          rmatvec=rmatvec)\n\n\ndef right_multiplied_operator(J, d):\n    \"\"\"Return J diag(d) as LinearOperator.\"\"\"\n    J = aslinearoperator(J)\n\n    def matvec(x):\n        return J.matvec(np.ravel(x) * d)\n\n    def matmat(X):\n        return J.matmat(X * d[:, np.newaxis])\n\n    def rmatvec(x):\n        return d * J.rmatvec(x)\n\n    return LinearOperator(J.shape, matvec=matvec, matmat=matmat,\n                          rmatvec=rmatvec)\n\n\ndef regularized_lsq_operator(J, diag):\n    \"\"\"Return a matrix arising in regularized least squares as LinearOperator.\n    \n    The matrix is\n        [ J ]\n        [ D ]\n    where D is diagonal matrix with elements from `diag`.\n    \"\"\"\n    J = aslinearoperator(J)\n    m, n = J.shape\n\n    def matvec(x):\n        return np.hstack((J.matvec(x), diag * x))\n\n    def rmatvec(x):\n        x1 = x[:m]\n        x2 = x[m:]\n        return J.rmatvec(x1) + diag * x2\n\n    return LinearOperator((m + n, n), matvec=matvec, rmatvec=rmatvec)\n\n\ndef right_multiply(J, d, copy=True):\n    \"\"\"Compute J diag(d).\n    \n    If `copy` is False, `J` is modified in place (unless being LinearOperator).\n    \"\"\"\n    if copy and not isinstance(J, LinearOperator):\n        J = J.copy()\n\n    if issparse(J):\n        J.data *= d.take(J.indices, mode='clip')  # scikit-learn recipe.\n    elif isinstance(J, LinearOperator):\n        J = right_multiplied_operator(J, d)\n    else:\n        J *= d\n\n    return J\n\n\ndef left_multiply(J, d, copy=True):\n    \"\"\"Compute diag(d) J.\n    \n    If `copy` is False, `J` is modified in place (unless being LinearOperator).\n    \"\"\"\n    if copy and not isinstance(J, LinearOperator):\n        J = J.copy()\n\n    if issparse(J):\n        J.data *= np.repeat(d, np.diff(J.indptr))  # scikit-learn recipe.\n    elif isinstance(J, LinearOperator):\n        J = left_multiplied_operator(J, d)\n    else:\n        J *= d[:, np.newaxis]\n\n    return J\n\n\ndef check_termination(dF, F, dx_norm, x_norm, ratio, ftol, xtol):\n    \"\"\"Check termination condition for nonlinear least squares.\"\"\"\n    ftol_satisfied = dF < ftol * F and ratio > 0.25\n    xtol_satisfied = dx_norm < xtol * (xtol + x_norm)\n\n    if ftol_satisfied and xtol_satisfied:\n        return 4\n    elif ftol_satisfied:\n        return 2\n    elif xtol_satisfied:\n        return 3\n    else:\n        return None\n\n\ndef scale_for_robust_loss_function(J, f, rho):\n    \"\"\"Scale Jacobian and residuals for a robust loss function.\n    \n    Arrays are modified in place.\n    \"\"\"\n    J_scale = rho[1] + 2 * rho[2] * f**2\n    J_scale[J_scale < EPS] = EPS\n    J_scale **= 0.5\n\n    f *= rho[1] / J_scale\n\n    return left_multiply(J, J_scale, copy=False), f\n", "import_text": ["math.copysign", "numpy", "numpy.linalg.norm", "scipy.linalg.cho_factor", "scipy.linalg.cho_solve", "scipy.linalg.LinAlgError", "scipy.sparse.issparse", "scipy.sparse.linalg.LinearOperator", "scipy.sparse.linalg.aslinearoperator"], "prompt": "\"\"\"\nDescription: This function calculates the minimum step size to reach the boundary of a given range.\n\nArgs:\n    x (numpy.ndarray): The current position.\n    s (numpy.ndarray): The step size.\n    lb (float): The lower boundary.\n    ub (float): The upper boundary.\n\nReturns:\n    tuple: A tuple containing the minimum step size and a boolean array indicating the direction of the step.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Compute a min_step size required to reach a bound.\n    \n    The function computes a positive scalar t, such that x + s * t is on\n    the bound.\n    \n    Returns\n    -------\n    step : float\n        Computed step. Non-negative value.\n    hits : ndarray of int with shape of x\n        Each element indicates whether a corresponding variable reaches the\n        bound:\n             \n             *  0 - the bound was not hit.\n             * -1 - the lower bound was hit.\n             *  1 - the upper bound was hit.\n    \"\"\"", "function_dependencies": ["numpy.nonzero", "numpy.empty_like", "numpy.empty_like.fill", "numpy.errstate", "numpy.maximum", "numpy.min", "numpy.equal", "numpy.sign", "numpy.sign.astype"], "project_create_time": "2017-07-18T05:29:04+00:00", "project_update_time": "2024-04-17T19:09:27+00:00", "file_create_time": "2021-12-13T16:11:19Z", "file_update_time": "2023-01-16T14:06:57Z", "function_update_time": "2021-12-13T16:11:19Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["numpy.min", "numpy.nonzero", "numpy.errstate"], "test_function": [{"file_path": "/catboost-v1.2.5/catboost-1.2.5/contrib/python/scipy/py2/scipy/optimize/tests/test_lsq_common.py", "class_name": "TestBounds", "function_name": "test_step_size_to_bounds", "code": "\n    def test_step_size_to_bounds(self):\n        lb = np.array([-1.0, 2.5, 10.0])\n        ub = np.array([1.0, 5.0, 100.0])\n        x = np.array([0.0, 2.5, 12.0])\n\n        s = np.array([0.1, 0.0, 0.0])\n        step, hits = step_size_to_bound(x, s, lb, ub)\n        assert_equal(step, 10)\n        assert_equal(hits, [1, 0, 0])\n\n        s = np.array([0.01, 0.05, -1.0])\n        step, hits = step_size_to_bound(x, s, lb, ub)\n        assert_equal(step, 2)\n        assert_equal(hits, [0, 0, -1])\n\n        s = np.array([10.0, -0.0001, 100.0])\n        step, hits = step_size_to_bound(x, s, lb, ub)\n        assert_equal(step, np.array(-0))\n        assert_equal(hits, [0, -1, 0])\n\n        s = np.array([1.0, 0.5, -2.0])\n        step, hits = step_size_to_bound(x, s, lb, ub)\n        assert_equal(step, 1.0)\n        assert_equal(hits, [1, 0, -1])\n\n        s = np.zeros(3)\n        step, hits = step_size_to_bound(x, s, lb, ub)\n        assert_equal(step, np.inf)\n        assert_equal(hits, [0, 0, 0])"}, {"file_path": "/catboost-v1.2.5/catboost-1.2.5/contrib/python/scipy/py3/scipy/optimize/tests/test_lsq_common.py", "class_name": "TestBounds", "function_name": "test_step_size_to_bounds", "code": "\n    def test_step_size_to_bounds(self):\n        lb = np.array([-1.0, 2.5, 10.0])\n        ub = np.array([1.0, 5.0, 100.0])\n        x = np.array([0.0, 2.5, 12.0])\n\n        s = np.array([0.1, 0.0, 0.0])\n        step, hits = step_size_to_bound(x, s, lb, ub)\n        assert_equal(step, 10)\n        assert_equal(hits, [1, 0, 0])\n\n        s = np.array([0.01, 0.05, -1.0])\n        step, hits = step_size_to_bound(x, s, lb, ub)\n        assert_equal(step, 2)\n        assert_equal(hits, [0, 0, -1])\n\n        s = np.array([10.0, -0.0001, 100.0])\n        step, hits = step_size_to_bound(x, s, lb, ub)\n        assert_equal(step, np.array(-0))\n        assert_equal(hits, [0, -1, 0])\n\n        s = np.array([1.0, 0.5, -2.0])\n        step, hits = step_size_to_bound(x, s, lb, ub)\n        assert_equal(step, 1.0)\n        assert_equal(hits, [1, 0, -1])\n\n        s = np.zeros(3)\n        step, hits = step_size_to_bound(x, s, lb, ub)\n        assert_equal(step, np.inf)\n        assert_equal(hits, [0, 0, 0])"}]}, {"git_group": "mrahtz", "git_name": "learning-from-human-preferences", "version": "master", "language": "Python", "project_name": "learning-from-human-preferences-master.zip", "file_path": "/learning-from-human-preferences-master/learning-from-human-preferences-master/utils.py", "file_name": "utils.py", "focal_class": null, "focal_name": "batch_iter", "focal_parameter": ["data", "batch_size"], "solution": "\ndef batch_iter(data, batch_size, shuffle=False):\n    idxs = list(range(len(data)))\n    if shuffle:\n        np.random.shuffle(idxs)  # in-place\n\n    start_idx = 0\n    end_idx = 0\n    while end_idx < len(data):\n        end_idx = start_idx + batch_size\n        if end_idx > len(data):\n            end_idx = len(data)\n\n        batch_idxs = idxs[start_idx:end_idx]\n        batch = []\n        for idx in batch_idxs:\n            batch.append(data[idx])\n\n        yield batch\n        start_idx += batch_size", "function_signature": "def batch_iter(data, batch_size, shuffle=False) :", "left_context": "import queue\nimport random\nimport socket\nimport time\nfrom multiprocessing import Process\n\nimport gym\nimport numpy as np\nimport pyglet\n\nfrom a2c.common.atari_wrappers import wrap_deepmind\nfrom scipy.ndimage import zoom\n\n\n# https://github.com/joschu/modular_rl/blob/master/modular_rl/running_stat.py\n# http://www.johndcook.com/blog/standard_deviation/\nclass RunningStat(object):\n    def __init__(self, shape=()):\n        self._n = 0\n        self._M = np.zeros(shape)\n        self._S = np.zeros(shape)\n\n    def push(self, x):\n        x = np.asarray(x)\n        assert x.shape == self._M.shape\n        self._n += 1\n        if self._n == 1:\n            self._M[...] = x\n        else:\n            oldM = self._M.copy()\n            self._M[...] = oldM + (x - oldM)/self._n\n            self._S[...] = self._S + (x - oldM)*(x - self._M)\n\n    @property\n    def n(self):\n        return self._n\n\n    @property\n    def mean(self):\n        return self._M\n\n    @property\n    def var(self):\n        if self._n >= 2:\n            return self._S/(self._n - 1)\n        else:\n            return np.square(self._M)\n\n    @property\n    def std(self):\n        return np.sqrt(self.var)\n\n    @property\n    def shape(self):\n        return self._M.shape\n\n\n# Based on SimpleImageViewer in OpenAI gym\nclass Im(object):\n    def __init__(self, display=None):\n        self.window = None\n        self.isopen = False\n        self.display = display\n\n    def imshow(self, arr):\n        if self.window is None:\n            height, width = arr.shape\n            self.window = pyglet.window.Window(\n                width=width, height=height, display=self.display)\n            self.width = width\n            self.height = height\n            self.isopen = True\n\n        assert arr.shape == (self.height, self.width), \\\n            \"You passed in an image with the wrong number shape\"\n\n        image = pyglet.image.ImageData(self.width, self.height,\n                                       'L', arr.tobytes(), pitch=-self.width)\n        self.window.clear()\n        self.window.switch_to()\n        self.window.dispatch_events()\n        image.blit(0, 0)\n        self.window.flip()\n\n    def close(self):\n        if self.isopen:\n            self.window.close()\n            self.isopen = False\n\n    def __del__(self):\n        self.close()\n\n\nclass VideoRenderer:\n    play_through_mode = 0\n    restart_on_get_mode = 1\n\n    def __init__(self, vid_queue, mode, zoom=1, playback_speed=1):\n        assert mode == VideoRenderer.restart_on_get_mode or mode == VideoRenderer.play_through_mode\n        self.mode = mode\n        self.vid_queue = vid_queue\n        self.zoom_factor = zoom\n        self.playback_speed = playback_speed\n        self.proc = Process(target=self.render)\n        self.proc.start()\n\n    def stop(self):\n        self.proc.terminate()\n\n    def render(self):\n        v = Im()\n        frames = self.vid_queue.get(block=True)\n        t = 0\n        while True:\n            # Add a grey dot on the last line showing position\n            width = frames[t].shape[1]\n            fraction_played = t / len(frames)\n            x = int(fraction_played * width)\n            frames[t][-1][x] = 128\n\n            zoomed_frame = zoom(frames[t], self.zoom_factor)\n            v.imshow(zoomed_frame)\n\n            if self.mode == VideoRenderer.play_through_mode:\n                # Wait until having finished playing the current\n                # set of frames. Then, stop, and get the most\n                # recent set of frames.\n                t += self.playback_speed\n                if t >= len(frames):\n                    frames = self.get_queue_most_recent()\n                    t = 0\n                else:\n                    time.sleep(1/60)\n            elif self.mode == VideoRenderer.restart_on_get_mode:\n                # Always try and get a new set of frames to show.\n                # If there is a new set of frames on the queue,\n                # restart playback with those frames immediately.\n                # Otherwise, just keep looping with the current frames.\n                try:\n                    frames = self.vid_queue.get(block=False)\n                    t = 0\n                except queue.Empty:\n                    t = (t + self.playback_speed) % len(frames)\n                    time.sleep(1/60)\n\n    def get_queue_most_recent(self):\n        # Make sure we at least get something\n        item = self.vid_queue.get(block=True)\n        while True:\n            try:\n                item = self.vid_queue.get(block=True, timeout=0.1)\n            except queue.Empty:\n                break\n        return item\n\n\ndef get_port_range(start_port, n_ports, random_stagger=False):\n    # If multiple runs try and call this function at the same time,\n    # the function could return the same port range.\n    # To guard against this, automatically offset the port range.\n    if random_stagger:\n        start_port += random.randint(0, 20) * n_ports\n\n    free_range_found = False\n    while not free_range_found:\n        ports = []\n        for port_n in range(n_ports):\n            port = start_port + port_n\n            try:\n                s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                s.bind((\"127.0.0.1\", port))\n                ports.append(port)\n            except socket.error as e:\n                if e.errno == 98 or e.errno == 48:\n                    print(\"Warning: port {} already in use\".format(port))\n                    break\n                else:\n                    raise e\n            finally:\n                s.close()\n        if len(ports) < n_ports:\n            # The last port we tried was in use\n            # Try again, starting from the next port\n            start_port = port + 1\n        else:\n            free_range_found = True\n\n    return ports\n\n\ndef profile_memory(log_path, pid):\n    import memory_profiler\n    def profile():\n        with open(log_path, 'w') as f:\n            # timeout=99999 is necessary because for external processes,\n            # memory_usage otherwise defaults to only returning a single sample\n            # Note that even with interval=1, because memory_profiler only\n            # flushes every 50 lines, we still have to wait 50 seconds before\n            # updates.\n            memory_profiler.memory_usage(pid, stream=f,\n                                         timeout=99999, interval=1)\n    p = Process(target=profile, daemon=True)\n    p.start()\n    return p\n\n", "right_context": "\n\ndef make_env(env_id, seed=0):\n    if env_id in ['MovingDot-v0', 'MovingDotNoFrameskip-v0']:\n        import gym_moving_dot\n    env = gym.make(env_id)\n    env.seed(seed)\n    if env_id == 'EnduroNoFrameskip-v4':\n        from enduro_wrapper import EnduroWrapper\n        env = EnduroWrapper(env)\n    return wrap_deepmind(env)\n", "import_text": ["queue", "random", "socket", "time", "multiprocessing.Process", "gym", "numpy", "pyglet", "a2c.common.atari_wrappers.wrap_deepmind", "scipy.ndimage.zoom"], "prompt": "\"\"\"\nDescription: This function generates batches of data from a given dataset.\n\nArgs:\n    data (list): The dataset to be divided into batches.\n    batch_size (int): The size of each batch.\n    shuffle (bool, optional): If True, the data will be shuffled before batching. Defaults to False.\n\nReturns:\n    generator: A generator that yields batches of data.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["numpy.random.shuffle"], "project_create_time": "2018-01-03T15:54:29+00:00", "project_update_time": "2024-04-11T12:52:33+00:00", "file_create_time": "2018-01-03T16:05:30Z", "file_update_time": "2018-03-27T09:04:39Z", "function_update_time": "2018-03-14T18:50:17Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["numpy.random.shuffle"], "test_function": [{"file_path": "/learning-from-human-preferences-master/learning-from-human-preferences-master/utils_test.py", "class_name": "TestUtils", "function_name": "test_batch_iter_1", "code": "    def test_batch_iter_1(self):\n        l1 = list(range(16))\n        l2 = list(range(15))\n        l3 = list(range(13))\n        for l in [l1, l2, l3]:\n            for shuffle in [True, False]:\n                expected_data = l\n                actual_data = set()\n                expected_n_batches = ceil(len(l) / 4)\n                actual_n_batches = 0\n                for batch_n, x in enumerate(batch_iter(l,\n                                                       batch_size=4,\n                                                       shuffle=shuffle)):\n                    if batch_n == expected_n_batches - 1 and len(l) % 4 != 0:\n                        self.assertEqual(len(x), len(l) % 4)\n                    else:\n                        self.assertEqual(len(x), 4)\n                    self.assertEqual(len(actual_data.intersection(set(x))), 0)\n                    actual_data = actual_data.union(set(x))\n                    actual_n_batches += 1\n                self.assertEqual(actual_n_batches, expected_n_batches)\n                np.testing.assert_array_equal(list(actual_data), expected_data)"}, {"file_path": "/learning-from-human-preferences-master/learning-from-human-preferences-master/utils_test.py", "class_name": "TestUtils", "function_name": "test_batch_iter_2", "code": "    def test_batch_iter_2(self):\n        expected_data = list(range(16))\n        actual_data = []\n        for x in batch_iter(expected_data, batch_size=4, shuffle=True):\n            actual_data.extend(x)\n        self.assertEqual(len(actual_data), len(expected_data))\n        self.assertEqual(set(actual_data), set(expected_data))\n        with self.assertRaises(AssertionError):\n            np.testing.assert_array_equal(actual_data, expected_data)"}, {"file_path": "/learning-from-human-preferences-master/learning-from-human-preferences-master/utils_test.py", "class_name": "TestUtils", "function_name": "test_batch_iter_3", "code": "    def test_batch_iter_3(self):\n        data = list(range(16))\n        out1 = []\n        for x in batch_iter(data, batch_size=4, shuffle=True):\n            out1.extend(x)\n        out2 = []\n        for x in batch_iter(data, batch_size=4, shuffle=True):\n            out2.extend(x)\n        self.assertEqual(set(out1), set(out2))\n        with self.assertRaises(AssertionError):\n            np.testing.assert_array_equal(out1, out2)"}]}, {"git_group": "holzschu", "git_name": "python3_ios", "version": "v1.0", "language": "Python", "project_name": "python3_ios-v1.0.zip", "file_path": "/python3_ios-v1.0/python3_ios-1.0/site-packages/sympy/physics/secondquant.py", "file_name": "secondquant.py", "focal_class": null, "focal_name": "simplify_index_permutations", "focal_parameter": ["expr", "permutation_operators"], "solution": "def simplify_index_permutations(expr, permutation_operators):\n\n    def _get_indices(expr, ind):\n        \"\"\"\n        Collects indices recursively in predictable order.\n        \"\"\"\n        result = []\n        for arg in expr.args:\n            if arg in ind:\n                result.append(arg)\n            else:\n                if arg.args:\n                    result.extend(_get_indices(arg, ind))\n        return result\n\n    def _choose_one_to_keep(a, b, ind):\n        # we keep the one where indices in ind are in order ind[0] < ind[1]\n        return min(a, b, key=lambda x: default_sort_key(_get_indices(x, ind)))\n\n    expr = expr.expand()\n    if isinstance(expr, Add):\n        terms = set(expr.args)\n\n        for P in permutation_operators:\n            new_terms = set([])\n            on_hold = set([])\n            while terms:\n                term = terms.pop()\n                permuted = P.get_permuted(term)\n                if permuted in terms | on_hold:\n                    try:\n                        terms.remove(permuted)\n                    except KeyError:\n                        on_hold.remove(permuted)\n                    keep = _choose_one_to_keep(term, permuted, P.args)\n                    new_terms.add(P*keep)\n                else:\n\n                    # Some terms must get a second chance because the permuted\n                    # term may already have canonical dummy ordering.  Then\n                    # substitute_dummies() does nothing.  However, the other\n                    # term, if it exists, will be able to match with us.\n                    permuted1 = permuted\n                    permuted = substitute_dummies(permuted)\n                    if permuted1 == permuted:\n                        on_hold.add(term)\n                    elif permuted in terms | on_hold:\n                        try:\n                            terms.remove(permuted)\n                        except KeyError:\n                            on_hold.remove(permuted)\n                        keep = _choose_one_to_keep(term, permuted, P.args)\n                        new_terms.add(P*keep)\n                    else:\n                        new_terms.add(term)\n            terms = new_terms | on_hold\n        return Add(*terms)\n    return expr", "function_signature": "def simplify_index_permutations(expr, permutation_operators) :", "left_context": "\"\"\"\nSecond quantization operators and states for bosons.\n\nThis follow the formulation of Fetter and Welecka, \"Quantum Theory\nof Many-Particle Systems.\"\n\"\"\"\nfrom __future__ import print_function, division\n\nfrom collections import defaultdict\n\nfrom sympy import (Add, Basic, cacheit, Dummy, Expr, Function, I,\n                   KroneckerDelta, Mul, Pow, S, sqrt, Symbol, sympify, Tuple,\n                   zeros)\nfrom sympy.printing.str import StrPrinter\n\nfrom sympy.core.compatibility import range\nfrom sympy.utilities.iterables import has_dups\nfrom sympy.utilities import default_sort_key\n\n__all__ = [\n    'Dagger',\n    'KroneckerDelta',\n    'BosonicOperator',\n    'AnnihilateBoson',\n    'CreateBoson',\n    'AnnihilateFermion',\n    'CreateFermion',\n    'FockState',\n    'FockStateBra',\n    'FockStateKet',\n    'FockStateBosonKet',\n    'FockStateBosonBra',\n    'BBra',\n    'BKet',\n    'FBra',\n    'FKet',\n    'F',\n    'Fd',\n    'B',\n    'Bd',\n    'apply_operators',\n    'InnerProduct',\n    'BosonicBasis',\n    'VarBosonicBasis',\n    'FixedBosonicBasis',\n    'Commutator',\n    'matrix_rep',\n    'contraction',\n    'wicks',\n    'NO',\n    'evaluate_deltas',\n    'AntiSymmetricTensor',\n    'substitute_dummies',\n    'PermutationOperator',\n    'simplify_index_permutations',\n]\n\n\nclass SecondQuantizationError(Exception):\n    pass\n\n\nclass AppliesOnlyToSymbolicIndex(SecondQuantizationError):\n    pass\n\n\nclass ContractionAppliesOnlyToFermions(SecondQuantizationError):\n    pass\n\n\nclass ViolationOfPauliPrinciple(SecondQuantizationError):\n    pass\n\n\nclass SubstitutionOfAmbigousOperatorFailed(SecondQuantizationError):\n    pass\n\n\nclass WicksTheoremDoesNotApply(SecondQuantizationError):\n    pass\n\n\nclass Dagger(Expr):\n    \"\"\"\n    Hermitian conjugate of creation/annihilation operators.\n\n    Examples\n    ========\n\n    >>> from sympy import I\n    >>> from sympy.physics.secondquant import Dagger, B, Bd\n    >>> Dagger(2*I)\n    -2*I\n    >>> Dagger(B(0))\n    CreateBoson(0)\n    >>> Dagger(Bd(0))\n    AnnihilateBoson(0)\n\n    \"\"\"\n\n    def __new__(cls, arg):\n        arg = sympify(arg)\n        r = cls.eval(arg)\n        if isinstance(r, Basic):\n            return r\n        obj = Basic.__new__(cls, arg)\n        return obj\n\n    @classmethod\n    def eval(cls, arg):\n        \"\"\"\n        Evaluates the Dagger instance.\n\n        Examples\n        ========\n\n        >>> from sympy import I\n        >>> from sympy.physics.secondquant import Dagger, B, Bd\n        >>> Dagger(2*I)\n        -2*I\n        >>> Dagger(B(0))\n        CreateBoson(0)\n        >>> Dagger(Bd(0))\n        AnnihilateBoson(0)\n\n        The eval() method is called automatically.\n\n        \"\"\"\n        try:\n            d = arg._dagger_()\n        except AttributeError:\n            if isinstance(arg, Basic):\n                if arg.is_Add:\n                    return Add(*tuple(map(Dagger, arg.args)))\n                if arg.is_Mul:\n                    return Mul(*tuple(map(Dagger, reversed(arg.args))))\n                if arg.is_Number:\n                    return arg\n                if arg.is_Pow:\n                    return Pow(Dagger(arg.args[0]), arg.args[1])\n                if arg == I:\n                    return -arg\n            else:\n                return None\n        else:\n            return d\n\n    def _dagger_(self):\n        return self.args[0]\n\n\nclass TensorSymbol(Expr):\n\n    is_commutative = True\n\n\nclass AntiSymmetricTensor(TensorSymbol):\n    \"\"\"Stores upper and lower indices in separate Tuple's.\n\n    Each group of indices is assumed to be antisymmetric.\n\n    Examples\n    ========\n\n    >>> from sympy import symbols\n    >>> from sympy.physics.secondquant import AntiSymmetricTensor\n    >>> i, j = symbols('i j', below_fermi=True)\n    >>> a, b = symbols('a b', above_fermi=True)\n    >>> AntiSymmetricTensor('v', (a, i), (b, j))\n    AntiSymmetricTensor(v, (a, i), (b, j))\n    >>> AntiSymmetricTensor('v', (i, a), (b, j))\n    -AntiSymmetricTensor(v, (a, i), (b, j))\n\n    As you can see, the indices are automatically sorted to a canonical form.\n\n    \"\"\"\n\n    def __new__(cls, symbol, upper, lower):\n\n        try:\n            upper, signu = _sort_anticommuting_fermions(\n                upper, key=cls._sortkey)\n            lower, signl = _sort_anticommuting_fermions(\n                lower, key=cls._sortkey)\n\n        except ViolationOfPauliPrinciple:\n            return S.Zero\n\n        symbol = sympify(symbol)\n        upper = Tuple(*upper)\n        lower = Tuple(*lower)\n\n        if (signu + signl) % 2:\n            return -TensorSymbol.__new__(cls, symbol, upper, lower)\n        else:\n\n            return TensorSymbol.__new__(cls, symbol, upper, lower)\n\n    @classmethod\n    def _sortkey(cls, index):\n        \"\"\"Key for sorting of indices.\n\n        particle < hole < general\n\n        FIXME: This is a bottle-neck, can we do it faster?\n        \"\"\"\n        h = hash(index)\n        label = str(index)\n        if isinstance(index, Dummy):\n            if index.assumptions0.get('above_fermi'):\n                return (20, label, h)\n            elif index.assumptions0.get('below_fermi'):\n                return (21, label, h)\n            else:\n                return (22, label, h)\n\n        if index.assumptions0.get('above_fermi'):\n            return (10, label, h)\n        elif index.assumptions0.get('below_fermi'):\n            return (11, label, h)\n        else:\n            return (12, label, h)\n\n    def _latex(self, printer):\n        return \"%s^{%s}_{%s}\" % (\n            self.symbol,\n            \"\".join([ i.name for i in self.args[1]]),\n            \"\".join([ i.name for i in self.args[2]])\n        )\n\n    @property\n    def symbol(self):\n        \"\"\"\n        Returns the symbol of the tensor.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols\n        >>> from sympy.physics.secondquant import AntiSymmetricTensor\n        >>> i, j = symbols('i,j', below_fermi=True)\n        >>> a, b = symbols('a,b', above_fermi=True)\n        >>> AntiSymmetricTensor('v', (a, i), (b, j))\n        AntiSymmetricTensor(v, (a, i), (b, j))\n        >>> AntiSymmetricTensor('v', (a, i), (b, j)).symbol\n        v\n\n        \"\"\"\n        return self.args[0]\n\n    @property\n    def upper(self):\n        \"\"\"\n        Returns the upper indices.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols\n        >>> from sympy.physics.secondquant import AntiSymmetricTensor\n        >>> i, j = symbols('i,j', below_fermi=True)\n        >>> a, b = symbols('a,b', above_fermi=True)\n        >>> AntiSymmetricTensor('v', (a, i), (b, j))\n        AntiSymmetricTensor(v, (a, i), (b, j))\n        >>> AntiSymmetricTensor('v', (a, i), (b, j)).upper\n        (a, i)\n\n\n        \"\"\"\n        return self.args[1]\n\n    @property\n    def lower(self):\n        \"\"\"\n        Returns the lower indices.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols\n        >>> from sympy.physics.secondquant import AntiSymmetricTensor\n        >>> i, j = symbols('i,j', below_fermi=True)\n        >>> a, b = symbols('a,b', above_fermi=True)\n        >>> AntiSymmetricTensor('v', (a, i), (b, j))\n        AntiSymmetricTensor(v, (a, i), (b, j))\n        >>> AntiSymmetricTensor('v', (a, i), (b, j)).lower\n        (b, j)\n\n        \"\"\"\n        return self.args[2]\n\n    def __str__(self):\n        return \"%s(%s,%s)\" % self.args\n\n    def doit(self, **kw_args):\n        \"\"\"\n        Returns self.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols\n        >>> from sympy.physics.secondquant import AntiSymmetricTensor\n        >>> i, j = symbols('i,j', below_fermi=True)\n        >>> a, b = symbols('a,b', above_fermi=True)\n        >>> AntiSymmetricTensor('v', (a, i), (b, j)).doit()\n        AntiSymmetricTensor(v, (a, i), (b, j))\n        \"\"\"\n        return self\n\n\nclass SqOperator(Expr):\n    \"\"\"\n    Base class for Second Quantization operators.\n    \"\"\"\n\n    op_symbol = 'sq'\n\n    is_commutative = False\n\n    def __new__(cls, k):\n        obj = Basic.__new__(cls, sympify(k))\n        return obj\n\n    @property\n    def state(self):\n        \"\"\"\n        Returns the state index related to this operator.\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F, Fd, B, Bd\n        >>> p = Symbol('p')\n        >>> F(p).state\n        p\n        >>> Fd(p).state\n        p\n        >>> B(p).state\n        p\n        >>> Bd(p).state\n        p\n\n        \"\"\"\n        return self.args[0]\n\n    @property\n    def is_symbolic(self):\n        \"\"\"\n        Returns True if the state is a symbol (as opposed to a number).\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> p = Symbol('p')\n        >>> F(p).is_symbolic\n        True\n        >>> F(1).is_symbolic\n        False\n\n        \"\"\"\n        if self.state.is_Integer:\n            return False\n        else:\n            return True\n\n    def doit(self, **kw_args):\n        \"\"\"\n        FIXME: hack to prevent crash further up...\n        \"\"\"\n        return self\n\n    def __repr__(self):\n        return NotImplemented\n\n    def __str__(self):\n        return \"%s(%r)\" % (self.op_symbol, self.state)\n\n    def apply_operator(self, state):\n        \"\"\"\n        Applies an operator to itself.\n        \"\"\"\n        raise NotImplementedError('implement apply_operator in a subclass')\n\n\nclass BosonicOperator(SqOperator):\n    pass\n\n\nclass Annihilator(SqOperator):\n    pass\n\n\nclass Creator(SqOperator):\n    pass\n\n\nclass AnnihilateBoson(BosonicOperator, Annihilator):\n    \"\"\"\n    Bosonic annihilation operator.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import B\n    >>> from sympy.abc import x\n    >>> B(x)\n    AnnihilateBoson(x)\n    \"\"\"\n\n    op_symbol = 'b'\n\n    def _dagger_(self):\n        return CreateBoson(self.state)\n\n    def apply_operator(self, state):\n        \"\"\"\n        Apply state to self if self is not symbolic and state is a FockStateKet, else\n        multiply self by state.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import B, BKet\n        >>> from sympy.abc import x, y, n\n        >>> B(x).apply_operator(y)\n        y*AnnihilateBoson(x)\n        >>> B(0).apply_operator(BKet((n,)))\n        sqrt(n)*FockStateBosonKet((n - 1,))\n\n        \"\"\"\n        if not self.is_symbolic and isinstance(state, FockStateKet):\n            element = self.state\n            amp = sqrt(state[element])\n            return amp*state.down(element)\n        else:\n            return Mul(self, state)\n\n    def __repr__(self):\n        return \"AnnihilateBoson(%s)\" % self.state\n\n    def _latex(self, printer):\n        return \"b_{%s}\" % self.state.name\n\n\nclass CreateBoson(BosonicOperator, Creator):\n    \"\"\"\n    Bosonic creation operator.\n    \"\"\"\n\n    op_symbol = 'b+'\n\n    def _dagger_(self):\n        return AnnihilateBoson(self.state)\n\n    def apply_operator(self, state):\n        \"\"\"\n        Apply state to self if self is not symbolic and state is a FockStateKet, else\n        multiply self by state.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import B, Dagger, BKet\n        >>> from sympy.abc import x, y, n\n        >>> Dagger(B(x)).apply_operator(y)\n        y*CreateBoson(x)\n        >>> B(0).apply_operator(BKet((n,)))\n        sqrt(n)*FockStateBosonKet((n - 1,))\n        \"\"\"\n        if not self.is_symbolic and isinstance(state, FockStateKet):\n            element = self.state\n            amp = sqrt(state[element] + 1)\n            return amp*state.up(element)\n        else:\n            return Mul(self, state)\n\n    def __repr__(self):\n        return \"CreateBoson(%s)\" % self.state\n\n    def _latex(self, printer):\n        return \"b^\\\\dagger_{%s}\" % self.state.name\n\nB = AnnihilateBoson\nBd = CreateBoson\n\n\nclass FermionicOperator(SqOperator):\n\n    @property\n    def is_restricted(self):\n        \"\"\"\n        Is this FermionicOperator restricted with respect to fermi level?\n\n        Return values:\n        1  : restricted to orbits above fermi\n        0  : no restriction\n        -1 : restricted to orbits below fermi\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F, Fd\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_restricted\n        1\n        >>> Fd(a).is_restricted\n        1\n        >>> F(i).is_restricted\n        -1\n        >>> Fd(i).is_restricted\n        -1\n        >>> F(p).is_restricted\n        0\n        >>> Fd(p).is_restricted\n        0\n\n        \"\"\"\n        ass = self.args[0].assumptions0\n        if ass.get(\"below_fermi\"):\n            return -1\n        if ass.get(\"above_fermi\"):\n            return 1\n        return 0\n\n    @property\n    def is_above_fermi(self):\n        \"\"\"\n        Does the index of this FermionicOperator allow values above fermi?\n\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_above_fermi\n        True\n        >>> F(i).is_above_fermi\n        False\n        >>> F(p).is_above_fermi\n        True\n\n        The same applies to creation operators Fd\n\n        \"\"\"\n        return not self.args[0].assumptions0.get(\"below_fermi\")\n\n    @property\n    def is_below_fermi(self):\n        \"\"\"\n        Does the index of this FermionicOperator allow values below fermi?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_below_fermi\n        False\n        >>> F(i).is_below_fermi\n        True\n        >>> F(p).is_below_fermi\n        True\n\n        The same applies to creation operators Fd\n\n        \"\"\"\n        return not self.args[0].assumptions0.get(\"above_fermi\")\n\n    @property\n    def is_only_below_fermi(self):\n        \"\"\"\n        Is the index of this FermionicOperator restricted to values below fermi?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_only_below_fermi\n        False\n        >>> F(i).is_only_below_fermi\n        True\n        >>> F(p).is_only_below_fermi\n        False\n\n        The same applies to creation operators Fd\n        \"\"\"\n        return self.is_below_fermi and not self.is_above_fermi\n\n    @property\n    def is_only_above_fermi(self):\n        \"\"\"\n        Is the index of this FermionicOperator restricted to values above fermi?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_only_above_fermi\n        True\n        >>> F(i).is_only_above_fermi\n        False\n        >>> F(p).is_only_above_fermi\n        False\n\n        The same applies to creation operators Fd\n        \"\"\"\n        return self.is_above_fermi and not self.is_below_fermi\n\n    def _sortkey(self):\n        h = hash(self)\n        label = str(self.args[0])\n\n        if self.is_only_q_creator:\n            return 1, label, h\n        if self.is_only_q_annihilator:\n            return 4, label, h\n        if isinstance(self, Annihilator):\n            return 3, label, h\n        if isinstance(self, Creator):\n            return 2, label, h\n\n\nclass AnnihilateFermion(FermionicOperator, Annihilator):\n    \"\"\"\n    Fermionic annihilation operator.\n    \"\"\"\n\n    op_symbol = 'f'\n\n    def _dagger_(self):\n        return CreateFermion(self.state)\n\n    def apply_operator(self, state):\n        \"\"\"\n        Apply state to self if self is not symbolic and state is a FockStateKet, else\n        multiply self by state.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import B, Dagger, BKet\n        >>> from sympy.abc import x, y, n\n        >>> Dagger(B(x)).apply_operator(y)\n        y*CreateBoson(x)\n        >>> B(0).apply_operator(BKet((n,)))\n        sqrt(n)*FockStateBosonKet((n - 1,))\n        \"\"\"\n        if isinstance(state, FockStateFermionKet):\n            element = self.state\n            return state.down(element)\n\n        elif isinstance(state, Mul):\n            c_part, nc_part = state.args_cnc()\n            if isinstance(nc_part[0], FockStateFermionKet):\n                element = self.state\n                return Mul(*(c_part + [nc_part[0].down(element)] + nc_part[1:]))\n            else:\n                return Mul(self, state)\n\n        else:\n            return Mul(self, state)\n\n    @property\n    def is_q_creator(self):\n        \"\"\"\n        Can we create a quasi-particle?  (create hole or create particle)\n        If so, would that be above or below the fermi surface?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_q_creator\n        0\n        >>> F(i).is_q_creator\n        -1\n        >>> F(p).is_q_creator\n        -1\n\n        \"\"\"\n        if self.is_below_fermi:\n            return -1\n        return 0\n\n    @property\n    def is_q_annihilator(self):\n        \"\"\"\n        Can we destroy a quasi-particle?  (annihilate hole or annihilate particle)\n        If so, would that be above or below the fermi surface?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=1)\n        >>> i = Symbol('i', below_fermi=1)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_q_annihilator\n        1\n        >>> F(i).is_q_annihilator\n        0\n        >>> F(p).is_q_annihilator\n        1\n\n        \"\"\"\n        if self.is_above_fermi:\n            return 1\n        return 0\n\n    @property\n    def is_only_q_creator(self):\n        \"\"\"\n        Always create a quasi-particle?  (create hole or create particle)\n\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_only_q_creator\n        False\n        >>> F(i).is_only_q_creator\n        True\n        >>> F(p).is_only_q_creator\n        False\n\n        \"\"\"\n        return self.is_only_below_fermi\n\n    @property\n    def is_only_q_annihilator(self):\n        \"\"\"\n        Always destroy a quasi-particle?  (annihilate hole or annihilate particle)\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import F\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> F(a).is_only_q_annihilator\n        True\n        >>> F(i).is_only_q_annihilator\n        False\n        >>> F(p).is_only_q_annihilator\n        False\n\n        \"\"\"\n        return self.is_only_above_fermi\n\n    def __repr__(self):\n        return \"AnnihilateFermion(%s)\" % self.state\n\n    def _latex(self, printer):\n        return \"a_{%s}\" % self.state.name\n\n\nclass CreateFermion(FermionicOperator, Creator):\n    \"\"\"\n    Fermionic creation operator.\n    \"\"\"\n\n    op_symbol = 'f+'\n\n    def _dagger_(self):\n        return AnnihilateFermion(self.state)\n\n    def apply_operator(self, state):\n        \"\"\"\n        Apply state to self if self is not symbolic and state is a FockStateKet, else\n        multiply self by state.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import B, Dagger, BKet\n        >>> from sympy.abc import x, y, n\n        >>> Dagger(B(x)).apply_operator(y)\n        y*CreateBoson(x)\n        >>> B(0).apply_operator(BKet((n,)))\n        sqrt(n)*FockStateBosonKet((n - 1,))\n        \"\"\"\n        if isinstance(state, FockStateFermionKet):\n            element = self.state\n            return state.up(element)\n\n        elif isinstance(state, Mul):\n            c_part, nc_part = state.args_cnc()\n            if isinstance(nc_part[0], FockStateFermionKet):\n                element = self.state\n                return Mul(*(c_part + [nc_part[0].up(element)] + nc_part[1:]))\n\n        return Mul(self, state)\n\n    @property\n    def is_q_creator(self):\n        \"\"\"\n        Can we create a quasi-particle?  (create hole or create particle)\n        If so, would that be above or below the fermi surface?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import Fd\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> Fd(a).is_q_creator\n        1\n        >>> Fd(i).is_q_creator\n        0\n        >>> Fd(p).is_q_creator\n        1\n\n        \"\"\"\n        if self.is_above_fermi:\n            return 1\n        return 0\n\n    @property\n    def is_q_annihilator(self):\n        \"\"\"\n        Can we destroy a quasi-particle?  (annihilate hole or annihilate particle)\n        If so, would that be above or below the fermi surface?\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import Fd\n        >>> a = Symbol('a', above_fermi=1)\n        >>> i = Symbol('i', below_fermi=1)\n        >>> p = Symbol('p')\n\n        >>> Fd(a).is_q_annihilator\n        0\n        >>> Fd(i).is_q_annihilator\n        -1\n        >>> Fd(p).is_q_annihilator\n        -1\n\n        \"\"\"\n        if self.is_below_fermi:\n            return -1\n        return 0\n\n    @property\n    def is_only_q_creator(self):\n        \"\"\"\n        Always create a quasi-particle?  (create hole or create particle)\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import Fd\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> Fd(a).is_only_q_creator\n        True\n        >>> Fd(i).is_only_q_creator\n        False\n        >>> Fd(p).is_only_q_creator\n        False\n\n        \"\"\"\n        return self.is_only_above_fermi\n\n    @property\n    def is_only_q_annihilator(self):\n        \"\"\"\n        Always destroy a quasi-particle?  (annihilate hole or annihilate particle)\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import Fd\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> Fd(a).is_only_q_annihilator\n        False\n        >>> Fd(i).is_only_q_annihilator\n        True\n        >>> Fd(p).is_only_q_annihilator\n        False\n\n        \"\"\"\n        return self.is_only_below_fermi\n\n    def __repr__(self):\n        return \"CreateFermion(%s)\" % self.state\n\n    def _latex(self, printer):\n        return \"a^\\\\dagger_{%s}\" % self.state.name\n\nFd = CreateFermion\nF = AnnihilateFermion\n\n\nclass FockState(Expr):\n    \"\"\"\n    Many particle Fock state with a sequence of occupation numbers.\n\n    Anywhere you can have a FockState, you can also have S.Zero.\n    All code must check for this!\n\n    Base class to represent FockStates.\n    \"\"\"\n    is_commutative = False\n\n    def __new__(cls, occupations):\n        \"\"\"\n        occupations is a list with two possible meanings:\n\n        - For bosons it is a list of occupation numbers.\n          Element i is the number of particles in state i.\n\n        - For fermions it is a list of occupied orbits.\n          Element 0 is the state that was occupied first, element i\n          is the i'th occupied state.\n        \"\"\"\n        occupations = list(map(sympify, occupations))\n        obj = Basic.__new__(cls, Tuple(*occupations))\n        return obj\n\n    def __getitem__(self, i):\n        i = int(i)\n        return self.args[0][i]\n\n    def __repr__(self):\n        return (\"FockState(%r)\") % (self.args)\n\n    def __str__(self):\n        return \"%s%r%s\" % (self.lbracket, self._labels(), self.rbracket)\n\n    def _labels(self):\n        return self.args[0]\n\n    def __len__(self):\n        return len(self.args[0])\n\n\nclass BosonState(FockState):\n    \"\"\"\n    Base class for FockStateBoson(Ket/Bra).\n    \"\"\"\n\n    def up(self, i):\n        \"\"\"\n        Performs the action of a creation operator.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import BBra\n        >>> b = BBra([1, 2])\n        >>> b\n        FockStateBosonBra((1, 2))\n        >>> b.up(1)\n        FockStateBosonBra((1, 3))\n        \"\"\"\n        i = int(i)\n        new_occs = list(self.args[0])\n        new_occs[i] = new_occs[i] + S.One\n        return self.__class__(new_occs)\n\n    def down(self, i):\n        \"\"\"\n        Performs the action of an annihilation operator.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import BBra\n        >>> b = BBra([1, 2])\n        >>> b\n        FockStateBosonBra((1, 2))\n        >>> b.down(1)\n        FockStateBosonBra((1, 1))\n        \"\"\"\n        i = int(i)\n        new_occs = list(self.args[0])\n        if new_occs[i] == S.Zero:\n            return S.Zero\n        else:\n            new_occs[i] = new_occs[i] - S.One\n            return self.__class__(new_occs)\n\n\nclass FermionState(FockState):\n    \"\"\"\n    Base class for FockStateFermion(Ket/Bra).\n    \"\"\"\n\n    fermi_level = 0\n\n    def __new__(cls, occupations, fermi_level=0):\n        occupations = list(map(sympify, occupations))\n        if len(occupations) > 1:\n            try:\n                (occupations, sign) = _sort_anticommuting_fermions(\n                    occupations, key=hash)\n            except ViolationOfPauliPrinciple:\n                return S.Zero\n        else:\n            sign = 0\n\n        cls.fermi_level = fermi_level\n\n        if cls._count_holes(occupations) > fermi_level:\n            return S.Zero\n\n        if sign % 2:\n            return S.NegativeOne*FockState.__new__(cls, occupations)\n        else:\n            return FockState.__new__(cls, occupations)\n\n    def up(self, i):\n        \"\"\"\n        Performs the action of a creation operator.\n\n        If below fermi we try to remove a hole,\n        if above fermi we try to create a particle.\n\n        if general index p we return Kronecker(p,i)*self\n        where i is a new symbol with restriction above or below.\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import FKet\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        >>> FKet([]).up(a)\n        FockStateFermionKet((a,))\n\n        A creator acting on vacuum below fermi vanishes\n\n        >>> FKet([]).up(i)\n        0\n\n\n        \"\"\"\n        present = i in self.args[0]\n\n        if self._only_above_fermi(i):\n            if present:\n                return S.Zero\n            else:\n                return self._add_orbit(i)\n        elif self._only_below_fermi(i):\n            if present:\n                return self._remove_orbit(i)\n            else:\n                return S.Zero\n        else:\n            if present:\n                hole = Dummy(\"i\", below_fermi=True)\n                return KroneckerDelta(i, hole)*self._remove_orbit(i)\n            else:\n                particle = Dummy(\"a\", above_fermi=True)\n                return KroneckerDelta(i, particle)*self._add_orbit(i)\n\n    def down(self, i):\n        \"\"\"\n        Performs the action of an annihilation operator.\n\n        If below fermi we try to create a hole,\n        if above fermi we try to remove a particle.\n\n        if general index p we return Kronecker(p,i)*self\n        where i is a new symbol with restriction above or below.\n\n        >>> from sympy import Symbol\n        >>> from sympy.physics.secondquant import FKet\n        >>> a = Symbol('a', above_fermi=True)\n        >>> i = Symbol('i', below_fermi=True)\n        >>> p = Symbol('p')\n\n        An annihilator acting on vacuum above fermi vanishes\n\n        >>> FKet([]).down(a)\n        0\n\n        Also below fermi, it vanishes, unless we specify a fermi level > 0\n\n        >>> FKet([]).down(i)\n        0\n        >>> FKet([],4).down(i)\n        FockStateFermionKet((i,))\n\n        \"\"\"\n        present = i in self.args[0]\n\n        if self._only_above_fermi(i):\n            if present:\n                return self._remove_orbit(i)\n            else:\n                return S.Zero\n\n        elif self._only_below_fermi(i):\n            if present:\n                return S.Zero\n            else:\n                return self._add_orbit(i)\n        else:\n            if present:\n                hole = Dummy(\"i\", below_fermi=True)\n                return KroneckerDelta(i, hole)*self._add_orbit(i)\n            else:\n                particle = Dummy(\"a\", above_fermi=True)\n                return KroneckerDelta(i, particle)*self._remove_orbit(i)\n\n    @classmethod\n    def _only_below_fermi(cls, i):\n        \"\"\"\n        Tests if given orbit is only below fermi surface.\n\n        If nothing can be concluded we return a conservative False.\n        \"\"\"\n        if i.is_number:\n            return i <= cls.fermi_level\n        if i.assumptions0.get('below_fermi'):\n            return True\n        return False\n\n    @classmethod\n    def _only_above_fermi(cls, i):\n        \"\"\"\n        Tests if given orbit is only above fermi surface.\n\n        If fermi level has not been set we return True.\n        If nothing can be concluded we return a conservative False.\n        \"\"\"\n        if i.is_number:\n            return i > cls.fermi_level\n        if i.assumptions0.get('above_fermi'):\n            return True\n        return not cls.fermi_level\n\n    def _remove_orbit(self, i):\n        \"\"\"\n        Removes particle/fills hole in orbit i. No input tests performed here.\n        \"\"\"\n        new_occs = list(self.args[0])\n        pos = new_occs.index(i)\n        del new_occs[pos]\n        if (pos) % 2:\n            return S.NegativeOne*self.__class__(new_occs, self.fermi_level)\n        else:\n            return self.__class__(new_occs, self.fermi_level)\n\n    def _add_orbit(self, i):\n        \"\"\"\n        Adds particle/creates hole in orbit i. No input tests performed here.\n        \"\"\"\n        return self.__class__((i,) + self.args[0], self.fermi_level)\n\n    @classmethod\n    def _count_holes(cls, list):\n        \"\"\"\n        returns number of identified hole states in list.\n        \"\"\"\n        return len([i for i in list if cls._only_below_fermi(i)])\n\n    def _negate_holes(self, list):\n        return tuple([-i if i <= self.fermi_level else i for i in list])\n\n    def __repr__(self):\n        if self.fermi_level:\n            return \"FockStateKet(%r, fermi_level=%s)\" % (self.args[0], self.fermi_level)\n        else:\n            return \"FockStateKet(%r)\" % (self.args[0],)\n\n    def _labels(self):\n        return self._negate_holes(self.args[0])\n\n\nclass FockStateKet(FockState):\n    \"\"\"\n    Representation of a ket.\n    \"\"\"\n    lbracket = '|'\n    rbracket = '>'\n\n\nclass FockStateBra(FockState):\n    \"\"\"\n    Representation of a bra.\n    \"\"\"\n    lbracket = '<'\n    rbracket = '|'\n\n    def __mul__(self, other):\n        if isinstance(other, FockStateKet):\n            return InnerProduct(self, other)\n        else:\n            return Expr.__mul__(self, other)\n\n\nclass FockStateBosonKet(BosonState, FockStateKet):\n    \"\"\"\n    Many particle Fock state with a sequence of occupation numbers.\n\n    Occupation numbers can be any integer >= 0.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import BKet\n    >>> BKet([1, 2])\n    FockStateBosonKet((1, 2))\n    \"\"\"\n    def _dagger_(self):\n        return FockStateBosonBra(*self.args)\n\n\nclass FockStateBosonBra(BosonState, FockStateBra):\n    \"\"\"\n    Describes a collection of BosonBra particles.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import BBra\n    >>> BBra([1, 2])\n    FockStateBosonBra((1, 2))\n    \"\"\"\n    def _dagger_(self):\n        return FockStateBosonKet(*self.args)\n\n\nclass FockStateFermionKet(FermionState, FockStateKet):\n    \"\"\"\n    Many-particle Fock state with a sequence of occupied orbits.\n\n    Each state can only have one particle, so we choose to store a list of\n    occupied orbits rather than a tuple with occupation numbers (zeros and ones).\n\n    states below fermi level are holes, and are represented by negative labels\n    in the occupation list.\n\n    For symbolic state labels, the fermi_level caps the number of allowed hole-\n    states.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import FKet\n    >>> FKet([1, 2]) #doctest: +SKIP\n    FockStateFermionKet((1, 2))\n    \"\"\"\n    def _dagger_(self):\n        return FockStateFermionBra(*self.args)\n\n\nclass FockStateFermionBra(FermionState, FockStateBra):\n    \"\"\"\n    See Also\n    ========\n\n    FockStateFermionKet\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import FBra\n    >>> FBra([1, 2]) #doctest: +SKIP\n    FockStateFermionBra((1, 2))\n    \"\"\"\n    def _dagger_(self):\n        return FockStateFermionKet(*self.args)\n\nBBra = FockStateBosonBra\nBKet = FockStateBosonKet\nFBra = FockStateFermionBra\nFKet = FockStateFermionKet\n\n\ndef _apply_Mul(m):\n    \"\"\"\n    Take a Mul instance with operators and apply them to states.\n\n    This method applies all operators with integer state labels\n    to the actual states.  For symbolic state labels, nothing is done.\n    When inner products of FockStates are encountered (like <a|b>),\n    they are converted to instances of InnerProduct.\n\n    This does not currently work on double inner products like,\n    <a|b><c|d>.\n\n    If the argument is not a Mul, it is simply returned as is.\n    \"\"\"\n    if not isinstance(m, Mul):\n        return m\n    c_part, nc_part = m.args_cnc()\n    n_nc = len(nc_part)\n    if n_nc == 0 or n_nc == 1:\n        return m\n    else:\n        last = nc_part[-1]\n        next_to_last = nc_part[-2]\n        if isinstance(last, FockStateKet):\n            if isinstance(next_to_last, SqOperator):\n                if next_to_last.is_symbolic:\n                    return m\n                else:\n                    result = next_to_last.apply_operator(last)\n                    if result == 0:\n                        return S.Zero\n                    else:\n                        return _apply_Mul(Mul(*(c_part + nc_part[:-2] + [result])))\n            elif isinstance(next_to_last, Pow):\n                if isinstance(next_to_last.base, SqOperator) and \\\n                        next_to_last.exp.is_Integer:\n                    if next_to_last.base.is_symbolic:\n                        return m\n                    else:\n                        result = last\n                        for i in range(next_to_last.exp):\n                            result = next_to_last.base.apply_operator(result)\n                            if result == 0:\n                                break\n                        if result == 0:\n                            return S.Zero\n                        else:\n                            return _apply_Mul(Mul(*(c_part + nc_part[:-2] + [result])))\n                else:\n                    return m\n            elif isinstance(next_to_last, FockStateBra):\n                result = InnerProduct(next_to_last, last)\n                if result == 0:\n                    return S.Zero\n                else:\n                    return _apply_Mul(Mul(*(c_part + nc_part[:-2] + [result])))\n            else:\n                return m\n        else:\n            return m\n\n\ndef apply_operators(e):\n    \"\"\"\n    Take a sympy expression with operators and states and apply the operators.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import apply_operators\n    >>> from sympy import sympify\n    >>> apply_operators(sympify(3)+4)\n    7\n    \"\"\"\n    e = e.expand()\n    muls = e.atoms(Mul)\n    subs_list = [(m, _apply_Mul(m)) for m in iter(muls)]\n    return e.subs(subs_list)\n\n\nclass InnerProduct(Basic):\n    \"\"\"\n    An unevaluated inner product between a bra and ket.\n\n    Currently this class just reduces things to a product of\n    Kronecker Deltas.  In the future, we could introduce abstract\n    states like ``|a>`` and ``|b>``, and leave the inner product unevaluated as\n    ``<a|b>``.\n\n    \"\"\"\n    is_commutative = True\n\n    def __new__(cls, bra, ket):\n        if not isinstance(bra, FockStateBra):\n            raise TypeError(\"must be a bra\")\n        if not isinstance(ket, FockStateKet):\n            raise TypeError(\"must be a key\")\n        return cls.eval(bra, ket)\n\n    @classmethod\n    def eval(cls, bra, ket):\n        result = S.One\n        for i, j in zip(bra.args[0], ket.args[0]):\n            result *= KroneckerDelta(i, j)\n            if result == 0:\n                break\n        return result\n\n    @property\n    def bra(self):\n        \"\"\"Returns the bra part of the state\"\"\"\n        return self.args[0]\n\n    @property\n    def ket(self):\n        \"\"\"Returns the ket part of the state\"\"\"\n        return self.args[1]\n\n    def __repr__(self):\n        sbra = repr(self.bra)\n        sket = repr(self.ket)\n        return \"%s|%s\" % (sbra[:-1], sket[1:])\n\n    def __str__(self):\n        return self.__repr__()\n\n\ndef matrix_rep(op, basis):\n    \"\"\"\n    Find the representation of an operator in a basis.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import VarBosonicBasis, B, matrix_rep\n    >>> b = VarBosonicBasis(5)\n    >>> o = B(0)\n    >>> matrix_rep(o, b)\n    Matrix([\n    [0, 1,       0,       0, 0],\n    [0, 0, sqrt(2),       0, 0],\n    [0, 0,       0, sqrt(3), 0],\n    [0, 0,       0,       0, 2],\n    [0, 0,       0,       0, 0]])\n    \"\"\"\n    a = zeros(len(basis))\n    for i in range(len(basis)):\n        for j in range(len(basis)):\n            a[i, j] = apply_operators(Dagger(basis[i])*op*basis[j])\n    return a\n\n\nclass BosonicBasis(object):\n    \"\"\"\n    Base class for a basis set of bosonic Fock states.\n    \"\"\"\n    pass\n\n\nclass VarBosonicBasis(object):\n    \"\"\"\n    A single state, variable particle number basis set.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import VarBosonicBasis\n    >>> b = VarBosonicBasis(5)\n    >>> b\n    [FockState((0,)), FockState((1,)), FockState((2,)),\n     FockState((3,)), FockState((4,))]\n    \"\"\"\n\n    def __init__(self, n_max):\n        self.n_max = n_max\n        self._build_states()\n\n    def _build_states(self):\n        self.basis = []\n        for i in range(self.n_max):\n            self.basis.append(FockStateBosonKet([i]))\n        self.n_basis = len(self.basis)\n\n    def index(self, state):\n        \"\"\"\n        Returns the index of state in basis.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import VarBosonicBasis\n        >>> b = VarBosonicBasis(3)\n        >>> state = b.state(1)\n        >>> b\n        [FockState((0,)), FockState((1,)), FockState((2,))]\n        >>> state\n        FockStateBosonKet((1,))\n        >>> b.index(state)\n        1\n        \"\"\"\n        return self.basis.index(state)\n\n    def state(self, i):\n        \"\"\"\n        The state of a single basis.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import VarBosonicBasis\n        >>> b = VarBosonicBasis(5)\n        >>> b.state(3)\n        FockStateBosonKet((3,))\n        \"\"\"\n        return self.basis[i]\n\n    def __getitem__(self, i):\n        return self.state(i)\n\n    def __len__(self):\n        return len(self.basis)\n\n    def __repr__(self):\n        return repr(self.basis)\n\n\nclass FixedBosonicBasis(BosonicBasis):\n    \"\"\"\n    Fixed particle number basis set.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.secondquant import FixedBosonicBasis\n    >>> b = FixedBosonicBasis(2, 2)\n    >>> state = b.state(1)\n    >>> b\n    [FockState((2, 0)), FockState((1, 1)), FockState((0, 2))]\n    >>> state\n    FockStateBosonKet((1, 1))\n    >>> b.index(state)\n    1\n    \"\"\"\n    def __init__(self, n_particles, n_levels):\n        self.n_particles = n_particles\n        self.n_levels = n_levels\n        self._build_particle_locations()\n        self._build_states()\n\n    def _build_particle_locations(self):\n        tup = [\"i%i\" % i for i in range(self.n_particles)]\n        first_loop = \"for i0 in range(%i)\" % self.n_levels\n        other_loops = ''\n        for cur, prev in zip(tup[1:], tup):\n            temp = \"for %s in range(%s + 1) \" % (cur, prev)\n            other_loops = other_loops + temp\n        tup_string = \"(%s)\" % \", \".join(tup)\n        list_comp = \"[%s %s %s]\" % (tup_string, first_loop, other_loops)\n        result = eval(list_comp)\n        if self.n_particles == 1:\n            result = [(item,) for item in result]\n        self.particle_locations = result\n\n    def _build_states(self):\n        self.basis = []\n        for tuple_of_indices in self.particle_locations:\n            occ_numbers = self.n_levels*[0]\n            for level in tuple_of_indices:\n                occ_numbers[level] += 1\n            self.basis.append(FockStateBosonKet(occ_numbers))\n        self.n_basis = len(self.basis)\n\n    def index(self, state):\n        \"\"\"Returns the index of state in basis.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import FixedBosonicBasis\n        >>> b = FixedBosonicBasis(2, 3)\n        >>> b.index(b.state(3))\n        3\n        \"\"\"\n        return self.basis.index(state)\n\n    def state(self, i):\n        \"\"\"Returns the state that lies at index i of the basis\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import FixedBosonicBasis\n        >>> b = FixedBosonicBasis(2, 3)\n        >>> b.state(3)\n        FockStateBosonKet((1, 0, 1))\n        \"\"\"\n        return self.basis[i]\n\n    def __getitem__(self, i):\n        return self.state(i)\n\n    def __len__(self):\n        return len(self.basis)\n\n    def __repr__(self):\n        return repr(self.basis)\n\n\nclass Commutator(Function):\n    \"\"\"\n    The Commutator:  [A, B] = A*B - B*A\n\n    The arguments are ordered according to .__cmp__()\n\n    >>> from sympy import symbols\n    >>> from sympy.physics.secondquant import Commutator\n    >>> A, B = symbols('A,B', commutative=False)\n    >>> Commutator(B, A)\n    -Commutator(A, B)\n\n    Evaluate the commutator with .doit()\n\n    >>> comm = Commutator(A,B); comm\n    Commutator(A, B)\n    >>> comm.doit()\n    A*B - B*A\n\n\n    For two second quantization operators the commutator is evaluated\n    immediately:\n\n    >>> from sympy.physics.secondquant import Fd, F\n    >>> a = symbols('a', above_fermi=True)\n    >>> i = symbols('i', below_fermi=True)\n    >>> p,q = symbols('p,q')\n\n    >>> Commutator(Fd(a),Fd(i))\n    2*NO(CreateFermion(a)*CreateFermion(i))\n\n    But for more complicated expressions, the evaluation is triggered by\n    a call to .doit()\n\n    >>> comm = Commutator(Fd(p)*Fd(q),F(i)); comm\n    Commutator(CreateFermion(p)*CreateFermion(q), AnnihilateFermion(i))\n    >>> comm.doit(wicks=True)\n    -KroneckerDelta(i, p)*CreateFermion(q) +\n     KroneckerDelta(i, q)*CreateFermion(p)\n\n    \"\"\"\n\n    is_commutative = False\n\n    @classmethod\n    def eval(cls, a, b):\n        \"\"\"\n        The Commutator [A,B] is on canonical form if A < B.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import Commutator, F, Fd\n        >>> from sympy.abc import x\n        >>> c1 = Commutator(F(x), Fd(x))\n        >>> c2 = Commutator(Fd(x), F(x))\n        >>> Commutator.eval(c1, c2)\n        0\n        \"\"\"\n        if not (a and b):\n            return S.Zero\n        if a == b:\n            return S.Zero\n        if a.is_commutative or b.is_commutative:\n            return S.Zero\n\n        #\n        # [A+B,C]  ->  [A,C] + [B,C]\n        #\n        a = a.expand()\n        if isinstance(a, Add):\n            return Add(*[cls(term, b) for term in a.args])\n        b = b.expand()\n        if isinstance(b, Add):\n            return Add(*[cls(a, term) for term in b.args])\n\n        #\n        # [xA,yB]  ->  xy*[A,B]\n        #\n        ca, nca = a.args_cnc()\n        cb, ncb = b.args_cnc()\n        c_part = list(ca) + list(cb)\n        if c_part:\n            return Mul(Mul(*c_part), cls(Mul._from_args(nca), Mul._from_args(ncb)))\n\n        #\n        # single second quantization operators\n        #\n        if isinstance(a, BosonicOperator) and isinstance(b, BosonicOperator):\n            if isinstance(b, CreateBoson) and isinstance(a, AnnihilateBoson):\n                return KroneckerDelta(a.state, b.state)\n            if isinstance(a, CreateBoson) and isinstance(b, AnnihilateBoson):\n                return S.NegativeOne*KroneckerDelta(a.state, b.state)\n            else:\n                return S.Zero\n        if isinstance(a, FermionicOperator) and isinstance(b, FermionicOperator):\n            return wicks(a*b) - wicks(b*a)\n\n        #\n        # Canonical ordering of arguments\n        #\n        if a.sort_key() > b.sort_key():\n            return S.NegativeOne*cls(b, a)\n\n    def doit(self, **hints):\n        \"\"\"\n        Enables the computation of complex expressions.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import Commutator, F, Fd\n        >>> from sympy import symbols\n        >>> i, j = symbols('i,j', below_fermi=True)\n        >>> a, b = symbols('a,b', above_fermi=True)\n        >>> c = Commutator(Fd(a)*F(i),Fd(b)*F(j))\n        >>> c.doit(wicks=True)\n        0\n        \"\"\"\n        a = self.args[0]\n        b = self.args[1]\n\n        if hints.get(\"wicks\"):\n            a = a.doit(**hints)\n            b = b.doit(**hints)\n            try:\n                return wicks(a*b) - wicks(b*a)\n            except ContractionAppliesOnlyToFermions:\n                pass\n            except WicksTheoremDoesNotApply:\n                pass\n\n        return (a*b - b*a).doit(**hints)\n\n    def __repr__(self):\n        return \"Commutator(%s,%s)\" % (self.args[0], self.args[1])\n\n    def __str__(self):\n        return \"[%s,%s]\" % (self.args[0], self.args[1])\n\n    def _latex(self, printer):\n        return \"\\\\left[%s,%s\\\\right]\" % tuple([\n            printer._print(arg) for arg in self.args])\n\n\nclass NO(Expr):\n    \"\"\"\n    This Object is used to represent normal ordering brackets.\n\n    i.e.  {abcd}  sometimes written  :abcd:\n\n    Applying the function NO(arg) to an argument means that all operators in\n    the argument will be assumed to anticommute, and have vanishing\n    contractions.  This allows an immediate reordering to canonical form\n    upon object creation.\n\n    >>> from sympy import symbols\n    >>> from sympy.physics.secondquant import NO, F, Fd\n    >>> p,q = symbols('p,q')\n    >>> NO(Fd(p)*F(q))\n    NO(CreateFermion(p)*AnnihilateFermion(q))\n    >>> NO(F(q)*Fd(p))\n    -NO(CreateFermion(p)*AnnihilateFermion(q))\n\n\n    Note:\n    If you want to generate a normal ordered equivalent of an expression, you\n    should use the function wicks().  This class only indicates that all\n    operators inside the brackets anticommute, and have vanishing contractions.\n    Nothing more, nothing less.\n\n    \"\"\"\n    is_commutative = False\n\n    def __new__(cls, arg):\n        \"\"\"\n        Use anticommutation to get canonical form of operators.\n\n        Employ associativity of normal ordered product: {ab{cd}} = {abcd}\n        but note that {ab}{cd} /= {abcd}.\n\n        We also employ distributivity: {ab + cd} = {ab} + {cd}.\n\n        Canonical form also implies expand() {ab(c+d)} = {abc} + {abd}.\n\n        \"\"\"\n\n        # {ab + cd} = {ab} + {cd}\n        arg = sympify(arg)\n        arg = arg.expand()\n        if arg.is_Add:\n            return Add(*[ cls(term) for term in arg.args])\n\n        if arg.is_Mul:\n\n            # take coefficient outside of normal ordering brackets\n            c_part, seq = arg.args_cnc()\n            if c_part:\n                coeff = Mul(*c_part)\n                if not seq:\n                    return coeff\n            else:\n                coeff = S.One\n\n            # {ab{cd}} = {abcd}\n            newseq = []\n            foundit = False\n            for fac in seq:\n                if isinstance(fac, NO):\n                    newseq.extend(fac.args)\n                    foundit = True\n                else:\n                    newseq.append(fac)\n            if foundit:\n                return coeff*cls(Mul(*newseq))\n\n            # We assume that the user don't mix B and F operators\n            if isinstance(seq[0], BosonicOperator):\n                raise NotImplementedError\n\n            try:\n                newseq, sign = _sort_anticommuting_fermions(seq)\n            except ViolationOfPauliPrinciple:\n                return S.Zero\n\n            if sign % 2:\n                return (S.NegativeOne*coeff)*cls(Mul(*newseq))\n            elif sign:\n                return coeff*cls(Mul(*newseq))\n            else:\n                pass  # since sign==0, no permutations was necessary\n\n            # if we couldn't do anything with Mul object, we just\n            # mark it as normal ordered\n            if coeff != S.One:\n                return coeff*cls(Mul(*newseq))\n            return Expr.__new__(cls, Mul(*newseq))\n\n        if isinstance(arg, NO):\n            return arg\n\n        # if object was not Mul or Add, normal ordering does not apply\n        return arg\n\n    @property\n    def has_q_creators(self):\n        \"\"\"\n        Return 0 if the leftmost argument of the first argument is a not a\n        q_creator, else 1 if it is above fermi or -1 if it is below fermi.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols\n        >>> from sympy.physics.secondquant import NO, F, Fd\n\n        >>> a = symbols('a', above_fermi=True)\n        >>> i = symbols('i', below_fermi=True)\n        >>> NO(Fd(a)*Fd(i)).has_q_creators\n        1\n        >>> NO(F(i)*F(a)).has_q_creators\n        -1\n        >>> NO(Fd(i)*F(a)).has_q_creators           #doctest: +SKIP\n        0\n\n        \"\"\"\n        return self.args[0].args[0].is_q_creator\n\n    @property\n    def has_q_annihilators(self):\n        \"\"\"\n        Return 0 if the rightmost argument of the first argument is a not a\n        q_annihilator, else 1 if it is above fermi or -1 if it is below fermi.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols\n        >>> from sympy.physics.secondquant import NO, F, Fd\n\n        >>> a = symbols('a', above_fermi=True)\n        >>> i = symbols('i', below_fermi=True)\n        >>> NO(Fd(a)*Fd(i)).has_q_annihilators\n        -1\n        >>> NO(F(i)*F(a)).has_q_annihilators\n        1\n        >>> NO(Fd(a)*F(i)).has_q_annihilators\n        0\n\n        \"\"\"\n        return self.args[0].args[-1].is_q_annihilator\n\n    def doit(self, **kw_args):\n        \"\"\"\n        Either removes the brackets or enables complex computations\n        in its arguments.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.secondquant import NO, Fd, F\n        >>> from textwrap import fill\n        >>> from sympy import symbols, Dummy\n        >>> p,q = symbols('p,q', cls=Dummy)\n        >>> print(fill(str(NO(Fd(p)*F(q)).doit())))\n        KroneckerDelta(_a, _p)*KroneckerDelta(_a,\n        _q)*CreateFermion(_a)*AnnihilateFermion(_a) + KroneckerDelta(_a,\n        _p)*KroneckerDelta(_i, _q)*CreateFermion(_a)*AnnihilateFermion(_i) -\n        KroneckerDelta(_a, _q)*KroneckerDelta(_i,\n        _p)*AnnihilateFermion(_a)*CreateFermion(_i) - KroneckerDelta(_i,\n        _p)*KroneckerDelta(_i, _q)*AnnihilateFermion(_i)*CreateFermion(_i)\n        \"\"\"\n        if kw_args.get(\"remove_brackets\", True):\n            return self._remove_brackets()\n        else:\n            return self.__new__(type(self), self.args[0].doit(**kw_args))\n\n    def _remove_brackets(self):\n        \"\"\"\n        Returns the sorted string without normal order brackets.\n\n        The returned string have the property that no nonzero\n        contractions exist.\n        \"\"\"\n\n        # check if any creator is also an annihilator\n        subslist = []\n        for i in self.iter_q_creators():\n            if self[i].is_q_annihilator:\n                assume = self[i].state.assumptions0\n\n                # only operators with a dummy index can be split in two terms\n                if isinstance(self[i].state, Dummy):\n\n                    # create indices with fermi restriction\n                    assume.pop(\"above_fermi\", None)\n                    assume[\"below_fermi\"] = True\n                    below = Dummy('i', **assume)\n                    assume.pop(\"below_fermi\", None)\n                    assume[\"above_fermi\"] = True\n                    above = Dummy('a', **assume)\n\n                    cls = type(self[i])\n                    split = (\n                        self[i].__new__(cls, below)\n                        * KroneckerDelta(below, self[i].state)\n                        + self[i].__new__(cls, above)\n                        * KroneckerDelta(above, self[i].state)\n                    )\n                    subslist.append((self[i], split))\n                else:\n                    raise SubstitutionOfAmbigousOperatorFailed(self[i])\n        if subslist:\n            result = NO(self.subs(subslist))\n            if isinstance(result, Add):\n                return Add(*[term.doit() for term in result.args])\n        else:\n            return self.args[0]\n\n    def _expand_operators(self):\n        \"\"\"\n        Returns a sum of NO objects that contain no ambiguous q-operators.\n\n        If an index q has range both above and below fermi, the operator F(q)\n        is ambiguous in the sense that it can be both a q-creator and a q-annihilator.\n        If q is dummy, it is assumed to be a summation variable and this method\n        rewrites it into a sum of NO terms with unambiguous operators:\n\n        {Fd(p)*F(q)} = {Fd(a)*F(b)} + {Fd(a)*F(i)} + {Fd(j)*F(b)} -{F(i)*Fd(j)}\n\n        where a,b are above and i,j are below fermi level.\n        \"\"\"\n        return NO(self._remove_brackets)\n\n    def __getitem__(self, i):\n        if isinstance(i, slice):\n            indices = i.indices(len(self))\n            return [self.args[0].args[i] for i in range(*indices)]\n        else:\n            return self.args[0].args[i]\n\n    def __len__(self):\n        return len(self.args[0].args)\n\n    def iter_q_annihilators(self):\n        \"\"\"\n        Iterates over the annihilation operators.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols\n        >>> i, j = symbols('i j', below_fermi=True)\n        >>> a, b = symbols('a b', above_fermi=True)\n        >>> from sympy.physics.secondquant import NO, F, Fd\n        >>> no = NO(Fd(a)*F(i)*F(b)*Fd(j))\n\n        >>> no.iter_q_creators()\n        <generator object... at 0x...>\n        >>> list(no.iter_q_creators())\n        [0, 1]\n        >>> list(no.iter_q_annihilators())\n        [3, 2]\n\n        \"\"\"\n        ops = self.args[0].args\n        iter = range(len(ops) - 1, -1, -1)\n        for i in iter:\n            if ops[i].is_q_annihilator:\n                yield i\n            else:\n                break\n\n    def iter_q_creators(self):\n        \"\"\"\n        Iterates over the creation operators.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols\n        >>> i, j = symbols('i j', below_fermi=True)\n        >>> a, b = symbols('a b', above_fermi=True)\n        >>> from sympy.physics.secondquant import NO, F, Fd\n        >>> no = NO(Fd(a)*F(i)*F(b)*Fd(j))\n\n        >>> no.iter_q_creators()\n        <generator object... at 0x...>\n        >>> list(no.iter_q_creators())\n        [0, 1]\n        >>> list(no.iter_q_annihilators())\n        [3, 2]\n\n        \"\"\"\n\n        ops = self.args[0].args\n        iter = range(0, len(ops))\n        for i in iter:\n            if ops[i].is_q_creator:\n                yield i\n            else:\n                break\n\n    def get_subNO(self, i):\n        \"\"\"\n        Returns a NO() without FermionicOperator at index i.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols\n        >>> from sympy.physics.secondquant import F, NO\n        >>> p,q,r = symbols('p,q,r')\n\n        >>> NO(F(p)*F(q)*F(r)).get_subNO(1)  # doctest: +SKIP\n        NO(AnnihilateFermion(p)*AnnihilateFermion(r))\n\n        \"\"\"\n        arg0 = self.args[0]  # it's a Mul by definition of how it's created\n        mul = arg0._new_rawargs(arg0.args[:i] + arg0.args[i + 1:])\n        return NO(mul)\n\n    def _latex(self, printer):\n        return \"\\\\left\\\\{%s\\\\right\\\\}\" % printer._print(self.args[0])\n\n    def __repr__(self):\n        return \"NO(%s)\" % self.args[0]\n\n    def __str__(self):\n        return \":%s:\" % self.args[0]\n\n\ndef contraction(a, b):\n    \"\"\"\n    Calculates contraction of Fermionic operators a and b.\n\n    Examples\n    ========\n\n    >>> from sympy import symbols\n    >>> from sympy.physics.secondquant import F, Fd, contraction\n    >>> p, q = symbols('p,q')\n    >>> a, b = symbols('a,b', above_fermi=True)\n    >>> i, j = symbols('i,j', below_fermi=True)\n\n    A contraction is non-zero only if a quasi-creator is to the right of a\n    quasi-annihilator:\n\n    >>> contraction(F(a),Fd(b))\n    KroneckerDelta(a, b)\n    >>> contraction(Fd(i),F(j))\n    KroneckerDelta(i, j)\n\n    For general indices a non-zero result restricts the indices to below/above\n    the fermi surface:\n\n    >>> contraction(Fd(p),F(q))\n    KroneckerDelta(_i, q)*KroneckerDelta(p, q)\n    >>> contraction(F(p),Fd(q))\n    KroneckerDelta(_a, q)*KroneckerDelta(p, q)\n\n    Two creators or two annihilators always vanishes:\n\n    >>> contraction(F(p),F(q))\n    0\n    >>> contraction(Fd(p),Fd(q))\n    0\n\n    \"\"\"\n    if isinstance(b, FermionicOperator) and isinstance(a, FermionicOperator):\n        if isinstance(a, AnnihilateFermion) and isinstance(b, CreateFermion):\n            if b.state.assumptions0.get(\"below_fermi\"):\n                return S.Zero\n            if a.state.assumptions0.get(\"below_fermi\"):\n                return S.Zero\n            if b.state.assumptions0.get(\"above_fermi\"):\n                return KroneckerDelta(a.state, b.state)\n            if a.state.assumptions0.get(\"above_fermi\"):\n                return KroneckerDelta(a.state, b.state)\n\n            return (KroneckerDelta(a.state, b.state)*\n                    KroneckerDelta(b.state, Dummy('a', above_fermi=True)))\n        if isinstance(b, AnnihilateFermion) and isinstance(a, CreateFermion):\n            if b.state.assumptions0.get(\"above_fermi\"):\n                return S.Zero\n            if a.state.assumptions0.get(\"above_fermi\"):\n                return S.Zero\n            if b.state.assumptions0.get(\"below_fermi\"):\n                return KroneckerDelta(a.state, b.state)\n            if a.state.assumptions0.get(\"below_fermi\"):\n                return KroneckerDelta(a.state, b.state)\n\n            return (KroneckerDelta(a.state, b.state)*\n                    KroneckerDelta(b.state, Dummy('i', below_fermi=True)))\n\n        # vanish if 2xAnnihilator or 2xCreator\n        return S.Zero\n\n    else:\n        #not fermion operators\n        t = ( isinstance(i, FermionicOperator) for i in (a, b) )\n        raise ContractionAppliesOnlyToFermions(*t)\n\n\ndef _sqkey(sq_operator):\n    \"\"\"Generates key for canonical sorting of SQ operators.\"\"\"\n    return sq_operator._sortkey()\n\n\ndef _sort_anticommuting_fermions(string1, key=_sqkey):\n    \"\"\"Sort fermionic operators to canonical order, assuming all pairs anticommute.\n\n    Uses a bidirectional bubble sort.  Items in string1 are not referenced\n    so in principle they may be any comparable objects.   The sorting depends on the\n    operators '>' and '=='.\n\n    If the Pauli principle is violated, an exception is raised.\n\n    Returns\n    =======\n\n    tuple (sorted_str, sign)\n\n    sorted_str: list containing the sorted operators\n    sign: int telling how many times the sign should be changed\n          (if sign==0 the string was already sorted)\n    \"\"\"\n\n    verified = False\n    sign = 0\n    rng = list(range(len(string1) - 1))\n    rev = list(range(len(string1) - 3, -1, -1))\n\n    keys = list(map(key, string1))\n    key_val = dict(list(zip(keys, string1)))\n\n    while not verified:\n        verified = True\n        for i in rng:\n            left = keys[i]\n            right = keys[i + 1]\n            if left == right:\n                raise ViolationOfPauliPrinciple([left, right])\n            if left > right:\n                verified = False\n                keys[i:i + 2] = [right, left]\n                sign = sign + 1\n        if verified:\n            break\n        for i in rev:\n            left = keys[i]\n            right = keys[i + 1]\n            if left == right:\n                raise ViolationOfPauliPrinciple([left, right])\n            if left > right:\n                verified = False\n                keys[i:i + 2] = [right, left]\n                sign = sign + 1\n    string1 = [ key_val[k] for k in keys ]\n    return (string1, sign)\n\n\ndef evaluate_deltas(e):\n    \"\"\"\n    We evaluate KroneckerDelta symbols in the expression assuming Einstein summation.\n\n    If one index is repeated it is summed over and in effect substituted with\n    the other one. If both indices are repeated we substitute according to what\n    is the preferred index.  this is determined by\n    KroneckerDelta.preferred_index and KroneckerDelta.killable_index.\n\n    In case there are no possible substitutions or if a substitution would\n    imply a loss of information, nothing is done.\n\n    In case an index appears in more than one KroneckerDelta, the resulting\n    substitution depends on the order of the factors.  Since the ordering is platform\n    dependent, the literal expression resulting from this function may be hard to\n    predict.\n\n    Examples\n    ========\n\n    We assume the following:\n\n    >>> from sympy import symbols, Function, Dummy, KroneckerDelta\n    >>> from sympy.physics.secondquant import evaluate_deltas\n    >>> i,j = symbols('i j', below_fermi=True, cls=Dummy)\n    >>> a,b = symbols('a b', above_fermi=True, cls=Dummy)\n    >>> p,q = symbols('p q', cls=Dummy)\n    >>> f = Function('f')\n    >>> t = Function('t')\n\n    The order of preference for these indices according to KroneckerDelta is\n    (a, b, i, j, p, q).\n\n    Trivial cases:\n\n    >>> evaluate_deltas(KroneckerDelta(i,j)*f(i))       # d_ij f(i) -> f(j)\n    f(_j)\n    >>> evaluate_deltas(KroneckerDelta(i,j)*f(j))       # d_ij f(j) -> f(i)\n    f(_i)\n    >>> evaluate_deltas(KroneckerDelta(i,p)*f(p))       # d_ip f(p) -> f(i)\n    f(_i)\n    >>> evaluate_deltas(KroneckerDelta(q,p)*f(p))       # d_qp f(p) -> f(q)\n    f(_q)\n    >>> evaluate_deltas(KroneckerDelta(q,p)*f(q))       # d_qp f(q) -> f(p)\n    f(_p)\n\n    More interesting cases:\n\n    >>> evaluate_deltas(KroneckerDelta(i,p)*t(a,i)*f(p,q))\n    f(_i, _q)*t(_a, _i)\n    >>> evaluate_deltas(KroneckerDelta(a,p)*t(a,i)*f(p,q))\n    f(_a, _q)*t(_a, _i)\n    >>> evaluate_deltas(KroneckerDelta(p,q)*f(p,q))\n    f(_p, _p)\n\n    Finally, here are some cases where nothing is done, because that would\n    imply a loss of information:\n\n    >>> evaluate_deltas(KroneckerDelta(i,p)*f(q))\n    f(_q)*KroneckerDelta(_i, _p)\n    >>> evaluate_deltas(KroneckerDelta(i,p)*f(i))\n    f(_i)*KroneckerDelta(_i, _p)\n    \"\"\"\n\n    # We treat Deltas only in mul objects\n    # for general function objects we don't evaluate KroneckerDeltas in arguments,\n    # but here we hard code exceptions to this rule\n    accepted_functions = (\n        Add,\n    )\n    if isinstance(e, accepted_functions):\n        return e.func(*[evaluate_deltas(arg) for arg in e.args])\n\n    elif isinstance(e, Mul):\n        # find all occurrences of delta function and count each index present in\n        # expression.\n        deltas = []\n        indices = {}\n        for i in e.args:\n            for s in i.free_symbols:\n                if s in indices:\n                    indices[s] += 1\n                else:\n                    indices[s] = 0  # geek counting simplifies logic below\n            if isinstance(i, KroneckerDelta):\n                deltas.append(i)\n\n        for d in deltas:\n            # If we do something, and there are more deltas, we should recurse\n            # to treat the resulting expression properly\n            if d.killable_index.is_Symbol and indices[d.killable_index]:\n                e = e.subs(d.killable_index, d.preferred_index)\n                if len(deltas) > 1:\n                    return evaluate_deltas(e)\n            elif (d.preferred_index.is_Symbol and indices[d.preferred_index]\n                  and d.indices_contain_equal_information):\n                e = e.subs(d.preferred_index, d.killable_index)\n                if len(deltas) > 1:\n                    return evaluate_deltas(e)\n            else:\n                pass\n\n        return e\n    # nothing to do, maybe we hit a Symbol or a number\n    else:\n        return e\n\n\ndef substitute_dummies(expr, new_indices=False, pretty_indices={}):\n    \"\"\"\n    Collect terms by substitution of dummy variables.\n\n    This routine allows simplification of Add expressions containing terms\n    which differ only due to dummy variables.\n\n    The idea is to substitute all dummy variables consistently depending on\n    the structure of the term.  For each term, we obtain a sequence of all\n    dummy variables, where the order is determined by the index range, what\n    factors the index belongs to and its position in each factor.  See\n    _get_ordered_dummies() for more inforation about the sorting of dummies.\n    The index sequence is then substituted consistently in each term.\n\n    Examples\n    ========\n\n    >>> from sympy import symbols, Function, Dummy\n    >>> from sympy.physics.secondquant import substitute_dummies\n    >>> a,b,c,d = symbols('a b c d', above_fermi=True, cls=Dummy)\n    >>> i,j = symbols('i j', below_fermi=True, cls=Dummy)\n    >>> f = Function('f')\n\n    >>> expr = f(a,b) + f(c,d); expr\n    f(_a, _b) + f(_c, _d)\n\n    Since a, b, c and d are equivalent summation indices, the expression can be\n    simplified to a single term (for which the dummy indices are still summed over)\n\n    >>> substitute_dummies(expr)\n    2*f(_a, _b)\n\n\n    Controlling output:\n\n    By default the dummy symbols that are already present in the expression\n    will be reused in a different permutation.  However, if new_indices=True,\n    new dummies will be generated and inserted.  The keyword 'pretty_indices'\n    can be used to control this generation of new symbols.\n\n    By default the new dummies will be generated on the form i_1, i_2, a_1,\n    etc.  If you supply a dictionary with key:value pairs in the form:\n\n        { index_group: string_of_letters }\n\n    The letters will be used as labels for the new dummy symbols.  The\n    index_groups must be one of 'above', 'below' or 'general'.\n\n    >>> expr = f(a,b,i,j)\n    >>> my_dummies = { 'above':'st', 'below':'uv' }\n    >>> substitute_dummies(expr, new_indices=True, pretty_indices=my_dummies)\n    f(_s, _t, _u, _v)\n\n    If we run out of letters, or if there is no keyword for some index_group\n    the default dummy generator will be used as a fallback:\n\n    >>> p,q = symbols('p q', cls=Dummy)  # general indices\n    >>> expr = f(p,q)\n    >>> substitute_dummies(expr, new_indices=True, pretty_indices=my_dummies)\n    f(_p_0, _p_1)\n\n    \"\"\"\n\n    # setup the replacing dummies\n    if new_indices:\n        letters_above = pretty_indices.get('above', \"\")\n        letters_below = pretty_indices.get('below', \"\")\n        letters_general = pretty_indices.get('general', \"\")\n        len_above = len(letters_above)\n        len_below = len(letters_below)\n        len_general = len(letters_general)\n\n        def _i(number):\n            try:\n                return letters_below[number]\n            except IndexError:\n                return 'i_' + str(number - len_below)\n\n        def _a(number):\n            try:\n                return letters_above[number]\n            except IndexError:\n                return 'a_' + str(number - len_above)\n\n        def _p(number):\n            try:\n                return letters_general[number]\n            except IndexError:\n                return 'p_' + str(number - len_general)\n\n    aboves = []\n    belows = []\n    generals = []\n\n    dummies = expr.atoms(Dummy)\n    if not new_indices:\n        dummies = sorted(dummies, key=default_sort_key)\n\n    # generate lists with the dummies we will insert\n    a = i = p = 0\n    for d in dummies:\n        assum = d.assumptions0\n\n        if assum.get(\"above_fermi\"):\n            if new_indices:\n                sym = _a(a)\n                a += 1\n            l1 = aboves\n        elif assum.get(\"below_fermi\"):\n            if new_indices:\n                sym = _i(i)\n                i += 1\n            l1 = belows\n        else:\n            if new_indices:\n                sym = _p(p)\n                p += 1\n            l1 = generals\n\n        if new_indices:\n            l1.append(Dummy(sym, **assum))\n        else:\n            l1.append(d)\n\n    expr = expr.expand()\n    terms = Add.make_args(expr)\n    new_terms = []\n    for term in terms:\n        i = iter(belows)\n        a = iter(aboves)\n        p = iter(generals)\n        ordered = _get_ordered_dummies(term)\n        subsdict = {}\n        for d in ordered:\n            if d.assumptions0.get('below_fermi'):\n                subsdict[d] = next(i)\n            elif d.assumptions0.get('above_fermi'):\n                subsdict[d] = next(a)\n            else:\n                subsdict[d] = next(p)\n        subslist = []\n        final_subs = []\n        for k, v in subsdict.items():\n            if k == v:\n                continue\n            if v in subsdict:\n                # We check if the sequence of substitutions end quickly.  In\n                # that case, we can avoid temporary symbols if we ensure the\n                # correct substitution order.\n                if subsdict[v] in subsdict:\n                    # (x, y) -> (y, x),  we need a temporary variable\n                    x = Dummy('x')\n                    subslist.append((k, x))\n                    final_subs.append((x, v))\n                else:\n                    # (x, y) -> (y, a),  x->y must be done last\n                    # but before temporary variables are resolved\n                    final_subs.insert(0, (k, v))\n            else:\n                subslist.append((k, v))\n        subslist.extend(final_subs)\n        new_terms.append(term.subs(subslist))\n    return Add(*new_terms)\n\n\nclass KeyPrinter(StrPrinter):\n    \"\"\"Printer for which only equal objects are equal in print\"\"\"\n    def _print_Dummy(self, expr):\n        return \"(%s_%i)\" % (expr.name, expr.dummy_index)\n\n\ndef __kprint(expr):\n    p = KeyPrinter()\n    return p.doprint(expr)\n\n\ndef _get_ordered_dummies(mul, verbose=False):\n    \"\"\"Returns all dummies in the mul sorted in canonical order\n\n    The purpose of the canonical ordering is that dummies can be substituted\n    consistently across terms with the result that equivalent terms can be\n    simplified.\n\n    It is not possible to determine if two terms are equivalent based solely on\n    the dummy order.  However, a consistent substitution guided by the ordered\n    dummies should lead to trivially (non-)equivalent terms, thereby revealing\n    the equivalence.  This also means that if two terms have identical sequences of\n    dummies, the (non-)equivalence should already be apparent.\n\n    Strategy\n    --------\n\n    The canoncial order is given by an arbitrary sorting rule.  A sort key\n    is determined for each dummy as a tuple that depends on all factors where\n    the index is present.  The dummies are thereby sorted according to the\n    contraction structure of the term, instead of sorting based solely on the\n    dummy symbol itself.\n\n    After all dummies in the term has been assigned a key, we check for identical\n    keys, i.e. unorderable dummies.  If any are found, we call a specialized\n    method, _determine_ambiguous(), that will determine a unique order based\n    on recursive calls to _get_ordered_dummies().\n\n    Key description\n    ---------------\n\n    A high level description of the sort key:\n\n        1. Range of the dummy index\n        2. Relation to external (non-dummy) indices\n        3. Position of the index in the first factor\n        4. Position of the index in the second factor\n\n    The sort key is a tuple with the following components:\n\n        1. A single character indicating the range of the dummy (above, below\n           or general.)\n        2. A list of strings with fully masked string representations of all\n           factors where the dummy is present.  By masked, we mean that dummies\n           are represented by a symbol to indicate either below fermi, above or\n           general.  No other information is displayed about the dummies at\n           this point.  The list is sorted stringwise.\n        3. An integer number indicating the position of the index, in the first\n           factor as sorted in 2.\n        4. An integer number indicating the position of the index, in the second\n           factor as sorted in 2.\n\n    If a factor is either of type AntiSymmetricTensor or SqOperator, the index\n    position in items 3 and 4 is indicated as 'upper' or 'lower' only.\n    (Creation operators are considered upper and annihilation operators lower.)\n\n    If the masked factors are identical, the two factors cannot be ordered\n    unambiguously in item 2.  In this case, items 3, 4 are left out.  If several\n    indices are contracted between the unorderable factors, it will be handled by\n    _determine_ambiguous()\n\n\n    \"\"\"\n    # setup dicts to avoid repeated calculations in key()\n    args = Mul.make_args(mul)\n    fac_dum = dict([ (fac, fac.atoms(Dummy)) for fac in args] )\n    fac_repr = dict([ (fac, __kprint(fac)) for fac in args] )\n    all_dums = set().union(*fac_dum.values())\n    mask = {}\n    for d in all_dums:\n        if d.assumptions0.get('below_fermi'):\n            mask[d] = '0'\n        elif d.assumptions0.get('above_fermi'):\n            mask[d] = '1'\n        else:\n            mask[d] = '2'\n    dum_repr = {d: __kprint(d) for d in all_dums}\n\n    def _key(d):\n        dumstruct = [ fac for fac in fac_dum if d in fac_dum[fac] ]\n        other_dums = set().union(*[fac_dum[fac] for fac in dumstruct])\n        fac = dumstruct[-1]\n        if other_dums is fac_dum[fac]:\n            other_dums = fac_dum[fac].copy()\n        other_dums.remove(d)\n        masked_facs = [ fac_repr[fac] for fac in dumstruct ]\n        for d2 in other_dums:\n            masked_facs = [ fac.replace(dum_repr[d2], mask[d2])\n                    for fac in masked_facs ]\n        all_masked = [ fac.replace(dum_repr[d], mask[d])\n                       for fac in masked_facs ]\n        masked_facs = dict(list(zip(dumstruct, masked_facs)))\n\n        # dummies for which the ordering cannot be determined\n        if has_dups(all_masked):\n            all_masked.sort()\n            return mask[d], tuple(all_masked)  # positions are ambiguous\n\n        # sort factors according to fully masked strings\n        keydict = dict(list(zip(dumstruct, all_masked)))\n        dumstruct.sort(key=lambda x: keydict[x])\n        all_masked.sort()\n\n        pos_val = []\n        for fac in dumstruct:\n            if isinstance(fac, AntiSymmetricTensor):\n                if d in fac.upper:\n                    pos_val.append('u')\n                if d in fac.lower:\n                    pos_val.append('l')\n            elif isinstance(fac, Creator):\n                pos_val.append('u')\n            elif isinstance(fac, Annihilator):\n                pos_val.append('l')\n            elif isinstance(fac, NO):\n                ops = [ op for op in fac if op.has(d) ]\n                for op in ops:\n                    if isinstance(op, Creator):\n                        pos_val.append('u')\n                    else:\n                        pos_val.append('l')\n            else:\n                # fallback to position in string representation\n                facpos = -1\n                while 1:\n                    facpos = masked_facs[fac].find(dum_repr[d], facpos + 1)\n                    if facpos == -1:\n                        break\n                    pos_val.append(facpos)\n        return (mask[d], tuple(all_masked), pos_val[0], pos_val[-1])\n    dumkey = dict(list(zip(all_dums, list(map(_key, all_dums)))))\n    result = sorted(all_dums, key=lambda x: dumkey[x])\n    if has_dups(iter(dumkey.values())):\n        # We have ambiguities\n        unordered = defaultdict(set)\n        for d, k in dumkey.items():\n            unordered[k].add(d)\n        for k in [ k for k in unordered if len(unordered[k]) < 2 ]:\n            del unordered[k]\n\n        unordered = [ unordered[k] for k in sorted(unordered) ]\n        result = _determine_ambiguous(mul, result, unordered)\n    return result\n\n\ndef _determine_ambiguous(term, ordered, ambiguous_groups):\n    # We encountered a term for which the dummy substitution is ambiguous.\n    # This happens for terms with 2 or more contractions between factors that\n    # cannot be uniquely ordered independent of summation indices.  For\n    # example:\n    #\n    # Sum(p, q) v^{p, .}_{q, .}v^{q, .}_{p, .}\n    #\n    # Assuming that the indices represented by . are dummies with the\n    # same range, the factors cannot be ordered, and there is no\n    # way to determine a consistent ordering of p and q.\n    #\n    # The strategy employed here, is to relabel all unambiguous dummies with\n    # non-dummy symbols and call _get_ordered_dummies again.  This procedure is\n    # applied to the entire term so there is a possibility that\n    # _determine_ambiguous() is called again from a deeper recursion level.\n\n    # break recursion if there are no ordered dummies\n    all_ambiguous = set()\n    for dummies in ambiguous_groups:\n        all_ambiguous |= dummies\n    all_ordered = set(ordered) - all_ambiguous\n    if not all_ordered:\n        # FIXME: If we arrive here, there are no ordered dummies. A method to\n        # handle this needs to be implemented.  In order to return something\n        # useful nevertheless, we choose arbitrarily the first dummy and\n        # determine the rest from this one.  This method is dependent on the\n        # actual dummy labels which violates an assumption for the\n        # canonicalization procedure.  A better implementation is needed.\n        group = [ d for d in ordered if d in ambiguous_groups[0] ]\n        d = group[0]\n        all_ordered.add(d)\n        ambiguous_groups[0].remove(d)\n\n    stored_counter = _symbol_factory._counter\n    subslist = []\n    for d in [ d for d in ordered if d in all_ordered ]:\n        nondum = _symbol_factory._next()\n        subslist.append((d, nondum))\n    newterm = term.subs(subslist)\n    neworder = _get_ordered_dummies(newterm)\n    _symbol_factory._set_counter(stored_counter)\n\n    # update ordered list with new information\n    for group in ambiguous_groups:\n        ordered_group = [ d for d in neworder if d in group ]\n        ordered_group.reverse()\n        result = []\n        for d in ordered:\n            if d in group:\n                result.append(ordered_group.pop())\n            else:\n                result.append(d)\n        ordered = result\n    return ordered\n\n\nclass _SymbolFactory(object):\n    def __init__(self, label):\n        self._counterVar = 0\n        self._label = label\n\n    def _set_counter(self, value):\n        \"\"\"\n        Sets counter to value.\n        \"\"\"\n        self._counterVar = value\n\n    @property\n    def _counter(self):\n        \"\"\"\n        What counter is currently at.\n        \"\"\"\n        return self._counterVar\n\n    def _next(self):\n        \"\"\"\n        Generates the next symbols and increments counter by 1.\n        \"\"\"\n        s = Symbol(\"%s%i\" % (self._label, self._counterVar))\n        self._counterVar += 1\n        return s\n_symbol_factory = _SymbolFactory('_]\"]_')  # most certainly a unique label\n\n\n@cacheit\ndef _get_contractions(string1, keep_only_fully_contracted=False):\n    \"\"\"\n    Returns Add-object with contracted terms.\n\n    Uses recursion to find all contractions. -- Internal helper function --\n\n    Will find nonzero contractions in string1 between indices given in\n    leftrange and rightrange.\n\n    \"\"\"\n\n    # Should we store current level of contraction?\n    if keep_only_fully_contracted and string1:\n        result = []\n    else:\n        result = [NO(Mul(*string1))]\n\n    for i in range(len(string1) - 1):\n        for j in range(i + 1, len(string1)):\n\n            c = contraction(string1[i], string1[j])\n\n            if c:\n                sign = (j - i + 1) % 2\n                if sign:\n                    coeff = S.NegativeOne*c\n                else:\n                    coeff = c\n\n                #\n                #  Call next level of recursion\n                #  ============================\n                #\n                # We now need to find more contractions among operators\n                #\n                # oplist = string1[:i]+ string1[i+1:j] + string1[j+1:]\n                #\n                # To prevent overcounting, we don't allow contractions\n                # we have already encountered. i.e. contractions between\n                #       string1[:i] <---> string1[i+1:j]\n                # and   string1[:i] <---> string1[j+1:].\n                #\n                # This leaves the case:\n                oplist = string1[i + 1:j] + string1[j + 1:]\n\n                if oplist:\n\n                    result.append(coeff*NO(\n                        Mul(*string1[:i])*_get_contractions( oplist,\n                            keep_only_fully_contracted=keep_only_fully_contracted)))\n\n                else:\n                    result.append(coeff*NO( Mul(*string1[:i])))\n\n        if keep_only_fully_contracted:\n            break   # next iteration over i leaves leftmost operator string1[0] uncontracted\n\n    return Add(*result)\n\n\ndef wicks(e, **kw_args):\n    \"\"\"\n    Returns the normal ordered equivalent of an expression using Wicks Theorem.\n\n\n    Examples\n    ========\n\n    >>> from sympy import symbols, Function, Dummy\n    >>> from sympy.physics.secondquant import wicks, F, Fd, NO\n    >>> p,q,r = symbols('p,q,r')\n    >>> wicks(Fd(p)*F(q))  # doctest: +SKIP\n    d(p, q)*d(q, _i) + NO(CreateFermion(p)*AnnihilateFermion(q))\n\n    By default, the expression is expanded:\n\n    >>> wicks(F(p)*(F(q)+F(r))) # doctest: +SKIP\n    NO(AnnihilateFermion(p)*AnnihilateFermion(q)) + NO(\n        AnnihilateFermion(p)*AnnihilateFermion(r))\n\n    With the keyword 'keep_only_fully_contracted=True', only fully contracted\n    terms are returned.\n\n    By request, the result can be simplified in the following order:\n     -- KroneckerDelta functions are evaluated\n     -- Dummy variables are substituted consistently across terms\n\n    >>> p, q, r = symbols('p q r', cls=Dummy)\n    >>> wicks(Fd(p)*(F(q)+F(r)), keep_only_fully_contracted=True) # doctest: +SKIP\n    KroneckerDelta(_i, _q)*KroneckerDelta(\n        _p, _q) + KroneckerDelta(_i, _r)*KroneckerDelta(_p, _r)\n\n    \"\"\"\n\n    if not e:\n        return S.Zero\n\n    opts = {\n        'simplify_kronecker_deltas': False,\n        'expand': True,\n        'simplify_dummies': False,\n        'keep_only_fully_contracted': False\n    }\n    opts.update(kw_args)\n\n    # check if we are already normally ordered\n    if isinstance(e, NO):\n        if opts['keep_only_fully_contracted']:\n            return S.Zero\n        else:\n            return e\n    elif isinstance(e, FermionicOperator):\n        if opts['keep_only_fully_contracted']:\n            return S.Zero\n        else:\n            return e\n\n    # break up any NO-objects, and evaluate commutators\n    e = e.doit(wicks=True)\n\n    # make sure we have only one term to consider\n    e = e.expand()\n    if isinstance(e, Add):\n        if opts['simplify_dummies']:\n            return substitute_dummies(Add(*[ wicks(term, **kw_args) for term in e.args]))\n        else:\n            return Add(*[ wicks(term, **kw_args) for term in e.args])\n\n    # For Mul-objects we can actually do something\n    if isinstance(e, Mul):\n\n        # we don't want to mess around with commuting part of Mul\n        # so we factorize it out before starting recursion\n        c_part = []\n        string1 = []\n        for factor in e.args:\n            if factor.is_commutative:\n                c_part.append(factor)\n            else:\n                string1.append(factor)\n        n = len(string1)\n\n        # catch trivial cases\n        if n == 0:\n            result = e\n        elif n == 1:\n            if opts['keep_only_fully_contracted']:\n                return S.Zero\n            else:\n                result = e\n\n        else:  # non-trivial\n\n            if isinstance(string1[0], BosonicOperator):\n                raise NotImplementedError\n\n            string1 = tuple(string1)\n\n            # recursion over higher order contractions\n            result = _get_contractions(string1,\n                keep_only_fully_contracted=opts['keep_only_fully_contracted'] )\n            result = Mul(*c_part)*result\n\n        if opts['expand']:\n            result = result.expand()\n        if opts['simplify_kronecker_deltas']:\n            result = evaluate_deltas(result)\n\n        return result\n\n    # there was nothing to do\n    return e\n\n\nclass PermutationOperator(Expr):\n    \"\"\"\n    Represents the index permutation operator P(ij).\n\n    P(ij)*f(i)*g(j) = f(i)*g(j) - f(j)*g(i)\n    \"\"\"\n    is_commutative = True\n\n    def __new__(cls, i, j):\n        i, j = sorted(map(sympify, (i, j)), key=default_sort_key)\n        obj = Basic.__new__(cls, i, j)\n        return obj\n\n    def get_permuted(self, expr):\n        \"\"\"\n        Returns -expr with permuted indices.\n\n        >>> from sympy import symbols, Function\n        >>> from sympy.physics.secondquant import PermutationOperator\n        >>> p,q = symbols('p,q')\n        >>> f = Function('f')\n        >>> PermutationOperator(p,q).get_permuted(f(p,q))\n        -f(q, p)\n\n        \"\"\"\n        i = self.args[0]\n        j = self.args[1]\n        if expr.has(i) and expr.has(j):\n            tmp = Dummy()\n            expr = expr.subs(i, tmp)\n            expr = expr.subs(j, i)\n            expr = expr.subs(tmp, j)\n            return S.NegativeOne*expr\n        else:\n            return expr\n\n    def _latex(self, printer):\n        return \"P(%s%s)\" % self.args\n\n", "right_context": "", "import_text": ["collections.defaultdict", "sympy.Add", "sympy.Basic", "sympy.cacheit", "sympy.Dummy", "sympy.Expr", "sympy.Function", "sympy.I", "sympy.KroneckerDelta", "sympy.Mul", "sympy.Pow", "sympy.S", "sympy.sqrt", "sympy.Symbol", "sympy.sympify", "sympy.Tuple", "sympy.zeros", "sympy.printing.str.StrPrinter", "sympy.core.compatibility.range", "sympy.utilities.iterables.has_dups", "sympy.utilities.default_sort_key"], "prompt": "\"\"\"\nDescription: This function simplifies index permutations in a given expression using a set of permutation operators.\n\nArgs:\n    expr (sympy.Add): The expression to be simplified.\n    permutation_operators (list): A list of permutation operators to be applied to the expression.\n\nReturns:\n    sympy.Add: The simplified expression.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Performs simplification by introducing PermutationOperators where appropriate.\n\n    Schematically:\n        [abij] - [abji] - [baij] + [baji] ->  P(ab)*P(ij)*[abij]\n\n    permutation_operators is a list of PermutationOperators to consider.\n\n    If permutation_operators=[P(ab),P(ij)] we will try to introduce the\n    permutation operators P(ij) and P(ab) in the expression.  If there are other\n    possible simplifications, we ignore them.\n\n    >>> from sympy import symbols, Function\n    >>> from sympy.physics.secondquant import simplify_index_permutations\n    >>> from sympy.physics.secondquant import PermutationOperator\n    >>> p,q,r,s = symbols('p,q,r,s')\n    >>> f = Function('f')\n    >>> g = Function('g')\n\n    >>> expr = f(p)*g(q) - f(q)*g(p); expr\n    f(p)*g(q) - f(q)*g(p)\n    >>> simplify_index_permutations(expr,[PermutationOperator(p,q)])\n    f(p)*g(q)*PermutationOperator(p, q)\n\n    >>> PermutList = [PermutationOperator(p,q),PermutationOperator(r,s)]\n    >>> expr = f(p,r)*g(q,s) - f(q,r)*g(p,s) + f(q,s)*g(p,r) - f(p,s)*g(q,r)\n    >>> simplify_index_permutations(expr,PermutList)\n    f(p, r)*g(q, s)*PermutationOperator(p, q)*PermutationOperator(r, s)\n\n    \"\"\"", "function_dependencies": ["sympy.utilities.default_sort_key", "sympy.Add"], "project_create_time": "2018-01-08T14:28:29+00:00", "project_update_time": "2024-04-16T22:36:42+00:00", "file_create_time": "2019-01-24T21:57:23Z", "file_update_time": "2019-01-24T21:57:23Z", "function_update_time": "2019-01-24T21:57:23Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["sympy.Add"], "test_function": [{"file_path": "/python3_ios-v1.0/python3_ios-1.0/site-packages/sympy/physics/tests/test_secondquant.py", "class_name": null, "function_name": "test_PermutationOperator", "code": "\ndef test_PermutationOperator():\n    p, q, r, s = symbols('p,q,r,s')\n    f, g, h, i = map(Function, 'fghi')\n    P = PermutationOperator\n    assert P(p, q).get_permuted(f(p)*g(q)) == -f(q)*g(p)\n    assert P(p, q).get_permuted(f(p, q)) == -f(q, p)\n    assert P(p, q).get_permuted(f(p)) == f(p)\n    expr = (f(p)*g(q)*h(r)*i(s)\n        - f(q)*g(p)*h(r)*i(s)\n        - f(p)*g(q)*h(s)*i(r)\n        + f(q)*g(p)*h(s)*i(r))\n    perms = [P(p, q), P(r, s)]\n    assert (simplify_index_permutations(expr, perms) ==\n        P(p, q)*P(r, s)*f(p)*g(q)*h(r)*i(s))"}, {"file_path": "/python3_ios-v1.0/python3_ios-1.0/site-packages/sympy/physics/tests/test_secondquant.py", "class_name": null, "function_name": "test_index_permutations_with_dummies", "code": "\ndef test_index_permutations_with_dummies():\n    a, b, c, d = symbols('a b c d')\n    p, q, r, s = symbols('p q r s', cls=Dummy)\n    f, g = map(Function, 'fg')\n    P = PermutationOperator\n\n    # No dummy substitution necessary\n    expr = f(a, b, p, q) - f(b, a, p, q)\n    assert simplify_index_permutations(\n        expr, [P(a, b)]) == P(a, b)*f(a, b, p, q)\n\n    # Cases where dummy substitution is needed\n    expected = P(a, b)*substitute_dummies(f(a, b, p, q))\n\n    expr = f(a, b, p, q) - f(b, a, q, p)\n    result = simplify_index_permutations(expr, [P(a, b)])\n    assert expected == substitute_dummies(result)\n\n    expr = f(a, b, q, p) - f(b, a, p, q)\n    result = simplify_index_permutations(expr, [P(a, b)])\n    assert expected == substitute_dummies(result)\n\n    # A case where nothing can be done\n    expr = f(a, b, q, p) - g(b, a, p, q)\n    result = simplify_index_permutations(expr, [P(a, b)])\n    assert expr == result"}]}, {"git_group": "spotify", "git_name": "basic-pitch", "version": "v0.3.3", "language": "Python", "project_name": "basic-pitch-v0.3.3.zip", "file_path": "/basic-pitch-v0.3.3/basic-pitch-0.3.3/basic_pitch/inference.py", "file_name": "inference.py", "focal_class": null, "focal_name": "get_audio_input", "focal_parameter": [], "solution": "def get_audio_input(\n    audio_path: Union[pathlib.Path, str], overlap_len: int, hop_size: int\n) -> Iterable[Tuple[npt.NDArray[np.float32], Dict[str, float], int]]:\n    assert overlap_len % 2 == 0, \"overlap_length must be even, got {}\".format(overlap_len)\n\n    audio_original, _ = librosa.load(str(audio_path), sr=AUDIO_SAMPLE_RATE, mono=True)\n\n    original_length = audio_original.shape[0]\n    audio_original = np.concatenate([np.zeros((int(overlap_len / 2),), dtype=np.float32), audio_original])\n    for window, window_time in window_audio_file(audio_original, hop_size):\n        yield np.expand_dims(window, axis=0), window_time, original_length", "function_signature": "def get_audio_input(\n    audio_path: Union[pathlib.Path, str], overlap_len: int, hop_size: int\n) -> Iterable[Tuple[npt.NDArray[np.float32], Dict[str, float], int]] :", "left_context": "#!/usr/bin/env python\n# encoding: utf-8\n#\n# Copyright 2022 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport csv\nimport enum\nimport json\nimport logging\nimport os\nimport pathlib\nfrom typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Union, cast\n\n\nfrom basic_pitch import CT_PRESENT, ICASSP_2022_MODEL_PATH, ONNX_PRESENT, TF_PRESENT, TFLITE_PRESENT\n\ntry:\n    import tensorflow as tf\nexcept ImportError:\n    pass\n\ntry:\n    import coremltools as ct\nexcept ImportError:\n    pass\n\ntry:\n    import tflite_runtime.interpreter as tflite\nexcept ImportError:\n    if TF_PRESENT:\n        import tensorflow.lite as tflite\n\ntry:\n    import onnxruntime as ort\nexcept ImportError:\n    pass\n\nimport numpy as np\nimport numpy.typing as npt\nimport librosa\nimport pretty_midi\n\nfrom basic_pitch.constants import (\n    AUDIO_SAMPLE_RATE,\n    AUDIO_N_SAMPLES,\n    ANNOTATIONS_FPS,\n    FFT_HOP,\n)\nfrom basic_pitch.commandline_printing import (\n    generating_file_message,\n    no_tf_warnings,\n    file_saved_confirmation,\n    failed_to_save,\n)\nimport basic_pitch.note_creation as infer\n\n\nclass Model:\n    class MODEL_TYPES(enum.Enum):\n        TENSORFLOW = enum.auto()\n        COREML = enum.auto()\n        TFLITE = enum.auto()\n        ONNX = enum.auto()\n\n    def __init__(self, model_path: Union[pathlib.Path, str]):\n        present = []\n        if TF_PRESENT:\n            present.append(\"TensorFlow\")\n            try:\n                self.model_type = Model.MODEL_TYPES.TENSORFLOW\n                self.model = tf.saved_model.load(str(model_path))\n                return\n            except Exception as e:\n                if os.path.isdir(model_path) and {\"saved_model.pb\", \"variables\"} & set(os.listdir(model_path)):\n                    logging.warning(\n                        \"Could not load TensorFlow saved model %s even \"\n                        \"though it looks like a saved model file with error %s. \"\n                        \"Are you sure it's a TensorFlow saved model?\",\n                        model_path,\n                        e.__repr__(),\n                    )\n\n        if CT_PRESENT:\n            present.append(\"CoreML\")\n            try:\n                self.model_type = Model.MODEL_TYPES.COREML\n                self.model = ct.models.MLModel(str(model_path))\n                return\n            except Exception as e:\n                if str(model_path).endswith(\".mlpackage\"):\n                    logging.warning(\n                        \"Could not load CoreML file %s even \"\n                        \"though it looks like a CoreML file with error %s. \"\n                        \"Are you sure it's a CoreML file?\",\n                        model_path,\n                        e.__repr__(),\n                    )\n\n        if TFLITE_PRESENT or TF_PRESENT:\n            present.append(\"TensorFlowLite\")\n            try:\n                self.model_type = Model.MODEL_TYPES.TFLITE\n                self.interpreter = tflite.Interpreter(str(model_path))\n                self.model = self.interpreter.get_signature_runner()\n                return\n            except Exception as e:\n                if str(model_path).endswith(\".tflite\"):\n                    logging.warning(\n                        \"Could not load TensorFlowLite file %s even \"\n                        \"though it looks like a TFLite file with error %s. \"\n                        \"Are you sure it's a TFLite file?\",\n                        model_path,\n                        e.__repr__(),\n                    )\n\n        if ONNX_PRESENT:\n            present.append(\"ONNX\")\n            try:\n                self.model_type = Model.MODEL_TYPES.ONNX\n                self.model = ort.InferenceSession(str(model_path), providers=[\"CPUExecutionProvider\"])\n                return\n            except Exception as e:\n                if str(model_path).endswith(\".onnx\"):\n                    logging.warning(\n                        \"Could not load ONNX file %s even \"\n                        \"though it looks like a ONNX file with error %s. \"\n                        \"Are you sure it's a ONNX file?\",\n                        model_path,\n                        e.__repr__(),\n                    )\n\n        raise ValueError(\n            f\"File {model_path} cannot be loaded into either \"\n            \"TensorFlow, CoreML, TFLite or ONNX. \"\n            \"Please check if it is a supported and valid serialized model \"\n            \"and that one of these packages are installed. On this system, \"\n            f\"{present} is installed.\"\n        )\n\n    def predict(self, x: npt.NDArray[np.float32]) -> Dict[str, npt.NDArray[np.float32]]:\n        if self.model_type == Model.MODEL_TYPES.TENSORFLOW:\n            return {k: v.numpy() for k, v in cast(tf.keras.Model, self.model(x)).items()}\n        elif self.model_type == Model.MODEL_TYPES.COREML:\n            print(f\"isfinite: {np.all(np.isfinite(x))}\", flush=True)\n            print(f\"shape: {x.shape}\", flush=True)\n            print(f\"dtype: {x.dtype}\", flush=True)\n            result = cast(ct.models.MLModel, self.model).predict({\"input_2\": x})\n            return {\n                \"note\": result[\"Identity_1\"],\n                \"onset\": result[\"Identity_2\"],\n                \"contour\": result[\"Identity\"],\n            }\n        elif self.model_type == Model.MODEL_TYPES.TFLITE:\n            return self.model(input_2=x)  # type: ignore\n        elif self.model_type == Model.MODEL_TYPES.ONNX:\n            return {\n                k: v\n                for k, v in zip(\n                    [\"note\", \"onset\", \"contour\"],\n                    cast(ort.InferenceSession, self.model).run(\n                        [\n                            \"StatefulPartitionedCall:1\",\n                            \"StatefulPartitionedCall:2\",\n                            \"StatefulPartitionedCall:0\",\n                        ],\n                        {\"serving_default_input_2:0\": x},\n                    ),\n                )\n            }\n\n\ndef window_audio_file(\n    audio_original: npt.NDArray[np.float32], hop_size: int\n) -> Iterable[Tuple[npt.NDArray[np.float32], Dict[str, float]]]:\n    \"\"\"\n    Pad appropriately an audio file, and return as\n    windowed signal, with window length = AUDIO_N_SAMPLES\n\n    Returns:\n        audio_windowed: tensor with shape (n_windows, AUDIO_N_SAMPLES, 1)\n            audio windowed into fixed length chunks\n        window_times: list of {'start':.., 'end':...} objects (times in seconds)\n\n    \"\"\"\n    for i in range(0, audio_original.shape[0], hop_size):\n        window = audio_original[i : i + AUDIO_N_SAMPLES]\n        if len(window) < AUDIO_N_SAMPLES:\n            window = np.pad(\n                window,\n                pad_width=[[0, AUDIO_N_SAMPLES - len(window)]],\n            )\n        t_start = float(i) / AUDIO_SAMPLE_RATE\n        window_time = {\n            \"start\": t_start,\n            \"end\": t_start + (AUDIO_N_SAMPLES / AUDIO_SAMPLE_RATE),\n        }\n        yield np.expand_dims(window, axis=-1), window_time\n\n", "right_context": "\n\ndef unwrap_output(\n    output: npt.NDArray[np.float32],\n    audio_original_length: int,\n    n_overlapping_frames: int,\n) -> np.array:\n    \"\"\"Unwrap batched model predictions to a single matrix.\n\n    Args:\n        output: array (n_batches, n_times_short, n_freqs)\n        audio_original_length: length of original audio signal (in samples)\n        n_overlapping_frames: number of overlapping frames in the output\n\n    Returns:\n        array (n_times, n_freqs)\n    \"\"\"\n    if len(output.shape) != 3:\n        return None\n\n    n_olap = int(0.5 * n_overlapping_frames)\n    if n_olap > 0:\n        # remove half of the overlapping frames from beginning and end\n        output = output[:, n_olap:-n_olap, :]\n\n    output_shape = output.shape\n    n_output_frames_original = int(np.floor(audio_original_length * (ANNOTATIONS_FPS / AUDIO_SAMPLE_RATE)))\n    unwrapped_output = output.reshape(output_shape[0] * output_shape[1], output_shape[2])\n    return unwrapped_output[:n_output_frames_original, :]  # trim to original audio length\n\n\ndef run_inference(\n    audio_path: Union[pathlib.Path, str],\n    model_or_model_path: Union[Model, pathlib.Path, str],\n    debug_file: Optional[pathlib.Path] = None,\n) -> Dict[str, np.array]:\n    \"\"\"Run the model on the input audio path.\n\n    Args:\n        audio_path: The audio to run inference on.\n        model_or_model_path: A loaded Model or path to a serialized model to load.\n        debug_file: An optional path to output debug data to. Useful for testing/verification.\n\n    Returns:\n       A dictionary with the notes, onsets and contours from model inference.\n    \"\"\"\n    if isinstance(model_or_model_path, Model):\n        model = model_or_model_path\n    else:\n        model = Model(model_or_model_path)\n\n    # overlap 30 frames\n    n_overlapping_frames = 30\n    overlap_len = n_overlapping_frames * FFT_HOP\n    hop_size = AUDIO_N_SAMPLES - overlap_len\n\n    output: Dict[str, Any] = {\"note\": [], \"onset\": [], \"contour\": []}\n    for audio_windowed, _, audio_original_length in get_audio_input(audio_path, overlap_len, hop_size):\n        for k, v in model.predict(audio_windowed).items():\n            output[k].append(v)\n\n    unwrapped_output = {\n        k: unwrap_output(np.concatenate(output[k]), audio_original_length, n_overlapping_frames) for k in output\n    }\n\n    if debug_file:\n        with open(debug_file, \"w\") as f:\n            json.dump(\n                {\n                    \"audio_windowed\": audio_windowed.numpy().tolist(),\n                    \"audio_original_length\": audio_original_length,\n                    \"hop_size_samples\": hop_size,\n                    \"overlap_length_samples\": overlap_len,\n                    \"unwrapped_output\": {k: v.tolist() for k, v in unwrapped_output.items()},\n                },\n                f,\n            )\n\n    return unwrapped_output\n\n\nclass OutputExtensions(enum.Enum):\n    MIDI = \"mid\"\n    MODEL_OUTPUT_NPZ = \"npz\"\n    MIDI_SONIFICATION = \"wav\"\n    NOTE_EVENTS = \"csv\"\n\n\ndef verify_input_path(audio_path: Union[pathlib.Path, str]) -> None:\n    \"\"\"Verify that an input path is valid and can be processed\n\n    Args:\n        audio_path: Path to an audio file.\n\n    Raises:\n        ValueError: If the audio file is invalid.\n    \"\"\"\n    if not os.path.isfile(audio_path):\n        raise ValueError(f\"\ud83d\udea8 {audio_path} is not a file path.\")\n\n    if not os.path.exists(audio_path):\n        raise ValueError(f\"\ud83d\udea8 {audio_path} does not exist.\")\n\n\ndef verify_output_dir(output_dir: Union[pathlib.Path, str]) -> None:\n    \"\"\"Verify that an output directory is valid and can be processed\n\n    Args:\n        output_dir: Path to an output directory.\n\n    Raises:\n        ValueError: If the output directory is invalid.\n    \"\"\"\n    if not os.path.isdir(output_dir):\n        raise ValueError(f\"\ud83d\udea8 {output_dir} is not a directory.\")\n\n    if not os.path.exists(output_dir):\n        raise ValueError(f\"\ud83d\udea8 {output_dir} does not exist.\")\n\n\ndef build_output_path(\n    audio_path: Union[pathlib.Path, str],\n    output_directory: Union[pathlib.Path, str],\n    output_type: OutputExtensions,\n) -> pathlib.Path:\n    \"\"\"Create an output path and make sure it doesn't already exist.\n\n    Args:\n        audio_path: The original file path.\n        output_directory: The directory we will output to.\n        output_type: The type of output file we are creating.\n\n    Raises:\n        IOError: If the generated path already exists.\n\n    Returns:\n        A new path in the output_directory with the stem audio_path and an extension\n        based on output_type.\n    \"\"\"\n    audio_path = str(audio_path)\n    if not isinstance(output_directory, pathlib.Path):\n        output_directory = pathlib.Path(output_directory)\n\n    basename, _ = os.path.splitext(os.path.basename(audio_path))\n\n    output_path = output_directory / f\"{basename}_basic_pitch.{output_type.value}\"\n\n    generating_file_message(output_type.name)\n\n    if output_path.exists():\n        raise IOError(\n            f\"  \ud83d\udea8 {str(output_path)} already exists and would be overwritten. Skipping output files for {audio_path}.\"\n        )\n\n    return output_path\n\n\ndef save_note_events(\n    note_events: List[Tuple[float, float, int, float, Optional[List[int]]]],\n    save_path: Union[pathlib.Path, str],\n) -> None:\n    \"\"\"Save note events to file\n\n    Args:\n        note_events: A list of note event tuples to save. Tuples have the format\n            (\"start_time_s\", \"end_time_s\", \"pitch_midi\", \"velocity\", \"list of pitch bend values\")\n        save_path: The location we're saving it\n    \"\"\"\n\n    with open(save_path, \"w\") as fhandle:\n        writer = csv.writer(fhandle, delimiter=\",\")\n        writer.writerow([\"start_time_s\", \"end_time_s\", \"pitch_midi\", \"velocity\", \"pitch_bend\"])\n        for start_time, end_time, note_number, amplitude, pitch_bend in note_events:\n            row = [start_time, end_time, note_number, int(np.round(127 * amplitude))]\n            if pitch_bend:\n                row.extend(pitch_bend)\n            writer.writerow(row)\n\n\ndef predict(\n    audio_path: Union[pathlib.Path, str],\n    model_or_model_path: Union[Model, pathlib.Path, str] = ICASSP_2022_MODEL_PATH,\n    onset_threshold: float = 0.5,\n    frame_threshold: float = 0.3,\n    minimum_note_length: float = 127.70,\n    minimum_frequency: Optional[float] = None,\n    maximum_frequency: Optional[float] = None,\n    multiple_pitch_bends: bool = False,\n    melodia_trick: bool = True,\n    debug_file: Optional[pathlib.Path] = None,\n    midi_tempo: float = 120,\n) -> Tuple[\n    Dict[str, np.array],\n    pretty_midi.PrettyMIDI,\n    List[Tuple[float, float, int, float, Optional[List[int]]]],\n]:\n    \"\"\"Run a single prediction.\n\n    Args:\n        audio_path: File path for the audio to run inference on.\n        model_or_model_path: A loaded Model or path to a serialized model to load.\n        onset_threshold: Minimum energy required for an onset to be considered present.\n        frame_threshold: Minimum energy requirement for a frame to be considered present.\n        minimum_note_length: The minimum allowed note length in milliseconds.\n        minimum_freq: Minimum allowed output frequency, in Hz. If None, all frequencies are used.\n        maximum_freq: Maximum allowed output frequency, in Hz. If None, all frequencies are used.\n        multiple_pitch_bends: If True, allow overlapping notes in midi file to have pitch bends.\n        melodia_trick: Use the melodia post-processing step.\n        debug_file: An optional path to output debug data to. Useful for testing/verification.\n    Returns:\n        The model output, midi data and note events from a single prediction\n    \"\"\"\n\n    with no_tf_warnings():\n        print(f\"Predicting MIDI for {audio_path}...\")\n\n        model_output = run_inference(audio_path, model_or_model_path, debug_file)\n        min_note_len = int(np.round(minimum_note_length / 1000 * (AUDIO_SAMPLE_RATE / FFT_HOP)))\n        midi_data, note_events = infer.model_output_to_notes(\n            model_output,\n            onset_thresh=onset_threshold,\n            frame_thresh=frame_threshold,\n            min_note_len=min_note_len,  # convert to frames\n            min_freq=minimum_frequency,\n            max_freq=maximum_frequency,\n            multiple_pitch_bends=multiple_pitch_bends,\n            melodia_trick=melodia_trick,\n            midi_tempo=midi_tempo,\n        )\n\n    if debug_file:\n        with open(debug_file) as f:\n            debug_data = json.load(f)\n        with open(debug_file, \"w\") as f:\n            json.dump(\n                {\n                    **debug_data,\n                    \"min_note_length\": min_note_len,\n                    \"onset_thresh\": onset_threshold,\n                    \"frame_thresh\": frame_threshold,\n                    \"estimated_notes\": [\n                        (\n                            float(start_time),\n                            float(end_time),\n                            int(pitch),\n                            float(amplitude),\n                            [int(b) for b in pitch_bends] if pitch_bends else None,\n                        )\n                        for start_time, end_time, pitch, amplitude, pitch_bends in note_events\n                    ],\n                },\n                f,\n            )\n\n    return model_output, midi_data, note_events\n\n\ndef predict_and_save(\n    audio_path_list: Sequence[Union[pathlib.Path, str]],\n    output_directory: Union[pathlib.Path, str],\n    save_midi: bool,\n    sonify_midi: bool,\n    save_model_outputs: bool,\n    save_notes: bool,\n    model_or_model_path: Union[Model, str, pathlib.Path],\n    onset_threshold: float = 0.5,\n    frame_threshold: float = 0.3,\n    minimum_note_length: float = 127.70,\n    minimum_frequency: Optional[float] = None,\n    maximum_frequency: Optional[float] = None,\n    multiple_pitch_bends: bool = False,\n    melodia_trick: bool = True,\n    debug_file: Optional[pathlib.Path] = None,\n    sonification_samplerate: int = 44100,\n    midi_tempo: float = 120,\n) -> None:\n    \"\"\"Make a prediction and save the results to file.\n\n    Args:\n        audio_path_list: List of file paths for the audio to run inference on.\n        output_directory: Directory to output MIDI and all other outputs derived from the model to.\n        save_midi: True to save midi.\n        sonify_midi: Whether or not to render audio from the MIDI and output it to a file.\n        save_model_outputs: True to save contours, onsets and notes from the model prediction.\n        save_notes: True to save note events.\n        model_or_model_path: A loaded Model or path to a serialized model to load.\n        onset_threshold: Minimum energy required for an onset to be considered present.\n        frame_threshold: Minimum energy requirement for a frame to be considered present.\n        minimum_note_length: The minimum allowed note length in milliseconds.\n        minimum_freq: Minimum allowed output frequency, in Hz. If None, all frequencies are used.\n        maximum_freq: Maximum allowed output frequency, in Hz. If None, all frequencies are used.\n        multiple_pitch_bends: If True, allow overlapping notes in midi file to have pitch bends.\n        melodia_trick: Use the melodia post-processing step.\n        debug_file: An optional path to output debug data to. Useful for testing/verification.\n        sonification_samplerate: Sample rate for rendering audio from MIDI.\n    \"\"\"\n    for audio_path in audio_path_list:\n        print(\"\")\n        try:\n            model_output, midi_data, note_events = predict(\n                pathlib.Path(audio_path),\n                model_or_model_path,\n                onset_threshold,\n                frame_threshold,\n                minimum_note_length,\n                minimum_frequency,\n                maximum_frequency,\n                multiple_pitch_bends,\n                melodia_trick,\n                debug_file,\n                midi_tempo,\n            )\n\n            if save_model_outputs:\n                model_output_path = build_output_path(audio_path, output_directory, OutputExtensions.MODEL_OUTPUT_NPZ)\n                try:\n                    np.savez(model_output_path, basic_pitch_model_output=model_output)\n                    file_saved_confirmation(OutputExtensions.MODEL_OUTPUT_NPZ.name, model_output_path)\n                except Exception as e:\n                    failed_to_save(OutputExtensions.MODEL_OUTPUT_NPZ.name, model_output_path)\n                    raise e\n\n            if save_midi:\n                try:\n                    midi_path = build_output_path(audio_path, output_directory, OutputExtensions.MIDI)\n                except IOError as e:\n                    raise e\n                try:\n                    midi_data.write(str(midi_path))\n                    file_saved_confirmation(OutputExtensions.MIDI.name, midi_path)\n                except Exception as e:\n                    failed_to_save(OutputExtensions.MIDI.name, midi_path)\n                    raise e\n\n            if sonify_midi:\n                midi_sonify_path = build_output_path(audio_path, output_directory, OutputExtensions.MIDI_SONIFICATION)\n                try:\n                    infer.sonify_midi(midi_data, midi_sonify_path, sr=sonification_samplerate)\n                    file_saved_confirmation(OutputExtensions.MIDI_SONIFICATION.name, midi_sonify_path)\n                except Exception as e:\n                    failed_to_save(OutputExtensions.MIDI_SONIFICATION.name, midi_sonify_path)\n                    raise e\n\n            if save_notes:\n                note_events_path = build_output_path(audio_path, output_directory, OutputExtensions.NOTE_EVENTS)\n                try:\n                    save_note_events(note_events, note_events_path)\n                    file_saved_confirmation(OutputExtensions.NOTE_EVENTS.name, note_events_path)\n                except Exception as e:\n                    failed_to_save(OutputExtensions.NOTE_EVENTS.name, note_events_path)\n                    raise e\n        except Exception as e:\n            raise e\n", "import_text": ["csv", "enum", "json", "logging", "os", "pathlib", "typing.Any", "typing.Dict", "typing.Iterable", "typing.List", "typing.Optional", "typing.Sequence", "typing.Tuple", "typing.Union", "typing.cast", "basic_pitch.CT_PRESENT", "basic_pitch.ICASSP_2022_MODEL_PATH", "basic_pitch.ONNX_PRESENT", "basic_pitch.TF_PRESENT", "basic_pitch.TFLITE_PRESENT", "numpy", "numpy.typing", "librosa", "pretty_midi", "basic_pitch.constants.AUDIO_SAMPLE_RATE", "basic_pitch.constants.AUDIO_N_SAMPLES", "basic_pitch.constants.ANNOTATIONS_FPS", "basic_pitch.constants.FFT_HOP", "basic_pitch.commandline_printing.generating_file_message", "basic_pitch.commandline_printing.no_tf_warnings", "basic_pitch.commandline_printing.file_saved_confirmation", "basic_pitch.commandline_printing.failed_to_save", "basic_pitch.note_creation"], "prompt": "\"\"\"\nDescription: This function is used to get audio input from a given audio file path. It also takes two parameters, overlap_len and hop_size.\n\nArgs:\n    audio_path (Union[pathlib.Path, str]): The path to the audio file. It can be either a string or a pathlib.Path object.\n    overlap_len (int): The length of the overlap between windows. It must be an even number.\n    hop_size (int): The size of the hop between windows.\n\nReturns:\n    Iterable[Tuple[npt.NDArray[np.float32], Dict[str, float], int]]: An iterable of tuples. Each tuple contains a 1D numpy array of float32, a dictionary with string keys and float values, and an integer. The numpy array represents the audio window, the dictionary contains metadata about the window, and the integer represents the original length of the audio.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Read wave file (as mono), pad appropriately, and return as\n    windowed signal, with window length = AUDIO_N_SAMPLES\n\n    Returns:\n        audio_windowed: tensor with shape (n_windows, AUDIO_N_SAMPLES, 1)\n            audio windowed into fixed length chunks\n        window_times: list of {'start':.., 'end':...} objects (times in seconds)\n        audio_original_length: int\n            length of original audio file, in frames, BEFORE padding.\n\n    \"\"\"", "function_dependencies": ["librosa.load", "numpy.concatenate", "numpy.zeros", "numpy.expand_dims"], "project_create_time": "2022-05-03T09:10:03+00:00", "project_update_time": "2024-04-17T17:18:50+00:00", "file_create_time": "2022-05-12T04:57:18Z", "file_update_time": "2024-04-19T23:52:41Z", "function_update_time": "2023-09-30T20:41:23Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["numpy.concatenate", "numpy.expand_dims"], "test_function": [{"file_path": "/basic-pitch-v0.3.3/basic-pitch-0.3.3/tests/test_inference.py", "class_name": null, "function_name": "test_get_audio_input", "code": "\ndef test_get_audio_input() -> None:\n    test_audio_path = RESOURCES_PATH / \"vocadito_10.wav\"\n    audio, _ = librosa.load(str(test_audio_path), sr=AUDIO_SAMPLE_RATE, mono=True)\n    overlap_len = 30 * FFT_HOP\n    audio = np.concatenate([np.zeros((overlap_len // 2,), dtype=np.float32), audio])\n    audio_windowed: List[npt.NDArray[np.float32]] = []\n    window_times: List[Dict[str, float]] = []\n    for audio_window, window_time, original_length in inference.get_audio_input(\n        test_audio_path, overlap_len, AUDIO_N_SAMPLES - overlap_len\n    ):\n        audio_windowed.append(audio_window)\n        window_times.append(window_time)\n    audio_windowed = np.array(audio_windowed)\n    assert len(audio_windowed) == 6\n    assert len(window_times) == 6\n    for time in window_times:\n        assert time[\"start\"] <= time[\"end\"]\n    np.testing.assert_equal(audio[:AUDIO_N_SAMPLES], np.squeeze(audio_windowed[0]))\n\n    assert original_length == 200607"}]}, {"git_group": "joeynmt", "git_name": "joeynmt", "version": "v2.3", "language": "Python", "project_name": "joeynmt-v2.3.zip", "file_path": "/joeynmt-v2.3/joeynmt-2.3/joeynmt/loss.py", "file_name": "loss.py", "focal_class": "XentLoss", "focal_name": "_smooth_targets", "focal_parameter": [], "solution": "    def _smooth_targets(self, targets: Tensor, vocab_size: int) -> Variable:\n        # batch*seq_len x vocab_size\n        smooth_dist = targets.new_zeros((targets.size(0), vocab_size)).float()\n        # fill distribution uniformly with smoothing\n        smooth_dist.fill_(self.smoothing / (vocab_size - 2))\n        # assign true label the probability of 1-smoothing (\"confidence\")\n        smooth_dist.scatter_(1, targets.unsqueeze(1).data, 1.0 - self.smoothing)\n        # give padding probability of 0 everywhere\n        smooth_dist[:, self.pad_index] = 0\n        # masking out padding area (sum of probabilities for padding area = 0)\n        padding_positions = torch.nonzero(\n            targets.data == self.pad_index, as_tuple=False\n        )\n        if len(padding_positions) > 0:\n            smooth_dist.index_fill_(0, padding_positions.squeeze(), 0.0)\n        return Variable(smooth_dist, requires_grad=False)", "function_signature": "def _smooth_targets(self, targets: Tensor, vocab_size: int) -> Variable :", "left_context": "# coding: utf-8\n\"\"\"\nModule to implement training loss\n\"\"\"\nimport torch\nfrom torch import Tensor, nn\nfrom torch.autograd import Variable\nfrom torch.nn.modules.loss import _Loss\n\n\nclass XentLoss(nn.Module):\n    \"\"\"\n    Cross-Entropy Loss with optional label smoothing\n    \"\"\"\n\n    def __init__(self, pad_index: int, smoothing: float = 0.0):\n        super().__init__()\n        self.smoothing = smoothing\n        self.pad_index = pad_index\n        self.criterion: _Loss  # (type annotation)\n        if self.smoothing <= 0.0:\n            # standard xent loss\n            self.criterion = nn.NLLLoss(ignore_index=self.pad_index, reduction=\"sum\")\n        else:\n            # custom label-smoothed loss, computed with KL divergence loss\n            self.criterion = nn.KLDivLoss(reduction=\"sum\")\n", "right_context": "\n    def _reshape(self, log_probs: Tensor, targets: Tensor) -> Tensor:\n        vocab_size = log_probs.size(-1)\n\n        # reshape log_probs to (batch*seq_len x vocab_size)\n        log_probs_flat = log_probs.contiguous().view(-1, vocab_size)\n\n        if self.smoothing > 0:\n            targets_flat = self._smooth_targets(\n                targets=targets.contiguous().view(-1), vocab_size=vocab_size\n            )\n            # targets: distributions with batch*seq_len x vocab_size\n            assert log_probs_flat.size() == targets_flat.size(), (\n                log_probs.size(),\n                targets_flat.size(),\n            )\n        else:\n            # targets: indices with batch*seq_len\n            targets_flat = targets.contiguous().view(-1)\n            assert log_probs_flat.size(0) == targets_flat.size(0), (\n                log_probs.size(0),\n                targets_flat.size(0),\n            )\n\n        return log_probs_flat, targets_flat\n\n    def forward(self, log_probs: Tensor, **kwargs) -> Tensor:\n        \"\"\"\n        Compute the cross-entropy between logits and targets.\n\n        If label smoothing is used, target distributions are not one-hot, but\n        \"1-smoothing\" for the correct target token and the rest of the\n        probability mass is uniformly spread across the other tokens.\n\n        :param log_probs: log probabilities as predicted by model\n        :return: logits\n        \"\"\"\n        assert \"trg\" in kwargs\n        log_probs, targets = self._reshape(log_probs, kwargs[\"trg\"])\n\n        # compute loss\n        logits = self.criterion(log_probs, targets)\n        return logits\n\n    def __repr__(self):\n        return (\n            f\"{self.__class__.__name__}(criterion={self.criterion}, \"\n            f\"smoothing={self.smoothing})\"\n        )\n", "import_text": ["torch", "torch.Tensor", "torch.nn", "torch.autograd.Variable", "torch.nn.modules.loss._Loss"], "prompt": "\"\"\"\nDescription: This function is used to smooth target distributions for a given vocabulary size.\n\nArgs:\n    targets (Tensor): The target tensor to be smoothed.\n    vocab_size (int): The size of the vocabulary.\n\nReturns:\n    Variable: The smoothed target distribution as a Variable.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"\n        Smooth target distribution. All non-reference words get uniform\n        probability mass according to \"smoothing\".\n\n        :param targets: target indices, batch*seq_len\n        :param vocab_size: size of the output vocabulary\n        :return: smoothed target distributions, batch*seq_len x vocab_size\n        \"\"\"", "function_dependencies": ["torch.nonzero", "torch.nonzero.squeeze", "torch.autograd.Variable"], "project_create_time": "2018-10-15T15:00:57+00:00", "project_update_time": "2024-04-10T09:50:49+00:00", "file_create_time": "2018-10-15T15:10:25Z", "file_update_time": "2024-01-20T21:54:42Z", "function_update_time": "2018-10-15T15:10:25Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["torch.nonzero"], "test_function": [{"file_path": "/joeynmt-v2.3/joeynmt-2.3/test/unit/test_loss.py", "class_name": "TestXentLoss", "function_name": "test_label_smoothing", "code": "\n    def test_label_smoothing(self):\n        pad_index = 0\n        smoothing = 0.4\n        criterion = XentLoss(pad_index=pad_index, smoothing=smoothing)\n\n        # batch x seq_len x vocab_size: 3 x 2 x 5\n        predict = torch.FloatTensor([\n            [[0.1, 0.1, 0.6, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]],\n            [[0.1, 0.1, 0.6, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]],\n            [[0.1, 0.1, 0.6, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]],\n        ])\n\n        # batch x seq_len: 3 x 2\n        targets = torch.LongTensor([[2, 1], [2, 0], [1, 0]])\n\n        # test the smoothing function\n        # pylint: disable=protected-access\n        smoothed_targets = criterion._smooth_targets(\n            targets=targets.view(-1), vocab_size=predict.size(-1)\n        )\n        # pylint: enable=protected-access\n        torch.testing.assert_close(\n            smoothed_targets,\n            torch.Tensor([\n                [0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n                [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n                [0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n                [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n                [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n            ]),\n            rtol=1e-4,\n            atol=1e-4,\n        )\n        self.assertEqual(torch.max(smoothed_targets), 1 - smoothing)\n\n        # test the loss computation\n        v = criterion(predict.log(), **{\"trg\": targets})\n        self.assertAlmostEqual(v.item(), 2.1326, places=4)"}, {"file_path": "/joeynmt-v2.3/joeynmt-2.3/test/unit/test_loss.py", "class_name": "TestXentLoss", "function_name": "test_no_label_smoothing", "code": "\n    def test_no_label_smoothing(self):\n        pad_index = 0\n        smoothing = 0.0\n        criterion = XentLoss(pad_index=pad_index, smoothing=smoothing)\n\n        # batch x seq_len x vocab_size: 3 x 2 x 5\n        predict = torch.FloatTensor([\n            [[0.1, 0.1, 0.6, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]],\n            [[0.1, 0.1, 0.6, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]],\n            [[0.1, 0.1, 0.6, 0.1, 0.1], [0.1, 0.1, 0.6, 0.1, 0.1]],\n        ])\n\n        # batch x seq_len: 3 x 2\n        targets = torch.LongTensor([[2, 1], [2, 0], [1, 0]])\n\n        # test the smoothing function: should still be one-hot\n        # pylint: disable=protected-access\n        smoothed_targets = criterion._smooth_targets(\n            targets=targets.view(-1), vocab_size=predict.size(-1)\n        )\n        # pylint: enable=protected-access\n\n        self.assertEqual(torch.max(smoothed_targets), 1)\n        self.assertEqual(torch.min(smoothed_targets), 0)\n\n        torch.testing.assert_close(\n            smoothed_targets,\n            torch.Tensor([\n                [0.0, 0.0, 1.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 1.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0, 0.0],\n            ]),\n            rtol=1e-4,\n            atol=1e-4,\n        )\n\n        v = criterion(predict.log(), **{\"trg\": targets})\n        self.assertAlmostEqual(v.item(), 5.6268, places=4)"}]}, {"git_group": "google", "git_name": "ldif", "version": "master", "language": "Python", "project_name": "ldif-master.zip", "file_path": "/ldif-master/ldif-master/ldif/torch/sif.py", "file_name": "sif.py", "focal_class": "Sif", "focal_name": "from_flat_tensor", "focal_parameter": ["cls", "tensor", "symmetry_count"], "solution": "  def from_flat_tensor(cls, tensor, symmetry_count):\n    if not torch.is_tensor(tensor):\n      raise ValueError(f'Input is not a tensor, but a {type(tensor)}: {tensor}')\n    if len(tensor.shape) != 3 or tensor.shape[-1] != 10:\n      raise ValueError(f'Could not parse flat tensor due to shape: {tensor.shape}')\n    constants = tensor[:, :, :1]\n    centers = tensor[:, :, 1:4]\n    radii = tensor[:, :, 4:7]\n    rotations = tensor[:, :, 7:10]\n    return cls(constants, centers, radii, rotations, symmetry_count)", "function_signature": "def from_flat_tensor(cls, tensor, symmetry_count) :", "left_context": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Minimal support for SIF evaluation in pytorch.\"\"\"\n\nimport numpy as np\n\nimport torch\n\nfrom ldif.util import file_util\nfrom ldif.util.file_util import log\n\nimport sif_evaluation\n\n\ndef ensure_are_tensors(named_tensors):\n  for name, tensor in named_tensors.items():\n    if not (torch.is_tensor(tensor)):\n      raise ValueError(f'Argument {name} is not a tensor, it is a {type(tensor)}')\n\ndef ensure_type(v, t, name):\n  if not isinstance(v, t):\n    raise ValueError(f'Error: variable {name} has type {type(v)}, not the expected type {t}')\n\ndef _load_v1_txt(path):\n  \"\"\"Parses a SIF V1 text file, returning numpy arrays.\n  \n  Args:\n    path: string containing the path to the ASCII file.\n    \n  Returns:\n    A tuple of 4 elements:\n      constants: A numpy array of shape (element_count). The constant\n        associated with each SIF element.\n      centers: A numpy array of shape (element_count, 3). The centers of the\n        SIF elements.\n      radii: A numpy array of shape (element_count, 3). The axis-aligned\n        radii of the gaussian falloffs.\n      rotations: A numpy array of shape (element_count, 3). The euler-angle\n        rotations of the SIF elements.\n      symmetry_count: An integer. The number of elements which are left-right\n        symmetric.\n      features: A numpy array of shape (element_count, implicit_len). The LDIF\n        neural features, if they are present. \n  \"\"\"\n  lines = file_util.readlines(path)\n  if lines[0] != 'SIF':\n    raise ValueError(f'Could not parse {path} as a sif txt. First line was {lines[0]}')\n  shape_count, version, implicit_len = [int(x) for x in lines[1].split(' ')]\n  version += 1\n  if version != 1:\n    raise ValueError(f'This function can only parse v1 files. This version: {version}.')\n  symmetry_count = 0\n  last_was_symmetric = True\n  constants = []\n  centers = []\n  radii = []\n  rotations = []\n  features = []\n  for row in lines[2:]:\n    elts = row.split(' ')\n    if len(elts) != 11 + implicit_len:\n      raise ValueError('Failed to parse the following row with '\n          f'implicit_len {implicit_len}: {row}')\n    explicit_params = np.array([float(x) for x in elts[:10]], dtype=np.float32)\n    is_symmetric = bool(int(elts[10]))\n    if is_symmetric:\n      symmetry_count += 1\n      if not last_was_symmetric:\n        raise ValueError(f'File not supported by parser: row {row} is '\n            'symmetric but follows an asymmetric element.')\n    constants.append(explicit_params[0])\n    centers.append(explicit_params[1:4])\n    radii.append(explicit_params[4:7])\n    rotations.append(explicit_params[7:10])\n    if implicit_len > 0:\n      implicit_params = np.array([float(x) for x in elts[11:]], dtype=np.float32)\n      features.append(implicit_params)\n  constants = np.stack(constants)\n  centers = np.stack(centers)\n  radii = np.stack(radii)\n  rotations = np.stack(rotations)\n  features = np.stack(features) if features else None\n  # Radii have their sqrt stored for GAPS:\n  radii = radii * radii\n  return constants, centers, radii, rotations, symmetry_count, features\n\n\ndef _tile_for_symgroups(elements, symmetry_count):\n  \"\"\"Tiles an input tensor along its element dimension based on symmetry.\n\n  Args:\n    elements: Tensor with shape [batch_size, element_count, ...].\n\n  Returns:\n    Tensor with shape [batch_size, element_count + tile_count, ...]. The\n    elements have been tiled according to the model configuration's symmetry\n    group description.\n  \"\"\"\n  left_right_sym_count = symmetry_count\n  assert len(elements.shape) >= 3\n  # The first K elements get reflected with left-right symmetry (z-axis) as\n  # needed.\n  if left_right_sym_count:\n    first_k = elements[:, :left_right_sym_count, ...]\n    elements = torch.cat([elements, first_k], axis=1)\n  # TODO(kgenova) As additional symmetry groups are added, add their tiling.\n  return elements\n\n\ndef reflect_z(samples):\n  \"\"\"Reflects the sample locations across the planes specified in xyz.\n\n  Args:\n    samples: Tensor with shape [..., 3].\n\n  Returns:\n    Tensor with shape [..., 3]. The reflected samples.\n  \"\"\"\n  to_keep = samples[..., :-1]\n  to_reflect = samples[..., -1:]\n  return torch.cat([to_keep, -to_reflect], dim=-1)\n\n\ndef _generate_symgroup_samples(samples, element_count, symmetry_count):\n  \"\"\"Duplicates and transforms samples as needed for symgroup queries.\n\n  Args:\n    samples: Tensor with shape [batch_size, sample_count, 3].\n\n  Returns:\n    Tensor with shape [batch_size, effective_element_count, sample_count, 3].\n  \"\"\"\n  if len(samples.shape) != 3:\n    raise ValueError(f'Internal Error: Samples have shape {samples.shape}')\n  bs, sample_count = samples.shape[:2]\n  samples = torch.reshape(samples, [bs, 1, sample_count, 3]).expand([-1, element_count, -1, -1])\n\n  left_right_sym_count = symmetry_count\n  if left_right_sym_count:\n    first_k = samples[:, :left_right_sym_count, :, :]\n    first_k = reflect_z(first_k)\n    samples = torch.cat([samples, first_k], axis=1)\n  return samples\n\ndef _generate_symgroup_frames(world2local, symmetry_count):\n  \"\"\"Duplicates and adds reflection transformation for symgroup matrices.\n\n  Args:\n    world2local: Tensor with shape [bs, element_count, 4, 4].\n    symmetry_count: Int, at most element_count. The number of LR symmetric frames.\n\n  Returns:\n    Tensor with shape [bs, effective_element_count, 4, 4].\n  \"\"\"\n  if len(world2local.shape) != 4:\n    raise ValueError(f'Invalid world2local shape: {world2local.shape}')\n  if symmetry_count:\n    bs = world2local.shape[0]\n    reflector = torch.eye(4).cuda()\n    first_two_rows = reflector[:2, :]\n    last_row = reflector[3:, :]\n    reflected = -1 * reflector[2:3, :]\n    reflector = torch.cat([first_two_rows, reflected, last_row], dim=0)\n    reflector = torch.reshape(reflector, [1, 1, 4, 4]).expand([bs, symmetry_count,\n        -1, -1])\n    first_k = world2local[:, :symmetry_count, :, :]\n    first_k = torch.matmul(first_k, reflector)\n    world2local = torch.cat([world2local, first_k], axis=1)\n  return world2local\n\n\n\nclass Sif(object):\n  \"\"\"A SIF for loading from txts, packing into a tensor, and evaluation.\"\"\"\n\n  def __init__(self, constants, centers, radii, rotations, symmetry_count):\n    \"\"\"The real initializer (from tensors). Not intended for direct use (see below).\"\"\"\n    ensure_are_tensors({'constants': constants, 'centers': centers,\n        'radii': radii, 'rotations': rotations})\n    if not (isinstance(symmetry_count, int)):\n      raise ValueError(f'symmetry_count is of type: {type(symmetry_count)}.')\n    has_batch_dim = len(centers.shape) == 3\n    if not has_batch_dim and len(centers.shape) != 2:\n      raise ValueError(f'Unable to parse input tensor shape: {centers.shape}')\n    bs = centers.shape[0] if has_batch_dim else 1\n    element_count = centers.shape[-2]\n    assert isinstance(element_count, int)\n    self._constants = torch.reshape(constants, (bs, element_count, 1))\n    self._centers = torch.reshape(centers, (bs, element_count, 3))\n    self._radii = torch.reshape(radii, (bs, element_count, 3))\n    self._rotations = torch.reshape(rotations, (bs, element_count, 3))\n    self.bs = bs\n    self.element_count = element_count\n    self.symmetry_count = symmetry_count\n\n  @classmethod\n  def from_file(cls, path):\n    \"\"\"Generates a SIF object from one or more txt files.\n    \n    Args:\n      path: Either a string containing the path to a txt file, or a list of\n        one or more strings, each containing a path to a text file.\n\n    Returns:\n      A SIF object. It will have a batch dimension containing each of the\n      txts in order, or no batch dimension if only one path was provided.\n    \"\"\"\n    if len(path) == 1 or isinstance(path, str):\n      loaded = _load_v1_txt(path)\n      symmetry_count, features = loaded[-2:]\n      explicits = [torch.Tensor(x).cuda() for x in loaded[:-2]]\n      # TODO(kgenova) Consider adding support to ignore the features, rather\n      # than just crashing. It would be fine to just throw them away.\n      if features is not None:\n        raise ValueError(f'This class cannot handle LDIFs, only SIFs.')\n      return cls(*explicits, symmetry_count)\n    flattened_shapes = []\n    symc = None\n    ec = None\n    for p in path:\n      shape = cls.from_file(p)\n      flat, cur_symc = shape.to_flat_tensor()\n      flattened_shapes.append(flat)\n      if symc is None:\n        symc = cur_symc\n      if symc != cur_symc:\n        raise ValueError('Trying to make a batched SIF with mismatched '\n          f'symmetry: {symc} vs {cur_symc}')\n      if ec is None:\n        ec = shape.element_count\n      if ec != shape.element_count:\n        raise ValueError('Trying to make a batched SIF with mismatched '\n          f'element counts: {ec} vs {shape.element_count}')\n    return cls.from_flat_tensor(torch.cat(flattened_shapes), symc)\n\n  @classmethod", "right_context": "     \n\n  def to_flat_tensor(self):\n    \"\"\"Generates a single tensor from the SIF. Can be batched with torch.cat.\n\n    Works with either single or batch SIFs. Useful for preloading the SIFs\n    so loading is not a bottleneck during training.\n    \n    Returns:\n      1) A tensor with shape [bs, element_count, 10]\n      2) An int. The symmetry count, needed to restore the SIFs. Note that\n        this must be the same for all SIFs in a batch, which is why only\n        a single int is returned. \n    \"\"\"\n    flat = torch.cat((self._constants, self._centers, self._radii, self._rotations),\n        dim=2)\n    return flat, self.symmetry_count\n  \n  def rbf_influence(self, samples):\n    \"\"\"Evaluates the influence of each RBF in the SIF at each sample.\n\n    Args:\n      samples: A tensor containing the samples, in the SIF global frame.\n        Has shape (sample_count, 3) or (bs, sample_count_3).\n\n    Returns:\n      A tensor with shape (sample_count, effective_element_count) or\n      (bs, sample_count, effective_element_count). The 'effective' element\n      count may be higher than the element count, depending on the symmetry\n      settings of the SIF. In the case were the SIF is at least partially\n      symmetric, then some elements have multiple RBF weights- their main\n      weight (given first) and the weight associated with the 'shadow'\n      element(s) transformed by their symmetry matrix. See get_symmetry_map()\n      for a mapping from original element indices to their symmetric \n      counterparts. Regardless of additional 'shadow' elements, the first \n      element_count RBF weights correspond to the 'real' elements with no\n      symmetry transforms applied, in order.\n    \"\"\"\n    if len(samples.shape) == 2:\n      if self.bs != 1:\n        raise ValueError('Samples must have a batch dimension if the SIF does.'\n            f' Input sample shape was {samples.shape} and SIF bs is {self.bs}')\n      samples = torch.unsqueeze(samples, dim=0)\n\n    samples = _generate_symgroup_samples(samples, self.element_count,\n      self.symmetry_count)\n    weights = sif_evaluation.compute_rbf_influences(\n      self._tiled_centers, self._tiled_radii, self._tiled_rotations, samples)\n    assert len(weights.shape) == 4\n    assert weights.shape[-1] == 1\n    # Currently last dim is always 1 and it's [bs, eec, sc, 1], not\n    # [bs, sc, eec] or [sc, eec] as needed\n    weights = weights[..., 0]\n    weights = torch.transpose(weights, -2, -1)\n    if self.bs == 1:\n      return weights[0, ...]\n    return weights\n\n  @property\n  def effective_element_count(self):\n    \"\"\"The number of elements, accounting for symmetry.\"\"\"\n    return self.element_count + self.symmetry_count\n\n  @property\n  def constants(self):\n    \"\"\"The constant parameters associated with the SIF.\n\n    Returns:\n      A tensor with shape (effective_element_count) or\n      (bs, effective_element_count). See rbf_influence for an explanation\n      of how to 'effective' samples.\n    \"\"\"\n    if self.bs == 1:\n      return self._tiled_constants[0, :, 0]\n    return self._tiled_constants[:, :, 0]\n\n  @property\n  def _tiled_constants(self):\n    \"\"\"The constants, tiled to account for symmetry.\"\"\"\n    if not hasattr(self, '__tiled_constants'):\n      self.__tiled_constants = _tile_for_symgroups(self._constants, self.symmetry_count)\n    return self.__tiled_constants\n  \n  @property\n  def _tiled_centers(self):\n    \"\"\"The centers, tiled to account for symmetry.\"\"\"\n    if not hasattr(self, '__tiled_centers'):\n      self.__tiled_centers = _tile_for_symgroups(self._centers, self.symmetry_count)\n    return self.__tiled_centers\n\n  @property\n  def _tiled_radii(self):\n    \"\"\"The radii, tiled to account for symmetry.\"\"\"\n    if not hasattr(self, '__tiled_radii'):\n      self.__tiled_radii = _tile_for_symgroups(self._radii, self.symmetry_count)\n    return self.__tiled_radii\n\n  @property\n  def _tiled_rotations(self):\n    \"\"\"The rotations, tiled to account for symmetry.\"\"\"\n    if not hasattr(self, '__tiled_rotations'):\n      self.__tiled_rotations = _tile_for_symgroups(self._rotations, self.symmetry_count)\n    return self.__tiled_rotations\n\n  @property\n  def world2local(self):\n    \"\"\"The 4x4 transformation matrices associated with the SIF elements.\n\n    Returns:\n      A tensor of shape (effective_element_count, 4, 4) or \n      (bs, effective_element_count, 4, 4). See rbf_influence for an explanation\n      of element_count vs effective_element_count.\n    \"\"\"\n    if not hasattr(self, '_world2local'):\n      self._world2local = sif_evaluation.compute_world2local(self._centers,\n          self._radii, self._rotations)\n      self._world2local = _generate_symgroup_frames(self._world2local,\n          self.symmetry_count)\n      if self.bs == 1:\n        self._world2local = torch.reshape(self._world2local,\n            [self.effective_element_count, 4, 4])\n    return self._world2local\n\n  def eval(self, samples):\n    \"\"\"Evaluates the SIF at the samples.\n\n    Args:\n      samples: A tensor of shape (sample_count, 3) or (bs, sample_count, 3).\n        The locations to evaluate the SIF, in the SIF's world coordinate frame.\n\n    Returns:\n      A tensor of shape (sample_count) or (bs, sample_count). The value of the\n        SIF at each sample point. Typically, values less than -0.07 are inside\n        and values greater than -0.07 are outside.\n    \"\"\"\n    # TODO(kgenova) A future version of the SIF txt file should contain the\n    # isosurface used for inside/outside determination, so users don't have\n    # to keep that information around.\n    if self.bs > 1 and len(samples.shape) == 2:\n      samples = torch.unsqueeze(samples, dim=0).expand([self.bs, -1, -1])\n    cs = self.constants\n    sample_count = samples.shape[-1]\n    cs = torch.unsqueeze(cs, dim=-2)\n    rbfs = self.rbf_influence(samples)\n    result = cs * rbfs\n    return torch.sum(result, dim=-1)\n\n\n", "import_text": ["numpy", "torch", "ldif.util.file_util", "ldif.util.file_util.log", "sif_evaluation"], "prompt": "\"\"\"\nDescription: This function is used to create an instance of the class from a flat tensor.\n\nArgs:\n    cls (type): The class to which the function belongs.\n    tensor (torch.Tensor): The input tensor to be parsed.\n    symmetry_count (int): The number of symmetries.\n\nRaises:\n    ValueError: If the input is not a tensor or the shape of the tensor is not (n, m, 10).\n\nReturns:\n    cls: An instance of the class with the parsed tensor values.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Generates a batch of SIFs from a batched tensor.\n\n    The symmetry_count is an integer because for batched SIFs, it is required\n    that all SIFs share the same symmetry count.\n\n    Args:\n      tensor: A single tensor previously generated by to_flat_tensor.\n      symmetry_count: The symmetry count variable for the SIFs (an int).\n\n    Returns:\n      A Sif object that can evaluate a batch of SIFs at once.\n    \"\"\"", "function_dependencies": ["torch.is_tensor"], "project_create_time": "2020-06-15T19:22:51+00:00", "project_update_time": "2024-03-26T14:30:13+00:00", "file_create_time": "2020-08-06T05:50:08Z", "file_update_time": "2020-08-11T03:06:50Z", "function_update_time": "2020-08-06T05:50:08Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["torch.is_tensor"], "test_function": [{"file_path": "/ldif-master/ldif-master/ldif/torch/sif_test.py", "class_name": "TestSif", "function_name": "test_batching", "code": "\n  def test_batching(self):\n    flattened_shapes = []\n    test_names = ['b831f60f211435df5bbc861d0124304c',\n        'b7eefc4c25dd9e49238581dd5a8af82c']\n    paths = [os.path.join(TEST_DATA_DIR, name + '.txt') for name in test_names]\n    for path in paths:\n      shape = sif.Sif.from_file(path)\n      flattened_shapes.append(shape.to_flat_tensor())\n    flats = [x[0] for x in flattened_shapes]\n    symcs = [x[1] for x in flattened_shapes]\n\n    for symc in symcs:\n      self.assertEqual(symc, symcs[0])\n\n    batched_tensor = torch.cat(flats, dim=0)\n    batched_sif = sif.Sif.from_flat_tensor(batched_tensor, symcs[0])\n    self.assertEqual(batched_sif.bs, 2)\n    auto_batched_sif = sif.Sif.from_file(paths)\n    self._ensure_sifs_equal(batched_sif, auto_batched_sif, -1)"}]}, {"git_group": "open2c", "git_name": "bioframe", "version": "v0.6.4", "language": "Python", "project_name": "bioframe-v0.6.4.zip", "file_path": "/bioframe-v0.6.4/bioframe-0.6.4/bioframe/core/checks.py", "file_name": "checks.py", "focal_class": null, "focal_name": "is_viewframe", "focal_parameter": ["region_df"], "solution": "def is_viewframe(region_df,\n                 raise_errors=False,\n                 view_name_col=\"name\",\n                 cols=None):\n\n    ck1, sk1, ek1 = _get_default_colnames() if cols is None else cols\n\n    if not _verify_columns(region_df, [ck1, sk1, ek1, view_name_col],\n                           return_as_bool=True):\n        if raise_errors:\n            raise TypeError(\"Invalid view: invalid column names\")\n        return False\n\n    if not is_bedframe(region_df, cols=cols):\n        if raise_errors:\n            raise ValueError(\"Invalid view: not a bedframe\")\n        return False\n\n    if pd.isna(region_df).values.any():\n        if raise_errors:\n            raise ValueError(\"Invalid view: cannot contain NAs\")\n        return False\n\n    if len(set(region_df[view_name_col])) < \\\n       len(region_df[view_name_col].values):\n        if raise_errors:\n            raise ValueError(\"Invalid view: entries in \\\n                region_df[view_name_col] must be unique\")\n        return False\n\n    if is_overlapping(region_df, cols=cols):\n        if raise_errors:\n            raise ValueError(\"Invalid view: entries must be non-overlapping\")\n        return False\n\n    return True", "function_signature": "def is_viewframe(region_df,\n                 raise_errors=False,\n                 view_name_col=\"name\",\n                 cols=None) :", "left_context": "import numpy as np\nimport pandas as pd\n\nfrom .. import ops\nfrom . import construction\nfrom .specs import _get_default_colnames, _verify_column_dtypes, _verify_columns\n\n__all__ = [\n    \"is_bedframe\",\n    \"is_cataloged\",\n    \"is_overlapping\",\n    \"is_viewframe\",\n    \"is_contained\",\n    \"is_covering\",\n    \"is_tiling\",\n    \"is_sorted\",\n]\n\n\ndef is_bedframe(\n    df,\n    raise_errors=False,\n    cols=None,\n):\n    \"\"\"\n    Checks that required bedframe properties are satisfied for dataframe `df`.\n\n    This includes:\n\n    - chrom, start, end columns\n    - columns have valid dtypes\n    - for each interval, if any of chrom, start, end are null, then all are\n        null\n    - all starts < ends.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n\n    raise_errors : bool, optional [default: False]\n        If True, raises errors instead of returning a boolean False for invalid\n        properties.\n\n    cols : (str, str, str) or None\n        The names of columns containing the chromosome, start and end of the\n        genomic intervals, provided separately for each set. The default\n        values are 'chrom', 'start', 'end'.\n\n    Returns\n    -------\n    is_bedframe:bool\n\n    Notes\n    -----\n    Valid dtypes for chrom are object, string, or categorical.\n    Valid dtypes for start and end are int/Int64Dtype.\n    \"\"\"\n    ck1, sk1, ek1 = _get_default_colnames() if cols is None else cols\n\n    if not _verify_columns(df, [ck1, sk1, ek1], return_as_bool=True):\n        if raise_errors:\n            raise TypeError(\"Invalid bedFrame: Invalid column names\")\n        return False\n\n    if not _verify_column_dtypes(df, cols=[ck1, sk1, ek1],\n                                 return_as_bool=True):\n        if raise_errors:\n            raise TypeError(\"Invalid bedFrame: Invalid column dtypes\")\n        return False\n\n    nan_intervals = pd.isnull(df[[ck1, sk1, ek1]])\n    if (~(~nan_intervals.any(axis=1) | nan_intervals.all(axis=1))).any():\n        if raise_errors:\n            raise ValueError(\n                \"Invalid bedFrame: Invalid null values \"\n                \"(if any of chrom, start, end are null, then all must be null)\"\n            )\n        return False\n\n    if ((df[ek1] - df[sk1]) < 0).any():\n        if raise_errors:\n            raise ValueError(f\"Invalid bedframe: starts exceed ends for \"\n                             f\"{sum((df[ek1] - df[sk1]) < 0)} intervals\")\n        return False\n\n    return True\n\n\ndef is_cataloged(df,\n                 view_df,\n                 raise_errors=False,\n                 df_view_col=\"view_region\",\n                 view_name_col=\"name\"):\n    \"\"\"\n    Tests if all region names in `df[df_view_col]` are present in\n    `view_df[view_name_col]`.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n\n    view_df : pandas.DataFrame\n\n    raise_errors : bool\n        If True, raises errors instead of returning a boolean False for invalid\n        properties. Default False.\n\n    df_view_col: str\n        Name of column from df that indicates region in view.\n\n    view_name_col: str\n        Name of column from view that specifies  region name.\n\n    Returns\n    -------\n    is_cataloged:bool\n\n    Notes\n    -----\n    Does not check if names in `view_df[view_name_col]` are unique.\n\n    \"\"\"\n    if not _verify_columns(df, [df_view_col], return_as_bool=True):\n        if raise_errors:\n            raise ValueError(f\"Could not find `{df_view_col}` column in df\")\n        return False\n\n    if not _verify_columns(view_df, [view_name_col], return_as_bool=True):\n        if raise_errors:\n            raise ValueError(f\"Could not find \\\n                `{view_name_col}` \\\n                column in view_df\")\n        return False\n\n    if not set(df[df_view_col].copy().dropna().values).issubset(\n            set(view_df[view_name_col].values)):\n        if raise_errors:\n            missing_regions = set(df[df_view_col].values).difference(\n                set(view_df[view_name_col].values))\n            raise ValueError(\n                f\"The following regions in df[df_view_col] not in \"\n                f\"view_df[view_name_col]: \\n{missing_regions}\")\n        return False\n\n    return True\n\n\ndef is_overlapping(df, cols=None):\n    \"\"\"\n    Tests if any genomic intervals in a bioframe `df` overlap.\n\n    Also see :func:`bioframe.ops.merge()`.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n\n    cols : (str, str, str) or None\n        The names of columns containing the chromosome, start and end of the\n        genomic intervals, provided separately for each set. The default\n        values are 'chrom', 'start', 'end'.\n\n    Returns\n    -------\n    is_overlapping:bool\n\n    \"\"\"\n    from ..ops import merge\n\n    ck1, sk1, ek1 = _get_default_colnames() if cols is None else cols\n\n    df_merged = merge(df, cols=cols)\n\n    total_interval_len = np.sum((df[ek1] - df[sk1]).values)\n    total_interval_len_merged = np.sum(\n        (df_merged[ek1] - df_merged[sk1]).values)\n\n    if total_interval_len > total_interval_len_merged:\n        return True\n    else:\n        return False\n\n", "right_context": "\n\ndef is_contained(\n    df,\n    view_df,\n    raise_errors=False,\n    df_view_col=None,\n    view_name_col=\"name\",\n    cols=None,\n    cols_view=None,\n):\n    \"\"\"\n    Tests if all genomic intervals in a bioframe `df` are cataloged and do not\n    extend beyond their associated region in the view `view_df`.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n\n    view_df : pandas.DataFrame\n        Valid viewframe.\n\n    raise_errors : bool\n        If True, raises errors instead of returning a boolean False for invalid\n        properties. Default False.\n\n    df_view_col:\n        Column from df used to associate interviews with view regions.\n        Default `view_region`.\n\n    view_name_col:\n        Column from view_df with view region names. Default `name`.\n\n    cols: (str, str, str)\n        Column names for chrom, start, end in df.\n    cols_view: (str, str, str)\n        Column names for chrom, start, end in view_df.\n\n    Returns\n    -------\n    is_contained:bool\n\n    \"\"\"\n    from ..ops import trim\n    ck1, sk1, ek1 = _get_default_colnames() if cols is None else cols\n    ck2, sk2, ek2 = _get_default_colnames() if cols_view is None else cols_view\n    if df_view_col is None:\n        try:\n            df_view_assigned = ops.overlap(df,\n                                           view_df,\n                                           cols1=cols,\n                                           cols2=cols_view)\n            # ek2 = end_ is the default value\n            assert (df_view_assigned[ek2 + \"_\"].isna()).sum() == 0\n            # sk2 = start_ is the default value\n            assert (df_view_assigned[sk2 + \"_\"].isna()).sum() == 0\n            assert (df_view_assigned[ek1] <= df_view_assigned[ek2 + \"_\"]).all()\n            # ek1 = end is the default value\n            # sk1 = start is the default value\n            assert (df_view_assigned[sk1] >= df_view_assigned[sk2 + \"_\"]).all()\n        except AssertionError:\n            if raise_errors:\n                raise AssertionError(\"df not contained in view_df\")\n            else:\n                return False\n        return True\n\n    if not is_cataloged(\n            df, view_df, df_view_col=df_view_col, view_name_col=view_name_col):\n        if raise_errors:\n            raise ValueError(\"df not cataloged in view_df\")\n        return False\n\n    df_trim = trim(df,\n                   view_df=view_df,\n                   df_view_col=df_view_col,\n                   view_name_col=view_name_col,\n                   cols=cols,\n                   cols_view=cols_view)\n\n    is_start_trimmed = np.any(df[sk1].values != df_trim[sk1].values)\n    is_end_trimmed = np.any(df[ek1].values != df_trim[ek1].values)\n\n    if is_start_trimmed or is_end_trimmed:\n        if raise_errors:\n            raise ValueError(\"df not contained in view_df\")\n        return False\n    else:\n        return True\n\n\ndef is_covering(df, view_df, view_name_col=\"name\", cols=None, cols_view=None):\n    \"\"\"\n    Tests if a view `view_df` is covered by the set of genomic intervals in\n    the bedframe `df`.\n\n    This test is true if ``complement(df,view_df)`` is empty. Also note this\n    test ignores regions assigned to intervals in `df` since regions are\n    re-assigned in :func:`bioframe.ops.complement`.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n\n    view_df : pandas.DataFrame\n        Valid viewFrame.\n\n    view_name_col:\n        Column from view_df with view region names. Default `name`.\n\n    cols : (str, str, str) or None\n        The names of columns containing the chromosome, start and end of the\n        genomic intervals, provided separately for each set. The default\n        values are 'chrom', 'start', 'end'.\n\n    cols_view: (str, str, str) or None\n        The names of columns containing the chromosome, start and end of the\n        genomic intervals in view_df, provided separately for\n        each set. The default\n        values are 'chrom', 'start', 'end'.\n\n    Returns\n    -------\n    is_covering:bool\n\n    \"\"\"\n    from ..ops import complement\n\n    if complement(\n            df,\n            view_df=view_df,\n            view_name_col=view_name_col,\n            cols=cols,\n            cols_view=cols_view,\n    ).empty:\n        return True\n    else:\n        return False\n\n\ndef is_tiling(\n    df,\n    view_df,\n    raise_errors=False,\n    df_view_col=\"view_region\",\n    view_name_col=\"name\",\n    cols=None,\n    cols_view=None,\n):\n    \"\"\"\n    Tests if a view `view_df` is tiled by the set of genomic intervals in the\n    bedframe `df`.\n\n    This is true if:\n\n    - df is not overlapping\n    - df is covering view_df\n    - df is contained in view_df\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n\n    view_df : pandas.DataFrame\n        valid viewFrame\n\n    raise_errors : bool\n        If True, raises errors instead of returning a boolean False for invalid\n        properties. Default False.\n\n    df_view_col: str\n        Name of column from df that indicates region in view.\n\n    view_name_col: str\n        Name of column from view that specifies unique region name.\n\n    cols : (str, str, str) or None\n        The names of columns containing the chromosome, start and end of the\n        genomic intervals, provided separately for each set. The default\n        values are 'chrom', 'start', 'end'.\n    cols_view: (str, str, str) or None\n        The names of columns containing the chromosome, start and end of the\n        genomic intervals in view_df, provided\n        separately for each set. The default\n        values are 'chrom', 'start', 'end'.\n\n    Returns\n    -------\n    is_tiling:bool\n\n    \"\"\"\n\n    view_df = construction.make_viewframe(view_df,\n                                          view_name_col=view_name_col,\n                                          cols=cols_view)\n\n    if is_overlapping(df, cols=cols):\n        if raise_errors:\n            raise ValueError(\"overlaps\")\n        return False\n    if not is_covering(df,\n                       view_df,\n                       view_name_col=view_name_col,\n                       cols=cols,\n                       cols_view=cols_view):\n        if raise_errors:\n            raise ValueError(\"not covered\")\n        return False\n    if not is_contained(df,\n                        view_df,\n                        df_view_col=df_view_col,\n                        view_name_col=view_name_col,\n                        cols=cols,\n                        cols_view=cols_view):\n        if raise_errors:\n            raise ValueError(\"not contained\")\n        return False\n    return True\n\n\ndef is_sorted(\n    df,\n    view_df=None,\n    reset_index=True,\n    df_view_col=None,\n    view_name_col=\"name\",\n    cols=None,\n    cols_view=None,\n):\n    \"\"\"\n    Tests if a bedframe is changed by sorting.\n\n    Also see :func:`bioframe.ops.sort_bedframe`.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n\n    view_df : pandas.DataFrame | dict-like\n        Optional view to pass to ``sort_bedframe``.\n        When it is dict-like :func:'bioframe.make_viewframe' will\n        be used to convert to viewframe. If view_df is not provided\n        df is assumed to be sorted by chrom and start.\n\n    reset_index : bool\n        Optional argument to pass to ``sort_bedframe``.\n\n    df_view_col: None | str\n        Name of column from df that indicates region in view.\n        If None, :func:'bioframe.assign_view' will be used to assign view\n        regions. Default None.\n\n    view_name_col: str\n        Name of column from view that specifies unique region name.\n\n    cols : (str, str, str) or None\n        The names of columns containing the chromosome, start and end of the\n        genomic intervals, provided separately for each set. The default\n        values are 'chrom', 'start', 'end'.\n\n    cols_view: (str, str, str) or None\n        The names of columns containing the chromosome, start and end of the\n        genomic intervals in view_df, provided separately for each set.\n        The default\n        values are 'chrom', 'start', 'end'.\n\n    Returns\n    -------\n    is_sorted : bool\n\n    \"\"\"\n    from ..ops import sort_bedframe\n\n    df_sorted = sort_bedframe(\n        df.copy(),\n        view_df=view_df,\n        reset_index=reset_index,\n        df_view_col=df_view_col,\n        view_name_col=view_name_col,\n        cols=cols,\n        cols_view=cols_view,\n    )\n\n    if df.equals(df_sorted):\n        return True\n    else:\n        return False\n", "import_text": ["numpy", "pandas"], "prompt": "\"\"\"\nDescription: This function checks if a given DataFrame is a valid viewframe.\n\nArgs:\n    region_df (pandas.DataFrame): The DataFrame to be checked.\n    raise_errors (bool, optional): If True, the function will raise exceptions when errors are found. Defaults to False.\n    view_name_col (str, optional): The name of the column that contains the names of the views. Defaults to \"name\".\n    cols (list, optional): A list of column names to be used for the check. If None, default column names will be used. Defaults to None.\n\nReturns:\n    bool: True if the DataFrame is a valid viewframe, False otherwise.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Checks that `region_df` is a valid viewFrame.\n\n    This includes:\n\n    - it satisfies requirements for a bedframe, including columns for\n      ('chrom', 'start', 'end')\n    - it has an additional column, view_name_col, with default 'name'\n    - it does not contain null values\n    - entries in the view_name_col are unique.\n    - intervals are non-overlapping\n\n    Parameters\n    ----------\n\n    region_df : pandas.DataFrame\n        Dataframe of genomic intervals to be tested.\n\n    raise_errors : bool\n        If True, raises errors instead of returning a boolean False for invalid\n        properties. Default False.\n\n    view_name_col : str\n        Specifies column name of the view regions. Default 'name'.\n\n    cols : (str, str, str) or None\n        The names of columns containing the chromosome, start and end of the\n        genomic intervals, provided separately for each set. The default\n        values are 'chrom', 'start', 'end'.\n\n    Returns\n    -------\n    is_viewframe:bool\n\n    \"\"\"", "function_dependencies": ["pandas.isna", "pandas.isna.values", "pandas.isna.values.any"], "project_create_time": "2016-10-03T19:09:54+00:00", "project_update_time": "2024-04-18T03:13:52+00:00", "file_create_time": "2021-06-01T05:44:19Z", "file_update_time": "2024-01-19T21:55:13Z", "function_update_time": "2024-01-19T21:55:13Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["pandas.isna"], "test_function": [{"file_path": "/bioframe-v0.6.4/bioframe-0.6.4/tests/test_core_checks.py", "class_name": null, "function_name": "test_is_viewframe", "code": "\ndef test_is_viewframe():\n    # not a bedframe\n    df1 = pd.DataFrame(\n        [\n            [\"chr1\", 10, 20, \"chr1p\"],\n            [\"chr1\", 15, 10, \"chr1q\"],\n        ],\n        columns=[\"chrom\", \"start\", \"end\", \"name\"],\n    )\n    assert not is_viewframe(df1)\n\n    # no column for region name\n    df1 = pd.DataFrame(\n        [\n            [\"chr1\", 10, 20],\n            [\"chr1\", 30, 40],\n        ],\n        columns=[\"chrom\", \"start\", \"end\"],\n    )\n    assert not is_viewframe(df1)\n\n    # contains null values\n    df1 = pd.DataFrame(\n        [\n            [\"chr1\", 10, 20, \"chr1p\"],\n            [\"chr1\", pd.NA, np.nan, \"chr1q\"],\n        ],\n        columns=[\"chrom\", \"start\", \"end\", \"name\"],\n    )\n    assert not is_viewframe(df1)\n\n    # overlapping intervals\n    df1 = pd.DataFrame(\n        [\n            [\"chr1\", 10, 20, \"chr1p\"],\n            [\"chr1\", 15, 25, \"chr1q\"],\n        ],\n        columns=[\"chrom\", \"start\", \"end\", \"name\"],\n    )\n    assert not is_viewframe(df1)\n\n    # valid view\n    df1 = pd.DataFrame(\n        [\n            [\"chr1\", 10, 20, \"chr1p\"],\n            [\"chr1\", 20, 25, \"chr1q\"],\n            [\"chr2\", 20, 25, \"chrTEST_2p\"],\n        ],\n        columns=[\"chrom\", \"start\", \"end\", \"name\"],\n    )\n    assert is_viewframe(df1)"}]}, {"git_group": "coin-or", "git_name": "rbfopt", "version": "4.2.6", "language": "Python", "project_name": "rbfopt-4.2.6.zip", "file_path": "/rbfopt-4.2.6/rbfopt-4.2.6/src/rbfopt/rbfopt_aux_problems.py", "file_name": "rbfopt_aux_problems.py", "focal_class": null, "focal_name": "generate_sample_points", "focal_parameter": ["settings", "n", "var_lower", "var_upper", "integer_vars", "categorical_info", "num_samples"], "solution": "def generate_sample_points(settings, n, var_lower, var_upper, integer_vars,\n                           categorical_info, num_samples):\n    assert(isinstance(var_lower, np.ndarray))\n    assert(isinstance(var_upper, np.ndarray))\n    assert(isinstance(integer_vars, np.ndarray))\n\n    assert(len(var_lower) == n)\n    assert(len(var_upper) == n)\n    assert(isinstance(settings, RbfoptSettings))\n\n    if (categorical_info is not None and categorical_info[2]):\n        # Map bounds and integer variables\n        var_lower = ru.compress_categorical_bounds(var_lower,\n                                                   *categorical_info)\n        var_upper = ru.compress_categorical_bounds(var_upper,\n                                                   *categorical_info)\n        n = len(var_lower)\n        integer_vars = ru.compress_categorical_integer_vars(\n            integer_vars, *categorical_info)\n        \n    # Generate samples\n    samples = (np.random.rand(num_samples, n) * (var_upper - var_lower) + \n               var_lower)\n\n    # Round integer vars\n    if (len(integer_vars)):\n        samples[:, integer_vars] = np.around(samples[:, integer_vars])\n\n    if (categorical_info is not None and categorical_info[2]):\n        # Uncompress\n        return ru.expand_categorical_vars(samples, *categorical_info)\n        \n    return samples", "function_signature": "def generate_sample_points(settings, n, var_lower, var_upper, integer_vars,\n                           categorical_info, num_samples) :", "left_context": "\"\"\"Auxiliary problems for the optimization process.\n\nThis module is responsible for constructing and solving all the\nauxiliary problems encountered during the optimization, such as the\nminimization of the surrogate model, of the bumpiness. The module acts\nas an interface between the high-level routines, the low-level PyOmo\nmodules, and the search algorithms.\n\nLicensed under Revised BSD license, see LICENSE.\n(C) Copyright Singapore University of Technology and Design 2014.\n(C) Copyright International Business Machines Corporation 2017.\n\n\"\"\"\n\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport math\nimport numpy as np\nimport scipy.spatial as ss\nimport pyomo.environ\nimport pyomo.opt\nimport logging\nimport rbfopt.rbfopt_utils as ru\nimport rbfopt.rbfopt_degree1_models as rbfopt_degree1_models\nimport rbfopt.rbfopt_degree0_models as rbfopt_degree0_models\nimport rbfopt.rbfopt_degreem1_models as rbfopt_degreem1_models\nfrom rbfopt.rbfopt_settings import RbfoptSettings\n\n\ndef pure_global_search(settings, n, k, var_lower, var_upper,\n                       integer_vars, categorical_info, node_pos, mat):\n    \"\"\"Pure global search that disregards objective function.\n\n    If using Gutmann's RBF method, Construct a PyOmo model to maximize\n    :math: `1/\\mu`. If using the Metric SRM, select a point purely\n    based on distance.\n\n    See paper by Costa and Nannicini, equation (7) pag 4, and the\n    references therein.\n\n    Parameters\n    ----------\n\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    n : int\n        The dimension of the problem, i.e. size of the space.\n\n    k : int\n        Number of nodes, i.e. interpolation points.\n\n    var_lower : 1D numpy.ndarray[float]\n        Vector of variable lower bounds.\n\n    var_upper : 1D numpy.ndarray[float]\n        Vector of variable upper bounds.\n\n    integer_vars : 1D numpy.ndarray[int]\n        A list containing the indices of the integrality constrained\n        variables. If empty list, all variables are assumed to be\n        continuous.\n\n    categorical_info : (1D numpy.ndarray[int], 1D numpy.ndarray[int],\n                        List[(int, 1D numpy.ndarray[int])]) or None\n        Information on categorical variables: array of indices of\n        categorical variables in original space, array of indices of\n        noncategorical variables in original space, and expansion of\n        each categorical variable, given as a tuple (original index,\n        indices of expanded variables).\n\n    node_pos : 2D numpy.ndarray[float]\n        List of coordinates of the nodes.\n\n    mat : 2D numpy.ndarray[float] or None\n        The matrix necessary for the computation. This is the inverse\n        of the matrix [Phi P; P^T 0], see paper as cited above. Must\n        be a square 2D numpy.ndarray[float] of appropriate dimension if\n        given. Can be None when using the MSRSM algorithm.\n\n    Returns\n    -------\n    List[float]\n        A maximizer. It is difficult to do global optimization so\n        typically this method returns a local maximum.\n\n    Raises\n    ------\n    ValueError\n        If some parameters are not supported.\n    RuntimeError\n        If the solver cannot be found.\n\n    \"\"\"\n    assert(isinstance(var_lower, np.ndarray))\n    assert(isinstance(var_upper, np.ndarray))\n    assert(isinstance(integer_vars, np.ndarray))\n    assert(isinstance(node_pos, np.ndarray))\n\n    assert(len(var_lower) == n)\n    assert(len(var_upper) == n)\n    assert(len(node_pos) == k)\n    assert(isinstance(settings, RbfoptSettings))\n\n    # Determine the size of the P matrix\n    p = ru.get_size_P_matrix(settings, n)\n    assert((mat is None and settings.algorithm.upper() == 'MSRSM')\n           or (isinstance(mat, np.ndarray) and mat.shape == (k + p, k + p)))\n\n    # Instantiate model\n    if (ru.get_degree_polynomial(settings) == 1):\n        model = rbfopt_degree1_models\n    elif (ru.get_degree_polynomial(settings) == 0):\n        model = rbfopt_degree0_models\n    elif (ru.get_degree_polynomial(settings) == -1):\n        model = rbfopt_degreem1_models\n    else:\n        raise ValueError('RBF type ' + settings.rbf + ' not supported')\n\n    if (settings.global_search_method == 'genetic'):\n        # Use a genetic algorithm to optimize\n        if (settings.algorithm.upper() == 'GUTMANN'):\n            fitness = GutmannMukObj(settings, n, k, node_pos, mat)\n        elif (settings.algorithm.upper() == 'MSRSM'):\n            fitness = MaximinDistanceObj(settings, n, k, node_pos)\n        else:\n            raise ValueError('Algorithm ' + settings.algorithm + \n                             ' not supported')\n        point = ga_optimize(settings, n, var_lower, var_upper, integer_vars,\n                            categorical_info, fitness.evaluate)\n    elif (settings.global_search_method == 'sampling'):\n        # Sample random points, and rank according to fitness\n        if (settings.algorithm.upper() == 'GUTMANN'):\n            fitness = GutmannMukObj(settings, n, k, node_pos, mat)\n        elif (settings.algorithm.upper() == 'MSRSM'):\n            fitness = MaximinDistanceObj(settings, n, k, node_pos)\n        else:\n            raise ValueError('Algorithm ' + settings.algorithm + \n                             ' not supported')\n        num_samples = n * settings.num_samples_aux_problems\n        samples = generate_sample_points(settings, n, var_lower, var_upper,\n                                         integer_vars, categorical_info,\n                                         num_samples)\n        scores = fitness.evaluate(samples)\n        point = samples[scores.argmin()]\n    elif (settings.global_search_method == 'solver'):\n        # Optimize using Pyomo\n        if (settings.algorithm.upper() == 'GUTMANN'):\n            instance = model.create_max_one_over_mu_model(\n                settings, n, k, var_lower, var_upper,\n                integer_vars, categorical_info, node_pos, mat)\n            # Initialize variables for local search\n            initialize_instance_variables(settings, instance)\n        elif (settings.algorithm.upper() == 'MSRSM'):\n            instance = model.create_maximin_dist_model(\n                settings, n, k, var_lower, var_upper, integer_vars,\n                categorical_info, node_pos)\n            # Initialize variables for local search\n            initialize_instance_variables(settings, instance)\n        else:\n            raise ValueError('Algorithm ' + settings.algorithm + \n                             ' not supported')\n        # Instantiate optimizer\n        opt = pyomo.opt.SolverFactory(\n            'bonmin', executable=settings.minlp_solver_path, solver_io='nl')\n        if (not opt.available()):\n            raise RuntimeError('Solver ' + 'bonmin' + ' not found')\n        set_minlp_solver_options(opt)\n\n        # Solve and load results\n        try:\n            results = opt.solve(instance, keepfiles=False,\n                                tee=settings.print_solver_output)\n            if ((results.solver.status == pyomo.opt.SolverStatus.ok) and\n                (results.solver.termination_condition ==\n                 pyomo.opt.TerminationCondition.optimal)):\n                # this is feasible and optimal\n                instance.solutions.load_from(results)\n                point = np.array([instance.x[i].value for i in instance.N])\n                ru.round_integer_vars(point, integer_vars)\n            else:\n                point = None\n        except:\n            point = None\n    else:\n        raise ValueError('Global search method ' + settings.algorithm +\n                         ' not supported')\n\n    return point\n\n# -- end function\n\n\ndef minimize_rbf(settings, n, k, var_lower, var_upper, integer_vars,\n                 categorical_info, node_pos, rbf_lambda, rbf_h,\n                 best_node_pos):\n    \"\"\"Compute the minimum of the RBF interpolant.\n\n    Compute the minimum of the RBF interpolant with a PyOmo model.\n\n    Parameters\n    ----------\n\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    n : int\n        The dimension of the problem, i.e. size of the space.\n\n    k : int\n        Number of nodes, i.e. interpolation points.\n\n    var_lower : 1D numpy.ndarray[float]\n        Vector of variable lower bounds.\n\n    var_upper : 1D numpy.ndarray[float]\n        Vector of variable upper bounds.\n\n    integer_vars : 1D numpy.ndarray[int]\n        A list containing the indices of the integrality constrained\n        variables. If empty list, all variables are assumed to be\n        continuous.\n\n    categorical_info : (1D numpy.ndarray[int], 1D numpy.ndarray[int],\n                        List[(int, 1D numpy.ndarray[int])]) or None\n        Information on categorical variables: array of indices of\n        categorical variables in original space, array of indices of\n        noncategorical variables in original space, and expansion of\n        each categorical variable, given as a tuple (original index,\n        indices of expanded variables).\n\n    node_pos : 2D numpy.ndarray[float]\n        List of coordinates of the nodes.\n\n    rbf_lambda : 1D numpy.ndarray[float]\n        The lambda coefficients of the RBF interpolant, corresponding\n        to the radial basis functions. List of dimension k.\n\n    rbf_h : 1D numpy.ndarray[float]\n        The h coefficients of the RBF interpolant, corresponding to\n        the polynomial. List of dimension n+1.\n\n    best_node_pos : 1D numpy.ndarray[float]\n        Coordinates of the best interpolation point.\n\n    Returns\n    -------\n    1D numpy.ndarray[float]\n        A minimizer. It is difficult to do global optimization so\n        typically this method returns a local minimum.\n\n    Raises\n    ------\n    ValueError\n        If some parameters are not supported.\n    RuntimeError\n        If the solver cannot be found.\n    \"\"\"\n    assert(isinstance(var_lower, np.ndarray))\n    assert(isinstance(var_upper, np.ndarray))\n    assert(isinstance(integer_vars, np.ndarray))\n    assert(isinstance(node_pos, np.ndarray))\n    assert(isinstance(rbf_lambda, np.ndarray))\n    assert(isinstance(rbf_h, np.ndarray))\n\n    assert(len(var_lower) == n)\n    assert(len(var_upper) == n)\n    assert(len(rbf_lambda) == k)\n    assert(len(node_pos) == k)\n    assert(isinstance(settings, RbfoptSettings))\n\n    # Determine the size of the P matrix\n    p = ru.get_size_P_matrix(settings, n)\n    assert(len(rbf_h) == p)\n\n    # Instantiate model\n    if (ru.get_degree_polynomial(settings) == 1):\n        model = rbfopt_degree1_models\n    elif (ru.get_degree_polynomial(settings) == 0):\n        model = rbfopt_degree0_models\n    elif (ru.get_degree_polynomial(settings) == -1):\n        model = rbfopt_degreem1_models\n    else:\n        raise ValueError('RBF type ' + settings.rbf + ' not supported')\n\n    instance = model.create_min_rbf_model(\n        settings, n, k, var_lower, var_upper, integer_vars,\n        categorical_info, node_pos, rbf_lambda, rbf_h)\n    # Initialize variables for local search\n    initialize_instance_variables(settings, instance,\n                                  start_point=best_node_pos)\n    # Instantiate optimizer\n    opt = pyomo.opt.SolverFactory(\n        'bonmin', executable=settings.minlp_solver_path, solver_io='nl')\n    if (not opt.available()):\n        raise RuntimeError('Solver ' + 'bonmin' + 'not found')\n    set_minlp_solver_options(opt)\n\n    # Solve and load results\n    try:\n        results = opt.solve(instance, keepfiles=False,\n                            tee=settings.print_solver_output)\n        if ((results.solver.status == pyomo.opt.SolverStatus.ok) and\n            (results.solver.termination_condition ==\n             pyomo.opt.TerminationCondition.optimal)):\n            # this is feasible and optimal\n            instance.solutions.load_from(results)\n            point = np.array([instance.x[i].value for i in instance.N])\n            ru.round_integer_vars(point, integer_vars)\n        else:\n            point = None\n    except:\n        point = None\n\n    return point\n\n# -- end function\n\n\ndef global_search(settings, n, k, var_lower, var_upper, integer_vars,\n                  categorical_info, node_pos, rbf_lambda, rbf_h, mat,\n                  target_val, dist_weight, fmin, fmax):\n    \"\"\"Global search that tries to balance exploration/exploitation.\n\n    If using Gutmann's RBF method, compute the maximum of the h_k\n    function, see equation (8) in the paper by Costa and\n    Nannicini. If using the Metric SRSM, select a point based on a\n    combination of distance and objective function value.\n\n    Parameters\n    ----------\n\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    n : int\n        The dimension of the problem, i.e. size of the space.\n\n    k : int\n        Number of nodes, i.e. interpolation points.\n\n    var_lower : 1D numpy.ndarray[float]\n        Vector of variable lower bounds.\n\n    var_upper : 1D numpy.ndarray[float]\n        Vector of variable upper bounds.\n\n    integer_vars : 1D numpy.ndarray[int]\n        A list containing the indices of the integrality constrained\n        variables. If empty list, all variables are assumed to be\n        continuous.\n\n    categorical_info : (1D numpy.ndarray[int], 1D numpy.ndarray[int],\n                        List[(int, 1D numpy.ndarray[int])]) or None\n        Information on categorical variables: array of indices of\n        categorical variables in original space, array of indices of\n        noncategorical variables in original space, and expansion of\n        each categorical variable, given as a tuple (original index,\n        indices of expanded variables).\n\n    node_pos : 2D numpy.ndarray[float]\n        List of coordinates of the nodes.\n\n    rbf_lambda : 1D numpy.ndarray[float]\n        The lambda coefficients of the RBF interpolant, corresponding\n        to the radial basis functions. List of dimension k.\n\n    rbf_h : 1D numpy.ndarray[float]\n        The h coefficients of the RBF interpolant, corresponding to\n        the polynomial. List of dimension n+1.\n\n    mat : 2D numpy.ndarray[float] or None\n        The matrix necessary for the computation. This is the inverse\n        of the matrix [Phi P; P^T 0], see paper as cited above. Must\n        be a square 2D numpy.ndarray[float] of appropriate dimension, or None if\n        using the MSRSM algorithm.\n\n    target_val : float\n        Value f* that we want to find in the unknown objective\n        function. Used by Gutmann's RBF method only.\n\n    dist_weight : float\n        Relative weight of the distance and objective function value,\n        when selecting the next point with a sampling strategy. A\n        weight of 1.0 corresponds to using solely distance, 0.0 to\n        objective function. Used by Metric SRSM only.\n\n    fmin : float\n        Minimum value among the interpolation nodes.\n\n    fmax : float\n        Maximum value among the interpolation nodes.\n\n    Returns\n    -------\n    1D numpy.ndarray[float]\n        A local optimum. It is difficult to do global optimization so\n        typically this method returns a local optimum.\n\n    Raises\n    ------\n    ValueError\n        If some parameters are not supported.\n    RuntimeError\n        If the solver cannot be found.\n\n    \"\"\"\n    assert(isinstance(var_lower, np.ndarray))\n    assert(isinstance(var_upper, np.ndarray))\n    assert(isinstance(integer_vars, np.ndarray))\n    assert(isinstance(node_pos, np.ndarray))\n    assert(isinstance(rbf_lambda, np.ndarray))\n    assert(isinstance(rbf_h, np.ndarray))\n    assert(len(var_lower) == n)\n    assert(len(var_upper) == n)\n    assert(len(rbf_lambda) == k)\n    assert(len(node_pos) == k)\n    assert(0 <= dist_weight <= 1)\n    assert(fmin <= fmax)\n    assert(isinstance(settings, RbfoptSettings))\n\n    # Determine the size of the P matrix\n    p = ru.get_size_P_matrix(settings, n)\n    assert((mat is None and settings.algorithm.upper() == 'MSRSM' )\n           or (isinstance(mat, np.ndarray) and mat.shape == (k + p, k + p)))\n    assert(len(rbf_h) == p)\n\n    # Instantiate model\n    if (ru.get_degree_polynomial(settings) == 1):\n        model = rbfopt_degree1_models\n    elif (ru.get_degree_polynomial(settings) == 0):\n        model = rbfopt_degree0_models\n    elif (ru.get_degree_polynomial(settings) == -1):\n        model = rbfopt_degreem1_models\n    else:\n        raise ValueError('RBF type ' + settings.rbf + ' not supported')\n\n    if (settings.global_search_method == 'genetic'):\n        # Use a genetic algorithm to optimize\n        if (settings.algorithm.upper() == 'GUTMANN'):\n            fitness = GutmannHkObj(settings, n, k, node_pos, rbf_lambda,\n                                   rbf_h, mat, target_val)\n        elif (settings.algorithm.upper() == 'MSRSM'):\n            fitness = MetricSRSMObj(settings, n, k, node_pos, rbf_lambda,\n                                    rbf_h, dist_weight)\n        else:\n            raise ValueError('Algorithm ' + settings.algorithm + \n                             ' not supported')\n        point = ga_optimize(settings, n, var_lower, var_upper, integer_vars,\n                            categorical_info, fitness.evaluate)\n    elif (settings.global_search_method == 'sampling'):\n        # Sample random points, and rank according to fitness\n        if (settings.algorithm.upper() == 'GUTMANN'):\n            fitness = GutmannHkObj(settings, n, k, node_pos, rbf_lambda,\n                                   rbf_h, mat, target_val)\n        elif (settings.algorithm.upper() == 'MSRSM'):\n            fitness = MetricSRSMObj(settings, n, k, node_pos, rbf_lambda,\n                                    rbf_h, dist_weight)\n        else:\n            raise ValueError('Algorithm ' + settings.algorithm + \n                             ' not supported')\n        num_samples = n * settings.num_samples_aux_problems\n        samples = generate_sample_points(\n            settings, n, var_lower, var_upper, integer_vars,\n            categorical_info, num_samples)\n        scores = fitness.evaluate(samples)\n        point = samples[scores.argmin()]\n    elif (settings.global_search_method == 'solver'):\n        # Optimize using Pyomo\n        if (settings.algorithm.upper() == 'GUTMANN'):\n            instance = model.create_max_h_k_model(\n                settings, n, k, var_lower, var_upper, integer_vars,\n                categorical_info, node_pos, rbf_lambda, rbf_h, mat,\n                target_val)\n            initialize_instance_variables(settings, instance)\n        elif (settings.algorithm.upper() == 'MSRSM'):\n            # Compute minimum and maximum distances between\n            # points. This computation could be avoided if\n            # RbfoptAlgorithm keeps track of them, but in the grand\n            # scheme of things the computation is rarely performed and\n            # is not as expensive as the subsequent optimization.\n            dist_mat = ss.distance.cdist(node_pos, node_pos)\n            dist_min = np.min(dist_mat[np.triu_indices(k, 1)])\n            dist_max = np.max(dist_mat[np.triu_indices(k, 1)])\n            instance = model.create_min_msrsm_model(\n                settings, n, k, var_lower, var_upper, integer_vars,\n                categorical_info, node_pos, rbf_lambda, rbf_h,\n                dist_weight, dist_min, dist_max, fmin, fmax)\n            initialize_instance_variables(settings, instance)\n            initialize_msrsm_aux_variables(settings, instance)\n        else:\n            raise ValueError('Algorithm ' + settings.algorithm + \n                             ' not supported')\n        # Instantiate optimizer\n        opt = pyomo.opt.SolverFactory(\n            'bonmin', executable=settings.minlp_solver_path, solver_io='nl')\n        if (not opt.available()):\n            raise RuntimeError('Solver ' + 'bonmin' + ' not found')\n        set_minlp_solver_options(opt)\n\n        # Solve and load results\n        try:\n            results = opt.solve(instance, keepfiles=False,\n                                tee=settings.print_solver_output)\n            if ((results.solver.status == pyomo.opt.SolverStatus.ok) and\n                (results.solver.termination_condition ==\n                 pyomo.opt.TerminationCondition.optimal)):\n                # this is feasible and optimal\n                instance.solutions.load_from(results)\n                point = np.array([instance.x[i].value for i in instance.N])\n                ru.round_integer_vars(point, integer_vars)\n            else:\n                point = None\n        except:\n            point = None\n    else:\n        raise ValueError('Global search method ' + settings.algorithm +\n                         ' not supported')\n    if point is not None:\n        point = np.array(point)\n\n    return point\n\n# -- end function\n\n\ndef initialize_instance_variables(settings, instance, start_point=None):\n    \"\"\"Initialize the variables of a problem instance.\n\n    Initialize the x variables of a problem instance, and set the\n    corresponding values for the vectors :math:`(u,\\pi)`. This helps\n    the local search by starting at a feasible point.\n\n    Parameters\n    ----------\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    instance : pyomo.ConcreteModel\n        A concrete instance of mathematical optimization model.\n\n    start_point : 1D numpy.ndarray[float] or None\n        The starting point for the local search, or None if it should\n        be randomly generated.\n\n    \"\"\"\n    assert(isinstance(settings, RbfoptSettings))\n\n    # Initialize variables for local search\n    if (start_point is not None):\n        assert(len(instance.N) == len(start_point))\n        for i in instance.N:\n            instance.x[i] = start_point[i]\n    else:\n        for i in instance.N:\n            instance.x[i] = np.random.uniform(instance.var_lower[i],\n                                              instance.var_upper[i])\n# -- end function\n\n\ndef initialize_msrsm_aux_variables(settings, instance):\n    \"\"\"Initialize auxiliary variables for the MSRSM model.\n\n    Initialize the rbfval and mindist variables of a problem\n    instance, using the values for for x and u_pi already given. This\n    helps the local search by starting at a feasible point.\n\n    Parameters\n    ----------\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    instance : pyomo.ConcreteModel\n        A concrete instance of mathematical optimization model.\n    \"\"\"\n    assert(isinstance(settings, RbfoptSettings))\n\n    dist = min(sum((instance.x[j].value - instance.node[i, j])**2\n                   for j in instance.N) for i in instance.K)\n    instance.mindistsq = dist\n\n# -- end function\n\n\ndef get_noisy_rbf_coefficients(settings, n, k, Phimat, Pmat, node_val,\n                               node_err_bounds, init_rbf_lambda=None,\n                               init_rbf_h=None):\n    \"\"\"Obtain coefficients for the noisy RBF interpolant.\n\n    Solve a quadratic problem to compute the coefficients of the RBF\n    interpolant that minimizes bumpiness and lets all points with\n    deviate by a given amount from their value.\n\n    Parameters\n    ----------\n\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    n : int\n        The dimension of the problem, i.e. size of the space.\n\n    k : int\n        Number of nodes, i.e. interpolation points.\n\n    Phimat : 2D numpy.ndarray[float]\n        Matrix Phi, i.e. top left part of the standard RBF matrix.\n\n    Pmat : 2D numpy.ndarray[float]\n        Matrix P, i.e. top right part of the standard RBF matrix.\n\n    node_val : 1D numpy.ndarray[float]\n        List of values of the function at the nodes.\n\n    node_err_bounds : 2D numpy.ndarray[float]\n        Allowed deviation from node values for nodes affected by\n        error. This is a matrix with rows (lower_deviation, \n        upper_deviation).\n\n    init_rbf_lambda : 1D numpy.ndarray[float] or None\n        Initial values that should be used for the lambda coefficients\n        of the RBF. Can be None.\n\n    init_rbf_h : 1D numpy.ndarray[float] or None\n        Initial values that should be used for the h coefficients of\n        the RBF. Can be None.\n\n    Returns\n    ---\n    (1D numpy.ndarray[float], 1D numpy.ndarray[float])\n        Two vectors: lambda coefficients (for the radial basis\n        functions), and h coefficients (for the polynomial). If\n        initialization information was provided and was valid, then\n        some values will always be returned. Otherwise, it will be\n        None.\n\n    Raises\n    ------\n    ValueError\n        If some parameters are not supported.\n    RuntimeError\n        If the solver cannot be found.\n    \"\"\"\n    assert(isinstance(settings, RbfoptSettings))\n    assert(isinstance(node_val, np.ndarray))\n    assert(isinstance(node_err_bounds, np.ndarray))\n    assert(len(node_val) == k)\n    assert(isinstance(Phimat, np.ndarray))\n    assert(isinstance(Pmat, np.ndarray))\n    assert(len(node_val) == len(node_err_bounds))\n    assert(init_rbf_lambda is None or (isinstance(init_rbf_lambda, \n                                                  np.ndarray) and\n                                       len(init_rbf_lambda) == k))\n    assert(init_rbf_h is None or (isinstance(init_rbf_h, np.ndarray) and\n                                  len(init_rbf_h) == Pmat.shape[1]))\n    \n    # Instantiate model\n    if (ru.get_degree_polynomial(settings) == 1):\n        model = rbfopt_degree1_models\n    elif (ru.get_degree_polynomial(settings) == 0):\n        model = rbfopt_degree0_models\n    elif (ru.get_degree_polynomial(settings) == -1):\n        model = rbfopt_degreem1_models\n    else:\n        raise ValueError('RBF type ' + settings.rbf + ' not supported')\n\n    instance = model.create_min_bump_model(settings, n, k, Phimat, Pmat,\n                                           node_val, node_err_bounds)\n\n    # Instantiate optimizer\n    opt = pyomo.opt.SolverFactory(\n        'ipopt', executable=settings.nlp_solver_path, solver_io='nl')\n    if (not opt.available()):\n        raise RuntimeError('Solver ' + 'ipopt' + ' not found')\n    set_nlp_solver_options(opt)\n\n    # Initialize instance variables with the solution provided (if\n    # available). This should avoid any infeasibility.\n    if (init_rbf_lambda is not None and init_rbf_h is not None):\n        for i in range(len(init_rbf_lambda)):\n            instance.rbf_lambda[i] = init_rbf_lambda[i]\n        for i in range(len(init_rbf_h)):\n            instance.rbf_h[i] = init_rbf_h[i]\n\n    # Solve and load results\n    try:\n        results = opt.solve(instance, keepfiles=False,\n                            tee=settings.print_solver_output)\n        if ((results.solver.status == pyomo.opt.SolverStatus.ok) and\n            (results.solver.termination_condition ==\n             pyomo.opt.TerminationCondition.optimal)):\n            # this is feasible and optimal\n            instance.solutions.load_from(results)\n            rbf_lambda = np.array([instance.rbf_lambda[i].value \n                                   for i in instance.K])\n            if (ru.get_size_P_matrix(settings, n) > 0):\n                rbf_h = np.array([instance.rbf_h[i].value for i in instance.P])\n            else:\n                rbf_h = np.array([])\n        else:\n            # If we have initialization information, return it. It is\n            # a feasible solution. Otherwise, this will be None.\n            rbf_lambda = init_rbf_lambda\n            rbf_h = init_rbf_h\n    except:\n        # If we have initialization information, return it. It is\n        # a feasible solution. Otherwise, this will be None.\n        rbf_lambda = init_rbf_lambda\n        rbf_h = init_rbf_h\n\n    return (rbf_lambda, rbf_h)\n\n# -- end function\n\n\ndef get_min_bump_node(settings, n, k, Amat, node_val, node_err_bounds,\n                      target_val):\n    \"\"\"Compute the bumpiness obtained by moving an interpolation point.\n\n    Compute the bumpiness of the interpolant obtained by moving a\n    single node (the one that yields minimum bumpiness, which is\n    determined by this function) within target_val plus or minus\n    error, to target_val.\n\n    Parameters\n    ----------\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    n : int\n        Dimension of the problem, i.e. the space where the point lives.\n\n    k : int\n        Number of nodes, i.e. interpolation points.\n\n    Amat : 2D numpy.ndarray[float]\n        The matrix A = [Phi P; P^T 0] of equation (3) in the paper by\n        Costa and Nannicini.\n\n    node_val : 1D numpy.ndarray[float]\n        List of values of the function at the nodes.\n\n    node_err_bounds : 2D numpy.ndarray[float]\n        Allowed deviation from node values for nodes affected by\n        error. This is a matrix with rows (lower_deviation, \n        upper_deviation).\n\n    target_val : float\n        Target function value at which we want to move the node.\n\n    Returns\n    -------\n    (int, float)\n        The index of the node and corresponding bumpiness value\n        indicating the sought node in the list node_pos.\n    \"\"\"\n    assert(isinstance(settings, RbfoptSettings))\n    assert(isinstance(node_val, np.ndarray))\n    assert(isinstance(node_err_bounds, np.ndarray))\n    assert(len(node_val) == k)\n    assert(isinstance(Amat, np.ndarray))\n    assert(len(node_val) == len(node_err_bounds))\n\n    # Extract the matrices Phi and P from\n    Phimat = Amat[:k, :k]\n    Pmat = Amat[:k, k:]\n\n    min_bump_index, min_bump = None, float('Inf')\n    # We only look at nodes that have some allowed variation, and for\n    # which target_val falls within the specified range.\n    nodes_to_process = np.where(\n        (node_err_bounds[:,1] - node_err_bounds[:,0] > settings.eps_zero) *\n        (node_val + node_err_bounds[:, 0] <= target_val) *\n        (node_val + node_err_bounds[:, 0] <= target_val))[0]\n\n    for i in nodes_to_process:\n        # Compute bumpiness. Save original data.\n        orig_node_val = node_val[i]\n        orig_node_err_bounds = node_err_bounds[i]\n        # Fix this node at the target value.\n        node_val[i] = target_val\n        node_err_bounds[i, :] = 0.0\n        # Compute RBF interpolant.\n        # Get coefficients for the exact RBF first\n        rbf_l, rbf_h = ru.get_rbf_coefficients(settings, n, k,\n                                               Amat, node_val)\n        # And now the noisy version\n        rbf_l, rbf_h = get_noisy_rbf_coefficients(\n            settings, n, k, Phimat, Pmat, node_val, node_err_bounds,\n            rbf_l, rbf_h)\n        # Restore original values\n        node_val[i] = orig_node_val\n        node_err_bounds[i] = orig_node_err_bounds\n        # Compute bumpiness using the formula \\lambda^T \\Phi \\lambda\n        bump = np.dot(np.dot(rbf_l, Phimat), rbf_l)\n        if (bump < min_bump):\n            min_bump_index, min_bump = i, bump\n\n    return (min_bump_index, min_bump)\n\n# -- end function\n\n\ndef get_bump_new_node(settings, n, k, node_pos, node_val, new_node,\n                      node_err_bounds, target_val):\n    \"\"\"Compute the bumpiness with a new interpolation point.\n\n    Computes the bumpiness of the interpolant obtained by setting a\n    new node in a specified location, at value target_val.\n\n    Parameters\n    ----------\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    n : int\n        Dimension of the problem, i.e. the space where the point lives.\n\n    k : int\n        Number of nodes, i.e. interpolation points.\n\n    node_pos : 2D numpy.ndarray[float]\n        Location of current interpolation nodes.\n\n    node_val : 1D numpy.ndarray[float]\n        List of values of the function at the nodes.\n\n    new_node : 1D numpy.ndarray[float]\n        Location of new interpolation node.\n\n    node_err_bounds : 2D numpy.ndarray[float]\n        Allowed deviation from node values for nodes affected by\n        error. This is a matrix with rows (lower_deviation, \n        upper_deviation).\n\n    target_val : float\n        Target function value at which we want to move the node.\n\n    Returns\n    -------\n    float\n        The bumpiness of the interpolant having a new node at the\n        specified location, with value target_val.\n    \"\"\"\n    assert(isinstance(node_pos, np.ndarray))\n    assert(isinstance(node_val, np.ndarray))\n    assert(isinstance(new_node, np.ndarray))\n    assert(isinstance(node_err_bounds, np.ndarray))\n    assert(isinstance(settings, RbfoptSettings))\n    assert(len(node_val) == k)\n    assert(len(node_pos) == k)\n    assert(len(node_val) == len(node_err_bounds))\n    assert(new_node is not None)\n\n    # Add the new node to existing ones\n    n_node_pos = np.vstack((node_pos, new_node))\n    n_node_val = np.append(node_val, target_val)\n    n_node_err_bounds = np.vstack((node_err_bounds, np.array([[0, 0]])))\n\n    # Compute the matrices necessary for the algorithm\n    Amat = ru.get_rbf_matrix(settings, n, k + 1, n_node_pos)\n\n    # Get coefficients for the exact RBF\n    rbf_l, rbf_h = ru.get_rbf_coefficients(settings, n, k + 1, Amat,\n                                             n_node_val)\n    # Get RBF coefficients for noisy interpolant\n    rbf_l, rbf_h = get_noisy_rbf_coefficients(\n        settings, n, k + 1, Amat[:(k + 1), :(k + 1)],\n        Amat[:(k + 1), (k + 1):], n_node_val,\n        n_node_err_bounds, rbf_l, rbf_h)\n\n    bumpiness = np.dot(np.dot(rbf_l, Amat[:(k+1), :(k+1)]), rbf_l)\n\n    return bumpiness\n\n# -- end function\n\n\ndef set_minlp_solver_options(solver):\n    \"\"\"Set MINLP solver options.\n\n    Set the options of the MINLP solver.\n\n    Parameters\n    ----------\n    solver: pyomo.opt.SolverFactory\n        The solver interface.\n    \"\"\"\n    minlp_solver_options = [('bonmin.num_resolve_at_root', 10),\n                            ('bonmin.num_retry_unsolved_random_point', 5),\n                            ('bonmin.num_resolve_at_infeasibles', 5),\n                            ('bonmin.algorithm', 'B-BB'),\n                            ('bonmin.time_limit', 45),\n                            ('acceptable_tol', 1.0e-3),\n                            ('max_cpu_time', 20),\n                            ('max_iter', 1000),\n                            ('bound_relax_factor', 0.0)]\n    minlp_solver_rand_seed_option = 'bonmin.random_generator_seed'\n    minlp_solver_max_seed = 2**31 - 2\n\n    for (opt_name, opt_value) in minlp_solver_options:\n        solver.options[opt_name] = opt_value\n    if (minlp_solver_rand_seed_option is not None):\n        solver.options[minlp_solver_rand_seed_option] = np.random.randint(minlp_solver_max_seed)\n\n# -- end function\n\n\ndef set_nlp_solver_options(solver):\n    \"\"\"Set NLP solver options.\n\n    Set the options of the NLP solver.\n\n    Parameters\n    ----------\n    solver: pyomo.opt.SolverFactory\n        The solver interface.\n    \"\"\"\n\n    nlp_solver_options = [('acceptable_tol', 1.0e-3),\n                          ('honor_original_bounds', 'yes'),\n                          ('max_cpu_time', 20),\n                          ('max_iter', 1000),\n                          ('bound_relax_factor', 0.0)]\n    nlp_solver_rand_seed_option = None\n    nlp_solver_max_seed = 2**31 - 2\n\n    for (opt_name, opt_value) in nlp_solver_options:\n        solver.options[opt_name] = opt_value\n    if (nlp_solver_rand_seed_option is not None):\n        solver.options[nlp_solver_rand_seed_option] = np.random.randint(nlp_solver_max_seed)\n\n# -- end function\n\n", "right_context": "\n# -- end function\n\ndef ga_optimize(settings, n, var_lower, var_upper, integer_vars,\n                categorical_info, objfun):\n    \"\"\"Compute and optimize a fitness function.\n\n    Use a simple genetic algorithm to quickly find a good solution for\n    a minimization subproblem.\n\n    Parameters\n    ----------\n\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    n : int\n        The dimension of the problem, i.e. size of the space.\n\n    var_lower : 1D numpy.ndarray[float]\n        Vector of variable lower bounds.\n\n    var_upper : 1D numpy.ndarray[float]\n        Vector of variable upper bounds.\n\n    integer_vars : 1D numpy.ndarray[int]\n        A list containing the indices of the integrality constrained\n        variables. If empty list, all variables are assumed to be\n        continuous.\n\n    categorical_info : (1D numpy.ndarray[int], 1D numpy.ndarray[int],\n                        List[(int, 1D numpy.ndarray[int])]) or None\n        Information on categorical variables: array of indices of\n        categorical variables in original space, array of indices of\n        noncategorical variables in original space, and expansion of\n        each categorical variable, given as a tuple (original index,\n        indices of expanded variables).\n\n    objfun : Callable[2D numpy.ndarray[float]]\n        The objective function. This must be a callable function that\n        can be applied to a list of points, and must return a list\n        containing one fitness vale for each point, such that lower\n        values are better.\n\n    Returns\n    -------\n    1D numpy.ndarray[float]\n        The best solution found.\n\n    \"\"\"\n    assert(isinstance(var_lower, np.ndarray))\n    assert(isinstance(var_upper, np.ndarray))\n    assert(isinstance(integer_vars, np.ndarray))\n    assert(len(var_lower) == n)\n    assert(len(var_upper) == n)\n    assert(isinstance(settings, RbfoptSettings))\n\n    # Define parameters here, for now. Will move them to\n    # rbfopt_settings later if it seems that the user should be able\n    # to change their value.\n    population_size = settings.ga_base_population_size + 20 * n//5\n    mutation_rate = 0.1\n\n    # Derived parameters. Since the best individual will always remain\n    # and mutate, there is a -1 in the count for new individuals.\n    num_surviving = population_size//4\n    num_new = population_size - 2*num_surviving - 1\n\n    if (categorical_info is not None and categorical_info[2]):\n        # We will have to work in both the extended and compressed\n        # space. Map bounds and integer variables\n        categorical, not_categorical, expansion = categorical_info\n        var_lower_comp = ru.compress_categorical_bounds(var_lower,\n                                                        *categorical_info)\n        var_upper_comp = ru.compress_categorical_bounds(var_upper,\n                                                        *categorical_info)\n        integer_vars_comp = ru.compress_categorical_integer_vars(\n            integer_vars, *categorical_info)\n        \n        n_comp = len(categorical) + len(not_categorical)\n        is_integer = np.zeros(len(var_lower_comp), dtype=bool)\n        for index in integer_vars_comp:\n            is_integer[index] = True\n\n    else:\n        # Generate boolean vector of integer variables for convenience\n        is_integer = np.zeros(n, dtype=bool)\n        if (len(integer_vars)):\n            is_integer[integer_vars] = True\n\n    # Compute initial population\n    population = generate_sample_points(\n        settings, n, var_lower, var_upper, integer_vars,\n        categorical_info, population_size)\n    for gen in range(settings.ga_num_generations):\n        # Mutation rate and maximum perturbed coordinates for this\n        # generation of individuals\n        curr_mutation_rate = (mutation_rate *\n                              (settings.ga_num_generations - gen) /\n                              settings.ga_num_generations)\n        # Compute fitness score to determine remaining individuals\n        fitness_val = objfun(population)\n        rank = np.argsort(fitness_val)\n        best_individuals = population[rank[:num_surviving]]\n        # Crossover: select how mating is done, then create offspring\n        father = np.random.permutation(best_individuals)\n        mother = np.random.permutation(best_individuals)\n        if (categorical_info is not None and categorical_info[2]):\n            father = ru.compress_categorical_vars(father, *categorical_info)\n            mother = ru.compress_categorical_vars(mother, *categorical_info)\n            offspring = ru.expand_categorical_vars(ga_mate(father, mother),\n                                                   *categorical_info)\n        else:\n            offspring = ga_mate(father, mother)\n        # New individuals\n        new_individuals = generate_sample_points(\n            settings, n, var_lower, var_upper, integer_vars,\n            categorical_info, num_new)\n        if (categorical_info is not None and categorical_info[2]):            \n            # If there are categorical variables, we work in\n            # compressed space, then expand again.\n            # Compute perturbation.\n            max_size_pert = min(n_comp,\n                                max(2, int(n_comp * curr_mutation_rate)))\n            # Map to compressed space\n            best_individuals = ru.compress_categorical_vars(\n                best_individuals, *categorical_info)\n            # Make a copy of best individual, and mutate it\n            best_mutated = best_individuals[0, :].copy()\n            ga_mutate(n_comp, var_lower_comp, var_upper_comp, is_integer,\n                      best_mutated, max_size_pert)\n            best_mutated = ru.expand_categorical_vars(\n                np.atleast_2d(best_mutated), *categorical_info)\n            # Mutate surviving (except best) if necessary\n            for point in best_individuals[1:]:\n                if (np.random.uniform() < curr_mutation_rate):\n                    ga_mutate(n_comp, var_lower_comp, var_upper_comp,\n                              is_integer, point, max_size_pert)\n            best_individuals = ru.expand_categorical_vars(\n                best_individuals, *categorical_info)\n        else:\n            # We can work in original space. Compute perturbation.\n            max_size_pert = min(n, max(2, int(n * curr_mutation_rate)))\n            # Make a copy of best individual, and mutate it\n            best_mutated = best_individuals[0, :].copy()\n            ga_mutate(n, var_lower, var_upper, is_integer,\n                      best_mutated, max_size_pert)\n            # Mutate surviving (except best) if necessary\n            for point in best_individuals[1:]:\n                if (np.random.uniform() < curr_mutation_rate):\n                    ga_mutate(n, var_lower, var_upper, is_integer,\n                              point, max_size_pert)\n        # Generate new population\n        population = np.vstack((best_individuals, offspring,\n                                new_individuals, best_mutated))\n    # Determine ranking of last generation.\n    # Compute fitness score to determine remaining individuals\n    fitness_val = objfun(population)\n    rank = np.argsort(fitness_val)\n\n    # Return best individual\n    return population[rank[0]]\n\n# -- end function\n\n\ndef ga_mate(father, mother):\n    \"\"\"Generate offspring for genetic algorithm.\n\n    The offspring will get genes uniformly at random from the mother\n    and the father.\n\n    Parameters\n    ----------\n    father : 2D numpy.ndarray[float]\n        First set of individuals for mating.\n\n    mother : 2D numpy.ndarray[float]\n        Second set of individuals for mating.\n\n    Returns\n    -------\n    2D numpy.ndarray(float)\n        The offspring. Same size as mother and father.\n    \"\"\"\n    assert(isinstance(mother, np.ndarray))\n    assert(isinstance(father, np.ndarray))\n    assert(father.shape == mother.shape)\n\n    # Take elements from father or mother, depending on coin toss\n    return np.where(np.random.uniform(size=father.shape) < 0.5,\n                    father, mother)\n# -- end function\n\n\ndef ga_mutate(n, var_lower, var_upper, is_integer, individual,\n              max_size_pert):\n    \"\"\"Mutate an individual (point) for the genetic algorithm.\n\n    The mutation is performed in place.\n\n    Parameters\n    ----------\n\n    n : int\n        The dimension of the problem, i.e. size of the space.\n\n    var_lower : 1D numpy.ndarray[float]\n        Vector of variable lower bounds.\n\n    var_upper : 1D numpy.ndarray[float]\n        Vector of variable upper bounds.\n\n    is_integer : 1D numpy.ndarray[bool]\n        List of size n, each element is True if the corresponding\n        variable is integer.\n\n    individual : 1D numpy.ndarray[float]\n        Point to be mutated.\n\n    max_size_pert : int\n        Maximum size of the perturbation for the mutation,\n        i.e. maximum number of coordinates that can change.\n\n    \"\"\"\n    assert(max_size_pert <= n)\n\n    assert(isinstance(var_lower, np.ndarray))\n    assert(isinstance(var_upper, np.ndarray))\n    assert(isinstance(is_integer, np.ndarray))\n    assert(isinstance(individual, np.ndarray))\n\n    # Randomly mutate some of the coordinates. First determine how\n    # many are mutated, then pick them randomly.\n    size_pert = np.random.randint(max_size_pert)\n    perturbed = np.random.choice(np.arange(n), size_pert, replace=False)\n    new = (var_lower[perturbed] + np.random.rand(size_pert) * \n           (var_upper[perturbed] - var_lower[perturbed]))\n    new[is_integer[perturbed]] = np.around(new[is_integer[perturbed]])\n    individual[perturbed] = new\n\n# -- end function\n\n\nclass MetricSRSMObj:\n    \"\"\"Objective function for the Metric SRM method.\n\n    This class facilitates the computation of the objective function\n    for the Metric SRSM. The objective function combines the distance\n    from the closest point, and the response surface (i.e. RBF\n    interpolant) value. Lower values are better.\n\n    Parameters\n    ----------\n\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    n : int\n        The dimension of the problem, i.e. size of the space.\n\n    k : int\n        Number of nodes, i.e. interpolation points.\n\n    node_pos : 2D numpy.ndarray[float]\n        List of coordinates of the nodes.\n\n    rbf_lambda : 1D numpy.ndarray[float]\n        The lambda coefficients of the RBF interpolant, corresponding\n        to the radial basis functions. List of dimension k. Can be\n        None if dist_weight is equal to 1, in which case RBF values\n        are not used.\n\n    rbf_h : 1D numpy.ndarray[float]\n        The h coefficients of the RBF interpolant, corresponding to\n        the polynomial. List of dimension n+1. Can be None if\n        dist_weight is equal to 1, in which case RBF values are not\n        used.\n\n    dist_weight : float\n        Relative weight of the distance and objective function value.\n        A weight of 1.0 corresponds to using solely distance, 0.0 to\n        objective function.\n    \"\"\"\n    def __init__(self, settings, n, k, node_pos, rbf_lambda,\n                 rbf_h, dist_weight):\n        \"\"\"Constructor.\n        \"\"\"\n        assert(isinstance(node_pos, np.ndarray))\n        assert(isinstance(rbf_lambda, np.ndarray))\n        assert(isinstance(rbf_h, np.ndarray))\n        assert(len(node_pos) == k)\n        assert(len(rbf_lambda) == k)\n        assert(0 <= dist_weight <= 1)\n        assert(isinstance(settings, RbfoptSettings))\n        p = ru.get_size_P_matrix(settings, n)\n        assert(len(rbf_h) == p)\n        self.settings = settings\n        self.n = n\n        self.k = k\n        self.node_pos = node_pos\n        self.rbf_lambda = rbf_lambda\n        self.rbf_h = rbf_h\n        self.dist_weight = dist_weight\n        self.obj_weight = (1.0 if settings.modified_msrsm_score\n                           else (1 - dist_weight))\n    # -- end function\n\n    def evaluate(self, points):\n        \"\"\"Evaluate the objective for Metric SRSM.\n\n        Evaluate the score of a set of points.\n\n        Parameters\n        ----------\n        points : 2D numpy.ndarray[float]\n            Points at which we want to evaluate the objective function\n            (one for each row).\n\n        Returns\n        -------\n        float\n            The score for the Metric SRSM algorithm (lower is better).\n        \"\"\"\n        assert(isinstance(points, np.ndarray))\n        # Determine distance and surrogate model value\n        obj, dist = ru.bulk_evaluate_rbf(self.settings, points, self.n,\n                                         self.k, self.node_pos,\n                                         self.rbf_lambda, self.rbf_h, 'min')\n        # Determine scaling factors\n        min_dist, max_dist = min(dist), max(dist)\n        min_obj, max_obj = min(obj), max(obj)\n        dist_denom = (max_dist - min_dist if max_dist > min_dist +\n                      self.settings.eps_zero else 1.0)\n        obj_denom = (max_obj - min_obj if max_obj > min_obj +\n                     self.settings.eps_zero else 1.0)\n        res = (self.dist_weight * (max_dist - dist) / dist_denom + \n               self.obj_weight * (obj - min_obj) / obj_denom)\n        res[dist <= self.settings.min_dist] = float('+inf')\n        return res\n    # -- end function\n\n# -- end class MetricSRSMObj\n\n\nclass MaximinDistanceObj:\n    \"\"\"Objective function for the Maximin Distance criterion.\n\n    This class facilitates the computation of the objective function\n    for the Maximin Distance criterion. The objective function is the\n    minimum distance from the closest point, multiplied by -1 so that\n    lower values are better (we always minimize).\n\n    Parameters\n    ----------\n\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    n : int\n        The dimension of the problem, i.e. size of the space.\n\n    k : int\n        Number of nodes, i.e. interpolation points.\n\n    node_pos : 2D numpy.ndarray[float]\n        List of coordinates of the nodes (one for each row).\n    \"\"\"\n    def __init__(self, settings, n, k, node_pos):\n        \"\"\"Constructor.\n        \"\"\"\n        assert (isinstance(node_pos, np.ndarray))\n        assert(len(node_pos) == k)\n        assert(isinstance(settings, RbfoptSettings))\n        self.settings = settings\n        self.n = n\n        self.k = k\n        self.node_pos = node_pos\n    # -- end function\n\n    def evaluate(self, points):\n        \"\"\"Evaluate the objective for Maximin Distance.\n\n        Evaluate the score of a set of points.\n\n        Parameters\n        ----------\n        points : 2D numpy.ndarray[float]\n            Points at which we want to evaluate the objective function\n            (one for each row).\n\n        Returns\n        -------\n        float\n            The score for Maximin Distance algorithm (lower is better).\n        \"\"\"\n        assert(isinstance(points, np.ndarray))\n        dist = ru.bulk_get_min_distance(points, self.node_pos)\n        return -dist\n    # -- end function\n# -- end class MaximinDistanceObj\n\n\nclass GutmannHkObj:\n    \"\"\"Objective function h_k for the Gutmann method.\n\n    This class computes the value of the h_k objective function for\n    the Gutmann method. Lower values are better.\n\n    Parameters\n    ----------\n\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    n : int\n        The dimension of the problem, i.e. size of the space.\n\n    k : int\n        Number of nodes, i.e. interpolation points.\n\n    node_pos : 2D numpy.ndarray[float]\n        List of coordinates of the nodes (one for each row).\n\n    rbf_lambda : 1D numpy.ndarray[float]\n        The lambda coefficients of the RBF interpolant, corresponding\n        to the radial basis functions. List of dimension k. Can be\n        None if dist_weight is equal to 1, in which case RBF values\n        are not used.\n\n    rbf_h : 1D numpy.ndarray[float]\n        The h coefficients of the RBF interpolant, corresponding to\n        the polynomial. List of dimension n+1. Can be None if\n        dist_weight is equal to 1, in which case RBF values are not\n        used.\n\n    Amatinv : 2D numpy.ndarray[float]\n        The matrix necessary for the computation. This is the inverse\n        of the matrix [Phi P; P^T 0]. Must be a square 2D numpy.ndarray[float] of\n        appropriate dimension.\n\n    target_val : float\n        Value f* that we want to find in the unknown objective\n        function. Used by Gutmann's RBF method only.\n\n    \"\"\"\n    def __init__(self, settings, n, k, node_pos, rbf_lambda,\n                 rbf_h, Amatinv, target_val):\n        \"\"\"Constructor.\n        \"\"\"\n        assert(isinstance(node_pos, np.ndarray))\n        assert(isinstance(rbf_lambda, np.ndarray))\n        assert(isinstance(rbf_h, np.ndarray))\n        assert(len(rbf_lambda) == k)\n        assert(len(node_pos) == k)\n        assert(isinstance(settings, RbfoptSettings))\n        # Determine the size of the P matrix\n        p = ru.get_size_P_matrix(settings, n)\n        assert(isinstance(Amatinv, np.ndarray) and\n               Amatinv.shape == (k + p, k + p))\n        assert(len(rbf_h) == p)\n\n        self.settings = settings\n        self.n = n\n        self.k = k\n        self.node_pos = node_pos\n        self.rbf_lambda = rbf_lambda\n        self.rbf_h = rbf_h\n        self.Amatinv = Amatinv\n        self.target_val = target_val\n    # -- end function\n\n    def evaluate(self, points):\n        \"\"\"Evaluate the objective for the Gutmann h_k objective.\n\n        Compute -1/(\\mu_k(x) [s_k(x) - f^\\ast]^2)), where s_k is\n        the value of the RBF interpolant, and f^\\ast is the target\n        value. This is because we want to maximize its negative.\n\n        Parameters\n        ----------\n        points : 2D numpy.ndarray[float]\n            Points at which we want to evaluate the objective function\n            (one for each row).\n\n        Returns\n        -------\n        float\n            The score for the h_k criterion (lower is better).\n\n        \"\"\"\n        assert(isinstance(points, np.ndarray))\n\n        rbf_function = ru.get_rbf_function(self.settings)\n        p = ru.get_size_P_matrix(self.settings, self.n)\n        # Formula:\n        # \\sum_{i=1}^k \\lambda_i \\phi(\\|x - x_i\\|) + h^T (x 1)\n\n        # Create distance matrix\n        dist_mat = ss.distance.cdist(points, self.node_pos)\n        # Evaluate radial basis function on each distance\n        rbf_vec = rbf_function(dist_mat.ravel())\n        u_mat = np.reshape(rbf_vec, (len(points), -1))\n        # Contributions to the RBF interpolant value s_k: the u part,\n        # the pi part, and the nonhomogenous part. At the same time,\n        # build the matrix with the vectors u_pi.\n        part1 = np.dot(u_mat, self.rbf_lambda)\n        if (ru.get_degree_polynomial(self.settings) == 1):\n            part2 = np.dot(points, self.rbf_h[:-1])\n            u_pi_mat = np.concatenate((u_mat, points,\n                                       np.ones((len(points), 1))), axis=1)\n        elif (ru.get_degree_polynomial(self.settings) == 0):\n            part2 = np.zeros(len(points))\n            u_pi_mat = np.concatenate((u_mat, np.ones((len(points), 1))),\n                                      axis=1)\n        else:\n            part2 = np.zeros(len(points))\n            u_pi_mat = u_mat\n        part3 = self.rbf_h[-1] if (p > 0) else 0.0\n        # The vector rbf_value contains the value of the RBF interpolant\n        rbf_value = part1 + part2 + part3\n        # This is the shift in the computation of \\mu_k\n        shift = rbf_function(0.0)\n        sign = (-1)**ru.get_degree_polynomial(self.settings)\n        \n        return -((sign * (np.sum(np.dot(u_pi_mat, np.array(self.Amatinv)) *\n                                 u_pi_mat, axis=1) - shift)) / \n                 np.maximum((rbf_value - self.target_val)**2,\n                            np.ones_like(rbf_value)*self.settings.eps_zero))\n\n        # -- end function\n# -- end class GutmannHkObj\n\n\nclass GutmannMukObj:\n    \"\"\"Objective function \\mu_k for the Gutmann method.\n\n    This class computes the value of the \\mu_k objective function for\n    the Gutmann method. Lower values are better.\n\n    Parameters\n    ----------\n\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    n : int\n        The dimension of the problem, i.e. size of the space.\n\n    k : int\n        Number of nodes, i.e. interpolation points.\n\n    node_pos : 2D numpy.ndarray[float]\n        List of coordinates of the nodes (one for each row).\n\n    Amatinv : 2D numpy.ndarray[float] or None\n        The matrix necessary for the computation. This is the inverse\n        of the matrix [Phi P; P^T 0]. Must be a square 2D numpy.ndarray[float]\n        of appropriate dimension.\n\n    \"\"\"\n    def __init__(self, settings, n, k, node_pos, Amatinv):\n        \"\"\"Constructor.\n        \"\"\"\n        assert(isinstance(node_pos, np.ndarray))\n        assert(len(node_pos) == k)\n        assert(isinstance(settings, RbfoptSettings))\n        # Determine the size of the P matrix\n        p = ru.get_size_P_matrix(settings, n)\n        assert(isinstance(Amatinv, np.ndarray) and\n               Amatinv.shape == (k + p, k + p))\n\n        self.settings = settings\n        self.n = n\n        self.k = k\n        self.node_pos = node_pos\n        self.Amatinv = Amatinv\n    # -- end function\n\n    def evaluate(self, points):\n        \"\"\"Evaluate the objective for the Gutmann \\mu objective.\n\n        Compute -1/\\mu_k(x), which we want to minimize.\n\n        Parameters\n        ----------\n        points : 2D numpy.ndarray[float]\n            Points at which we want to evaluate the objective function\n            (one for each row).\n\n        Returns\n        -------\n        float\n            The score for the \\mu_k criterion (lower is better).\n\n        \"\"\"\n        assert(isinstance(points, np.ndarray))\n\n        rbf_function = ru.get_rbf_function(self.settings)\n        p = ru.get_size_P_matrix(self.settings, self.n)\n        # Formula:\n        # \\sum_{i=1}^k \\lambda_i \\phi(\\|x - x_i\\|) + h^T (x 1)\n\n        # Create distance matrix\n        dist_mat = ss.distance.cdist(points, self.node_pos)\n        # Evaluate radial basis function on each distance\n        rbf_vec = rbf_function(dist_mat.ravel())\n        u_mat = np.reshape(rbf_vec, (len(points), -1))\n        # Build the matrix with the vectors u_pi.\n        if (ru.get_degree_polynomial(self.settings) == 1):\n            u_pi_mat = np.concatenate((u_mat, points,\n                                       np.ones((len(points), 1))), axis=1)\n        elif (ru.get_degree_polynomial(self.settings) == 0):\n            u_pi_mat = np.concatenate((u_mat, np.ones((len(points), 1))),\n                                      axis=1)\n        elif (ru.get_degree_polynomial(self.settings) == -1):\n            u_pi_mat = u_mat\n        # This is the shift in the computation of \\mu_k\n        shift = rbf_function(0.0)\n        sign = (-1)**ru.get_degree_polynomial(self.settings)\n\n        return -(sign * (np.sum(np.dot(u_pi_mat, np.array(self.Amatinv)) * \n                                u_pi_mat, axis=1) +\n                         shift))\n    # -- end function\n# -- end class GutmannMukObj\n", "import_text": ["math", "numpy", "scipy.spatial", "pyomo.environ", "pyomo.opt", "logging", "rbfopt.rbfopt_utils", "rbfopt.rbfopt_degree1_models", "rbfopt.rbfopt_degree0_models", "rbfopt.rbfopt_degreem1_models", "rbfopt.rbfopt_settings.RbfoptSettings"], "prompt": "\"\"\"\nDescription: This function generates a specified number of sample points within given bounds.\n\nArgs:\n    settings (RbfoptSettings): An instance of RbfoptSettings class.\n    n (int): The number of variables.\n    var_lower (np.ndarray): A numpy array of lower bounds for each variable.\n    var_upper (np.ndarray): A numpy array of upper bounds for each variable.\n    integer_vars (np.ndarray): A numpy array indicating which variables are integer.\n    categorical_info (tuple): A tuple containing information about categorical variables.\n    num_samples (int): The number of samples to generate.\n\nReturns:\n    np.ndarray: A numpy array of generated sample points.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Generate sample points uniformly at random.\n\n    Generate a given number of points uniformly at random in the\n    bounding box, ensuring that integer variables take on integer\n    values.\n\n    Parameters\n    ----------\n\n    settings : :class:`rbfopt_settings.RbfoptSettings`\n        Global and algorithmic settings.\n\n    n : int\n        The dimension of the problem, i.e. size of the space.\n\n    var_lower : 1D numpy.ndarray[float]\n        Vector of variable lower bounds.\n\n    var_upper : 1D numpy.ndarray[float]\n        Vector of variable upper bounds.\n\n    integer_vars : 1D numpy.ndarray[int]\n        A list containing the indices of the integrality constrained\n        variables. If empty list, all variables are assumed to be\n        continuous.\n\n    categorical_info : (1D numpy.ndarray[int], 1D numpy.ndarray[int],\n                        List[(int, 1D numpy.ndarray[int])]) or None\n        Information on categorical variables: array of indices of\n        categorical variables in original space, array of indices of\n        noncategorical variables in original space, and expansion of\n        each categorical variable, given as a tuple (original index,\n        indices of expanded variables).\n\n    num_samples : int\n        Number of samples to generate\n\n    Returns\n    -------\n    2D numpy.ndarray[float]\n        A list of sample points (one for each row).\n    \"\"\"", "function_dependencies": ["rbfopt.rbfopt_utils.compress_categorical_bounds", "rbfopt.rbfopt_utils.compress_categorical_integer_vars", "numpy.random.rand", "numpy.around", "rbfopt.rbfopt_utils.expand_categorical_vars"], "project_create_time": "2015-06-11T12:09:41+00:00", "project_update_time": "2024-04-13T20:55:15+00:00", "file_create_time": "2017-08-22T19:45:03Z", "file_update_time": "2021-02-16T21:02:54Z", "function_update_time": "2020-02-29T03:34:37Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["numpy.random.rand"], "test_function": [{"file_path": "/rbfopt-4.2.6/rbfopt-4.2.6/tests/test_rbfopt_aux_problems.py", "class_name": "TestAuxProblems", "function_name": "test_generate_sample_points", "code": "    def test_generate_sample_points(self):\n\n        samples = aux.generate_sample_points(self.settings, self.n, \n                                             self.var_lower, self.var_upper,\n                                             self.integer_vars, None, 123)\n        self.assertEqual(len(samples), 123,\n                         msg='Wrong number of sample points')\n        for sample in samples:\n            self.assertEqual(len(sample), self.n, msg='Wrong point length')\n            for i in self.integer_vars:\n                msg='Variable {:d} not integer in sample'.format(i)\n                self.assertAlmostEqual(abs(sample[i] - round(sample[i])),\n                                       0.0, msg=msg)\n        # Now test some limit cases\n        samples = aux.generate_sample_points(self.settings, 0, np.array([]),\n                                             np.array([]), np.array([]),\n                                             None, 45)\n        self.assertListEqual(samples.tolist(), [[] for i in range(45)],\n                             msg='Samples are not empty when n = 0')\n        samples = aux.generate_sample_points(self.settings, self.n, \n                                             self.var_lower, self.var_upper,\n                                             self.integer_vars, None, 0)\n        self.assertFalse(samples, msg='List of samples should be empty')\n        samples = aux.generate_sample_points(\n            self.settings, 10, np.array([0] * 10), np.array([1] * 10),\n            np.array([i for i in range(10)]),\n            (np.array([0]), np.array([]),\n             [(0, 0, np.array([i for i in range(10)]))]), 10)\n        for sample in samples:\n            msg='Sum of categorical variables not equal to 1'\n            self.assertEqual(sum(sample), 1, msg=msg)"}]}, {"git_group": "google", "git_name": "paranoid_crypto", "version": "main", "language": "Python", "project_name": "paranoid_crypto-main.zip", "file_path": "/paranoid_crypto-main/paranoid_crypto-main/paranoid_crypto/lib/small_roots.py", "file_name": "small_roots.py", "focal_class": null, "focal_name": "multivariate_modn", "focal_parameter": [], "solution": "def multivariate_modn(\n    f: sympy.Poly, bounds: list[int], m: int = 1\n) -> Optional[list[int]]:\n  n = f.get_modulus()\n  fz = f.monic().set_domain(sympy.ZZ)\n  xs = fz.gens\n  # Let l be the leading monomial\n  l = _get_monomials(fz)[0]\n  # Define the sets M_k of monomials\n  mks = []\n  mons = _get_monomials(fz**m)\n  for k in range(m + 1):\n    fmk_mons = _get_monomials(fz ** (m - k))\n    mk = {mon for mon in mons if mon // l**k in fmk_mons}\n    mks.append(mk)\n  mks.append(set())\n  # Define the shift polynomials\n  pols = []\n  for k in range(m + 1):\n    diffs = mks[k] - mks[k + 1]\n    g = fz**k * n ** (m - k)\n    for mon in diffs:\n      pols.append((mon // l**k) * g)\n  dim = len(pols)\n\n  # create the lattice using the coefficients\n  lat = [[0] * dim for _ in range(dim)]\n  params = [sympy.Poly(xs[i] * bounds[i]) for i in range(len(xs))]\n  for i in range(dim):\n    pol = pols[i]\n    for v in range(len(xs)):\n      pol = sympy.compose(pol, params[v], xs[v])\n    pol = sympy.Poly(pol, *xs)\n    lat[i] = [pol.coeff_monomial(mons[j].as_expr()) for j in range(dim)]\n\n  lat = lll.reduce(lat)\n\n  # reconstruct polynomials\n  for i in range(dim):\n    for j in range(dim):\n      lat[i][j] = gmpy.mpz(lat[i][j]) // int(sympy.Poly(mons[j], *xs)(*bounds))\n  a = [lat[i][:-1] for i in range(dim - 1)]\n  b = [-lat[i][-1] for i in range(dim - 1)]\n  solutions = linalg_util.solve_right(a, b)\n  if not solutions:\n    return None\n  pols = [mons[j] - int(solutions[j]) for j in range(len(solutions))]\n  # TODO(pedroysb): Is there a better way to solve this without using sympy? For\n  # very complex polynomials/monomials this may take a while...\n  for roots in sympy.solve(pols, *xs, check=False, manual=True):\n    if int(f(*roots)) % n == 0:\n      return list(roots)\n  return None", "function_signature": "def multivariate_modn(\n    f: sympy.Poly, bounds: list[int], m: int = 1\n) -> Optional[list[int]] :", "left_context": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Set of functions to find small roots of polynomials modulo an integer.\"\"\"\n\nimport itertools\nfrom typing import Optional\nimport gmpy2 as gmpy\nfrom paranoid_crypto.lib import linalg_util\nfrom paranoid_crypto.lib import lll\nimport sympy\n\n\ndef _get_monomials(f: sympy.Poly) -> list[sympy.Poly]:\n  mons = []\n  for exps in f.monoms():\n    mon = 1\n    for i in range(len(exps)):\n      mon *= f.gens[i] ** exps[i]\n    mons.append(sympy.Poly(mon, *f.gens))\n  return mons\n\n\ndef univariate_modp(f: sympy.Poly, b: int, k: int = 3) -> Optional[int]:\n  \"\"\"Returns a small root of a univariate polynomial modulo an unknown factor.\n\n  For a composite n, given a polynomial f(x) = 0 mod p, where p is an unknown\n  factor of n, this function applies Coppersmith/Howgrave-Graham method to find\n  a small root of f. The expected root should be smaller than the bound b. This\n  function assumes that n is a balanced RSA modulus, i.e., n = p*q, where p and\n  q have equal bit-lengths (beta = 1/2 by original Coppersmith method).\n\n  E.g., for a polynomial f(x) = p0 + x and by using k = 2 the generated lattice\n  has the triangular form:\n  | n^2       0        0       0  |\n  | n*p0     n*b       0       0  |\n  | p0^2    2*p0*b    b^2      0  |\n  |  0      p0^2*b  2*p0*b^2  b^3 |\n\n  I.e., every new polynomial is a multiple of p and introduces exactly one new\n  monomial to the lattice. The size of this lattice might be good enough for\n  some situations but in order to achieve better results for a larger bound b,\n  one can use a larger value of k. The maximum bound possible should be about\n  half of the size of p, according to Howgrave-Graham theorem.\n\n  Args:\n    f: Univariate polynomial modulus n. This polynomial has to be 0 when mod p,\n      where p is a factor of n.\n    b: The maximum value of the expected root.\n    k: A value that determines the size of the lattice. The dimension of the\n      lattice is 2*k*d, where d is the degree of f.\n\n  Returns:\n    a small -b < root < b for the polynomial f if found, None otherwise.\n  \"\"\"\n  if not f.is_univariate:\n    raise ValueError('Polynomial is not univariate.')\n\n  d = f.degree()\n  dim = 2 * k * d\n  n = f.get_modulus()\n  fz = f.monic().set_domain(sympy.ZZ)\n  x = fz.gen\n\n  # compute polynomials\n  pols = []\n  xb = sympy.Poly(x * b)\n  t = fz.compose(xb)\n  for i in range(k):\n    g = t**i * n ** (k - i)\n    for j in range(d):\n      pols.append(xb**j * g)\n  g = t**k\n  for i in range(d * k):\n    pols.append(xb**i * g)\n\n  # create the lattice using the coefficients\n  lat = [[0] * dim for _ in range(dim)]\n  for i in range(dim):\n    coeffs = pols[i].all_coeffs()[::-1]\n    for j in range(i + 1):\n      lat[i][j] = coeffs[j]\n\n  lat = lll.reduce(lat)\n\n  # reconstruct polynomial\n  poly = sympy.Poly(sum(x**i * (lat[0][i] // b**i) for i in range(dim)))\n\n  # Look for a linear/irreducible factor of the form a*x + b. Thus, the root\n  # will be -b/a. This approach is much faster than calling one of the root\n  # methods provided by sympy.\n  for factor in poly.factor_list_include():\n    rx = -factor[0].TC() // factor[0].LC()\n    y = f(rx)\n    if y != 0 and n % y == 0:\n      return rx\n  return None\n\n\ndef multivariate_modp(\n    f: sympy.Poly, bounds: list[int], m: int = 4\n) -> Optional[list[int]]:\n  \"\"\"Returns small roots of a multivariate polynomial modulo an unknown factor.\n\n  Proposal of M. Herrmann and A. May., 'Solving Linear Equations Modulo\n  Divisors: On Factoring Given Any Bits', ASIACRYPT 2008. For a composite n,\n  given a polynomial f = 0 mod p, where p is an unknown factor of n, this\n  function applies Herrmann & May  method to find small roots of f. f has to be\n  a linear equation f(x1, x2, ..., xl) = a0 + a1*x1 + a2*x2 + ... + al*xl.\n  The expected roots should be smaller than the bounds. This function assumes\n  that n is a balanced RSA modulus, i.e., n = p*q, where p and q have equal\n  bit-lengths (beta = 1/2).\n\n  The generated polynomials for the lattice are of the form\n  g_i2,...,il,k = x2^i2 * x3^i3 * ... * xn^in * f^k * N^max{t-k, 0}, for an\n  optimized t value, 0 <= k <= m, ij in {0...m} and sum(ij) <= m-k.\n  I.e., every new polynomial is a multiple of p and introduces exactly one new\n  monomial to the lattice. In order to achieve better results for larger bounds,\n  one can use a larger value of m. The maximum product of bounds possible\n  should be about n^0.25, according to Howgrave-Graham theorem. However, as\n  discussed by Herrmann and May, with more unknowns it is natural to achieve\n  smaller bounds. E.g., with two balanced unknowns, the product of the bounds\n  is about n^0.207.\n\n  E.g., for a polynomial f(x1, x2) = a0 + a1*x1 + a2*x2, the generated lattice\n  has the triangular form:\n  | n^t   0                                                        ... 0 |\n  |    x2*n^t    0                                                 ... 0 |\n  |       ...                                                      ... 0 |\n  |          x2^m*n^t     0                                        ... 0 |\n  |                  x1*n^(t-1)          0                         ... 0 |\n  |                         ...                                    ... 0 |\n  |                            x1*x2^(m-1)*n^(t-1)       0         ... 0 |\n  |                                               x1^2*n^(t-2)   0 ... 0 |\n  |                                                        ...     ... 0 |\n  |                                                                 x1^m |\n\n  Args:\n    f: Multivariate polynomial modulus n. This polynomial has to be linear and 0\n      when mod p, where p is a factor of n.\n    bounds: A list with the maximum values of the expected roots.\n    m: A value that determines the size of the lattice.\n\n  Returns:\n    list r of roots, with -bounds[i] < r[i] < bounds[i], for the polynomial f\n      if found, None otherwise.\n  \"\"\"\n\n  if not f.is_multivariate:\n    raise ValueError('Polynomial is not multivariate.')\n  if not f.is_linear:\n    raise ValueError('Polynomial is not linear.')\n\n  n = f.get_modulus()\n  fz = f.monic().set_domain(sympy.ZZ)\n  xs = fz.gens\n  l = len(xs)\n  # Herrmann and May suggest optimzed value t = tau*m, where\n  # tau = 1-(1-beta)**(1/l). Assuming balanced RSA modulus, beta = 0.5:\n  t = max(1, int((1 - (0.5) ** (1 / l)) * m))\n\n  # Compute polynomials. Each polynomial adds a new monomial.\n  pols = []\n  mons = []\n  all_idxs = list(itertools.product(range(m + 1), repeat=l - 1))\n  for k in range(m + 1):\n    idxs = [idx for idx in all_idxs if sum(idx) <= m - k]\n    g = fz**k * n ** max(t - k, 0)\n    mon = xs[0] ** k\n    for ijs in idxs:\n      t1 = 1\n      for i in range(1, l):\n        t1 *= xs[i] ** ijs[i - 1]\n      pols.append(t1 * g)\n      mons.append(t1 * mon)\n  dim = len(pols)\n\n  # create the lattice using the coefficients\n  lat = [[0] * dim for _ in range(dim)]\n  params = [sympy.Poly(xs[i] * bounds[i]) for i in range(l)]\n  for i in range(dim):\n    lat[i][0] = pols[i].TC()\n    pol = pols[i]\n    for v in range(l):\n      pol = sympy.compose(pol, params[v], xs[v])\n    pol = sympy.Poly(pol, *xs)\n    for j in range(1, i + 1):\n      lat[i][j] = pol.coeff_monomial(mons[j])\n\n  lat = lll.reduce(lat)\n\n  # reconstruct polynomials\n  for i in range(dim):\n    for j in range(dim):\n      lat[i][j] = gmpy.mpz(lat[i][j]) // int(sympy.Poly(mons[j], *xs)(*bounds))\n\n  # NOTE(pedroysb): Under the assumption that the lattice-based construction\n  # yields algebraically independent polynomials, some papers suggest computing\n  # resultants, Grobner basis, or multidimensional Newton method to find roots.\n  #\n  # As the assumption seems to hold for all rows of the lattice, a simpler (and\n  # maybe faster) approach seems to be to consider every monomial as an\n  # independent variable. The result is a linear system of dim-1 variables and\n  # dim-1 equations.\n  a = [lat[i][1:] for i in range(dim)]\n  b = [-lat[i][0] for i in range(dim)]\n  solutions = linalg_util.solve_right(a, b)\n  if not solutions:\n    return None\n  mons_dict = dict(zip(mons[1:], range(len(solutions))))\n  roots = [int(solutions[mons_dict[xi]]) for xi in xs]\n  y = int(f(*roots))\n  if y != 0 and n % y == 0:\n    return roots\n  return None\n\n", "right_context": "", "import_text": ["itertools", "typing.Optional", "gmpy2", "paranoid_crypto.lib.linalg_util", "paranoid_crypto.lib.lll", "sympy"], "prompt": "\"\"\"\nDescription: This function is for finding roots of a multivariate polynomial modulo n.\n\nArgs:\n    f (sympy.Poly): The polynomial to find roots for.\n    bounds (list[int]): The bounds for the variables in the polynomial.\n    m (int): The degree of the polynomial. Defaults to 1.\n\nReturns:\n    Optional[list[int]]: A list of roots of the polynomial modulo n, or None if no roots are found.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "  \"\"\"Returns small roots of a multivariate polynomial modulo an integer.\n\n  Proposal of E. Jochemsz and A. May., 'A Strategy for Finding Roots of\n  Multivariate Polynomials with New Applications in Attacking RSA Variants',\n  ASIACRYPT 2006. Given a polynomial f = 0 mod n, this function applies Jochemsz\n  & May method to find small roots of f. f can be any polynomial with arbitrary\n  degree. The expected roots should be smaller than the bounds.\n\n  Compared to univariate_modp and multivariate_modp, this function is more\n  appropriate to be used on vulnerabilities where the product of factors of n\n  generate properties (e.g., unknowns) that do not exist or do not have enough\n  information on the individual factors.\n\n  First it defines the set of monomials:\n  m_k = {x1^i1 * x2^i2 * ... * xn^in |\n         x1^i1 * x2^i2 * ... * xn^in is a monomial of f^m and\n        (x1^i1 * x2^i2 * ... * xn^in)/l^k is a monomial of f^(m-k)},\n  where l is the leading monomial of f and 0 <= k <= m+1.\n  Thus, the generated polynomials for the lattice are of the form\n  g_i1,...,in,k = ((x1^i1 * x2^i2 * ... * xn^in)/l^k) * f^k * N^(m-k), for\n  0 <= k <= m and x1^i1 * x2^i2 * ... * xn^in is an element of the set\n  m_k - m_k+1. I.e., every new polynomial is a multiple of n and introduces\n  exactly one new monomial to the lattice. In order to achieve better results\n  for larger bounds, one can use a larger value of m. The maximum product of\n  bounds possible should be about n^0.5. However, with more unknowns it is\n  natural to achieve smaller bounds.\n\n  Args:\n    f: Multivariate polynomial modulus n.\n    bounds: A list with the maximum values of the expected roots.\n    m: A value that determines the size of the lattice.\n\n  Returns:\n    list r of roots, with -bounds[i] < r[i] < bounds[i], for the polynomial f if\n      found, None otherwise.\n  \"\"\"", "function_dependencies": ["sympy.Poly", "sympy.compose", "sympy.Poly.coeff_monomial", "paranoid_crypto.lib.lll.reduce", "gmpy2.mpz", "sympy", "paranoid_crypto.lib.linalg_util.solve_right", "sympy.solve"], "project_create_time": "2022-05-09T13:54:17+00:00", "project_update_time": "2024-04-16T23:23:12+00:00", "file_create_time": "2022-06-30T09:05:43Z", "file_update_time": "2024-04-12T09:51:35Z", "function_update_time": "2024-04-12T09:51:35Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["sympy.solve"], "test_function": [{"file_path": "/paranoid_crypto-main/paranoid_crypto-main/paranoid_crypto/lib/small_roots_test.py", "class_name": "SmallRootsTest", "function_name": "testBivariateModn", "code": "\n  def testBivariateModn(self):\n    x1, x2 = sympy.symbols(\"x1, x2\")\n\n    def bivariate_helper(unknown_bits, m=1):\n      unknown_bits_x1, unknown_bits_x2 = unknown_bits\n      b1, b2 = 2**unknown_bits_x1, 2**unknown_bits_x2\n      p0 = (p >> unknown_bits_x1) << unknown_bits_x1\n      q0 = (n // p >> unknown_bits_x2) << unknown_bits_x2\n      f = sympy.Poly((p0 + x1) * (q0 + x2), modulus=n)\n      roots = small_roots.multivariate_modn(f, [b1, b2], m)\n      return roots, p0, q0\n\n    # Even using a small lattice (default m = 1), we are able to factorize.\n    roots, p0, q0 = bivariate_helper([340, 340])\n    rx1, rx2 = roots\n    self.assertEqual((p0 + rx1) * (q0 + rx2), n)\n    # Testing with unbalanced bounds:\n    roots, p0, q0 = bivariate_helper([820, 100])\n    rx1, rx2 = roots\n    self.assertEqual((p0 + rx1) * (q0 + rx2), n)\n    # With less known bits the factorization fails:\n    roots, _, _ = bivariate_helper([400, 400])\n    self.assertIsNone(roots)\n    # By increasing the size of the lattice, we are able to factor:\n    roots, p0, q0 = bivariate_helper([400, 400], m=2)\n    rx1, rx2 = roots\n    self.assertEqual((p0 + rx1) * (q0 + rx2), n)"}]}, {"git_group": "bexxmodd", "git_name": "vizex", "version": "v2.1.1", "language": "Python", "project_name": "vizex-v2.1.1.zip", "file_path": "/vizex-v2.1.1/vizex-2.1.1/vizex/tools.py", "file_name": "tools.py", "focal_class": null, "focal_name": "save_to_csv", "focal_parameter": [], "solution": "def save_to_csv(data: dict,\n                filename: str,\n                orient: str = 'index') -> None:\n    if filename.split(\".\")[-1].lower() == 'csv':\n        df = pd.DataFrame.from_dict(data, orient=orient)\n        df.to_csv(filename, mode='a')\n    else:\n        raise NameError('Please include \".csv\" in the filename')", "function_signature": "def save_to_csv(data: dict,\n                filename: str,\n                orient: str = 'index') -> None :", "left_context": "'''\nUtility functions for vizex/vizexdf\n'''\n\nimport os\nimport re\nimport json\nimport time\nimport pandas as pd\n\nfrom typing import Optional, Match\nfrom math import ceil\nfrom colored import fg, attr, stylize\nfrom dataclasses import dataclass\n\n\n@dataclass(order=True)\nclass DecoratedData:\n    \"\"\"\n    Custom class to compare numerical data for sorting\n    which appears in the stylized representation of a string.\n    \"\"\"\n    size: int\n    to_string: str\n\n    def __str__(self):\n        \"\"\"String representation of the class\"\"\"\n        return self.to_string\n\n\ndef bytes_to_human_readable(bytes_in: int, suffix='B') -> str:\n    \"\"\"\n    Converts bytes into the appropriate human\n    readable unit with a relevant suffix.\n\n    Args:\n        bytes_in (int): to convert\n        suffix (str, optional): suffix of a size string.\n                                Defaults to 'b'.\n\n    Returns:\n        str: size in a human readable\n    \"\"\"\n    for unit in ['', 'k', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(bytes_in) < 1024.0:\n            return f'{bytes_in:3.1f} {unit}{suffix}'\n        bytes_in /= 1024.0\n    return f'{bytes_in:.1f} {\"Y\"}{suffix}'\n\n\ndef ints_to_human_readable(disk: dict) -> dict:\n    \"\"\"Converts the dictionary of integers\n    into the human readable size formats.\n\n    Args:\n        disk [dict]: of large byte numbers\n\n    Returns:\n        dict: same dict but in human readable format\n    \"\"\"\n    result = {}\n    for key in disk:\n        try:\n            result[key] = bytes_to_human_readable(disk[key])\n        except Exception:\n            result[key] = disk[key]  # If not able to convert return input back\n    return result\n\n\ndef printml(folder: list, cols: int = 1) -> None:\n    \"\"\"Prints multiline strings side by side.\n\n    Args:\n        folder (list): list of folders to print\n        cols (int, optional): On how many lines should it be printed.\n                            Defaults to 1.\n    \"\"\"\n    size = len(folder)\n    incr = ceil(size / cols)\n    end, start = 0, 0\n    while True:\n        if end >= size:\n            break\n        end += incr\n        # Check if the end index exceeds the last index\n        if end > size:\n            end = size\n        lines = [folder[i].splitlines() for i in range(start, end)]\n        for line in zip(*lines):\n            print(*line, sep='  ')\n        print()\n        start = end\n\n\ndef create_usage_warning(usage_pct: float,\n                         red_flag: int,\n                         yellow_flag: int) -> str:\n    \"\"\"Create disk usage percent with warning color\n\n    Args:\n        usage_pct (float): of a given space\n        red_flag (int): threshold that decides if print should be red\n        yellow_flag (int): threshold that decides if print is yellow\n\n    Returns:\n        str: stylized warning string\n    \"\"\"\n    if usage_pct < 0:\n        usage_pct = 0\n\n    if usage_pct > 100:\n        usage_pct = 100\n\n    use = str(usage_pct) + '% used'\n\n    if usage_pct >= red_flag:\n        return f\"{stylize(use, attr('blink') + fg(9))}\"\n    if usage_pct >= yellow_flag:\n        return f\"{stylize(use, fg(214))}\"\n    return f\"{stylize(use, fg(82))}\"\n\n", "right_context": "\n\ndef save_to_json(data: dict,\n                 filename: str,\n                 indent: int = 4) -> None:\n    \"\"\"Saves disk/partitions data as a JSON file\n\n    Args:\n        data (dict): to be saved to a JSON file\n        filename (str): to name a saved file\n        indent (int, optional): of each new line.\n                                Defaults to 4.\n\n    Raises:\n        NameError: if filename doesn't contain .json at the end\n    \"\"\"\n    if filename.split(\".\")[-1].lower() == 'json':\n        with open(filename, \"w\") as file:\n            json.dump(data, file, indent=indent)\n    else:\n        raise NameError('Please include \".json\" in the filename')\n\n\ndef append_to_bash(alias: str, line: str) -> None:\n    \"\"\"\n    Appends terminal command line as an alias in .bashrc for reuse\n\n    Args:\n        alias[str]: To set up in the bash\n        line[str]: line which will be assigned to an alias\n    \"\"\"\n    bash = os.path.expanduser(\"~\") + '/.bash_aliases'\n    print(remove_if_exists(alias, bash))\n    with open(bash, 'a+') as file:\n        file.write('alias ' + alias + f\"='{line}'\")\n\n\ndef remove_if_exists(alias: str, path: str) -> None:\n    \"\"\"\n    Removes if the given line/alias exists in a given file\n\n    Args:\n        alias (str): to check if exists in bash\n        path (str): path to the file to read\n    \"\"\"\n    if not os.path.exists(path):\n        return\n    with open(path, \"r\") as file:\n        lines = file.readlines()\n\n    with open(path, \"w\") as file:\n        for line in lines:\n            # We only write back lines which is not alias\n            if f'alias {alias}' not in line.strip(\"\\n\"):\n                file.write(line)\n\n\ndef normalize_date(formatting: str, date: int) -> str:\n    \"\"\"\n    Converts date from nanoseconds to the human readable form\n\n    Args:\n        format (str): example %h-%d-%Y for mm-dd-yyyy\n        date (int): date in nanoseconds\n\n    Returns:\n        str: Human readable format of a date\n    \"\"\"\n    return time.strftime(formatting, time.localtime(date))\n\n\ndef find_word(word, src) -> Optional[Match[str]]:\n    \"\"\"Find word in a src using regex\"\"\"\n    return re.compile(r'\\b({0})\\b'.format(word),\n                      flags=re.IGNORECASE).search(src)\n\n\nif __name__ == '__main__':\n    file1 = DecoratedData(55456, '54.2 kB')\n    file2 = DecoratedData(123233419, '117.5 MB')\n    print(f'{file1} is less than {file2} : {file1 < file2}')\n", "import_text": ["os", "re", "json", "time", "pandas", "typing.Optional", "typing.Match", "math.ceil", "colored.fg", "colored.attr", "colored.stylize", "dataclasses.dataclass"], "prompt": "\"\"\"\nDescription: This function saves a dictionary to a CSV file using pandas.\n\nArgs:\n    data (dict): The dictionary to be saved.\n    filename (str): The name of the file to save the data to.\n    orient (str, optional): The orientation of the data in the CSV file. Defaults to 'index'.\n\nRaises:\n    NameError: If the filename does not end with '.csv'.\n\nReturns:\n    None: This function does not return anything.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Outputs disks/partitions data as a CSV file\n\n    Args:\n        data (dict): to be saved to a CSV file\n        filename (str): to name a saved file\n        orient (str, optional): how lines are saved.\n                                Defaults to 'index'.\n\n    Raises:\n        NameError: if filename doesn't contain .csv at the end\n    \"\"\"", "function_dependencies": ["pandas.DataFrame.from_dict", "pandas.DataFrame.from_dict.to_csv"], "project_create_time": "2020-09-04T00:17:25+00:00", "project_update_time": "2024-04-02T20:49:15+00:00", "file_create_time": "2022-01-09T19:02:13Z", "file_update_time": "2022-01-18T20:45:55Z", "function_update_time": "2022-01-09T19:02:13Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["pandas.DataFrame.from_dict"], "test_function": [{"file_path": "/vizex-v2.1.1/vizex-2.1.1/tests/test_tools.py", "class_name": "TestTools", "function_name": "test_save_to_csv_create_file", "code": "\n    def test_save_to_csv_create_file(self):\n        data = {}\n        try:\n            with tempfile.NamedTemporaryFile(mode='w+', suffix='.csv') as tmpf:\n                filename = tmpf.name\n                save_to_csv(data=data, filename=filename)\n                self.assertTrue(os.path.isfile(filename),\n                                msg=f'{filename} was not created')\n                self.assertTrue(os.path.getsize(filename),\n                                msg=f'{filename} is not empty')\n        except Exception as e:\n            self.fail(f'Exception occured when trying to save an empty CSV file {e}')"}, {"file_path": "/vizex-v2.1.1/vizex-2.1.1/tests/test_tools.py", "class_name": "TestTools", "function_name": "test_save_to_csv_full_data", "code": "\n    def test_save_to_csv_full_data(self):\n        data = {\n            'test_01': [11, 33, 55],\n            'test_02': [22, 44, 66],\n            'test_03': [33, 77, 99]\n        }\n        try:\n            with tempfile.NamedTemporaryFile(mode='w+', suffix='.csv') as tmpf:\n                save_to_csv(data=data, filename=tmpf.name, orient='index')\n                self.assertTrue(os.path.isfile(tmpf.name))\n\n                other_f = [\n                    ['', '0', '1', '2\\n'],\n                    ['test_01', '11', '33', '55\\n'],\n                    ['test_02', '22', '44', '66\\n'],\n                    ['test_03', '33', '77', '99\\n']\n                ]\n\n                with open(tmpf.name) as f:\n                    for line, other in zip(f, other_f):\n                        self.assertListEqual(other, line.split(','),\n                                    msg='Given two rows in a CSV files are not the same')\n        except Exception as e:\n            self.fail(f'Exception occured when trying to save a CSV file {e}')"}]}, {"git_group": "lyst", "git_name": "lightfm", "version": "1.17", "language": "Python", "project_name": "lightfm-1.17.zip", "file_path": "/lightfm-1.17/lightfm-1.17/lightfm/evaluation.py", "file_name": "evaluation.py", "focal_class": null, "focal_name": "auc_score", "focal_parameter": ["model", "test_interactions"], "solution": "def auc_score(\n    model,\n    test_interactions,\n    train_interactions=None,\n    user_features=None,\n    item_features=None,\n    preserve_rows=False,\n    num_threads=1,\n    check_intersections=True,\n):\n\n    if num_threads < 1:\n        raise ValueError(\"Number of threads must be 1 or larger.\")\n\n    ranks = model.predict_rank(\n        test_interactions,\n        train_interactions=train_interactions,\n        user_features=user_features,\n        item_features=item_features,\n        num_threads=num_threads,\n        check_intersections=check_intersections,\n    )\n\n    assert np.all(ranks.data >= 0)\n\n    auc = np.zeros(ranks.shape[0], dtype=np.float32)\n\n    if train_interactions is not None:\n        num_train_positives = np.squeeze(\n            np.array(train_interactions.getnnz(axis=1)).astype(np.int32)\n        )\n    else:\n        num_train_positives = np.zeros(test_interactions.shape[0], dtype=np.int32)\n\n    # The second argument is modified in-place, but\n    # here we don't care about the inconsistency\n    # introduced into the ranks matrix.\n    calculate_auc_from_rank(\n        CSRMatrix(ranks), num_train_positives, ranks.data, auc, num_threads\n    )\n\n    if not preserve_rows:\n        auc = auc[test_interactions.getnnz(axis=1) > 0]\n\n    return auc", "function_signature": "def auc_score(\n    model,\n    test_interactions,\n    train_interactions=None,\n    user_features=None,\n    item_features=None,\n    preserve_rows=False,\n    num_threads=1,\n    check_intersections=True,\n) :", "left_context": "# coding=utf-8\n\"\"\"\nModule containing evaluation functions suitable for judging the performance of\na fitted LightFM model.\n\"\"\"\n\nimport numpy as np\n\nfrom ._lightfm_fast import CSRMatrix, calculate_auc_from_rank\n\n__all__ = [\"precision_at_k\", \"recall_at_k\", \"auc_score\", \"reciprocal_rank\"]\n\n\ndef precision_at_k(\n    model,\n    test_interactions,\n    train_interactions=None,\n    k=10,\n    user_features=None,\n    item_features=None,\n    preserve_rows=False,\n    num_threads=1,\n    check_intersections=True,\n):\n    \"\"\"\n    Measure the precision at k metric for a model: the fraction of known\n    positives in the first k positions of the ranked list of results.\n    A perfect score is 1.0.\n\n    Parameters\n    ----------\n\n    model: LightFM instance\n         the fitted model to be evaluated\n    test_interactions: np.float32 csr_matrix of shape [n_users, n_items]\n         Non-zero entries representing known positives in the evaluation set.\n    train_interactions: np.float32 csr_matrix of shape [n_users, n_items], optional\n         Non-zero entries representing known positives in the train set. These\n         will be omitted from the score calculations to avoid re-recommending\n         known positives.\n    k: integer, optional\n         The k parameter.\n    user_features: np.float32 csr_matrix of shape [n_users, n_user_features], optional\n         Each row contains that user's weights over features.\n    item_features: np.float32 csr_matrix of shape [n_items, n_item_features], optional\n         Each row contains that item's weights over features.\n    preserve_rows: boolean, optional\n         When False (default), the number of rows in the output will be equal\n         to the number of users with interactions in the evaluation set.\n         When True, the number of rows in the output will be equal to the\n         number of users.\n    num_threads: int, optional\n         Number of parallel computation threads to use. Should\n         not be higher than the number of physical cores.\n    check_intersections: bool, optional, True by default,\n        Only relevant when train_interactions are supplied.\n        A flag that signals whether the test and train matrices should be checked\n        for intersections to prevent optimistic ranks / wrong evaluation / bad data split.\n\n    Returns\n    -------\n\n    np.array of shape [n_users with interactions or n_users,]\n         Numpy array containing precision@k scores for each user. If there are\n         no interactions for a given user the returned precision will be 0.\n    \"\"\"\n\n    if num_threads < 1:\n        raise ValueError(\"Number of threads must be 1 or larger.\")\n\n    ranks = model.predict_rank(\n        test_interactions,\n        train_interactions=train_interactions,\n        user_features=user_features,\n        item_features=item_features,\n        num_threads=num_threads,\n        check_intersections=check_intersections,\n    )\n\n    ranks.data = np.less(ranks.data, k, ranks.data)\n\n    precision = np.squeeze(np.array(ranks.sum(axis=1))) / k\n\n    if not preserve_rows:\n        precision = precision[test_interactions.getnnz(axis=1) > 0]\n\n    return precision\n\n\ndef recall_at_k(\n    model,\n    test_interactions,\n    train_interactions=None,\n    k=10,\n    user_features=None,\n    item_features=None,\n    preserve_rows=False,\n    num_threads=1,\n    check_intersections=True,\n):\n    \"\"\"\n    Measure the recall at k metric for a model: the number of positive items in\n    the first k positions of the ranked list of results divided by the number\n    of positive items in the test period. A perfect score is 1.0.\n\n    Parameters\n    ----------\n\n    model: LightFM instance\n         the fitted model to be evaluated\n    test_interactions: np.float32 csr_matrix of shape [n_users, n_items]\n         Non-zero entries representing known positives in the evaluation set.\n    train_interactions: np.float32 csr_matrix of shape [n_users, n_items], optional\n         Non-zero entries representing known positives in the train set. These\n         will be omitted from the score calculations to avoid re-recommending\n         known positives.\n    k: integer, optional\n         The k parameter.\n    user_features: np.float32 csr_matrix of shape [n_users, n_user_features], optional\n         Each row contains that user's weights over features.\n    item_features: np.float32 csr_matrix of shape [n_items, n_item_features], optional\n         Each row contains that item's weights over features.\n    preserve_rows: boolean, optional\n         When False (default), the number of rows in the output will be equal\n         to the number of users with interactions in the evaluation set.\n         When True, the number of rows in the output will be equal to the\n         number of users.\n    num_threads: int, optional\n         Number of parallel computation threads to use. Should\n         not be higher than the number of physical cores.\n    check_intersections: bool, optional, True by default,\n        Only relevant when train_interactions are supplied.\n        A flag that signals whether the test and train matrices should be checked\n        for intersections to prevent optimistic ranks / wrong evaluation / bad data split.\n\n    Returns\n    -------\n\n    np.array of shape [n_users with interactions or n_users,]\n         Numpy array containing recall@k scores for each user. If there are no\n         interactions for a given user having items in the test period, the\n         returned recall will be 0.\n    \"\"\"\n\n    if num_threads < 1:\n        raise ValueError(\"Number of threads must be 1 or larger.\")\n\n    ranks = model.predict_rank(\n        test_interactions,\n        train_interactions=train_interactions,\n        user_features=user_features,\n        item_features=item_features,\n        num_threads=num_threads,\n        check_intersections=check_intersections,\n    )\n\n    ranks.data = np.less(ranks.data, k, ranks.data)\n\n    retrieved = np.squeeze(test_interactions.getnnz(axis=1))\n    hit = np.squeeze(np.array(ranks.sum(axis=1)))\n\n    if not preserve_rows:\n        hit = hit[test_interactions.getnnz(axis=1) > 0]\n        retrieved = retrieved[test_interactions.getnnz(axis=1) > 0]\n\n    return hit / retrieved\n\n", "right_context": "\n\ndef reciprocal_rank(\n    model,\n    test_interactions,\n    train_interactions=None,\n    user_features=None,\n    item_features=None,\n    preserve_rows=False,\n    num_threads=1,\n    check_intersections=True,\n):\n    \"\"\"\n    Measure the reciprocal rank metric for a model: 1 / the rank of the highest\n    ranked positive example. A perfect score is 1.0.\n\n    Parameters\n    ----------\n\n    model: LightFM instance\n         the fitted model to be evaluated\n    test_interactions: np.float32 csr_matrix of shape [n_users, n_items]\n         Non-zero entries representing known positives in the evaluation set.\n    train_interactions: np.float32 csr_matrix of shape [n_users, n_items], optional\n         Non-zero entries representing known positives in the train set. These\n         will be omitted from the score calculations to avoid re-recommending\n         known positives.\n    user_features: np.float32 csr_matrix of shape [n_users, n_user_features], optional\n         Each row contains that user's weights over features.\n    item_features: np.float32 csr_matrix of shape [n_items, n_item_features], optional\n         Each row contains that item's weights over features.\n    preserve_rows: boolean, optional\n         When False (default), the number of rows in the output will be equal\n         to the number of users with interactions in the evaluation set.\n         When True, the number of rows in the output will be equal to the\n         number of users.\n    num_threads: int, optional\n         Number of parallel computation threads to use. Should\n         not be higher than the number of physical cores.\n    check_intersections: bool, optional, True by default,\n        Only relevant when train_interactions are supplied.\n        A flag that signals whether the test and train matrices should be checked\n        for intersections to prevent optimistic ranks / wrong evaluation / bad data split.\n\n    Returns\n    -------\n\n    np.array of shape [n_users with interactions or n_users,]\n         Numpy array containing reciprocal rank scores for each user.\n         If there are no interactions for a given user the returned value will\n         be 0.0.\n    \"\"\"\n\n    if num_threads < 1:\n        raise ValueError(\"Number of threads must be 1 or larger.\")\n\n    ranks = model.predict_rank(\n        test_interactions,\n        train_interactions=train_interactions,\n        user_features=user_features,\n        item_features=item_features,\n        num_threads=num_threads,\n        check_intersections=check_intersections,\n    )\n\n    ranks.data = 1.0 / (ranks.data + 1.0)\n\n    ranks = np.squeeze(np.array(ranks.max(axis=1).todense()))\n\n    if not preserve_rows:\n        ranks = ranks[test_interactions.getnnz(axis=1) > 0]\n\n    return ranks\n", "import_text": ["numpy"], "prompt": "\"\"\"\nDescription: This function calculates the Area Under the Curve (AUC) score for a given model.\n\nArgs:\n    model (object): The model to be evaluated.\n    test_interactions (CSRMatrix): The test interactions for the model.\n    train_interactions (CSRMatrix, optional): The training interactions for the model. Defaults to None.\n    user_features (CSRMatrix, optional): The user features for the model. Defaults to None.\n    item_features (CSRMatrix, optional): The item features for the model. Defaults to None.\n    preserve_rows (bool, optional): Whether to preserve rows. Defaults to False.\n    num_threads (int, optional): The number of threads to use. Defaults to 1.\n    check_intersections (bool, optional): Whether to check intersections. Defaults to True.\n\nReturns:\n    numpy.ndarray: The AUC score for the model.\n\nRaises:\n    ValueError: If the number of threads is less than 1.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Measure the ROC AUC metric for a model: the probability that a randomly\n    chosen positive example has a higher score than a randomly chosen negative\n    example.\n    A perfect score is 1.0.\n\n    Parameters\n    ----------\n\n    model: LightFM instance\n         the fitted model to be evaluated\n    test_interactions: np.float32 csr_matrix of shape [n_users, n_items]\n         Non-zero entries representing known positives in the evaluation set.\n    train_interactions: np.float32 csr_matrix of shape [n_users, n_items], optional\n         Non-zero entries representing known positives in the train set. These\n         will be omitted from the score calculations to avoid re-recommending\n         known positives.\n    user_features: np.float32 csr_matrix of shape [n_users, n_user_features], optional\n         Each row contains that user's weights over features.\n    item_features: np.float32 csr_matrix of shape [n_items, n_item_features], optional\n         Each row contains that item's weights over features.\n    preserve_rows: boolean, optional\n         When False (default), the number of rows in the output will be equal\n         to the number of users with interactions in the evaluation set.\n         When True, the number of rows in the output will be equal to the\n         number of users.\n    num_threads: int, optional\n         Number of parallel computation threads to use. Should\n         not be higher than the number of physical cores.\n    check_intersections: bool, optional, True by default,\n        Only relevant when train_interactions are supplied.\n        A flag that signals whether the test and train matrices should be checked\n        for intersections to prevent optimistic ranks / wrong evaluation / bad data split.\n\n    Returns\n    -------\n\n    np.array of shape [n_users with interactions or n_users,]\n         Numpy array containing AUC scores for each user. If there are no\n         interactions for a given user the returned AUC will be 0.5.\n    \"\"\"", "function_dependencies": ["numpy.all", "numpy.zeros", "numpy.squeeze", "numpy.array", "numpy.array.astype"], "project_create_time": "2015-07-30T08:34:00+00:00", "project_update_time": "2024-04-17T13:01:14+00:00", "file_create_time": "2016-04-20T16:31:35Z", "file_update_time": "2018-12-06T06:20:45Z", "function_update_time": "2018-07-14T11:14:13Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["numpy.zeros", "numpy.all"], "test_function": [{"file_path": "/lightfm-1.17/lightfm-1.17/tests/test_evaluation.py", "class_name": null, "function_name": "test_auc_score", "code": "\ndef test_auc_score():\n\n    no_users, no_items = (10, 100)\n\n    train, test = _generate_data(no_users, no_items)\n\n    model = LightFM(loss=\"bpr\")\n    model.fit_partial(train)\n\n    auc = evaluation.auc_score(model, test, num_threads=2)\n    expected_auc = np.array(_auc(model, test))\n\n    assert auc.shape == expected_auc.shape\n    assert np.abs(auc.mean() - expected_auc.mean()) < 0.01\n    assert len(auc) == (test.getnnz(axis=1) > 0).sum()\n    assert len(evaluation.auc_score(model, train, preserve_rows=True)) == test.shape[0]\n\n    # With omitting train interactions\n    auc = evaluation.auc_score(model, test, train_interactions=train, num_threads=2)\n    expected_auc = np.array(_auc(model, test, train))\n    assert np.abs(auc.mean() - expected_auc.mean()) < 0.01"}, {"file_path": "/lightfm-1.17/lightfm-1.17/tests/test_evaluation.py", "class_name": null, "function_name": "test_intersections_check", "code": "\ndef test_intersections_check():\n\n    no_users, no_items = (10, 100)\n\n    train, test = _generate_data(no_users, no_items)\n\n    model = LightFM(loss=\"bpr\")\n    model.fit_partial(train)\n\n    # check error is raised when train and test have interactions in common\n    with pytest.raises(ValueError):\n        evaluation.auc_score(\n            model, train, train_interactions=train, check_intersections=True\n        )\n\n    with pytest.raises(ValueError):\n        evaluation.recall_at_k(\n            model, train, train_interactions=train, check_intersections=True\n        )\n\n    with pytest.raises(ValueError):\n        evaluation.precision_at_k(\n            model, train, train_interactions=train, check_intersections=True\n        )\n\n    with pytest.raises(ValueError):\n        evaluation.reciprocal_rank(\n            model, train, train_interactions=train, check_intersections=True\n        )\n\n    # check no errors raised when train and test have no interactions in common\n    evaluation.auc_score(\n        model, test, train_interactions=train, check_intersections=True\n    )\n    evaluation.recall_at_k(\n        model, test, train_interactions=train, check_intersections=True\n    )\n    evaluation.precision_at_k(\n        model, test, train_interactions=train, check_intersections=True\n    )\n    evaluation.reciprocal_rank(\n        model, test, train_interactions=train, check_intersections=True\n    )\n\n    # check no error is raised when there are intersections but flag is False\n    evaluation.auc_score(\n        model, train, train_interactions=train, check_intersections=False\n    )\n    evaluation.recall_at_k(\n        model, train, train_interactions=train, check_intersections=False\n    )\n    evaluation.precision_at_k(\n        model, train, train_interactions=train, check_intersections=False\n    )\n    evaluation.reciprocal_rank(\n        model, train, train_interactions=train, check_intersections=False\n    )"}]}, {"git_group": "mwaskom", "git_name": "seaborn", "version": "v0.13.2", "language": "Python", "project_name": "seaborn-v0.13.2.zip", "file_path": "/seaborn-v0.13.2/seaborn-0.13.2/seaborn/_marks/base.py", "file_name": "base.py", "focal_class": null, "focal_name": "resolve_color", "focal_parameter": [], "solution": "def resolve_color(\n    mark: Mark,\n    data: DataFrame | dict,\n    prefix: str = \"\",\n    scales: dict[str, Scale] | None = None,\n) -> RGBATuple | ndarray:\n    color = mark._resolve(data, f\"{prefix}color\", scales)\n\n    if f\"{prefix}alpha\" in mark._mappable_props:\n        alpha = mark._resolve(data, f\"{prefix}alpha\", scales)\n    else:\n        alpha = mark._resolve(data, \"alpha\", scales)\n\n    def visible(x, axis=None):\n        \"\"\"Detect \"invisible\" colors to set alpha appropriately.\"\"\"\n        # TODO First clause only needed to handle non-rgba arrays,\n        # which we are trying to handle upstream\n        return np.array(x).dtype.kind != \"f\" or np.isfinite(x).all(axis)\n\n    # Second check here catches vectors of strings with identity scale\n    # It could probably be handled better upstream. This is a tricky problem\n    if np.ndim(color) < 2 and all(isinstance(x, float) for x in color):\n        if len(color) == 4:\n            return mpl.colors.to_rgba(color)\n        alpha = alpha if visible(color) else np.nan\n        return mpl.colors.to_rgba(color, alpha)\n    else:\n        if np.ndim(color) == 2 and color.shape[1] == 4:\n            return mpl.colors.to_rgba_array(color)\n        alpha = np.where(visible(color, axis=1), alpha, np.nan)\n        return mpl.colors.to_rgba_array(color, alpha)\n\n    # TODO should we be implementing fill here too?\n    # (i.e. set fillalpha to 0 when fill=False)", "function_signature": "def resolve_color(\n    mark: Mark,\n    data: DataFrame | dict,\n    prefix: str = \"\",\n    scales: dict[str, Scale] | None = None,\n) -> RGBATuple | ndarray :", "left_context": "from __future__ import annotations\nfrom dataclasses import dataclass, fields, field\nimport textwrap\nfrom typing import Any, Callable, Union\nfrom collections.abc import Generator\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\n\nfrom numpy import ndarray\nfrom pandas import DataFrame\nfrom matplotlib.artist import Artist\n\nfrom seaborn._core.scales import Scale\nfrom seaborn._core.properties import (\n    PROPERTIES,\n    Property,\n    RGBATuple,\n    DashPattern,\n    DashPatternWithOffset,\n)\nfrom seaborn._core.exceptions import PlotSpecError\n\n\nclass Mappable:\n    def __init__(\n        self,\n        val: Any = None,\n        depend: str | None = None,\n        rc: str | None = None,\n        auto: bool = False,\n        grouping: bool = True,\n    ):\n        \"\"\"\n        Property that can be mapped from data or set directly, with flexible defaults.\n\n        Parameters\n        ----------\n        val : Any\n            Use this value as the default.\n        depend : str\n            Use the value of this feature as the default.\n        rc : str\n            Use the value of this rcParam as the default.\n        auto : bool\n            The default value will depend on other parameters at compile time.\n        grouping : bool\n            If True, use the mapped variable to define groups.\n\n        \"\"\"\n        if depend is not None:\n            assert depend in PROPERTIES\n        if rc is not None:\n            assert rc in mpl.rcParams\n\n        self._val = val\n        self._rc = rc\n        self._depend = depend\n        self._auto = auto\n        self._grouping = grouping\n\n    def __repr__(self):\n        \"\"\"Nice formatting for when object appears in Mark init signature.\"\"\"\n        if self._val is not None:\n            s = f\"<{repr(self._val)}>\"\n        elif self._depend is not None:\n            s = f\"<depend:{self._depend}>\"\n        elif self._rc is not None:\n            s = f\"<rc:{self._rc}>\"\n        elif self._auto:\n            s = \"<auto>\"\n        else:\n            s = \"<undefined>\"\n        return s\n\n    @property\n    def depend(self) -> Any:\n        \"\"\"Return the name of the feature to source a default value from.\"\"\"\n        return self._depend\n\n    @property\n    def grouping(self) -> bool:\n        return self._grouping\n\n    @property\n    def default(self) -> Any:\n        \"\"\"Get the default value for this feature, or access the relevant rcParam.\"\"\"\n        if self._val is not None:\n            return self._val\n        elif self._rc is not None:\n            return mpl.rcParams.get(self._rc)\n\n\n# TODO where is the right place to put this kind of type aliasing?\n\nMappableBool = Union[bool, Mappable]\nMappableString = Union[str, Mappable]\nMappableFloat = Union[float, Mappable]\nMappableColor = Union[str, tuple, Mappable]\nMappableStyle = Union[str, DashPattern, DashPatternWithOffset, Mappable]\n\n\n@dataclass\nclass Mark:\n    \"\"\"Base class for objects that visually represent data.\"\"\"\n\n    artist_kws: dict = field(default_factory=dict)\n\n    @property\n    def _mappable_props(self):\n        return {\n            f.name: getattr(self, f.name) for f in fields(self)\n            if isinstance(f.default, Mappable)\n        }\n\n    @property\n    def _grouping_props(self):\n        # TODO does it make sense to have variation within a Mark's\n        # properties about whether they are grouping?\n        return [\n            f.name for f in fields(self)\n            if isinstance(f.default, Mappable) and f.default.grouping\n        ]\n\n    # TODO make this method private? Would extender every need to call directly?\n    def _resolve(\n        self,\n        data: DataFrame | dict[str, Any],\n        name: str,\n        scales: dict[str, Scale] | None = None,\n    ) -> Any:\n        \"\"\"Obtain default, specified, or mapped value for a named feature.\n\n        Parameters\n        ----------\n        data : DataFrame or dict with scalar values\n            Container with data values for features that will be semantically mapped.\n        name : string\n            Identity of the feature / semantic.\n        scales: dict\n            Mapping from variable to corresponding scale object.\n\n        Returns\n        -------\n        value or array of values\n            Outer return type depends on whether `data` is a dict (implying that\n            we want a single value) or DataFrame (implying that we want an array\n            of values with matching length).\n\n        \"\"\"\n        feature = self._mappable_props[name]\n        prop = PROPERTIES.get(name, Property(name))\n        directly_specified = not isinstance(feature, Mappable)\n        return_multiple = isinstance(data, pd.DataFrame)\n        return_array = return_multiple and not name.endswith(\"style\")\n\n        # Special case width because it needs to be resolved and added to the dataframe\n        # during layer prep (so the Move operations use it properly).\n        # TODO how does width *scaling* work, e.g. for violin width by count?\n        if name == \"width\":\n            directly_specified = directly_specified and name not in data\n\n        if directly_specified:\n            feature = prop.standardize(feature)\n            if return_multiple:\n                feature = [feature] * len(data)\n            if return_array:\n                feature = np.array(feature)\n            return feature\n\n        if name in data:\n            if scales is None or name not in scales:\n                # TODO Might this obviate the identity scale? Just don't add a scale?\n                feature = data[name]\n            else:\n                scale = scales[name]\n                value = data[name]\n                try:\n                    feature = scale(value)\n                except Exception as err:\n                    raise PlotSpecError._during(\"Scaling operation\", name) from err\n\n            if return_array:\n                feature = np.asarray(feature)\n            return feature\n\n        if feature.depend is not None:\n            # TODO add source_func or similar to transform the source value?\n            # e.g. set linewidth as a proportion of pointsize?\n            return self._resolve(data, feature.depend, scales)\n\n        default = prop.standardize(feature.default)\n        if return_multiple:\n            default = [default] * len(data)\n        if return_array:\n            default = np.array(default)\n        return default\n\n    def _infer_orient(self, scales: dict) -> str:  # TODO type scales\n\n        # TODO The original version of this (in seaborn._base) did more checking.\n        # Paring that down here for the prototype to see what restrictions make sense.\n\n        # TODO rethink this to map from scale type to \"DV priority\" and use that?\n        # e.g. Nominal > Discrete > Continuous\n\n        x = 0 if \"x\" not in scales else scales[\"x\"]._priority\n        y = 0 if \"y\" not in scales else scales[\"y\"]._priority\n\n        if y > x:\n            return \"y\"\n        else:\n            return \"x\"\n\n    def _plot(\n        self,\n        split_generator: Callable[[], Generator],\n        scales: dict[str, Scale],\n        orient: str,\n    ) -> None:\n        \"\"\"Main interface for creating a plot.\"\"\"\n        raise NotImplementedError()\n\n    def _legend_artist(\n        self, variables: list[str], value: Any, scales: dict[str, Scale],\n    ) -> Artist | None:\n\n        return None\n\n\ndef resolve_properties(\n    mark: Mark, data: DataFrame, scales: dict[str, Scale]\n) -> dict[str, Any]:\n\n    props = {\n        name: mark._resolve(data, name, scales) for name in mark._mappable_props\n    }\n    return props\n\n", "right_context": "\n\ndef document_properties(mark):\n\n    properties = [f.name for f in fields(mark) if isinstance(f.default, Mappable)]\n    text = [\n        \"\",\n        \"    This mark defines the following properties:\",\n        textwrap.fill(\n            \", \".join([f\"|{p}|\" for p in properties]),\n            width=78, initial_indent=\" \" * 8, subsequent_indent=\" \" * 8,\n        ),\n    ]\n\n    docstring_lines = mark.__doc__.split(\"\\n\")\n    new_docstring = \"\\n\".join([\n        *docstring_lines[:2],\n        *text,\n        *docstring_lines[2:],\n    ])\n    mark.__doc__ = new_docstring\n    return mark\n", "import_text": ["dataclasses.dataclass", "dataclasses.fields", "dataclasses.field", "textwrap", "typing.Any", "typing.Callable", "typing.Union", "collections.abc.Generator", "numpy", "pandas", "matplotlib", "numpy.ndarray", "pandas.DataFrame", "matplotlib.artist.Artist", "seaborn._core.scales.Scale", "seaborn._core.properties.PROPERTIES", "seaborn._core.properties.Property", "seaborn._core.properties.RGBATuple", "seaborn._core.properties.DashPattern", "seaborn._core.properties.DashPatternWithOffset", "seaborn._core.exceptions.PlotSpecError"], "prompt": "\"\"\"\nDescription: This function resolves the color of a mark based on the provided data and scales.\n\nArgs:\n    mark (Mark): The mark object to resolve the color for.\n    data (DataFrame | dict): The data to use for resolving the color.\n    prefix (str, optional): A prefix to prepend to the color property name. Defaults to \"\".\n    scales (dict[str, Scale] | None, optional): The scales to use for resolving the color. Defaults to None.\n\nReturns:\n    RGBATuple | ndarray: The resolved color as an RGBA tuple or an array of RGBA tuples.\n\nRaises:\n    None\n\nNotes:\n    - The function uses the `_resolve` method of the mark object to resolve the color and alpha values.\n    - The alpha value is determined based on the presence of the `{prefix}alpha` property in the mark's mappable properties.\n    - The `visible` function is used to determine if a color is visible, based on its type and values.\n    - If the color is a single value or a vector of strings with an identity scale, the function returns the color with the appropriate alpha value.\n    - If the color is a 2D array with 4 columns, the function returns the color array with the appropriate alpha values.\n    - The function uses the `mpl.colors.to_rgba` and `mpl.colors.to_rgba_array` functions from matplotlib to convert colors to RGBA format.\n    - The function uses the `numpy.isfinite` function to check if the color values are finite.\n    - The function does not handle the `fill` property.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Obtain a default, specified, or mapped value for a color feature.\n\n    This method exists separately to support the relationship between a\n    color and its corresponding alpha. We want to respect alpha values that\n    are passed in specified (or mapped) color values but also make use of a\n    separate `alpha` variable, which can be mapped. This approach may also\n    be extended to support mapping of specific color channels (i.e.\n    luminance, chroma) in the future.\n\n    Parameters\n    ----------\n    mark :\n        Mark with the color property.\n    data :\n        Container with data values for features that will be semantically mapped.\n    prefix :\n        Support \"color\", \"fillcolor\", etc.\n\n    \"\"\"", "function_dependencies": ["numpy.array", "numpy.isfinite", "numpy.isfinite.all", "numpy.ndim", "matplotlib.colors.to_rgba", "matplotlib.colors.to_rgba_array", "numpy.where"], "project_create_time": "2012-06-18T18:41:19+00:00", "project_update_time": "2024-04-18T02:55:57+00:00", "file_create_time": "2021-06-03T21:56:46Z", "file_update_time": "2023-08-31T12:54:00Z", "function_update_time": "2022-05-03T23:37:41Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["matplotlib.colors.to_rgba_array", "numpy.isfinite"], "test_function": [{"file_path": "/seaborn-v0.13.2/seaborn-0.13.2/tests/_marks/test_base.py", "class_name": "TestMappable", "function_name": "test_color_mapped_alpha", "code": "\n    def test_color_mapped_alpha(self):\n\n        c = \"r\"\n        values = {\"a\": .2, \"b\": .5, \"c\": .8}\n\n        m = self.mark(color=c, alpha=Mappable(1))\n        scales = {\"alpha\": lambda s: np.array([values[s_i] for s_i in s])}\n\n        assert resolve_color(m, {\"alpha\": \"b\"}, \"\", scales) == mpl.colors.to_rgba(c, .5)\n\n        df = pd.DataFrame({\"alpha\": list(values.keys())})\n\n        # Do this in two steps for mpl 3.2 compat\n        expected = mpl.colors.to_rgba_array([c] * len(df))\n        expected[:, 3] = list(values.values())\n\n        assert_array_equal(resolve_color(m, df, \"\", scales), expected)"}, {"file_path": "/seaborn-v0.13.2/seaborn-0.13.2/tests/_marks/test_base.py", "class_name": "TestMappable", "function_name": "test_fillcolor", "code": "\n    def test_fillcolor(self):\n\n        c, a = \"green\", .8\n        fa = .2\n        m = self.mark(\n            color=c, alpha=a,\n            fillcolor=Mappable(depend=\"color\"), fillalpha=Mappable(fa),\n        )\n\n        assert resolve_color(m, {}) == mpl.colors.to_rgba(c, a)\n        assert resolve_color(m, {}, \"fill\") == mpl.colors.to_rgba(c, fa)\n\n        df = pd.DataFrame(index=pd.RangeIndex(10))\n        cs = [c] * len(df)\n        assert_array_equal(resolve_color(m, df), mpl.colors.to_rgba_array(cs, a))\n        assert_array_equal(\n            resolve_color(m, df, \"fill\"), mpl.colors.to_rgba_array(cs, fa)\n        )"}]}, {"git_group": "holoviz", "git_name": "holoviews", "version": "v1.19.0a1", "language": "Python", "project_name": "holoviews-v1.19.0a1.zip", "file_path": "/holoviews-v1.19.0a1/holoviews-1.19.0a1/holoviews/plotting/mpl/util.py", "file_name": "util.py", "focal_class": null, "focal_name": "polygons_to_path_patches", "focal_parameter": ["element"], "solution": "def polygons_to_path_patches(element):\n    paths = element.split(datatype='array', dimensions=element.kdims)\n    has_holes = isinstance(element, Polygons) and element.interface.has_holes(element)\n    holes = element.interface.holes(element) if has_holes else None\n    mpl_paths = []\n    for i, path in enumerate(paths):\n        splits = np.where(np.isnan(path[:, :2].astype('float')).sum(axis=1))[0]\n        arrays = np.split(path, splits+1) if len(splits) else [path]\n        subpath = []\n        for j, array in enumerate(arrays):\n            if j != (len(arrays)-1):\n                array = array[:-1]\n            if (array[0] != array[-1]).any():\n                array = np.append(array, array[:1], axis=0)\n            interiors = []\n            for interior in (holes[i][j] if has_holes else []):\n                if (interior[0] != interior[-1]).any():\n                    interior = np.append(interior, interior[:1], axis=0)\n                interiors.append(interior)\n            vertices = np.concatenate([array]+interiors)\n            codes = np.concatenate([ring_coding(array)]+\n                                   [ring_coding(h) for h in interiors])\n            subpath.append(PathPatch(Path(vertices, codes)))\n        mpl_paths.append(subpath)\n    return mpl_paths", "function_signature": "def polygons_to_path_patches(element) :", "left_context": "import inspect\nimport re\nimport warnings\n\nimport matplotlib as mpl\nimport numpy as np\nfrom matplotlib import (\n    ticker,\n    units as munits,\n)\nfrom matplotlib.colors import Normalize, cnames\nfrom matplotlib.lines import Line2D\nfrom matplotlib.markers import MarkerStyle\nfrom matplotlib.patches import Path, PathPatch\nfrom matplotlib.rcsetup import validate_fontsize, validate_fonttype, validate_hatch\nfrom matplotlib.transforms import Affine2D, Bbox, TransformedBbox\nfrom packaging.version import Version\n\ntry:  # starting Matplotlib 3.4.0\n    from matplotlib._enums import (\n        CapStyle as validate_capstyle,\n        JoinStyle as validate_joinstyle,\n    )\nexcept ImportError:  # before Matplotlib 3.4.0\n    from matplotlib.rcsetup import validate_capstyle, validate_joinstyle\n\ntry:\n    from nc_time_axis import CalendarDateTime, NetCDFTimeConverter\n    nc_axis_available = True\nexcept ImportError:\n    from matplotlib.dates import DateConverter\n    NetCDFTimeConverter = DateConverter\n    nc_axis_available = False\n\nfrom ...core.util import arraylike_types, cftime_types, is_number\nfrom ...element import RGB, Polygons, Raster\nfrom ..util import COLOR_ALIASES, RGB_HEX_REGEX\n\nmpl_version = Version(mpl.__version__)\n\n\ndef is_color(color):\n    \"\"\"\n    Checks if supplied object is a valid color spec.\n    \"\"\"\n    if not isinstance(color, str):\n        return False\n    elif RGB_HEX_REGEX.match(color):\n        return True\n    elif color in COLOR_ALIASES:\n        return True\n    elif color in cnames:\n        return True\n    return False\n\nvalidators = {\n    'alpha': lambda x: is_number(x) and (0 <= x <= 1),\n    'capstyle': validate_capstyle,\n    'color': is_color,\n    'fontsize': validate_fontsize,\n    'fonttype': validate_fonttype,\n    'hatch': validate_hatch,\n    'joinstyle': validate_joinstyle,\n    'marker': lambda x: (\n        x in Line2D.markers\n        or isinstance(x, (MarkerStyle, Path))\n        or (isinstance(x, str) and x.startswith('$') and x.endswith('$'))\n    ),\n    's': lambda x: is_number(x) and (x >= 0)\n}\n\ndef get_old_rcparams():\n    deprecated_rcparams = [\n        'text.latex.unicode',\n        'examples.directory',\n        'savefig.frameon', # deprecated in MPL 3.1, to be removed in 3.3\n        'verbose.level', # deprecated in MPL 3.1, to be removed in 3.3\n        'verbose.fileo', # deprecated in MPL 3.1, to be removed in 3.3\n        'datapath', # deprecated in MPL 3.2.1, to be removed in 3.3\n        'text.latex.preview', # deprecated in MPL 3.3.1\n        'animation.avconv_args', # deprecated in MPL 3.3.1\n        'animation.avconv_path', # deprecated in MPL 3.3.1\n        'animation.html_args', # deprecated in MPL 3.3.1\n        'keymap.all_axes', # deprecated in MPL 3.3.1\n        'savefig.jpeg_quality' # deprecated in MPL 3.3.1\n    ]\n    old_rcparams = {\n        k: v for k, v in mpl.rcParams.items()\n        if mpl_version < Version('3.0') or k not in deprecated_rcparams\n    }\n    return old_rcparams\n\n\ndef get_validator(style):\n    for k, v in validators.items():\n        if style.endswith(k) and (len(style) != 1 or style == k):\n            return v\n\n\ndef validate(style, value, vectorized=True):\n    \"\"\"\n    Validates a style and associated value.\n\n    Arguments\n    ---------\n    style: str\n       The style to validate (e.g. 'color', 'size' or 'marker')\n    value:\n       The style value to validate\n    vectorized: bool\n       Whether validator should allow vectorized setting\n\n    Returns\n    -------\n    valid: boolean or None\n       If validation is supported returns boolean, otherwise None\n    \"\"\"\n    validator = get_validator(style)\n    if validator is None:\n        return None\n    if isinstance(value, arraylike_types+(list,)) and vectorized:\n        return all(validator(v) for v in value)\n    try:\n        valid = validator(value)\n        return False if valid == False else True\n    except Exception:\n        return False\n\n\ndef filter_styles(style, group, other_groups, blacklist=None):\n    \"\"\"\n    Filters styles which are specific to a particular artist, e.g.\n    for a GraphPlot this will filter options specific to the nodes and\n    edges.\n\n    Arguments\n    ---------\n    style: dict\n        Dictionary of styles and values\n    group: str\n        Group within the styles to filter for\n    other_groups: list\n        Other groups to filter out\n    blacklist: list (optional)\n        List of options to filter out\n\n    Returns\n    -------\n    filtered: dict\n        Filtered dictionary of styles\n    \"\"\"\n    if blacklist is None:\n        blacklist = []\n    group = group+'_'\n    filtered = {}\n    for k, v in style.items():\n        if (any(k.startswith(p) for p in other_groups)\n            or k.startswith(group) or k in blacklist):\n            continue\n        filtered[k] = v\n    for k, v in style.items():\n        if not k.startswith(group) or k in blacklist:\n            continue\n        filtered[k[len(group):]] = v\n    return filtered\n\n\ndef wrap_formatter(formatter):\n    \"\"\"\n    Wraps formatting function or string in\n    appropriate matplotlib formatter type.\n    \"\"\"\n    if isinstance(formatter, ticker.Formatter):\n        return formatter\n    elif callable(formatter):\n        args = [arg for arg in inspect.getfullargspec(formatter).args\n                if arg != 'self']\n        wrapped = formatter\n        if len(args) == 1:\n            def wrapped(val, pos=None):\n                return formatter(val)\n        return ticker.FuncFormatter(wrapped)\n    elif isinstance(formatter, str):\n        if re.findall(r\"\\{(\\w+)\\}\", formatter):\n            return ticker.StrMethodFormatter(formatter)\n        else:\n            return ticker.FormatStrFormatter(formatter)\n\ndef unpack_adjoints(ratios):\n    new_ratios = {}\n    offset = 0\n    for k, (num, ratio_values) in sorted(ratios.items()):\n        unpacked = [[] for _ in range(num)]\n        for r in ratio_values:\n            nr = len(r)\n            for i in range(num):\n                unpacked[i].append(r[i] if i < nr else np.nan)\n        for i, r in enumerate(unpacked):\n            new_ratios[k+i+offset] = r\n        offset += num-1\n    return new_ratios\n\ndef normalize_ratios(ratios):\n    normalized = {}\n    for i, v in enumerate(zip(*ratios.values())):\n        arr = np.array(v)\n        normalized[i] = arr/float(np.nanmax(arr))\n    return normalized\n\ndef compute_ratios(ratios, normalized=True):\n    unpacked = unpack_adjoints(ratios)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n        if normalized:\n            unpacked = normalize_ratios(unpacked)\n        sorted_ratios = sorted(unpacked.items())\n        return np.nanmax(np.vstack([v for _, v in sorted_ratios]), axis=0)\n\n\ndef axis_overlap(ax1, ax2):\n    \"\"\"\n    Tests whether two axes overlap vertically\n    \"\"\"\n    b1, t1 = ax1.get_position().intervaly\n    b2, t2 = ax2.get_position().intervaly\n    return t1 > b2 and b1 < t2\n\n\ndef resolve_rows(rows):\n    \"\"\"\n    Recursively iterate over lists of axes merging\n    them by their vertical overlap leaving a list\n    of rows.\n    \"\"\"\n    merged_rows = []\n    for row in rows:\n        overlap = False\n        for mrow in merged_rows:\n            if any(axis_overlap(ax1, ax2) for ax1 in row\n                   for ax2 in mrow):\n                mrow += row\n                overlap = True\n                break\n        if not overlap:\n            merged_rows.append(row)\n    if rows == merged_rows:\n        return rows\n    else:\n        return resolve_rows(merged_rows)\n\n\ndef fix_aspect(fig, nrows, ncols, title=None, extra_artists=None,\n               vspace=0.2, hspace=0.2):\n    \"\"\"\n    Calculate heights and widths of axes and adjust\n    the size of the figure to match the aspect.\n    \"\"\"\n    if extra_artists is None:\n        extra_artists = []\n    fig.canvas.draw()\n    w, h = fig.get_size_inches()\n\n    # Compute maximum height and width of each row and columns\n    rows = resolve_rows([[ax] for ax in fig.axes])\n    rs, cs = len(rows), max([len(r) for r in rows])\n    heights = [[] for i in range(cs)]\n    widths = [[] for i in range(rs)]\n    for r, row in enumerate(rows):\n        for c, ax in enumerate(row):\n            bbox = ax.get_tightbbox(fig.canvas.get_renderer())\n            heights[c].append(bbox.height)\n            widths[r].append(bbox.width)\n    height = (max([sum(c) for c in heights])) + nrows*vspace*fig.dpi\n    width = (max([sum(r) for r in widths])) + ncols*hspace*fig.dpi\n\n    # Compute aspect and set new size (in inches)\n    aspect = height/width\n    offset = 0\n    if title and title.get_text():\n        offset = title.get_window_extent().height/fig.dpi\n    fig.set_size_inches(w, (w*aspect)+offset)\n\n    # Redraw and adjust title position if defined\n    fig.canvas.draw()\n    if title and title.get_text():\n        extra_artists = [a for a in extra_artists\n                         if a is not title]\n        bbox = get_tight_bbox(fig, extra_artists)\n        top = bbox.intervaly[1]\n        if title and title.get_text():\n            title.set_y(top/(w*aspect))\n\n\ndef get_tight_bbox(fig, bbox_extra_artists=None, pad=None):\n    \"\"\"\n    Compute a tight bounding box around all the artists in the figure.\n    \"\"\"\n    if bbox_extra_artists is None:\n        bbox_extra_artists = []\n    renderer = fig.canvas.get_renderer()\n    bbox_inches = fig.get_tightbbox(renderer)\n    bbox_artists = bbox_extra_artists[:]\n    bbox_artists += fig.get_default_bbox_extra_artists()\n    bbox_filtered = []\n    for a in bbox_artists:\n        bbox = a.get_window_extent(renderer)\n        if isinstance(bbox, tuple):\n            continue\n        if a.get_clip_on():\n            clip_box = a.get_clip_box()\n            if clip_box is not None:\n                bbox = Bbox.intersection(bbox, clip_box)\n            clip_path = a.get_clip_path()\n            if clip_path is not None and bbox is not None:\n                clip_path = clip_path.get_fully_transformed_path()\n                bbox = Bbox.intersection(bbox,\n                                         clip_path.get_extents())\n        if (\n            bbox is not None and\n            (bbox.width != 0 or bbox.height != 0) and\n            np.isfinite(bbox).all()\n        ):\n            bbox_filtered.append(bbox)\n    if bbox_filtered:\n        _bbox = Bbox.union(bbox_filtered)\n        trans = Affine2D().scale(1.0 / fig.dpi)\n        bbox_extra = TransformedBbox(_bbox, trans)\n        bbox_inches = Bbox.union([bbox_inches, bbox_extra])\n    return bbox_inches.padded(pad) if pad else bbox_inches\n\n\ndef get_raster_array(image):\n    \"\"\"\n    Return the array data from any Raster or Image type\n    \"\"\"\n    if isinstance(image, RGB):\n        rgb = image.rgb\n        data = np.dstack([np.flipud(rgb.dimension_values(d, flat=False))\n                          for d in rgb.vdims])\n    else:\n        data = image.dimension_values(2, flat=False)\n        if type(image) is Raster:\n            data = data.T\n        else:\n            data = np.flipud(data)\n    return data\n\n\ndef ring_coding(array):\n    \"\"\"\n    Produces matplotlib Path codes for exterior and interior rings\n    of a polygon geometry.\n    \"\"\"\n    # The codes will be all \"LINETO\" commands, except for \"MOVETO\"s at the\n    # beginning of each subpath\n    n = len(array)\n    codes = np.ones(n, dtype=Path.code_type) * Path.LINETO\n    codes[0] = Path.MOVETO\n    codes[-1] = Path.CLOSEPOLY\n    return codes\n\n", "right_context": "\n\nclass CFTimeConverter(NetCDFTimeConverter):\n    \"\"\"\n    Defines conversions for cftime types by extending nc_time_axis.\n    \"\"\"\n\n    @classmethod\n    def convert(cls, value, unit, axis):\n        if not nc_axis_available:\n            raise ValueError('In order to display cftime types with '\n                             'matplotlib install the nc_time_axis '\n                             'library using pip or from conda-forge '\n                             'using:\\n\\tconda install -c conda-forge '\n                             'nc_time_axis')\n        if isinstance(value, cftime_types):\n            value = CalendarDateTime(value.datetime, value.calendar)\n        elif isinstance(value, np.ndarray):\n            value = np.array([CalendarDateTime(v.datetime, v.calendar) for v in value])\n        return super().convert(value, unit, axis)\n\n\nclass EqHistNormalize(Normalize):\n\n    def __init__(self, vmin=None, vmax=None, clip=False, rescale_discrete_levels=True, nbins=256**2, ncolors=256):\n        super().__init__(vmin, vmax, clip)\n        self._nbins = nbins\n        self._bin_edges = None\n        self._ncolors = ncolors\n        self._color_bins = np.linspace(0, 1, ncolors+1)\n        self._rescale = rescale_discrete_levels\n\n    def binning(self, data, n=256):\n        low = data.min() if self.vmin is None else self.vmin\n        high = data.max() if self.vmax is None else self.vmax\n        nbins = self._nbins\n        eq_bin_edges = np.linspace(low, high, nbins+1)\n        full_hist, _ = np.histogram(data, eq_bin_edges)\n\n        # Remove zeros, leaving extra element at beginning for rescale_discrete_levels\n        nonzero = np.nonzero(full_hist)[0]\n        nhist = len(nonzero)\n        if nhist > 1:\n            hist = np.zeros(nhist+1)\n            hist[1:] = full_hist[nonzero]\n            eq_bin_centers = np.concatenate([[0.], (eq_bin_edges[nonzero] + eq_bin_edges[nonzero+1]) / 2.])\n            eq_bin_centers[0] = 2*eq_bin_centers[1] - eq_bin_centers[-1]\n        else:\n            hist = full_hist\n            eq_bin_centers = np.convolve(eq_bin_edges, [0.5, 0.5], mode='valid')\n\n        # CDF scaled from 0 to 1 except for first value\n        cdf = np.cumsum(hist)\n        lo = cdf[1]\n        diff = cdf[-1] - lo\n        with np.errstate(divide='ignore', invalid='ignore'):\n            cdf = (cdf - lo) / diff\n        cdf[0] = -1.0\n\n        lower_span = 0\n        if self._rescale:\n            discrete_levels = nhist\n            m = -0.5/98.0\n            c = 1.5 - 2*m\n            multiple = m*discrete_levels + c\n            if (multiple > 1):\n                lower_span = 1 - multiple\n\n        cdf_bins = np.linspace(lower_span, 1, n+1)\n        binning = np.interp(cdf_bins, cdf, eq_bin_centers)\n        if not self._rescale:\n            binning[0] = low\n        binning[-1] = high\n        return binning\n\n    def __call__(self, data, clip=None):\n        return self.process_value(data)[0]\n\n    def process_value(self, data):\n        if isinstance(data, np.ndarray):\n            self._bin_edges = self.binning(data, self._ncolors)\n        isscalar = np.isscalar(data)\n        data = np.array([data]) if isscalar else data\n        interped = np.interp(data, self._bin_edges, self._color_bins)\n        return np.ma.array(interped), isscalar\n\n    def inverse(self, value):\n        if self._bin_edges is None:\n            raise ValueError(\"Not invertible until eq_hist has been computed\")\n        return np.interp([value], self._color_bins, self._bin_edges)[0]\n\n\nfor cft in cftime_types:\n    munits.registry[cft] = CFTimeConverter()\n", "import_text": ["inspect", "re", "warnings", "matplotlib", "numpy", "matplotlib.ticker", "matplotlib.units", "matplotlib.colors.Normalize", "matplotlib.colors.cnames", "matplotlib.lines.Line2D", "matplotlib.markers.MarkerStyle", "matplotlib.patches.Path", "matplotlib.patches.PathPatch", "matplotlib.rcsetup.validate_fontsize", "matplotlib.rcsetup.validate_fonttype", "matplotlib.rcsetup.validate_hatch", "matplotlib.transforms.Affine2D", "matplotlib.transforms.Bbox", "matplotlib.transforms.TransformedBbox", "packaging.version.Version"], "prompt": "\"\"\"\nDescription: This function converts an element into a path patch for matplotlib.\n\nArgs:\n    element (type): The element to be converted into a path patch.\n\nReturns:\n    list: A list of path patches. Each path patch is a list of subpaths.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Converts Polygons into list of lists of matplotlib.patches.PathPatch\n    objects including any specified holes. Each list represents one\n    (multi-)polygon.\n    \"\"\"", "function_dependencies": ["numpy.where", "numpy.isnan", "numpy.isnan.sum", "numpy.split", "numpy.append", "numpy.concatenate", "matplotlib.patches.PathPatch", "matplotlib.patches.Path"], "project_create_time": "2014-05-07T16:59:22+00:00", "project_update_time": "2024-04-16T01:42:40+00:00", "file_create_time": "2015-09-16T12:10:01Z", "file_update_time": "2023-10-16T10:15:12Z", "function_update_time": "2018-10-23T00:30:06Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["matplotlib.patches.PathPatch", "numpy.where", "numpy.isnan", "numpy.append"], "test_function": [{"file_path": "/holoviews-v1.19.0a1/holoviews-1.19.0a1/holoviews/tests/plotting/matplotlib/test_utils.py", "class_name": "TestUtils", "function_name": "test_polygon_to_path_patches", "code": "\n    def test_polygon_to_path_patches(self):\n        xs = [1, 2, 3, np.nan, 3, 7, 6, np.nan, 0, 0, 0]\n        ys = [2, 0, 7, np.nan, 2, 5, 7, np.nan, 0, 1, 0]\n\n        holes = [\n            [[(1.5, 2), (2, 3), (1.6, 1.6)], [(2.1, 4.5), (2.5, 5), (2.3, 3.5)]],\n            [],\n            []\n        ]\n        polys = Polygons([{'x': xs, 'y': ys, 'holes': holes}])\n        paths = polygons_to_path_patches(polys)\n\n\n        self.assertEqual(len(paths), 1)\n        self.assertEqual(len(paths[0]), 3)\n        self.assertEqual(paths[0][0].get_path().vertices, np.array([\n            (1, 2), (2, 0), (3, 7), (1, 2),\n            (1.5, 2), (2, 3), (1.6, 1.6), (1.5, 2),\n            (2.1, 4.5), (2.5, 5), (2.3, 3.5), (2.1, 4.5)]))\n        self.assertEqual(paths[0][0].get_path().codes, np.array([1, 2, 2, 79, 1, 2, 2, 79, 1, 2, 2, 79], dtype='uint8'))\n        self.assertEqual(paths[0][1].get_path().vertices, np.array([(3, 2), (7, 5), (6, 7),  (3, 2),]))\n        self.assertEqual(paths[0][1].get_path().codes, np.array([1, 2, 2, 79], dtype='uint8'))\n        self.assertEqual(paths[0][2].get_path().vertices, np.array([(0, 0), (0, 1), (0, 0)]))\n        self.assertEqual(paths[0][1].get_path().codes, np.array([1, 2, 2, 79], dtype='uint8'))"}]}, {"git_group": "py-why", "git_name": "dowhy", "version": "v0.11.1", "language": "Python", "project_name": "dowhy-v0.11.1.zip", "file_path": "/dowhy-v0.11.1/dowhy-0.11.1/dowhy/gcm/stats.py", "file_name": "stats.py", "focal_class": null, "focal_name": "quantile_based_fwer", "focal_parameter": [], "solution": "def quantile_based_fwer(\n    p_values: Union[np.ndarray, List[float]], p_values_scaling: Optional[np.ndarray] = None, quantile: float = 0.5\n) -> float:\n\n    if quantile <= 0 or abs(quantile - 1) >= 1:\n        raise ValueError(\"The given quantile is %f, but it needs to be on (0, 1]!\" % quantile)\n\n    p_values = np.array(p_values)\n    if p_values_scaling is None:\n        p_values_scaling = np.ones(p_values.shape[0])\n\n    if p_values.shape != p_values_scaling.shape:\n        raise ValueError(\"The p-value scaling array needs to have the same dimension as the given p-values.\")\n\n    p_values_scaling = p_values_scaling[~np.isnan(p_values)]\n    p_values = p_values[~np.isnan(p_values)]\n\n    p_values = p_values * p_values_scaling\n    p_values[p_values > 1] = 1.0\n\n    if p_values.shape[0] == 1:\n        return float(p_values[0])\n    else:\n        return float(min(1.0, np.quantile(p_values / quantile, quantile)))", "function_signature": "def quantile_based_fwer(\n    p_values: Union[np.ndarray, List[float]], p_values_scaling: Optional[np.ndarray] = None, quantile: float = 0.5\n) -> float :", "left_context": "from typing import Callable, List, Optional, Union\n\nimport numpy as np\nfrom numpy.matlib import repmat\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\n\nfrom dowhy.gcm.constant import EPS\nfrom dowhy.gcm.util.general import shape_into_2d\n\n", "right_context": "\n\ndef marginal_expectation(\n    prediction_method: Callable[[np.ndarray], np.ndarray],\n    feature_samples: np.ndarray,\n    baseline_samples: np.ndarray,\n    baseline_feature_indices: List[int],\n    return_averaged_results: bool = True,\n    feature_perturbation: str = \"randomize_columns_jointly\",\n    max_batch_size: int = -1,\n) -> np.ndarray:\n    \"\"\"Estimates the marginal expectation for samples in baseline_noise_samples when randomizing features that are not\n    part of baseline_feature_indices. This is, this function estimates\n        y^i = E[Y | do(x^i_s)] := \\\\int_x_s' E[Y | x^i_s, x_s'] p(x_s') d x_s',\n    where x^i_s is the i-th sample from baseline_noise_samples, s denotes the baseline_feature_indices and\n    x_s' ~ X_s' denotes the randomized features that are not in s. For an approximation of the integral, the given\n    prediction_method is evaluated multiple times for the same x^i_s, but different x_s' ~ X_s'.\n\n    :param prediction_method: Prediction method of interest. This should expect a numpy array as input for making\n    predictions.\n    :param feature_samples: Samples from the joint distribution. These are used for randomizing the features that are not in\n                            baseline_feature_indices.\n    :param baseline_samples: Samples for which the marginal expectation should be estimated.\n    :param baseline_feature_indices: Column indices of the features in s. These values for these features are remain constant\n                                     when estimating the expectation.\n    :param return_averaged_results: If set to True, the expectation over all evaluated samples for the i-th\n    baseline_noise_samples is returned. If set to False, all corresponding results for the i-th sample are returned.\n    :param feature_perturbation: Type of feature permutation:\n        'randomize_columns_independently': Each feature not in s is randomly permuted separately.\n        'randomize_columns_jointly': All features not in s are jointly permuted. Note that this still represents an\n        interventional distribution.\n    :param max_batch_size: Maximum batch size for a estimating the predictions. This has a significant influence on the\n    overall memory usage. If set to -1, all samples are used in one batch.\n    :return: If return_averaged_results is False, a numpy array where the i-th entry belongs to the marginal expectation\n    of x^i_s when randomizing the remaining features.\n    If return_averaged_results is True, a two dimensional numpy array where the i-th entry contains all\n    predictions for x^i_s when randomizing the remaining features.\n    \"\"\"\n    feature_samples, baseline_samples = shape_into_2d(feature_samples, baseline_samples)\n\n    batch_size = baseline_samples.shape[0] if max_batch_size == -1 else max_batch_size\n    result = [np.nan] * baseline_samples.shape[0]\n\n    # Make copy to avoid manipulating the original matrix.\n    feature_samples = np.array(feature_samples)\n\n    features_to_randomize = np.delete(np.arange(0, feature_samples.shape[1]), baseline_feature_indices)\n\n    if feature_perturbation == \"randomize_columns_independently\":\n        feature_samples = permute_features(feature_samples, features_to_randomize, False)\n    elif feature_perturbation == \"randomize_columns_jointly\":\n        feature_samples = permute_features(feature_samples, features_to_randomize, True)\n    else:\n        raise ValueError(\"Unknown argument %s as feature_perturbation type!\" % feature_perturbation)\n\n    # The given prediction method has to be evaluated multiple times on a large amount of different inputs. Typically,\n    # the batch evaluation of a prediction model on multiple inputs at the same time is significantly faster\n    # than evaluating it on single simples in a for-loop. To make use of this, we try to evaluate as many samples as\n    # possible in one batch call of the prediction method. However, this also requires a lot of memory for many samples.\n    # To overcome potential memory issues, multiple batch calls are performed, each with at most batch_size many\n    # samples. The number of samples that are evaluated is normally\n    # baseline_noise_samples.shape[0] * feature_samples.shape[0]. Here, we reduce it to\n    # batch_size * feature_samples.shape[0]. If the batch_size would be set 1, then each baseline_noise_samples is\n    # evaluated one by one in a for-loop.\n    inputs = repmat(feature_samples, batch_size, 1)\n    for offset in range(0, baseline_samples.shape[0], batch_size):\n        # Each batch consist of at most batch_size * feature_samples.shape[0] many samples. If there are multiple\n        # batches, the offset indicates the index of the current baseline_noise_samples that has not been evaluated yet.\n        if offset + batch_size > baseline_samples.shape[0]:\n            # If the batch size would be larger than the remaining amount of samples, it is reduced to only include the\n            # remaining baseline_noise_samples.\n            adjusted_batch_size = baseline_samples.shape[0] - offset\n            inputs = inputs[: adjusted_batch_size * feature_samples.shape[0]]\n        else:\n            adjusted_batch_size = batch_size\n\n        for index in range(adjusted_batch_size):\n            # The inputs consist of batch_size many copies of feature_samples. Here, we set the columns of the features\n            # in baseline_feature_indices to their respective values in baseline_noise_samples.\n            inputs[\n                index * feature_samples.shape[0] : (index + 1) * feature_samples.shape[0], baseline_feature_indices\n            ] = baseline_samples[offset + index, baseline_feature_indices]\n\n        # After creating the (potentially large) input data matrix, we can evaluate the prediction method.\n        predictions = np.array(prediction_method(inputs))\n\n        for index in range(adjusted_batch_size):\n            # Here, offset + index now indicates the sample index in baseline_noise_samples.\n            if return_averaged_results:\n                # This would average all prediction results obtained for the 'offset + index'-th sample in\n                # baseline_noise_samples. This is, y^(offset + index) = E[Y | do(x^(offset + index)_s)].\n                result[offset + index] = np.mean(\n                    predictions[index * feature_samples.shape[0] : (index + 1) * feature_samples.shape[0]], axis=0\n                )\n            else:\n                # This would return all prediction results obtained for the 'offset + index'-th sample in\n                # baseline_noise_samples, i.e. the results are not averaged.\n                result[offset + index] = predictions[\n                    index * feature_samples.shape[0] : (index + 1) * feature_samples.shape[0]\n                ]\n\n    return np.array(result)\n\n\ndef permute_features(\n    feature_samples: np.ndarray, features_to_permute: Union[List[int], np.ndarray], randomize_features_jointly: bool\n) -> np.ndarray:\n    # Making copy to ensure that the original object is not modified.\n    feature_samples = np.array(feature_samples)\n\n    if randomize_features_jointly:\n        # Permute samples jointly. This still represents an interventional distribution.\n        feature_samples[:, features_to_permute] = feature_samples[\n            np.random.choice(feature_samples.shape[0], feature_samples.shape[0], replace=False)\n        ][:, features_to_permute]\n    else:\n        # Permute samples independently.\n        for feature in features_to_permute:\n            np.random.shuffle(feature_samples[:, feature])\n\n    return feature_samples\n\n\ndef estimate_ftest_pvalue(\n    X_training_a: np.ndarray,\n    X_training_b: np.ndarray,\n    Y_training: np.ndarray,\n    X_test_a: np.ndarray,\n    X_test_b: np.ndarray,\n    Y_test: np.ndarray,\n) -> float:\n    \"\"\"Estimates the p-value for the null hypothesis that the same regression error with less parameters can be\n    achieved. This is, a linear model trained on a data set A with d number of features has the same performance\n    (in terms of squared error) relative to the number of features as a model trained on a data set B with k number\n    features, where k < d. Here, both data sets need to have the same target values. A small p-value would\n    indicate that the model performances are significantly different.\n\n    Note that all given test samples are utilized in the f-test.\n\n    See https://en.wikipedia.org/wiki/F-test#Regression_problems for more details.\n\n    :param X_training_a: Input training samples for model A.\n    :param X_training_b: Input training samples for model B. These samples should have less features than samples in X_training_a.\n    :param Y_training: Target training values.\n    :param X_test_a: Test samples for model A.\n    :param X_test_b: Test samples for model B.\n    :param Y_test: Test values.\n    :return: A p-value on [0, 1].\n    \"\"\"\n    X_training_a, X_test_a = shape_into_2d(X_training_a, X_test_a)\n\n    if X_training_b.size > 0:\n        X_training_b, X_test_b = shape_into_2d(X_training_b, X_test_b)\n    else:\n        X_training_b = X_training_b.reshape(0, 0)\n        X_test_b = X_test_b.reshape(0, 0)\n\n    if X_training_a.shape[1] <= X_training_b.shape[1]:\n        raise ValueError(\n            \"The data X_training_a should have more dimensions (model parameters) than the data \" \"X_training_b!\"\n        )\n\n    ssr_a = np.sum((Y_test - LinearRegression().fit(X_training_a, Y_training).predict(X_test_a)) ** 2)\n\n    if X_training_b.shape[1] > 0:\n        ssr_b = np.sum((Y_test - LinearRegression().fit(X_training_b, Y_training).predict(X_test_b)) ** 2)\n    else:\n        ssr_b = np.sum((Y_test - np.mean(Y_training)) ** 2)\n\n    dof_diff_1 = X_test_a.shape[1] - X_test_b.shape[1]  # p1 - p2\n    dof_diff_2 = X_test_a.shape[0] - X_test_a.shape[1] - 1  # n - p2 (parameters include intercept)\n\n    f_statistic = (ssr_b - ssr_a) / dof_diff_1 * dof_diff_2\n\n    if ssr_a < EPS:\n        ssr_a = 0\n    if ssr_b < EPS:\n        ssr_b = 0\n\n    if ssr_a == 0 and ssr_b == 0:\n        f_statistic = 0\n    elif ssr_a != 0:\n        f_statistic /= ssr_a\n\n    return stats.f.sf(f_statistic, dof_diff_1, dof_diff_2)\n", "import_text": ["typing.Callable", "typing.List", "typing.Optional", "typing.Union", "numpy", "numpy.matlib.repmat", "scipy.stats", "sklearn.linear_model.LinearRegression", "dowhy.gcm.constant.EPS", "dowhy.gcm.util.general.shape_into_2d"], "prompt": "\"\"\"\nDescription: This function calculates the quantile-based Family-Wise Error Rate (FWER) for a given list of p-values.\n\nArgs:\n    p_values (Union[np.ndarray, List[float]]): A list or numpy array of p-values.\n    p_values_scaling (Optional[np.ndarray]): An optional numpy array of scaling factors for the p-values. Defaults to None.\n    quantile (float): The quantile value to use for the FWER calculation. Defaults to 0.5.\n\nReturns:\n    float: The adjusted p-value based on the quantile-based FWER.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Applies a quantile based family wise error rate (FWER) control to the given p-values. This is based on the\n    approach described in:\n\n    Meinshausen, N., Meier, L. and Buehlmann, P. (2009).\n    p-values for high-dimensional regression. J. Amer. Statist. Assoc.104 1671\u20131681\n\n    :param p_values: A list or array of p-values.\n    :param p_values_scaling: An optional list of scaling factors for each p-value.\n    :param quantile: The quantile used for the p-value adjustment. By default, this is the median (0.5).\n    :return: The p-value that lies on the quantile threshold. Note that this is the quantile based on scaled values\n             p_values / quantile.\n    \"\"\"", "function_dependencies": ["numpy.array", "numpy.ones", "numpy.isnan", "numpy.quantile"], "project_create_time": "2018-05-31T13:07:04+00:00", "project_update_time": "2024-04-17T23:56:14+00:00", "file_create_time": "2022-05-02T10:31:23Z", "file_update_time": "2023-11-10T00:04:10Z", "function_update_time": "2022-08-19T22:51:25Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["numpy.isnan"], "test_function": [{"file_path": "/dowhy-v0.11.1/dowhy-0.11.1/tests/gcm/test_stats.py", "class_name": null, "function_name": "test_when_apply_quantile_based_fwer_control_then_returns_single_adjusted_pvalue", "code": "\ndef test_when_apply_quantile_based_fwer_control_then_returns_single_adjusted_pvalue():\n    p_values = np.array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])\n    assert quantile_based_fwer(p_values, quantile=0.5) == 0.055 / 0.5\n    assert quantile_based_fwer(p_values, quantile=0.25) == 0.0325 / 0.25\n    assert quantile_based_fwer(p_values, quantile=0.75) == 0.0775 / 0.75\n\n    assert (\n        quantile_based_fwer(np.array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 1]), quantile=0.5)\n        == 0.06 / 0.5\n    )\n    assert quantile_based_fwer(np.array([0.9, 0.95, 1]), quantile=0.5) == 1\n    assert quantile_based_fwer(np.array([0, 0, 0]), quantile=0.5) == 0\n    assert quantile_based_fwer(np.array([0.33]), quantile=0.5) == 0.33"}, {"file_path": "/dowhy-v0.11.1/dowhy-0.11.1/tests/gcm/test_stats.py", "class_name": null, "function_name": "test_given_invalid_inputs_when_apply_quantile_based_fwer_control_then_raises_error", "code": "\ndef test_given_invalid_inputs_when_apply_quantile_based_fwer_control_then_raises_error():\n    with pytest.raises(ValueError):\n        assert quantile_based_fwer(np.array([0.1, 0.5, 1]), quantile=0)\n\n    with pytest.raises(ValueError):\n        assert quantile_based_fwer(np.array([0.1, 0.5, 1]), np.array([1, 2]), quantile=0.1)\n\n    with pytest.raises(ValueError):\n        assert quantile_based_fwer(np.array([0.1, 0.5, 1]), quantile=1.1)\n\n    with pytest.raises(ValueError):\n        assert quantile_based_fwer(np.array([0.1, 0.5, 1]), quantile=-0.5)"}]}, {"git_group": "HumanCompatibleAI", "git_name": "imitation", "version": "v1.0.0", "language": "Python", "project_name": "imitation-v1.0.0.zip", "file_path": "/imitation-v1.0.0/imitation-1.0.0/src/imitation/algorithms/bc.py", "file_name": "bc.py", "focal_class": null, "focal_name": "reconstruct_policy", "focal_parameter": [], "solution": "def reconstruct_policy(\n    policy_path: str,\n    device: Union[th.device, str] = \"auto\",\n) -> policies.ActorCriticPolicy:\n    policy = th.load(policy_path, map_location=utils.get_device(device))\n    assert isinstance(policy, policies.ActorCriticPolicy)\n    return policy", "function_signature": "def reconstruct_policy(\n    policy_path: str,\n    device: Union[th.device, str] = \"auto\",\n) -> policies.ActorCriticPolicy :", "left_context": "\"\"\"Behavioural Cloning (BC).\n\nTrains policy by applying supervised learning to a fixed dataset of (observation,\naction) pairs generated by some expert demonstrator.\n\"\"\"\n\nimport dataclasses\nimport itertools\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    Iterator,\n    Mapping,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n)\n\nimport gymnasium as gym\nimport numpy as np\nimport torch as th\nimport tqdm\nfrom stable_baselines3.common import policies, torch_layers, utils, vec_env\n\nfrom imitation.algorithms import base as algo_base\nfrom imitation.data import rollout, types\nfrom imitation.policies import base as policy_base\nfrom imitation.util import logger as imit_logger\nfrom imitation.util import util\n\n\n@dataclasses.dataclass(frozen=True)\nclass BatchIteratorWithEpochEndCallback:\n    \"\"\"Loops through batches from a batch loader and calls a callback after every epoch.\n\n    Will throw an exception when an epoch contains no batches.\n    \"\"\"\n\n    batch_loader: Iterable[types.TransitionMapping]\n    n_epochs: Optional[int]\n    n_batches: Optional[int]\n    on_epoch_end: Optional[Callable[[int], None]]\n\n    def __post_init__(self) -> None:\n        epochs_and_batches_specified = (\n            self.n_epochs is not None and self.n_batches is not None\n        )\n        neither_epochs_nor_batches_specified = (\n            self.n_epochs is None and self.n_batches is None\n        )\n        if epochs_and_batches_specified or neither_epochs_nor_batches_specified:\n            raise ValueError(\n                \"Must provide exactly one of `n_epochs` and `n_batches` arguments.\",\n            )\n\n    def __iter__(self) -> Iterator[types.TransitionMapping]:\n        def batch_iterator() -> Iterator[types.TransitionMapping]:\n            # Note: the islice here ensures we do not exceed self.n_epochs\n            for epoch_num in itertools.islice(itertools.count(), self.n_epochs):\n                some_batch_was_yielded = False\n                for batch in self.batch_loader:\n                    yield batch\n                    some_batch_was_yielded = True\n\n                if not some_batch_was_yielded:\n                    raise AssertionError(\n                        f\"Data loader returned no data during epoch \"\n                        f\"{epoch_num} -- did it reset correctly?\",\n                    )\n                if self.on_epoch_end is not None:\n                    self.on_epoch_end(epoch_num)\n\n        # Note: the islice here ensures we do not exceed self.n_batches\n        return itertools.islice(batch_iterator(), self.n_batches)\n\n\n@dataclasses.dataclass(frozen=True)\nclass BCTrainingMetrics:\n    \"\"\"Container for the different components of behavior cloning loss.\"\"\"\n\n    neglogp: th.Tensor\n    entropy: Optional[th.Tensor]\n    ent_loss: th.Tensor  # set to 0 if entropy is None\n    prob_true_act: th.Tensor\n    l2_norm: th.Tensor\n    l2_loss: th.Tensor\n    loss: th.Tensor\n\n\n@dataclasses.dataclass(frozen=True)\nclass BehaviorCloningLossCalculator:\n    \"\"\"Functor to compute the loss used in Behavior Cloning.\"\"\"\n\n    ent_weight: float\n    l2_weight: float\n\n    def __call__(\n        self,\n        policy: policies.ActorCriticPolicy,\n        obs: Union[\n            types.AnyTensor,\n            types.DictObs,\n            Dict[str, np.ndarray],\n            Dict[str, th.Tensor],\n        ],\n        acts: Union[th.Tensor, np.ndarray],\n    ) -> BCTrainingMetrics:\n        \"\"\"Calculate the supervised learning loss used to train the behavioral clone.\n\n        Args:\n            policy: The actor-critic policy whose loss is being computed.\n            obs: The observations seen by the expert.\n            acts: The actions taken by the expert.\n\n        Returns:\n            A BCTrainingMetrics object with the loss and all the components it\n            consists of.\n        \"\"\"\n        tensor_obs = types.map_maybe_dict(\n            util.safe_to_tensor,\n            types.maybe_unwrap_dictobs(obs),\n        )\n        acts = util.safe_to_tensor(acts)\n\n        # policy.evaluate_actions's type signatures are incorrect.\n        # See https://github.com/DLR-RM/stable-baselines3/issues/1679\n        (_, log_prob, entropy) = policy.evaluate_actions(\n            tensor_obs,  # type: ignore[arg-type]\n            acts,\n        )\n        prob_true_act = th.exp(log_prob).mean()\n        log_prob = log_prob.mean()\n        entropy = entropy.mean() if entropy is not None else None\n\n        l2_norms = [th.sum(th.square(w)) for w in policy.parameters()]\n        l2_norm = sum(l2_norms) / 2  # divide by 2 to cancel with gradient of square\n        # sum of list defaults to float(0) if len == 0.\n        assert isinstance(l2_norm, th.Tensor)\n\n        ent_loss = -self.ent_weight * (entropy if entropy is not None else th.zeros(1))\n        neglogp = -log_prob\n        l2_loss = self.l2_weight * l2_norm\n        loss = neglogp + ent_loss + l2_loss\n\n        return BCTrainingMetrics(\n            neglogp=neglogp,\n            entropy=entropy,\n            ent_loss=ent_loss,\n            prob_true_act=prob_true_act,\n            l2_norm=l2_norm,\n            l2_loss=l2_loss,\n            loss=loss,\n        )\n\n\ndef enumerate_batches(\n    batch_it: Iterable[types.TransitionMapping],\n) -> Iterable[Tuple[Tuple[int, int, int], types.TransitionMapping]]:\n    \"\"\"Prepends batch stats before the batches of a batch iterator.\"\"\"\n    num_samples_so_far = 0\n    for num_batches, batch in enumerate(batch_it):\n        batch_size = len(batch[\"obs\"])\n        num_samples_so_far += batch_size\n        yield (num_batches, batch_size, num_samples_so_far), batch\n\n\n@dataclasses.dataclass(frozen=True)\nclass RolloutStatsComputer:\n    \"\"\"Computes statistics about rollouts.\n\n    Args:\n        venv: The vectorized environment in which to compute the rollouts.\n        n_episodes: The number of episodes to base the statistics on.\n    \"\"\"\n\n    venv: Optional[vec_env.VecEnv]\n    n_episodes: int\n\n    # TODO(shwang): Maybe instead use a callback that can be shared between\n    #   all algorithms' `.train()` for generating rollout stats.\n    #   EvalCallback could be a good fit:\n    #   https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html#evalcallback\n\n    def __call__(\n        self,\n        policy: policies.ActorCriticPolicy,\n        rng: np.random.Generator,\n    ) -> Mapping[str, float]:\n        if self.venv is not None and self.n_episodes > 0:\n            trajs = rollout.generate_trajectories(\n                policy,\n                self.venv,\n                rollout.make_min_episodes(self.n_episodes),\n                rng=rng,\n            )\n            return rollout.rollout_stats(trajs)\n        else:\n            return dict()\n\n\nclass BCLogger:\n    \"\"\"Utility class to help logging information relevant to Behavior Cloning.\"\"\"\n\n    def __init__(self, logger: imit_logger.HierarchicalLogger):\n        \"\"\"Create new BC logger.\n\n        Args:\n            logger: The logger to feed all the information to.\n        \"\"\"\n        self._logger = logger\n        self._tensorboard_step = 0\n        self._current_epoch = 0\n\n    def reset_tensorboard_steps(self):\n        self._tensorboard_step = 0\n\n    def log_epoch(self, epoch_number):\n        self._current_epoch = epoch_number\n\n    def log_batch(\n        self,\n        batch_num: int,\n        batch_size: int,\n        num_samples_so_far: int,\n        training_metrics: BCTrainingMetrics,\n        rollout_stats: Mapping[str, float],\n    ):\n        self._logger.record(\"batch_size\", batch_size)\n        self._logger.record(\"bc/epoch\", self._current_epoch)\n        self._logger.record(\"bc/batch\", batch_num)\n        self._logger.record(\"bc/samples_so_far\", num_samples_so_far)\n        for k, v in training_metrics.__dict__.items():\n            self._logger.record(f\"bc/{k}\", float(v) if v is not None else None)\n\n        for k, v in rollout_stats.items():\n            if \"return\" in k and \"monitor\" not in k:\n                self._logger.record(\"rollout/\" + k, v)\n        self._logger.dump(self._tensorboard_step)\n        self._tensorboard_step += 1\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        del state[\"_logger\"]\n        return state\n\n", "right_context": "\n\nclass BC(algo_base.DemonstrationAlgorithm):\n    \"\"\"Behavioral cloning (BC).\n\n    Recovers a policy via supervised learning from observation-action pairs.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        observation_space: gym.Space,\n        action_space: gym.Space,\n        rng: np.random.Generator,\n        policy: Optional[policies.ActorCriticPolicy] = None,\n        demonstrations: Optional[algo_base.AnyTransitions] = None,\n        batch_size: int = 32,\n        minibatch_size: Optional[int] = None,\n        optimizer_cls: Type[th.optim.Optimizer] = th.optim.Adam,\n        optimizer_kwargs: Optional[Mapping[str, Any]] = None,\n        ent_weight: float = 1e-3,\n        l2_weight: float = 0.0,\n        device: Union[str, th.device] = \"auto\",\n        custom_logger: Optional[imit_logger.HierarchicalLogger] = None,\n    ):\n        \"\"\"Builds BC.\n\n        Args:\n            observation_space: the observation space of the environment.\n            action_space: the action space of the environment.\n            rng: the random state to use for the random number generator.\n            policy: a Stable Baselines3 policy; if unspecified,\n                defaults to `FeedForward32Policy`.\n            demonstrations: Demonstrations from an expert (optional). Transitions\n                expressed directly as a `types.TransitionsMinimal` object, a sequence\n                of trajectories, or an iterable of transition batches (mappings from\n                keywords to arrays containing observations, etc).\n            batch_size: The number of samples in each batch of expert data.\n            minibatch_size: size of minibatch to calculate gradients over.\n                The gradients are accumulated until `batch_size` examples\n                are processed before making an optimization step. This\n                is useful in GPU training to reduce memory usage, since\n                fewer examples are loaded into memory at once,\n                facilitating training with larger batch sizes, but is\n                generally slower. Must be a factor of `batch_size`.\n                Optional, defaults to `batch_size`.\n            optimizer_cls: optimiser to use for supervised training.\n            optimizer_kwargs: keyword arguments, excluding learning rate and\n                weight decay, for optimiser construction.\n            ent_weight: scaling applied to the policy's entropy regularization.\n            l2_weight: scaling applied to the policy's L2 regularization.\n            device: name/identity of device to place policy on.\n            custom_logger: Where to log to; if None (default), creates a new logger.\n\n        Raises:\n            ValueError: If `weight_decay` is specified in `optimizer_kwargs` (use the\n                parameter `l2_weight` instead), or if the batch size is not a multiple\n                of the minibatch size.\n        \"\"\"\n        self._demo_data_loader: Optional[Iterable[types.TransitionMapping]] = None\n        self.batch_size = batch_size\n        self.minibatch_size = minibatch_size or batch_size\n        if self.batch_size % self.minibatch_size != 0:\n            raise ValueError(\"Batch size must be a multiple of minibatch size.\")\n        super().__init__(\n            demonstrations=demonstrations,\n            custom_logger=custom_logger,\n        )\n        self._bc_logger = BCLogger(self.logger)\n\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n        self.rng = rng\n\n        if policy is None:\n            extractor = (\n                torch_layers.CombinedExtractor\n                if isinstance(observation_space, gym.spaces.Dict)\n                else torch_layers.FlattenExtractor\n            )\n            policy = policy_base.FeedForward32Policy(\n                observation_space=observation_space,\n                action_space=action_space,\n                # Set lr_schedule to max value to force error if policy.optimizer\n                # is used by mistake (should use self.optimizer instead).\n                lr_schedule=lambda _: th.finfo(th.float32).max,\n                features_extractor_class=extractor,\n            )\n        self._policy = policy.to(utils.get_device(device))\n        # TODO(adam): make policy mandatory and delete observation/action space params?\n        assert self.policy.observation_space == self.observation_space\n        assert self.policy.action_space == self.action_space\n\n        if optimizer_kwargs:\n            if \"weight_decay\" in optimizer_kwargs:\n                raise ValueError(\"Use the parameter l2_weight instead of weight_decay.\")\n        optimizer_kwargs = optimizer_kwargs or {}\n        self.optimizer = optimizer_cls(\n            self.policy.parameters(),\n            **optimizer_kwargs,\n        )\n\n        self.loss_calculator = BehaviorCloningLossCalculator(ent_weight, l2_weight)\n\n    @property\n    def policy(self) -> policies.ActorCriticPolicy:\n        return self._policy\n\n    def set_demonstrations(self, demonstrations: algo_base.AnyTransitions) -> None:\n        self._demo_data_loader = algo_base.make_data_loader(\n            demonstrations,\n            self.minibatch_size,\n        )\n\n    def train(\n        self,\n        *,\n        n_epochs: Optional[int] = None,\n        n_batches: Optional[int] = None,\n        on_epoch_end: Optional[Callable[[], None]] = None,\n        on_batch_end: Optional[Callable[[], None]] = None,\n        log_interval: int = 500,\n        log_rollouts_venv: Optional[vec_env.VecEnv] = None,\n        log_rollouts_n_episodes: int = 5,\n        progress_bar: bool = True,\n        reset_tensorboard: bool = False,\n    ):\n        \"\"\"Train with supervised learning for some number of epochs.\n\n        Here an 'epoch' is just a complete pass through the expert data loader,\n        as set by `self.set_expert_data_loader()`. Note, that when you specify\n        `n_batches` smaller than the number of batches in an epoch, the `on_epoch_end`\n        callback will never be called.\n\n        Args:\n            n_epochs: Number of complete passes made through expert data before ending\n                training. Provide exactly one of `n_epochs` and `n_batches`.\n            n_batches: Number of batches loaded from dataset before ending training.\n                Provide exactly one of `n_epochs` and `n_batches`.\n            on_epoch_end: Optional callback with no parameters to run at the end of each\n                epoch.\n            on_batch_end: Optional callback with no parameters to run at the end of each\n                batch.\n            log_interval: Log stats after every log_interval batches.\n            log_rollouts_venv: If not None, then this VecEnv (whose observation and\n                actions spaces must match `self.observation_space` and\n                `self.action_space`) is used to generate rollout stats, including\n                average return and average episode length. If None, then no rollouts\n                are generated.\n            log_rollouts_n_episodes: Number of rollouts to generate when calculating\n                rollout stats. Non-positive number disables rollouts.\n            progress_bar: If True, then show a progress bar during training.\n            reset_tensorboard: If True, then start plotting to Tensorboard from x=0\n                even if `.train()` logged to Tensorboard previously. Has no practical\n                effect if `.train()` is being called for the first time.\n        \"\"\"\n        if reset_tensorboard:\n            self._bc_logger.reset_tensorboard_steps()\n        self._bc_logger.log_epoch(0)\n\n        compute_rollout_stats = RolloutStatsComputer(\n            log_rollouts_venv,\n            log_rollouts_n_episodes,\n        )\n\n        def _on_epoch_end(epoch_number: int):\n            if tqdm_progress_bar is not None:\n                total_num_epochs_str = f\"of {n_epochs}\" if n_epochs is not None else \"\"\n                tqdm_progress_bar.display(\n                    f\"Epoch {epoch_number} {total_num_epochs_str}\",\n                    pos=1,\n                )\n            self._bc_logger.log_epoch(epoch_number + 1)\n            if on_epoch_end is not None:\n                on_epoch_end()\n\n        mini_per_batch = self.batch_size // self.minibatch_size\n        n_minibatches = n_batches * mini_per_batch if n_batches is not None else None\n\n        assert self._demo_data_loader is not None\n        demonstration_batches = BatchIteratorWithEpochEndCallback(\n            self._demo_data_loader,\n            n_epochs,\n            n_minibatches,\n            _on_epoch_end,\n        )\n        batches_with_stats = enumerate_batches(demonstration_batches)\n        tqdm_progress_bar: Optional[tqdm.tqdm] = None\n\n        if progress_bar:\n            batches_with_stats = tqdm.tqdm(\n                batches_with_stats,\n                unit=\"batch\",\n                total=n_minibatches,\n            )\n            tqdm_progress_bar = batches_with_stats\n\n        def process_batch():\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n            if batch_num % log_interval == 0:\n                rollout_stats = compute_rollout_stats(self.policy, self.rng)\n\n                self._bc_logger.log_batch(\n                    batch_num,\n                    minibatch_size,\n                    num_samples_so_far,\n                    training_metrics,\n                    rollout_stats,\n                )\n\n            if on_batch_end is not None:\n                on_batch_end()\n\n        self.optimizer.zero_grad()\n        for (\n            batch_num,\n            minibatch_size,\n            num_samples_so_far,\n        ), batch in batches_with_stats:\n            obs_tensor: Union[th.Tensor, Dict[str, th.Tensor]]\n            # unwraps the observation if it's a dictobs and converts arrays to tensors\n            obs_tensor = types.map_maybe_dict(\n                lambda x: util.safe_to_tensor(x, device=self.policy.device),\n                types.maybe_unwrap_dictobs(batch[\"obs\"]),\n            )\n            acts = util.safe_to_tensor(batch[\"acts\"], device=self.policy.device)\n            training_metrics = self.loss_calculator(self.policy, obs_tensor, acts)\n\n            # Renormalise the loss to be averaged over the whole\n            # batch size instead of the minibatch size.\n            # If there is an incomplete batch, its gradients will be\n            # smaller, which may be helpful for stability.\n            loss = training_metrics.loss * minibatch_size / self.batch_size\n            loss.backward()\n\n            batch_num = batch_num * self.minibatch_size // self.batch_size\n            if num_samples_so_far % self.batch_size == 0:\n                process_batch()\n        if num_samples_so_far % self.batch_size != 0:\n            # if there remains an incomplete batch\n            batch_num += 1\n            process_batch()\n", "import_text": ["dataclasses", "itertools", "typing.Any", "typing.Callable", "typing.Dict", "typing.Iterable", "typing.Iterator", "typing.Mapping", "typing.Optional", "typing.Tuple", "typing.Type", "typing.Union", "gymnasium", "numpy", "torch", "tqdm", "stable_baselines3.common.policies", "stable_baselines3.common.torch_layers", "stable_baselines3.common.utils", "stable_baselines3.common.vec_env", "imitation.algorithms.base", "imitation.data.rollout", "imitation.data.types", "imitation.policies.base", "imitation.util.logger", "imitation.util.util"], "prompt": "\"\"\"\nDescription: This function reconstructs a policy from a given path using torch.load.\n\nArgs:\n    policy_path (str): The path to the saved policy.\n    device (Union[th.device, str]): The device to load the policy to. Defaults to \"auto\".\n\nReturns:\n    policies.ActorCriticPolicy: The reconstructed policy.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Reconstruct a saved policy.\n\n    Args:\n        policy_path: path where `.save_policy()` has been run.\n        device: device on which to load the policy.\n\n    Returns:\n        policy: policy with reloaded weights.\n    \"\"\"", "function_dependencies": ["torch.load", "stable_baselines3.common.utils.get_device"], "project_create_time": "2018-12-08T05:15:33+00:00", "project_update_time": "2024-04-17T14:17:20+00:00", "file_create_time": "2019-08-15T10:03:28Z", "file_update_time": "2023-10-05T19:32:34Z", "function_update_time": "2022-03-29T12:05:49Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["torch.load"], "test_function": [{"file_path": "/imitation-v1.0.0/imitation-1.0.0/tests/algorithms/test_bc.py", "class_name": null, "function_name": "test_that_policy_reconstruction_preserves_parameters", "code": "\ndef test_that_policy_reconstruction_preserves_parameters(\n    cartpole_bc_trainer: bc.BC,\n    tmpdir,\n):\n    # GIVEN\n    pol_path = os.path.join(tmpdir, \"policy.pt\")\n    original_parameters = list(cartpole_bc_trainer.policy.parameters())\n\n    # WHEN\n    util.save_policy(cartpole_bc_trainer.policy, pol_path)\n    reconstructed_policy = bc.reconstruct_policy(pol_path)\n\n    # THEN\n    reconstructed_parameters = list(reconstructed_policy.parameters())\n    assert len(original_parameters) == len(reconstructed_parameters)\n    for original, reconstructed in zip(original_parameters, reconstructed_parameters):\n        th.testing.assert_close(original, reconstructed)"}]}, {"git_group": "unit8co", "git_name": "darts", "version": "0.29.0", "language": "Python", "project_name": "darts-0.29.0.zip", "file_path": "/darts-0.29.0/darts-0.29.0/darts/metrics/metrics.py", "file_name": "metrics.py", "focal_class": null, "focal_name": "rmse", "focal_parameter": [], "solution": "def rmse(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    return np.sqrt(\n        _get_wrapped_metric(mse)(\n            actual_series,\n            pred_series,\n            intersect,\n        )\n    )", "function_signature": "def rmse(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE :", "left_context": "\"\"\"\nMetrics\n-------\n\nSome metrics to compare time series.\n\"\"\"\n\nimport inspect\nfrom functools import wraps\nfrom inspect import signature\nfrom typing import Callable, List, Optional, Sequence, Tuple, Union\n\nimport numpy as np\n\nfrom darts import TimeSeries\nfrom darts.dataprocessing import dtw\nfrom darts.logging import get_logger, raise_log\nfrom darts.utils import _build_tqdm_iterator, _parallel_apply, n_steps_between\nfrom darts.utils.ts_utils import SeriesType, get_series_seq_type, series2seq\n\nlogger = get_logger(__name__)\nTIME_AX = 0\nCOMP_AX = 1\n\n# Note: for new metrics added to this module to be able to leverage the two decorators, it is required both having\n# the `actual_series` and `pred_series` parameters, and not having other ``Sequence`` as args (since these decorators\n# don't \"unpack\" parameters different from `actual_series` and `pred_series`). In those cases, the new metric must take\n# care of dealing with Sequence[TimeSeries] and multivariate TimeSeries on its own (See mase() implementation).\nMETRIC_OUTPUT_TYPE = Union[float, List[float], np.ndarray, List[np.ndarray]]\nMETRIC_TYPE = Callable[\n    ...,\n    METRIC_OUTPUT_TYPE,\n]\n\n\ndef multi_ts_support(func) -> Callable[..., METRIC_OUTPUT_TYPE]:\n    \"\"\"\n    This decorator further adapts the metrics that took as input two (or three for scaled metrics with `insample`)\n    univariate/multivariate ``TimeSeries`` instances, adding support for equally-sized sequences of ``TimeSeries``\n    instances. The decorator computes the pairwise metric for ``TimeSeries`` with the same indices, and returns a float\n    value that is computed as a function of all the pairwise metrics using a `series_reduction` subroutine passed as\n    argument to the metric function.\n\n    If a 'Sequence[TimeSeries]' is passed as input, this decorator provides also parallelisation of the metric\n    evaluation regarding different ``TimeSeries`` (if the `n_jobs` parameter is not set 1).\n    \"\"\"\n\n    @wraps(func)\n    def wrapper_multi_ts_support(*args, **kwargs):\n        actual_series = (\n            kwargs[\"actual_series\"] if \"actual_series\" in kwargs else args[0]\n        )\n        pred_series = (\n            kwargs[\"pred_series\"]\n            if \"pred_series\" in kwargs\n            else args[0] if \"actual_series\" in kwargs else args[1]\n        )\n\n        params = signature(func).parameters\n        n_jobs = kwargs.pop(\"n_jobs\", params[\"n_jobs\"].default)\n        if not isinstance(n_jobs, int):\n            raise_log(ValueError(\"n_jobs must be an integer\"), logger=logger)\n\n        verbose = kwargs.pop(\"verbose\", params[\"verbose\"].default)\n        if not isinstance(verbose, bool):\n            raise_log(ValueError(\"verbose must be a bool\"), logger=logger)\n\n        # sanity check reduction functions\n        _ = _get_reduction(\n            kwargs=kwargs,\n            params=params,\n            red_name=\"time_reduction\",\n            axis=TIME_AX,\n            sanity_check=True,\n        )\n        _ = _get_reduction(\n            kwargs=kwargs,\n            params=params,\n            red_name=\"component_reduction\",\n            axis=COMP_AX,\n            sanity_check=True,\n        )\n        series_reduction = _get_reduction(\n            kwargs=kwargs,\n            params=params,\n            red_name=\"series_reduction\",\n            axis=0,\n            sanity_check=True,\n        )\n\n        series_seq_type = get_series_seq_type(actual_series)\n        actual_series = series2seq(actual_series)\n        pred_series = series2seq(pred_series)\n\n        if len(actual_series) != len(pred_series):\n            raise_log(\n                ValueError(\n                    f\"Mismatch between number of series in `actual_series` (n={len(actual_series)}) and \"\n                    f\"`pred_series` (n={len(pred_series)}).\"\n                ),\n                logger=logger,\n            )\n        num_series_in_args = int(\"actual_series\" not in kwargs) + int(\n            \"pred_series\" not in kwargs\n        )\n        input_series = (actual_series, pred_series)\n\n        kwargs.pop(\"actual_series\", 0)\n        kwargs.pop(\"pred_series\", 0)\n\n        # handle `insample` parameter for scaled metrics\n        if \"insample\" in params:\n            insample = kwargs.get(\"insample\")\n            if insample is None:\n                insample = args[\n                    2 - (\"actual_series\" in kwargs) - (\"pred_series\" in kwargs)\n                ]\n\n            insample = [insample] if not isinstance(insample, Sequence) else insample\n            if len(actual_series) != len(insample):\n                raise_log(\n                    ValueError(\n                        f\"Mismatch between number of series in `actual_series` (n={len(actual_series)}) and \"\n                        f\"`insample` series (n={len(insample)}).\"\n                    ),\n                    logger=logger,\n                )\n            input_series += (insample,)\n            num_series_in_args += int(\"insample\" not in kwargs)\n            kwargs.pop(\"insample\", 0)\n\n        iterator = _build_tqdm_iterator(\n            iterable=zip(*input_series),\n            verbose=verbose,\n            total=len(actual_series),\n        )\n\n        # `vals` is a list of series metrics of length `len(actual_series)`. Each metric has shape\n        # `(n time steps, n components)`;\n        # - n times step is `1` if `time_reduction` is other than `None`\n        # - n components: is 1 if `component_reduction` is other than `None`\n        vals = _parallel_apply(\n            iterator=iterator,\n            fn=func,\n            n_jobs=n_jobs,\n            fn_args=args[num_series_in_args:],\n            fn_kwargs=kwargs,\n        )\n\n        # we flatten metrics along the time axis if n time steps == 1,\n        # and/or along component axis if n components == 1\n        vals = [\n            val[\n                slice(None) if val.shape[TIME_AX] != 1 else 0,\n                slice(None) if val.shape[COMP_AX] != 1 else 0,\n            ]\n            for val in vals\n        ]\n\n        # reduce metrics along series axis\n        if series_reduction is not None:\n            vals = kwargs[\"series_reduction\"](vals, axis=0)\n        elif series_seq_type == SeriesType.SINGLE:\n            vals = vals[0]\n\n        # flatten along series axis if n series == 1\n        return vals\n\n    return wrapper_multi_ts_support\n\n\ndef multivariate_support(func) -> Callable[..., METRIC_OUTPUT_TYPE]:\n    \"\"\"\n    This decorator transforms a metric function that takes as input two univariate TimeSeries instances\n    into a function that takes two equally-sized multivariate TimeSeries instances, computes the pairwise univariate\n    metrics for components with the same indices, and returns a float value that is computed as a function of all the\n    univariate metrics using a `component_reduction` subroutine passed as argument to the metric function.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper_multivariate_support(*args, **kwargs) -> METRIC_OUTPUT_TYPE:\n        params = signature(func).parameters\n        # we can avoid checks about args and kwargs since the input is adjusted by the previous decorator\n        actual_series = args[0]\n        pred_series = args[1]\n        num_series_in_args = 2\n\n        if actual_series.width != pred_series.width:\n            raise_log(\n                ValueError(\n                    f\"Mismatch between number of components in `actual_series` \"\n                    f\"(n={actual_series.width}) and `pred_series` (n={pred_series.width}.\"\n                ),\n                logger=logger,\n            )\n\n        # handle `insample` parameters for scaled metrics\n        input_series = (actual_series, pred_series)\n        if \"insample\" in params:\n            insample = args[2]\n            if actual_series.width != insample.width:\n                raise_log(\n                    ValueError(\n                        f\"Mismatch between number of components in `actual_series` \"\n                        f\"(n={actual_series.width}) and `insample` (n={insample.width}.\"\n                    ),\n                    logger=logger,\n                )\n            input_series += (insample,)\n            num_series_in_args += 1\n\n        vals = func(*input_series, *args[num_series_in_args:], **kwargs)\n        if not 1 <= len(vals.shape) <= 2:\n            raise_log(\n                ValueError(\n                    \"Metric output must have 1 dimension for aggregated metrics (e.g. `mae()`, ...), \"\n                    \"or 2 dimension for time dependent metrics (e.g. `ae()`, ...)\"\n                ),\n                logger=logger,\n            )\n        elif len(vals.shape) == 1:\n            vals = np.expand_dims(vals, TIME_AX)\n\n        time_reduction = _get_reduction(\n            kwargs=kwargs,\n            params=params,\n            red_name=\"time_reduction\",\n            axis=TIME_AX,\n            sanity_check=False,\n        )\n        if time_reduction is not None:\n            vals = np.expand_dims(time_reduction(vals, axis=TIME_AX), axis=TIME_AX)\n\n        component_reduction = _get_reduction(\n            kwargs=kwargs,\n            params=params,\n            red_name=\"component_reduction\",\n            axis=COMP_AX,\n            sanity_check=False,\n        )\n        if component_reduction is not None:\n            vals = np.expand_dims(component_reduction(vals, axis=COMP_AX), axis=COMP_AX)\n        return vals\n\n    return wrapper_multivariate_support\n\n\ndef _get_values(\n    vals: np.ndarray, stochastic_quantile: Optional[float] = 0.5\n) -> np.ndarray:\n    \"\"\"\n    Returns a deterministic or probabilistic numpy array from the values of a time series.\n    For stochastic input values, return either all sample values with (stochastic_quantile=None) or the quantile sample\n    value with (stochastic_quantile {>=0,<=1})\n    \"\"\"\n    if vals.shape[2] == 1:  # deterministic\n        out = vals[:, :, 0]\n    else:  # stochastic\n        if stochastic_quantile is None:\n            out = vals\n        else:\n            out = np.quantile(vals, stochastic_quantile, axis=2)\n    return out\n\n\ndef _get_values_or_raise(\n    series_a: TimeSeries,\n    series_b: TimeSeries,\n    intersect: bool,\n    stochastic_quantile: Optional[float] = 0.5,\n    remove_nan_union: bool = False,\n    is_insample: bool = False,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns the processed numpy values of two time series. Processing can be customized with arguments\n    `intersect, stochastic_quantile, remove_nan_union`.\n\n    Parameters\n    ----------\n    series_a\n        A deterministic ``TimeSeries`` instance. If `is_insample=False`, it is the `actual_series`.\n        Otherwise, it is the `insample` series.\n    series_b\n        A deterministic or stochastic ``TimeSeries`` instance (the predictions `pred_series`).\n    intersect\n        A boolean for whether to only consider the time intersection between `series_a` and `series_b`\n    stochastic_quantile\n        Optionally, for stochastic predicted series, return either all sample values with (`stochastic_quantile=None`)\n        or any deterministic quantile sample values by setting `stochastic_quantile=quantile` {>=0,<=1}.\n    remove_nan_union\n        By setting `remove_non_union` to True, sets all values from `series_a` and `series_b` to `np.nan` at indices\n        where any of the two series contain a NaN value. Only effective when `is_insample=False`.\n    is_insample\n        Whether `series_a` corresponds to the `insample` series for scaled metrics.\n\n    Raises\n    ------\n    ValueError\n        If `is_insample=False` and the two time series do not have at least a partially overlapping time index.\n    \"\"\"\n\n    if not series_a.width == series_b.width:\n        raise_log(\n            ValueError(\"The two time series must have the same number of components\"),\n            logger=logger,\n        )\n\n    if not isinstance(intersect, bool):\n        raise_log(ValueError(\"The intersect parameter must be a bool\"), logger=logger)\n\n    make_copy = False\n    if not is_insample:\n        # get the time intersection and values of the two series (corresponds to `actual_series` and `pred_series`\n        if series_a.has_same_time_as(series_b) or not intersect:\n            vals_a_common = series_a.all_values(copy=make_copy)\n            vals_b_common = series_b.all_values(copy=make_copy)\n        else:\n            vals_a_common = series_a.slice_intersect_values(series_b, copy=make_copy)\n            vals_b_common = series_b.slice_intersect_values(series_a, copy=make_copy)\n\n        if not len(vals_a_common) == len(vals_b_common):\n            raise_log(\n                ValueError(\n                    \"The two time series must have at least a partially overlapping time index.\"\n                ),\n                logger=logger,\n            )\n\n        vals_b_det = _get_values(vals_b_common, stochastic_quantile=stochastic_quantile)\n    else:\n        # for `insample` series we extract only values up until before start of `pred_series`\n        # find how many steps `insample` overlaps into `series_b`\n        end = (\n            n_steps_between(\n                end=series_b.start_time(), start=series_a.end_time(), freq=series_a.freq\n            )\n            - 1\n        )\n        if end > 0 or abs(end) >= len(series_a):\n            raise_log(\n                ValueError(\n                    \"The `insample` series must start before the `pred_series` and \"\n                    \"extend at least until one time step before the start of `pred_series`.\"\n                ),\n                logger=logger,\n            )\n        end = end or None\n        vals_a_common = series_a.all_values(copy=make_copy)[:end]\n        vals_b_det = None\n    vals_a_det = _get_values(vals_a_common, stochastic_quantile=stochastic_quantile)\n\n    if not remove_nan_union or is_insample:\n        return vals_a_det, vals_b_det\n\n    b_is_deterministic = bool(len(vals_b_det.shape) == 2)\n    if b_is_deterministic:\n        isnan_mask = np.logical_or(np.isnan(vals_a_det), np.isnan(vals_b_det))\n        isnan_mask_pred = isnan_mask\n    else:\n        isnan_mask = np.logical_or(\n            np.isnan(vals_a_det), np.isnan(vals_b_det).any(axis=2)\n        )\n        isnan_mask_pred = np.repeat(\n            np.expand_dims(isnan_mask, axis=-1), vals_b_det.shape[2], axis=2\n        )\n    return np.where(isnan_mask, np.nan, vals_a_det), np.where(\n        isnan_mask_pred, np.nan, vals_b_det\n    )\n\n\ndef _get_wrapped_metric(\n    func: Callable[..., METRIC_OUTPUT_TYPE]\n) -> Callable[..., METRIC_OUTPUT_TYPE]:\n    \"\"\"Returns the inner metric function `func` which bypasses the decorators `multi_ts_support` and\n    `multivariate_support`. It significantly decreases process time compared to calling `func` directly.\n    Only use this to compute a pre-defined metric within the scope of another metric.\n    \"\"\"\n    return func.__wrapped__.__wrapped__\n\n\ndef _get_reduction(\n    kwargs, params, red_name, axis, sanity_check: bool = True\n) -> Optional[Callable[..., np.ndarray]]:\n    \"\"\"Returns the reduction function either from user kwargs or metric default.\n    Optionally performs sanity checks for presence of `axis` parameter, and correct output type and\n    reduced shape.\"\"\"\n    if red_name not in params:\n        return None\n\n    red_fn = kwargs[red_name] if red_name in kwargs else params[red_name].default\n    if not sanity_check:\n        return red_fn\n\n    if red_fn is not None:\n        red_params = inspect.signature(red_fn).parameters\n        if \"axis\" not in red_params:\n            raise_log(\n                ValueError(\n                    f\"Invalid `{red_name}` function: Must have a parameter called `axis`.\"\n                ),\n                logger=logger,\n            )\n        # verify `red_fn` reduces to array with correct shape\n        shape_in = (2, 1) if axis == 0 else (1, 2)\n        out = red_fn(np.zeros(shape_in), axis=axis)\n\n        if not isinstance(out, np.ndarray):\n            raise_log(\n                ValueError(\n                    f\"Invalid `{red_name}` function output type: Expected type \"\n                    f\"`np.ndarray`, received type=`{type(out)}`.\"\n                ),\n                logger=logger,\n            )\n        shape_invalid = out.shape != (1,)\n        if shape_invalid:\n            raise_log(\n                ValueError(\n                    f\"Invalid `{red_name}` function output shape: The function must reduce an input \"\n                    f\"`np.ndarray` of shape (t, c) to a `np.ndarray` of shape `(c,)`. \"\n                    f\"However, the function reduced a test array of shape `{shape_in}` to \"\n                    f\"`{out.shape}`.\"\n                ),\n                logger=logger,\n            )\n    return red_fn\n\n\ndef _get_error_scale(\n    insample: TimeSeries,\n    pred_series: TimeSeries,\n    m: int,\n    metric: str,\n):\n    \"\"\"Computes the error scale based on a naive seasonal forecasts on `insample` values with seasonality `m`.\"\"\"\n    if not isinstance(m, int):\n        raise_log(\n            ValueError(f\"Seasonality `m` must be of type `int`, recevied `m={m}`\"),\n            logger=logger,\n        )\n\n    # `x_t` are the true `y` values before the start of `y_pred`\n    x_t, _ = _get_values_or_raise(\n        insample, pred_series, intersect=False, remove_nan_union=False, is_insample=True\n    )\n    diff = x_t[m:] - x_t[:-m]\n    if metric == \"mae\":\n        scale = np.nanmean(np.abs(diff), axis=TIME_AX)\n    elif metric == \"mse\":\n        scale = np.nanmean(np.power(diff, 2), axis=TIME_AX)\n    elif metric == \"rmse\":\n        scale = np.sqrt(np.nanmean(np.power(diff, 2), axis=TIME_AX))\n    else:\n        raise_log(\n            ValueError(\n                f\"unknown `metric={metric}`. Must be one of ('mae', 'mse', 'rmse').\"\n            ),\n            logger=logger,\n        )\n\n    if np.isclose(scale, 0.0).any():\n        raise_log(ValueError(\"cannot use MASE with periodical signals\"), logger=logger)\n    return scale\n\n\n@multi_ts_support\n@multivariate_support\ndef err(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    time_reduction: Optional[Callable[..., np.ndarray]] = None,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Error (ERR).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column and time step :math:`t` as:\n\n    .. math:: y_t - \\\\hat{y}_t\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    time_reduction\n        Optionally, a function to aggregate the metrics over the time axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(c,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        time axis. If `None`, will return a metric per time step.\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - a sequence (list) of uni/multivariate series with `series_reduction`, `component_reduction` and\n          `time_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n time steps, n components) without time\n        and component reductions. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - single uni/multivariate series and at least `time_reduction=None`.\n        - a sequence of uni/multivariate series including `series_reduction` and at least one of\n          `component_reduction=None` or `time_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n\n    y_true, y_pred = _get_values_or_raise(\n        actual_series, pred_series, intersect, remove_nan_union=False\n    )\n    return y_true - y_pred\n\n\n@multi_ts_support\n@multivariate_support\ndef merr(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Mean Error (MERR).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column as:\n\n    .. math:: \\\\frac{1}{T}\\\\sum_{t=1}^T{(y_t - \\\\hat{y}_t)}\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n    return np.nanmean(\n        _get_wrapped_metric(err)(\n            actual_series,\n            pred_series,\n            intersect,\n        ),\n        axis=TIME_AX,\n    )\n\n\n@multi_ts_support\n@multivariate_support\ndef ae(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    time_reduction: Optional[Callable[..., np.ndarray]] = None,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Absolute Error (AE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column and time step :math:`t` as:\n\n    .. math:: |y_t - \\\\hat{y}_t|\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    time_reduction\n        Optionally, a function to aggregate the metrics over the time axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(c,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        time axis. If `None`, will return a metric per time step.\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    series_reduction\n        Optionally, a function taking as input a ``np.ndarray`` and returning either a scalar value or a ``np.ndarray``.\n        This function is used to aggregate the metrics in case the metric is evaluated on multiple series\n        (e.g., on a ``Sequence[TimeSeries]``). By default, returns the metric for each series.\n        Example: ``series_reduction=np.nanmean``, will return the average over all series metrics.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - a sequence (list) of uni/multivariate series with `series_reduction`, `component_reduction` and\n          `time_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n time steps, n components) without time\n        and component reductions. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - single uni/multivariate series and at least `time_reduction=None`.\n        - a sequence of uni/multivariate series including `series_reduction` and at least one of\n          `component_reduction=None` or `time_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n\n    y_true, y_pred = _get_values_or_raise(\n        actual_series, pred_series, intersect, remove_nan_union=False\n    )\n    return np.abs(y_true - y_pred)\n\n\n@multi_ts_support\n@multivariate_support\ndef mae(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Mean Absolute Error (MAE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column as:\n\n    .. math:: \\\\frac{1}{T}\\\\sum_{t=1}^T{|y_t - \\\\hat{y}_t|}\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    series_reduction\n        Optionally, a function taking as input a ``np.ndarray`` and returning either a scalar value or a ``np.ndarray``.\n        This function is used to aggregate the metrics in case the metric is evaluated on multiple series\n        (e.g., on a ``Sequence[TimeSeries]``). By default, returns the metric for each series.\n        Example: ``series_reduction=np.nanmean``, will return the average over all series metrics.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n    return np.nanmean(\n        _get_wrapped_metric(ae)(\n            actual_series,\n            pred_series,\n            intersect,\n        ),\n        axis=TIME_AX,\n    )\n\n\n@multi_ts_support\n@multivariate_support\ndef ase(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    insample: Union[TimeSeries, Sequence[TimeSeries]],\n    m: int = 1,\n    intersect: bool = True,\n    *,\n    time_reduction: Optional[Callable[..., np.ndarray]] = None,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Absolute Scaled Error (ASE) (see [1]_ for more information on scaled forecasting errors).\n\n    It is the Absolute Error (AE) scaled by the Mean AE (MAE) of the naive m-seasonal forecast.\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column and time step :math:`t` as:\n\n    .. math:: \\\\frac{AE(y_{t_p+1:t_p+T}, \\\\hat{y}_{t_p+1:t_p+T})}{E_m},\n\n    where :math:`t_p` is the prediction time (one step before the first forecasted point), :math:`AE` is the Absolute\n    Error (:func:`~darts.metrics.metrics.ae`), and :math:`E_m` is the Mean AE (MAE) of the naive m-seasonal\n    forecast on the `insample` series :math:`y_{0:t_p}` (the true series ending at :math:`t_p`):\n\n    .. math:: E_m = MAE(y_{m:t_p}, y_{0:t_p - m}).\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    insample\n        The training series used to forecast `pred_series` . This series serves to compute the scale of the error\n        obtained by a naive forecaster on the training data.\n    m\n        The seasonality to use for differencing to compute the error scale :math:`E_m` (as described in the metric\n        description). :math:`m=1` corresponds to a non-seasonal :math:`E_m` (e.g. naive repetition of the last observed\n        value), whereas :math:`m>1` corresponds to a seasonal :math:`E_m`.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    time_reduction\n        Optionally, a function to aggregate the metrics over the time axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(c,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        time axis. If `None`, will return a metric per time step.\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Raises\n    ------\n    ValueError\n        If the `insample` series is periodic ( :math:`y_t = y_{t-m}` ) or any series in `insample` does not end one\n        time step before the start of the corresponding forecast in `pred_series`.\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - a sequence (list) of uni/multivariate series with `series_reduction`, `component_reduction` and\n          `time_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n time steps, n components) without time\n        and component reductions. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - single uni/multivariate series and at least `time_reduction=None`.\n        - a sequence of uni/multivariate series including `series_reduction` and at least one of\n          `component_reduction=None` or `time_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n\n    References\n    ----------\n    .. [1] https://www.pmorgan.com.au/tutorials/mae%2C-mape%2C-mase-and-the-scaled-rmse/\n    \"\"\"\n    error_scale = _get_error_scale(insample, pred_series, m=m, metric=\"mae\")\n    errors = _get_wrapped_metric(ae)(\n        actual_series,\n        pred_series,\n        intersect,\n    )\n    return errors / error_scale\n\n\n@multi_ts_support\n@multivariate_support\ndef mase(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    insample: Union[TimeSeries, Sequence[TimeSeries]],\n    m: int = 1,\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Mean Absolute Scaled Error (MASE) (see [1]_ for more information on scaled forecasting errors).\n\n    It is the Mean Absolute Error (MAE) scaled by the MAE of the naive m-seasonal forecast.\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column as:\n\n    .. math:: \\\\frac{MAE(y_{t_p+1:t_p+T}, \\\\hat{y}_{t_p+1:t_p+T})}{E_m},\n\n    where :math:`t_p` is the prediction time (one step before the first forecasted point), :math:`MAE` is the Mean\n    Absolute Error (:func:`~darts.metrics.metrics.mae`), and :math:`E_m` is the MAE of the naive m-seasonal\n    forecast on the `insample` series :math:`y_{0:t_p}` (the true series ending at :math:`t_p`):\n\n    .. math:: E_m = MAE(y_{m:t_p}, y_{0:t_p - m}).\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    insample\n        The training series used to forecast `pred_series` . This series serves to compute the scale of the error\n        obtained by a naive forecaster on the training data.\n    m\n        The seasonality to use for differencing to compute the error scale :math:`E_m` (as described in the metric\n        description). :math:`m=1` corresponds to a non-seasonal :math:`E_m` (e.g. naive repetition of the last observed\n        value), whereas :math:`m>1` corresponds to a seasonal :math:`E_m`.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Raises\n    ------\n    ValueError\n        If the `insample` series is periodic ( :math:`y_t = y_{t-m}` ) or any series in `insample` does not end one\n        time step before the start of the corresponding forecast in `pred_series`.\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n\n    References\n    ----------\n    .. [1] https://www.pmorgan.com.au/tutorials/mae%2C-mape%2C-mase-and-the-scaled-rmse/\n    \"\"\"\n    return np.nanmean(\n        _get_wrapped_metric(ase)(\n            actual_series,\n            pred_series,\n            insample,\n            m=m,\n            intersect=intersect,\n        ),\n        axis=TIME_AX,\n    )\n\n\n@multi_ts_support\n@multivariate_support\ndef se(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    time_reduction: Optional[Callable[..., np.ndarray]] = None,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Squared Error (SE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column and time step :math:`t` as:\n\n    .. math:: (y_t - \\\\hat{y}_t)^2.\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    time_reduction\n        Optionally, a function to aggregate the metrics over the time axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(c,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        time axis. If `None`, will return a metric per time step.\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - a sequence (list) of uni/multivariate series with `series_reduction`, `component_reduction` and\n          `time_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n time steps, n components) without time\n        and component reductions. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - single uni/multivariate series and at least `time_reduction=None`.\n        - a sequence of uni/multivariate series including `series_reduction` and at least one of\n          `component_reduction=None` or `time_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n\n    y_true, y_pred = _get_values_or_raise(\n        actual_series, pred_series, intersect, remove_nan_union=False\n    )\n    return (y_true - y_pred) ** 2\n\n\n@multi_ts_support\n@multivariate_support\ndef mse(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Mean Squared Error (MSE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column as:\n\n    .. math:: \\\\frac{1}{T}\\\\sum_{t=1}^T{(y_t - \\\\hat{y}_t)^2}.\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n    return np.nanmean(\n        _get_wrapped_metric(se)(\n            actual_series,\n            pred_series,\n            intersect,\n        ),\n        axis=TIME_AX,\n    )\n\n\n@multi_ts_support\n@multivariate_support\ndef sse(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    insample: Union[TimeSeries, Sequence[TimeSeries]],\n    m: int = 1,\n    intersect: bool = True,\n    *,\n    time_reduction: Optional[Callable[..., np.ndarray]] = None,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Squared Scaled Error (SSE) (see [1]_ for more information on scaled forecasting errors).\n\n    It is the Squared Error (SE) scaled by the Mean SE (MSE) of the naive m-seasonal forecast.\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column and time step :math:`t` as:\n\n    .. math:: \\\\frac{SE(y_{t_p+1:t_p+T}, \\\\hat{y}_{t_p+1:t_p+T})}{E_m},\n\n    where :math:`t_p` is the prediction time (one step before the first forecasted point), :math:`SE` is the Squared\n    Error (:func:`~darts.metrics.metrics.se`), and :math:`E_m` is the Mean SE (MSE) of the naive m-seasonal\n    forecast on the `insample` series :math:`y_{0:t_p}` (the true series ending at :math:`t_p`):\n\n    .. math:: E_m = MSE(y_{m:t_p}, y_{0:t_p - m}).\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    insample\n        The training series used to forecast `pred_series` . This series serves to compute the scale of the error\n        obtained by a naive forecaster on the training data.\n    m\n        The seasonality to use for differencing to compute the error scale :math:`E_m` (as described in the metric\n        description). :math:`m=1` corresponds to a non-seasonal :math:`E_m` (e.g. naive repetition of the last observed\n        value), whereas :math:`m>1` corresponds to a seasonal :math:`E_m`.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    time_reduction\n        Optionally, a function to aggregate the metrics over the time axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(c,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        time axis. If `None`, will return a metric per time step.\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Raises\n    ------\n    ValueError\n        If the `insample` series is periodic ( :math:`y_t = y_{t-m}` ) or any series in `insample` does not end one\n        time step before the start of the corresponding forecast in `pred_series`.\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - a sequence (list) of uni/multivariate series with `series_reduction`, `component_reduction` and\n          `time_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n time steps, n components) without time\n        and component reductions. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - single uni/multivariate series and at least `time_reduction=None`.\n        - a sequence of uni/multivariate series including `series_reduction` and at least one of\n          `component_reduction=None` or `time_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n\n    References\n    ----------\n    .. [1] https://www.pmorgan.com.au/tutorials/mae%2C-mape%2C-mase-and-the-scaled-rmse/\n    \"\"\"\n    error_scale = _get_error_scale(insample, pred_series, m=m, metric=\"mse\")\n    errors = _get_wrapped_metric(se)(\n        actual_series,\n        pred_series,\n        intersect,\n    )\n    return errors / error_scale\n\n\n@multi_ts_support\n@multivariate_support\ndef msse(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    insample: Union[TimeSeries, Sequence[TimeSeries]],\n    m: int = 1,\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Mean Squared Scaled Error (MSSE) (see [1]_ for more information on scaled forecasting errors).\n\n    It is the Mean Squared Error (MSE) scaled by the MSE of the naive m-seasonal forecast.\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column as:\n\n    .. math:: \\\\frac{MSE(y_{t_p+1:t_p+T}, \\\\hat{y}_{t_p+1:t_p+T})}{E_m},\n\n    where :math:`t_p` is the prediction time (one step before the first forecasted point), :math:`MSE` is the Mean\n    Squared Error (:func:`~darts.metrics.metrics.mse`), and :math:`E_m` is the MSE of the naive m-seasonal\n    forecast on the `insample` series :math:`y_{0:t_p}` (the true series ending at :math:`t_p`):\n\n    .. math:: E_m = MSE(y_{m:t_p}, y_{0:t_p - m}).\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    insample\n        The training series used to forecast `pred_series` . This series serves to compute the scale of the error\n        obtained by a naive forecaster on the training data.\n    m\n        The seasonality to use for differencing to compute the error scale :math:`E_m` (as described in the metric\n        description). :math:`m=1` corresponds to a non-seasonal :math:`E_m` (e.g. naive repetition of the last observed\n        value), whereas :math:`m>1` corresponds to a seasonal :math:`E_m`.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Raises\n    ------\n    ValueError\n        If the `insample` series is periodic ( :math:`y_t = y_{t-m}` ) or any series in `insample` does not end one\n        time step before the start of the corresponding forecast in `pred_series`.\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n\n    References\n    ----------\n    .. [1] https://www.pmorgan.com.au/tutorials/mae%2C-mape%2C-mase-and-the-scaled-rmse/\n    \"\"\"\n    return np.nanmean(\n        _get_wrapped_metric(sse)(\n            actual_series,\n            pred_series,\n            insample,\n            m=m,\n            intersect=intersect,\n        ),\n        axis=TIME_AX,\n    )\n\n\n@multi_ts_support\n@multivariate_support", "right_context": "\n\n@multi_ts_support\n@multivariate_support\ndef rmsse(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    insample: Union[TimeSeries, Sequence[TimeSeries]],\n    m: int = 1,\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Root Mean Squared Scaled Error (RMSSE) (see [1]_ for more information on scaled forecasting errors).\n\n    It is the Root Mean Squared Error (RMSE) scaled by the RMSE of the naive m-seasonal forecast.\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column as:\n\n    .. math:: \\\\frac{RMSE(y_{t_p+1:t_p+T}, \\\\hat{y}_{t_p+1:t_p+T})}{E_m},\n\n    where :math:`t_p` is the prediction time (one step before the first forecasted point), :math:`RMSE` is the Root\n    Mean Squared Error (:func:`~darts.metrics.metrics.rmse`), and :math:`E_m` is the RMSE of the naive m-seasonal\n    forecast on the `insample` series :math:`y_{0:t_p}` (the true series ending at :math:`t_p`):\n\n    .. math:: E_m = RMSE(y_{m:t_p}, y_{0:t_p - m}).\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    insample\n        The training series used to forecast `pred_series` . This series serves to compute the scale of the error\n        obtained by a naive forecaster on the training data.\n    m\n        The seasonality to use for differencing to compute the error scale :math:`E_m` (as described in the metric\n        description). :math:`m=1` corresponds to a non-seasonal :math:`E_m` (e.g. naive repetition of the last observed\n        value), whereas :math:`m>1` corresponds to a seasonal :math:`E_m`.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Raises\n    ------\n    ValueError\n        If the `insample` series is periodic ( :math:`y_t = y_{t-m}` ) or any series in `insample` does not end one\n        time step before the start of the corresponding forecast in `pred_series`.\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n\n    References\n    ----------\n    .. [1] https://www.pmorgan.com.au/tutorials/mae%2C-mape%2C-mase-and-the-scaled-rmse/\n    \"\"\"\n    error_scale = _get_error_scale(insample, pred_series, m=m, metric=\"rmse\")\n    errors = _get_wrapped_metric(rmse)(\n        actual_series,\n        pred_series,\n        intersect,\n    )\n    return errors / error_scale\n\n\n@multi_ts_support\n@multivariate_support\ndef sle(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    time_reduction: Optional[Callable[..., np.ndarray]] = None,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Squared Log Error (SLE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column and time step :math:`t` as:\n\n    .. math:: \\\\left(\\\\log{(y_t + 1)} - \\\\log{(\\\\hat{y} + 1)}\\\\right)^2\n\n    using the natural logarithm.\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    time_reduction\n        Optionally, a function to aggregate the metrics over the time axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(c,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        time axis. If `None`, will return a metric per time step.\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - a sequence (list) of uni/multivariate series with `series_reduction`, `component_reduction` and\n          `time_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n time steps, n components) without time\n        and component reductions. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - single uni/multivariate series and at least `time_reduction=None`.\n        - a sequence of uni/multivariate series including `series_reduction` and at least one of\n          `component_reduction=None` or `time_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n\n    y_true, y_pred = _get_values_or_raise(\n        actual_series, pred_series, intersect, remove_nan_union=False\n    )\n    y_true, y_pred = np.log(y_true + 1), np.log(y_pred + 1)\n    return (y_true - y_pred) ** 2\n\n\n@multi_ts_support\n@multivariate_support\ndef rmsle(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Root Mean Squared Log Error (RMSLE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column as:\n\n    .. math:: \\\\sqrt{\\\\frac{1}{T}\\\\sum_{t=1}^T{\\\\left(\\\\log{(y_t + 1)} - \\\\log{(\\\\hat{y}_t + 1)}\\\\right)^2}}\n\n    using the natural logarithm.\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n    return np.sqrt(\n        np.nanmean(\n            _get_wrapped_metric(sle)(\n                actual_series,\n                pred_series,\n                intersect,\n            ),\n            axis=TIME_AX,\n        )\n    )\n\n\n@multi_ts_support\n@multivariate_support\ndef ape(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    time_reduction: Optional[Callable[..., np.ndarray]] = None,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Absolute Percentage Error (APE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed as a\n    percentage value per component/column and time step :math:`t` with:\n\n    .. math:: 100 \\\\cdot \\\\left| \\\\frac{y_t - \\\\hat{y}_t}{y_t} \\\\right|\n\n    Note that it will raise a `ValueError` if :math:`y_t = 0` for some :math:`t`. Consider using\n    the Absolute Scaled Error (:func:`~darts.metrics.metrics.ase`) in these cases.\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    time_reduction\n        Optionally, a function to aggregate the metrics over the time axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(c,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        time axis. If `None`, will return a metric per time step.\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Raises\n    ------\n    ValueError\n        If `actual_series` contains some zeros.\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - a sequence (list) of uni/multivariate series with `series_reduction`, `component_reduction` and\n          `time_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n time steps, n components) without time\n        and component reductions. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - single uni/multivariate series and at least `time_reduction=None`.\n        - a sequence of uni/multivariate series including `series_reduction` and at least one of\n          `component_reduction=None` or `time_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n\n    y_true, y_pred = _get_values_or_raise(\n        actual_series, pred_series, intersect, remove_nan_union=False\n    )\n    if not (y_true != 0).all():\n        raise_log(\n            ValueError(\n                \"`actual_series` must be strictly positive to compute the MAPE.\"\n            ),\n            logger=logger,\n        )\n    return 100.0 * np.abs((y_true - y_pred) / y_true)\n\n\n@multi_ts_support\n@multivariate_support\ndef mape(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Mean Absolute Percentage Error (MAPE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed as a\n    percentage value per component/column with:\n\n    .. math:: 100 \\\\cdot \\\\frac{1}{T} \\\\sum_{t=1}^{T}{\\\\left| \\\\frac{y_t - \\\\hat{y}_t}{y_t} \\\\right|}\n\n    Note that it will raise a `ValueError` if :math:`y_t = 0` for some :math:`t`. Consider using\n    the Mean Absolute Scaled Error (:func:`~darts.metrics.metrics.mase`) in these cases.\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Raises\n    ------\n    ValueError\n        If `actual_series` contains some zeros.\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n\n    return np.nanmean(\n        _get_wrapped_metric(ape)(\n            actual_series,\n            pred_series,\n            intersect,\n        ),\n        axis=TIME_AX,\n    )\n\n\n@multi_ts_support\n@multivariate_support\ndef sape(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    time_reduction: Optional[Callable[..., np.ndarray]] = None,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"symmetric Absolute Percentage Error (sAPE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed as a\n    percentage value per component/column and time step :math:`t` with:\n\n    .. math::\n        200 \\\\cdot \\\\frac{\\\\left| y_t - \\\\hat{y}_t \\\\right|}{\\\\left| y_t \\\\right| + \\\\left| \\\\hat{y}_t \\\\right|}\n\n    Note that it will raise a `ValueError` if :math:`\\\\left| y_t \\\\right| + \\\\left| \\\\hat{y}_t \\\\right| = 0` for some\n    :math:`t`. Consider using the Absolute Scaled Error (:func:`~darts.metrics.metrics.ase`)  in these cases.\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    time_reduction\n        Optionally, a function to aggregate the metrics over the time axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(c,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        time axis. If `None`, will return a metric per time step.\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Raises\n    ------\n    ValueError\n        If `actual_series` and `pred_series` contain some zeros at the same time index.\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - a sequence (list) of uni/multivariate series with `series_reduction`, `component_reduction` and\n          `time_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n time steps, n components) without time\n        and component reductions. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - single uni/multivariate series and at least `time_reduction=None`.\n        - a sequence of uni/multivariate series including `series_reduction` and at least one of\n          `component_reduction=None` or `time_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n\n    y_true, y_pred = _get_values_or_raise(\n        actual_series, pred_series, intersect, remove_nan_union=True\n    )\n    if not np.logical_or(y_true != 0, y_pred != 0).all():\n        raise_log(\n            ValueError(\n                \"`actual_series` must be strictly positive to compute the sMAPE.\"\n            ),\n            logger=logger,\n        )\n    return 200.0 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))\n\n\n@multi_ts_support\n@multivariate_support\ndef smape(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"symmetric Mean Absolute Percentage Error (sMAPE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed as a\n    percentage value per component/column with:\n\n    .. math::\n        200 \\\\cdot \\\\frac{1}{T}\n        \\\\sum_{t=1}^{T}{\\\\frac{\\\\left| y_t - \\\\hat{y}_t \\\\right|}{\\\\left| y_t \\\\right| + \\\\left| \\\\hat{y}_t \\\\right|} }\n\n    Note that it will raise a `ValueError` if :math:`\\\\left| y_t \\\\right| + \\\\left| \\\\hat{y}_t \\\\right| = 0`\n    for some :math:`t`. Consider using the Mean Absolute Scaled Error (:func:`~darts.metrics.metrics.mase`) in these\n    cases.\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Raises\n    ------\n    ValueError\n        If the `actual_series` and the `pred_series` contain some zeros at the same time index.\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n\n    return np.nanmean(\n        _get_wrapped_metric(sape)(\n            actual_series,\n            pred_series,\n            intersect,\n        ),\n        axis=TIME_AX,\n    )\n\n\n@multi_ts_support\n@multivariate_support\ndef ope(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Overall Percentage Error (OPE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed as a\n    percentage value per component/column with:\n\n    .. math:: 100 \\\\cdot \\\\left| \\\\frac{\\\\sum_{t=1}^{T}{y_t}\n              - \\\\sum_{t=1}^{T}{\\\\hat{y}_t}}{\\\\sum_{t=1}^{T}{y_t}} \\\\right|.\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Raises\n    ------\n    ValueError\n        If :math:`\\\\sum_{t=1}^{T}{y_t} = 0`.\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n\n    y_true, y_pred = _get_values_or_raise(\n        actual_series, pred_series, intersect, remove_nan_union=True\n    )\n    y_true_sum, y_pred_sum = np.nansum(y_true, axis=TIME_AX), np.nansum(\n        y_pred, axis=TIME_AX\n    )\n    if not (y_true_sum > 0).all():\n        raise_log(\n            ValueError(\n                \"The series of actual value cannot sum to zero when computing OPE.\"\n            ),\n            logger=logger,\n        )\n    return np.abs((y_true_sum - y_pred_sum) / y_true_sum) * 100.0\n\n\n@multi_ts_support\n@multivariate_support\ndef arre(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    time_reduction: Optional[Callable[..., np.ndarray]] = None,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Absolute Ranged Relative Error (ARRE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed as a\n    percentage value per component/column and time step :math:`t` with:\n\n    .. math:: 100 \\\\cdot \\\\left| \\\\frac{y_t - \\\\hat{y}_t} {\\\\max_t{y_t} - \\\\min_t{y_t}} \\\\right|\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    time_reduction\n        Optionally, a function to aggregate the metrics over the time axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(c,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        time axis. If `None`, will return a metric per time step.\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Raises\n    ------\n    ValueError\n        If :math:`\\\\max_t{y_t} = \\\\min_t{y_t}`.\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - a sequence (list) of uni/multivariate series with `series_reduction`, `component_reduction` and\n          `time_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n time steps, n components) without time\n        and component reductions. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - single uni/multivariate series and at least `time_reduction=None`.\n        - a sequence of uni/multivariate series including `series_reduction` and at least one of\n          `component_reduction=None` or `time_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n\n    y_true, y_pred = _get_values_or_raise(\n        actual_series, pred_series, intersect, remove_nan_union=True\n    )\n    y_max, y_min = np.nanmax(y_true, axis=TIME_AX), np.nanmin(y_true, axis=TIME_AX)\n    if not (y_max > y_min).all():\n        raise_log(\n            ValueError(\n                \"The difference between the max and min values must \"\n                \"be strictly positive to compute the MARRE.\"\n            ),\n            logger=logger,\n        )\n    true_range = y_max - y_min\n    return 100.0 * np.abs((y_true - y_pred) / true_range)\n\n\n@multi_ts_support\n@multivariate_support\ndef marre(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Mean Absolute Ranged Relative Error (MARRE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed as a\n    percentage value per component/column with:\n\n    .. math:: 100 \\\\cdot \\\\frac{1}{T} \\\\sum_{t=1}^{T} {\\\\left| \\\\frac{y_t - \\\\hat{y}_t} {\\\\max_t{y_t} -\n              \\\\min_t{y_t}} \\\\right|}\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Raises\n    ------\n    ValueError\n        If :math:`\\\\max_t{y_t} = \\\\min_t{y_t}`.\n\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n    return np.nanmean(\n        _get_wrapped_metric(arre)(\n            actual_series,\n            pred_series,\n            intersect,\n        ),\n        axis=TIME_AX,\n    )\n\n\n@multi_ts_support\n@multivariate_support\ndef r2_score(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Coefficient of Determination :math:`R^2` (see [1]_ for more details).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column as:\n\n    .. math:: 1 - \\\\frac{\\\\sum_{t=1}^T{(y_t - \\\\hat{y}_t)^2}}{\\\\sum_{t=1}^T{(y_t - \\\\bar{y})^2}},\n\n    where :math:`\\\\bar{y}` is the mean of :math:`y` over all time steps.\n\n    This metric is not symmetric.\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Coefficient_of_determination\n    \"\"\"\n    y_true, y_pred = _get_values_or_raise(\n        actual_series, pred_series, intersect, remove_nan_union=True\n    )\n    ss_errors = np.nansum((y_true - y_pred) ** 2, axis=TIME_AX)\n    y_hat = np.nanmean(y_true, axis=TIME_AX)\n    ss_tot = np.nansum((y_true - y_hat) ** 2, axis=TIME_AX)\n    return 1 - ss_errors / ss_tot\n\n\n@multi_ts_support\n@multivariate_support\ndef coefficient_of_variation(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Coefficient of Variation (percentage).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column as a percentage value with:\n\n    .. math:: 100 \\\\cdot \\\\text{RMSE}(y_t, \\\\hat{y}_t) / \\\\bar{y},\n\n    where :math:`RMSE` is the Root Mean Squared Error (:func:`~darts.metrics.metrics.rmse`), and :math:`\\\\bar{y}` is\n    the average of :math:`y` over all time steps.\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n\n    y_true, y_pred = _get_values_or_raise(\n        actual_series, pred_series, intersect, remove_nan_union=True\n    )\n    # not calling rmse as y_true and y_pred are np.ndarray\n    return (\n        100\n        * np.sqrt(np.nanmean((y_true - y_pred) ** 2, axis=TIME_AX))\n        / np.nanmean(y_true, axis=TIME_AX)\n    )\n\n\n# Dynamic Time Warping\n@multi_ts_support\n@multivariate_support\ndef dtw_metric(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    metric: Callable[\n        [\n            Union[TimeSeries, Sequence[TimeSeries]],\n            Union[TimeSeries, Sequence[TimeSeries]],\n        ],\n        METRIC_OUTPUT_TYPE,\n    ] = mae,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n    **kwargs,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"\n    Applies Dynamic Time Warping to `actual_series` and `pred_series` before passing it into the metric.\n    Enables comparison between series of different lengths, phases and time indices.\n\n    Defaults to using :func:`~darts.metrics.metrics.mae` as a metric.\n\n    See :func:`~darts.dataprocessing.dtw.dtw.dtw` for more supported parameters.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    metric\n        The selected metric with signature '[[TimeSeries, TimeSeries], float]' to use. Default: `mae`.\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n\n    alignment = dtw.dtw(actual_series, pred_series, **kwargs)\n    warped_actual_series, warped_pred_series = alignment.warped()\n    return _get_wrapped_metric(metric)(\n        warped_actual_series,\n        warped_pred_series,\n    )\n\n\n@multi_ts_support\n@multivariate_support\ndef qr(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    q: float = 0.5,\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Quantile Risk (QR)\n\n    QR is a metric that quantifies the accuracy of a specific quantile :math:`q` from the predicted value\n    distribution of a stochastic/probabilistic `pred_series` containing N samples.\n\n    The main difference to the Quantile Loss (QL) is that QR computes the quantile and loss on the aggregate of all\n    sample values summed up along the time axis (QL computes the quantile and loss per time step).\n\n    For the true series :math:`y` and predicted stochastic/probabilistic series (containing N samples) :math:`\\\\hat{y}`\n    of of shape :math:`T \\\\times N`, it is computed per column/component as:\n\n    .. math:: 2 \\\\frac{QL(Z, \\\\hat{Z}_q)}{Z},\n\n    where :math:`QL` is the Quantile Loss (:func:`~darts.metrics.metrics.ql`), :math:`Z = \\\\sum_{t=1}^{T} y_t` is\n    the sum of all target/actual values, :math:`\\\\hat{Z} = \\\\sum_{t=1}^{T} \\\\hat{y}_t` is the sum of all predicted\n    samples along the time axis, and :math:`\\\\hat{Z}_q` is the quantile :math:`q` of that sum.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    q\n        The quantile (float [0, 1]) of interest for the risk evaluation.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n    if not pred_series.is_stochastic:\n        raise_log(\n            ValueError(\n                \"quantile risk (qr) should only be computed for stochastic predicted TimeSeries.\"\n            ),\n            logger=logger,\n        )\n\n    z_true, z_hat = _get_values_or_raise(\n        actual_series,\n        pred_series,\n        intersect,\n        stochastic_quantile=None,\n        remove_nan_union=True,\n    )\n    z_true = np.nansum(z_true, axis=TIME_AX)\n    z_hat = np.nansum(\n        z_hat, axis=TIME_AX\n    )  # aggregate all individual sample realizations\n    z_hat_rho = np.quantile(\n        z_hat, q=q, axis=1\n    )  # get the quantile from aggregated samples\n\n    # quantile loss\n    errors = z_true - z_hat_rho\n    losses = 2 * np.maximum((q - 1) * errors, q * errors)\n    return losses / z_true\n\n\n@multi_ts_support\n@multivariate_support\ndef ql(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    q: float = 0.5,\n    intersect: bool = True,\n    *,\n    time_reduction: Optional[Callable[..., np.ndarray]] = None,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Quantile Loss (QL).\n\n    Also known as Pinball Loss. QL is a metric that quantifies the accuracy of a specific quantile :math:`q` from the\n    predicted value distribution of a stochastic/probabilistic `pred_series` containing N samples.\n\n    QL computes the quantile of all sample values and the loss per time step.\n\n    For the true series :math:`y` and predicted stochastic/probabilistic series (containing N samples) :math:`\\\\hat{y}`\n    of of shape :math:`T \\\\times N`, it is computed per column/component and time step :math:`t` as:\n\n    .. math:: 2 \\\\max((q - 1) (y_t - \\\\hat{y}_{t,q}), q (y_t - \\\\hat{y}_{t,q})),\n\n    where :math:`\\\\hat{y}_{t,q}` is the quantile :math:`q` of all predicted sample values at time :math:`t`.\n    The factor `2` makes the loss more interpretable, as for `q=0.5` the loss is identical to the Absolute Error\n    (:func:`~darts.metrics.metrics.ae`).\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    q\n        The quantile (float [0, 1]) of interest for the loss.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    time_reduction\n        Optionally, a function to aggregate the metrics over the time axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(c,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        time axis. If `None`, will return a metric per time step.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - a sequence (list) of uni/multivariate series with `series_reduction`, `component_reduction` and\n          `time_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n time steps, n components) without time\n        and component reductions. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - single uni/multivariate series and at least `time_reduction=None`.\n        - a sequence of uni/multivariate series including `series_reduction` and at least one of\n          `component_reduction=None` or `time_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n    if not pred_series.is_stochastic:\n        raise_log(\n            ValueError(\n                \"quantile/pinball loss (ql) should only be computed for \"\n                \"stochastic predicted TimeSeries.\"\n            ),\n            logger=logger,\n        )\n\n    y_true, y_pred = _get_values_or_raise(\n        actual_series,\n        pred_series,\n        intersect,\n        stochastic_quantile=q,\n        remove_nan_union=True,\n    )\n    errors = y_true - y_pred\n    losses = 2.0 * np.maximum((q - 1) * errors, q * errors)\n    return losses\n\n\n@multi_ts_support\n@multivariate_support\ndef mql(\n    actual_series: Union[TimeSeries, Sequence[TimeSeries]],\n    pred_series: Union[TimeSeries, Sequence[TimeSeries]],\n    q: float = 0.5,\n    intersect: bool = True,\n    *,\n    component_reduction: Optional[Callable[[np.ndarray], float]] = np.nanmean,\n    series_reduction: Optional[Callable[[np.ndarray], Union[float, np.ndarray]]] = None,\n    n_jobs: int = 1,\n    verbose: bool = False,\n) -> METRIC_OUTPUT_TYPE:\n    \"\"\"Mean Quantile Loss (MQL).\n\n    Also known as Pinball Loss. QL is a metric that quantifies the accuracy of a specific quantile :math:`q` from the\n    predicted value distribution of a stochastic/probabilistic `pred_series` containing N samples.\n\n    MQL first computes the quantile of all sample values and the loss per time step, and then takes the mean over the\n    time axis.\n\n    For the true series :math:`y` and predicted stochastic/probabilistic series (containing N samples) :math:`\\\\hat{y}`\n    of of shape :math:`T \\\\times N`, it is computed per column/component as:\n\n    .. math:: 2 \\\\frac{1}{T}\\\\sum_{t=1}^T{\\\\max((q - 1) (y_t - \\\\hat{y}_{t,q}), q (y_t - \\\\hat{y}_{t,q}))},\n\n    where :math:`\\\\hat{y}_{t,q}` is the quantile :math:`q` of all predicted sample values at time :math:`t`.\n    The factor `2` makes the loss more interpretable, as for `q=0.5` the loss is identical to the Mean Absolute Error\n    (:func:`~darts.metrics.metrics.mae`).\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    q\n        The quantile (float [0, 1]) of interest for the loss.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"\n    return np.nanmean(\n        _get_wrapped_metric(ql)(\n            actual_series,\n            pred_series,\n            q=q,\n            intersect=intersect,\n        ),\n        axis=TIME_AX,\n    )\n", "import_text": ["inspect", "functools.wraps", "inspect.signature", "typing.Callable", "typing.List", "typing.Optional", "typing.Sequence", "typing.Tuple", "typing.Union", "numpy", "darts.TimeSeries", "darts.dataprocessing.dtw", "darts.logging.get_logger", "darts.logging.raise_log", "darts.utils._build_tqdm_iterator", "darts.utils._parallel_apply", "darts.utils.n_steps_between", "darts.utils.ts_utils.SeriesType", "darts.utils.ts_utils.get_series_seq_type", "darts.utils.ts_utils.series2seq"], "prompt": "\"\"\"\nDescription: This function calculates the Root Mean Squared Error (RMSE) between the actual and predicted series.\n\nArgs:\n    actual_series (Union[TimeSeries, Sequence[TimeSeries]]): The actual series.\n    pred_series (Union[TimeSeries, Sequence[TimeSeries]]): The predicted series.\n    intersect (bool): Whether to intersect the actual and predicted series.\n    component_reduction (Optional[Callable[[np.ndarray], float]]): The reduction function to apply on the component level.\n    series_reduction (Optional[Callable[[np.ndarray], Union[float, np.ndarray]]]): The reduction function to apply on the series level.\n    n_jobs (int): The number of jobs to run in parallel.\n    verbose (bool): Whether to print progress.\n\nReturns:\n    METRIC_OUTPUT_TYPE: The Root Mean Squared Error (RMSE) between the actual and predicted series.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Root Mean Squared Error (RMSE).\n\n    For the true series :math:`y` and predicted series :math:`\\\\hat{y}` of length :math:`T`, it is computed per\n    component/column as:\n\n    .. math:: \\\\sqrt{\\\\frac{1}{T}\\\\sum_{t=1}^T{(y_t - \\\\hat{y}_t)^2}}\n\n    If any of the series is stochastic (containing several samples), :math:`\\\\hat{y}_t` is the median over all samples\n    for time step :math:`t`.\n\n    Parameters\n    ----------\n    actual_series\n        The (sequence of) actual series.\n    pred_series\n        The (sequence of) predicted series.\n    intersect\n        For time series that are overlapping in time without having the same time index, setting `True`\n        will consider the values only over their common time interval (intersection in time).\n    component_reduction\n        Optionally, a function to aggregate the metrics over the component/column axis. It must reduce a `np.ndarray`\n        of shape `(t, c)` to a `np.ndarray` of shape `(t,)`. The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `1` corresponding to the\n        component axis. If `None`, will return a metric per component.\n    series_reduction\n        Optionally, a function to aggregate the metrics over the series axis. It must reduce a `np.ndarray`\n        of shape `(s, t, c)` to a `np.ndarray` of shape `(t, c)` The function takes as input a ``np.ndarray`` and a\n        parameter named `axis`, and returns the reduced array. The `axis` receives value `0` corresponding to the\n        series axis. If `None`, will return a metric per series.\n    n_jobs\n        The number of jobs to run in parallel. Parallel jobs are created only when a ``Sequence[TimeSeries]`` is\n        passed as input, parallelising operations regarding different ``TimeSeries``. Defaults to `1`\n        (sequential). Setting the parameter to `-1` means using all the available processors.\n    verbose\n        Optionally, whether to print operations progress\n\n    Returns\n    -------\n    float\n        A single metric score for:\n\n        - single univariate series.\n        - single multivariate series with `component_reduction`.\n        - sequence (list) of uni/multivariate series with `series_reduction` and `component_reduction`.\n    np.ndarray\n        A numpy array of metric scores. The array has shape (n components,) without component reduction. For:\n\n        - single multivariate series and at least `component_reduction=None`.\n        - sequence of uni/multivariate series including `series_reduction` and `component_reduction=None`.\n    List[float]\n        Same as for type `float` but for a sequence of series.\n    List[np.ndarray]\n        Same as for type `np.ndarray` but for a sequence of series.\n    \"\"\"", "function_dependencies": ["numpy.sqrt"], "project_create_time": "2018-09-13T15:17:28+00:00", "project_update_time": "2024-04-17T20:01:47+00:00", "file_create_time": "2020-06-09T16:18:54Z", "file_update_time": "2024-04-04T14:09:31Z", "function_update_time": "2022-01-12T08:02:57Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["numpy.sqrt"], "test_function": [{"file_path": "/darts-0.29.0/darts-0.29.0/darts/tests/metrics/test_metrics.py", "class_name": "TestMetrics", "function_name": "test_rmse", "code": "\n    def test_rmse(self):\n        self.helper_test_multivariate_duplication_equality(metrics.rmse)\n        self.helper_test_multiple_ts_duplication_equality(metrics.rmse)\n\n        np.testing.assert_array_almost_equal(\n            metrics.rmse(\n                self.series1.append(self.series2b),\n                self.series2.append(self.series1b),\n            ),\n            metrics.mse(\n                self.series12,\n                self.series21,\n                component_reduction=lambda x, axis: np.sqrt(np.mean(x, axis=axis)),\n            ),\n        )\n        self.helper_test_nan(metrics.rmse)"}, {"file_path": "/darts-0.29.0/darts-0.29.0/darts/tests/metrics/test_metrics.py", "class_name": "TestMetrics", "function_name": "test_multiple_ts_rmse", "code": "\n    def test_multiple_ts_rmse(self):\n        # simple test\n        multi_ts_1 = [self.series1 + 1, self.series1 + 1]\n        multi_ts_2 = [self.series1 + 2, self.series1 + 1]\n        assert (\n            metrics.rmse(\n                multi_ts_1,\n                multi_ts_2,\n                component_reduction=np.mean,\n                series_reduction=np.mean,\n            )\n            == 0.5\n        )"}]}, {"git_group": "pyupio", "git_name": "safety", "version": "3.1.0", "language": "Python", "project_name": "safety-3.1.0.zip", "file_path": "/safety-3.1.0/safety-3.1.0/safety/safety.py", "file_name": "safety.py", "focal_class": null, "focal_name": "get_closest_ver", "focal_parameter": ["versions", "version"], "solution": "\ndef get_closest_ver(versions, version, spec: SpecifierSet):\n    results = {'upper': None, 'lower': None}\n\n    if (not version and not spec) or not versions:\n        return results\n\n    sorted_versions = sorted(versions, key=lambda ver: parse_version(ver), reverse=True)\n\n    if not version:\n        sorted_versions = spec.filter(sorted_versions, prereleases=False)\n\n        upper = None\n        lower = None\n\n        try:\n            sorted_versions = list(sorted_versions)\n            upper = sorted_versions[0]\n            lower = sorted_versions[-1]\n            results['upper'] = upper\n            results['lower'] = lower if upper != lower else None\n        except IndexError:\n            pass\n\n        return results\n\n    current_v = parse_version(version)\n\n    for v in sorted_versions:\n        index = parse_version(v)\n\n        if index > current_v:\n            results['upper'] = index\n\n        if index < current_v:\n            results['lower'] = index\n            break\n\n    return results", "function_signature": "def get_closest_ver(versions, version, spec: SpecifierSet) :", "left_context": "# -*- coding: utf-8 -*-\nfrom dataclasses import asdict\nimport errno\nimport itertools\nimport json\nimport logging\nimport os\nfrom pathlib import Path\nimport random\nimport sys\nimport tempfile\nimport time\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom typing import Dict, Optional, List\n\nimport click\nimport requests\nfrom requests.models import PreparedRequest\nfrom packaging.specifiers import SpecifierSet\nfrom packaging.utils import canonicalize_name\nfrom packaging.version import parse as parse_version, Version\nfrom pydantic.json import pydantic_encoder\n\nfrom safety_schemas.models import Ecosystem, FileType\n\n\n\nfrom .constants import (API_MIRRORS, DB_CACHE_FILE, OPEN_MIRRORS, REQUEST_TIMEOUT, DATA_API_BASE_URL, JSON_SCHEMA_VERSION,\n                        IGNORE_UNPINNED_REQ_REASON)\nfrom .errors import (DatabaseFetchError, DatabaseFileNotFoundError, InvalidCredentialError,\n                     TooManyRequestsError, NetworkConnectionError,\n                     RequestTimeoutError, ServerError, MalformedDatabase)\nfrom .models import Vulnerability, CVE, Severity, Fix, is_pinned_requirement, SafetyRequirement\nfrom .output_utils import print_service, get_applied_msg, prompt_service, get_skipped_msg, get_fix_opt_used_msg, \\\n    is_using_api_key, get_specifier_range_info\nfrom .util import build_remediation_info_url, pluralize, read_requirements, Package, build_telemetry_data, sync_safety_context, \\\n    SafetyContext, validate_expiration_date, is_a_remote_mirror, get_requirements_content, SafetyPolicyFile, \\\n    get_terminal_size, is_ignore_unpinned_mode, get_hashes\n\nLOG = logging.getLogger(__name__)\n\n\ndef get_from_cache(db_name, cache_valid_seconds=0, skip_time_verification=False):\n    if os.path.exists(DB_CACHE_FILE):\n        with open(DB_CACHE_FILE) as f:\n            try:\n                data = json.loads(f.read())\n                if db_name in data:\n\n                    if \"cached_at\" in data[db_name]:\n                        if data[db_name][\"cached_at\"] + cache_valid_seconds > time.time() or skip_time_verification:\n                            LOG.debug('Getting the database from cache at %s, cache setting: %s',\n                                      data[db_name][\"cached_at\"], cache_valid_seconds)\n                            \n                            try:\n                                data[db_name][\"db\"][\"meta\"][\"base_domain\"] = \"https://data.safetycli.com\"\n                            except KeyError as e:\n                                pass\n\n                            return data[db_name][\"db\"]\n\n                        LOG.debug('Cached file is too old, it was cached at %s', data[db_name][\"cached_at\"])\n                    else:\n                        LOG.debug('There is not the cached_at key in %s database', data[db_name])\n\n            except json.JSONDecodeError:\n                LOG.debug('JSONDecodeError trying to get the cached database.')\n    else:\n        LOG.debug(\"Cache file doesn't exist...\")\n    return False\n\n\ndef write_to_cache(db_name, data):\n    # cache is in: ~/safety/cache.json\n    # and has the following form:\n    # {\n    #   \"insecure.json\": {\n    #       \"cached_at\": 12345678\n    #       \"db\": {}\n    #   },\n    #   \"insecure_full.json\": {\n    #       \"cached_at\": 12345678\n    #       \"db\": {}\n    #   },\n    # }\n    if not os.path.exists(os.path.dirname(DB_CACHE_FILE)):\n        try:\n            os.makedirs(os.path.dirname(DB_CACHE_FILE))\n            with open(DB_CACHE_FILE, \"w\") as _:\n                _.write(json.dumps({}))\n                LOG.debug('Cache file created')\n        except OSError as exc:  # Guard against race condition\n            LOG.debug('Unable to create the cache file because: %s', exc.errno)\n            if exc.errno != errno.EEXIST:\n                raise\n\n    with open(DB_CACHE_FILE, \"r\") as f:\n        try:\n            cache = json.loads(f.read())\n        except json.JSONDecodeError:\n            LOG.debug('JSONDecodeError in the local cache, dumping the full cache file.')\n            cache = {}\n\n    with open(DB_CACHE_FILE, \"w\") as f:\n        cache[db_name] = {\n            \"cached_at\": time.time(),\n            \"db\": data\n        }\n        f.write(json.dumps(cache))\n        LOG.debug('Safety updated the cache file for %s database.', db_name)\n\n\ndef fetch_database_url(session, mirror, db_name, cached, telemetry=True,\n                       ecosystem: Ecosystem = Ecosystem.PYTHON, from_cache=True):\n    headers = {'schema-version': JSON_SCHEMA_VERSION, 'ecosystem': ecosystem.value}    \n\n    if cached and from_cache:\n        cached_data = get_from_cache(db_name=db_name, cache_valid_seconds=cached)\n        if cached_data:\n            LOG.info('Database %s returned from cache.', db_name)\n            return cached_data\n    url = mirror + db_name\n\n    \n    telemetry_data = {\n        'telemetry': json.dumps(build_telemetry_data(telemetry=telemetry), \n                                default=pydantic_encoder)}\n\n    try:\n        r = session.get(url=url, timeout=REQUEST_TIMEOUT, \n                        headers=headers, params=telemetry_data)\n    except requests.exceptions.ConnectionError:\n        raise NetworkConnectionError()\n    except requests.exceptions.Timeout:\n        raise RequestTimeoutError()\n    except requests.exceptions.RequestException:\n        raise DatabaseFetchError()\n\n    if r.status_code == 403:\n        raise InvalidCredentialError(credential=session.get_credential(), reason=r.text)\n\n    if r.status_code == 429:\n        raise TooManyRequestsError(reason=r.text)\n\n    if r.status_code != 200:\n        raise ServerError(reason=r.reason)\n\n    try:\n        data = r.json()\n    except json.JSONDecodeError as e:\n        raise MalformedDatabase(reason=e)\n\n    if cached:\n        LOG.info('Writing %s to cache because cached value was %s', db_name, cached)\n        write_to_cache(db_name, data)\n\n    return data\n\n\ndef fetch_policy(session):\n    url = f\"{DATA_API_BASE_URL}policy/\"\n\n    try:\n        LOG.debug(f'Getting policy')\n        r = session.get(url=url, timeout=REQUEST_TIMEOUT)\n        LOG.debug(r.text)\n        return r.json()\n    except Exception:\n        LOG.exception(\"Error fetching policy\")\n\n        return {\"safety_policy\": \"\", \"audit_and_monitor\": False}\n\n\ndef post_results(session, safety_json, policy_file):\n    url = f\"{DATA_API_BASE_URL}result/\"\n\n    # safety_json is in text form already. policy_file is a text YAML\n    audit_report = {\n        \"safety_json\": json.loads(safety_json),\n        \"policy_file\": policy_file\n    }\n\n    try:\n        LOG.debug(f'Posting results to: {url}')\n        LOG.debug(f'Posting results: {audit_report}')\n        r = session.post(url=url, timeout=REQUEST_TIMEOUT, json=audit_report)\n        LOG.debug(r.text)\n\n        return r.json()\n    except:\n        LOG.exception(\"Error posting results\")\n        click.secho(\n            \"Warning: couldn't upload results to safetycli.com.\",\n            fg=\"yellow\",\n            file=sys.stderr\n        )\n\n        return {}\n\n\ndef fetch_database_file(path, db_name, ecosystem: Ecosystem = Ecosystem.PYTHON):\n    full_path = os.path.join(path, db_name)\n    if not os.path.exists(full_path):\n        raise DatabaseFileNotFoundError(db=path)\n    with open(full_path) as f:\n        return json.loads(f.read())\n\n\ndef is_valid_database(db) -> bool:\n    try:\n        if db['meta']['schema_version'] == JSON_SCHEMA_VERSION:\n            return True\n    except Exception:\n        return False\n\n    return False\n\n\ndef fetch_database(session, full=False, db=False, cached=0, telemetry=True, \n                   ecosystem: Ecosystem = Ecosystem.PYTHON, from_cache=True):\n    if session.is_using_auth_credentials():\n        mirrors = API_MIRRORS\n    elif db:\n        mirrors = [db]\n    else:\n        mirrors = OPEN_MIRRORS\n\n    db_name = \"insecure_full.json\" if full else \"insecure.json\"\n    for mirror in mirrors:\n        # mirror can either be a local path or a URL\n        if is_a_remote_mirror(mirror):\n            data = fetch_database_url(session, mirror, db_name=db_name, cached=cached, \n                                      telemetry=telemetry, ecosystem=ecosystem, from_cache=from_cache)\n        else:\n            data = fetch_database_file(mirror, db_name=db_name, ecosystem=ecosystem)\n        if data:\n            if is_valid_database(data):\n                return data\n            raise MalformedDatabase(fetched_from=mirror,\n                                    reason=f'Not supported schema version. '\n                                           f'This Safety version supports only schema version {JSON_SCHEMA_VERSION}')\n\n    raise DatabaseFetchError()\n\n\ndef get_vulnerabilities(pkg, spec, db):\n    for entry in db['vulnerable_packages'][pkg]:\n        for entry_spec in entry[\"specs\"]:\n            if entry_spec == spec:\n                yield entry\n\n\ndef get_vulnerability_from(vuln_id, cve, data, specifier, db, name, pkg, ignore_vulns, affected):\n    base_domain = db.get('meta', {}).get('base_domain')\n    unpinned_ignored = ignore_vulns.get(vuln_id, {}).get('requirements', None)\n    should_ignore = not unpinned_ignored or str(affected.specifier) in unpinned_ignored\n\n    ignored = (ignore_vulns and vuln_id in ignore_vulns and should_ignore and (\n            not ignore_vulns[vuln_id]['expires'] or ignore_vulns[vuln_id]['expires'] > datetime.utcnow()))\n    more_info_url = f\"{base_domain}{data.get('more_info_path', '')}\"\n    severity = None\n\n    if cve and (cve.cvssv2 or cve.cvssv3):\n        severity = Severity(source=cve.name, cvssv2=cve.cvssv2, cvssv3=cve.cvssv3)\n\n    analyzed_requirement = affected\n    analyzed_version = next(iter(analyzed_requirement.specifier)).version if is_pinned_requirement(\n        analyzed_requirement.specifier) else None\n\n    vulnerable_spec = set()\n    vulnerable_spec.add(specifier)\n\n    return Vulnerability(\n        vulnerability_id=vuln_id,\n        package_name=name,\n        pkg=pkg,\n        ignored=ignored,\n        ignored_reason=ignore_vulns.get(vuln_id, {}).get('reason', None) if ignore_vulns else None,\n        ignored_expires=ignore_vulns.get(vuln_id, {}).get('expires', None) if ignore_vulns else None,\n        vulnerable_spec=vulnerable_spec,\n        all_vulnerable_specs=data.get(\"specs\", []),\n        analyzed_version=analyzed_version,\n        analyzed_requirement=analyzed_requirement,\n        advisory=data.get(\"advisory\"),\n        is_transitive=data.get(\"transitive\", False),\n        published_date=data.get(\"published_date\"),\n        fixed_versions=[ver for ver in data.get(\"fixed_versions\", []) if ver],\n        closest_versions_without_known_vulnerabilities=data.get(\"closest_secure_versions\", []),\n        resources=data.get(\"vulnerability_resources\"),\n        CVE=cve,\n        severity=severity,\n        affected_versions=data.get(\"affected_versions\", []),\n        more_info_url=more_info_url\n    )\n\n\ndef get_cve_from(data, db_full):\n    try:\n        xve_id: str = str(\n            next(filter(lambda i: i.get('type', None) in ['cve', 'pve'], data.get('ids', []))).get('id', ''))\n    except StopIteration:\n        xve_id: str = ''\n\n    if not xve_id:\n        return None\n\n    cve_meta = db_full.get(\"meta\", {}).get(\"severities\", {}).get(xve_id, {})\n    return CVE(name=xve_id, cvssv2=cve_meta.get(\"cvssv2\", None),\n               cvssv3=cve_meta.get(\"cvssv3\", None))\n\n\ndef ignore_vuln_if_needed(pkg: Package, vuln_id, cve, ignore_vulns, ignore_severity_rules, req):\n    if not ignore_severity_rules:\n        ignore_severity_rules = {}\n\n    if not isinstance(ignore_vulns, dict):\n        return\n\n    severity = None\n\n    if cve:\n        if cve.cvssv2 and cve.cvssv2.get(\"base_score\", None):\n            severity = cve.cvssv2.get(\"base_score\", None)\n\n        if cve.cvssv3 and cve.cvssv3.get(\"base_score\", None):\n            severity = cve.cvssv3.get(\"base_score\", None)\n\n    ignore_severity_below = float(ignore_severity_rules.get('ignore-cvss-severity-below', 0.0))\n    ignore_unknown_severity = bool(ignore_severity_rules.get('ignore-cvss-unknown-severity', False))\n\n    if severity:\n        if float(severity) < ignore_severity_below:\n            reason = 'Ignored by severity rule in policy file, {0} < {1}'.format(float(severity), ignore_severity_below)\n            ignore_vulns[vuln_id] = {'reason': reason, 'expires': None}\n    elif ignore_unknown_severity:\n        reason = 'Unknown CVSS severity, ignored by severity rule in policy file.'\n        ignore_vulns[vuln_id] = {'reason': reason, 'expires': None}\n\n    version = next(iter(req.specifier)).version if is_pinned_requirement(req.specifier) else pkg.version\n\n    is_prev_not_ignored: bool = vuln_id not in ignore_vulns\n    is_req_not_ignored: bool = 'requirements' in ignore_vulns.get(vuln_id, {}) and str(req.specifier) not in ignore_vulns.get(vuln_id, {}).get('requirements', set())\n\n    if (is_prev_not_ignored or is_req_not_ignored) and is_ignore_unpinned_mode(version):\n        reason = IGNORE_UNPINNED_REQ_REASON\n        requirements = set()\n        requirements.add(str(req.specifier))\n        ignore_vulns[vuln_id] = {'reason': reason, 'expires': None, 'requirements': requirements}\n\n\ndef is_vulnerable(vulnerable_spec: SpecifierSet, requirement, package):\n\n    if is_pinned_requirement(requirement.specifier):\n        try:\n            return vulnerable_spec.contains(next(iter(requirement.specifier)).version)\n        except Exception:\n            # Ugly for now...\n            message = f'Version {requirement.specifier} for {package.name} is invalid and is ignored by Safety. Please See PEP 440.'\n            if message not in [a['message'] for a in SafetyContext.local_announcements]:\n                SafetyContext.local_announcements.append(\n                    {'message': message,\n                    'type': 'warning', 'local': True})\n            return False\n\n    return any(requirement.specifier.filter(vulnerable_spec.filter(package.insecure_versions, prereleases=True),\n                                            prereleases=True))\n\n\n@sync_safety_context\ndef check(*, session=None, packages=[], db_mirror=False, cached=0, ignore_vulns=None, ignore_severity_rules=None, proxy=None,\n          include_ignored=False, is_env_scan=True, telemetry=True, params=None, project=None):\n    SafetyContext().command = 'check'\n    db = fetch_database(session, db=db_mirror, cached=cached, telemetry=telemetry)\n    db_full = None\n    vulnerable_packages = frozenset(db.get('vulnerable_packages', []))\n    vulnerabilities = []\n    found_pkgs = {}\n    requirements = iter([])\n\n    for p in packages:\n        requirements = itertools.chain(p.requirements, requirements)\n        found_pkgs[canonicalize_name(p.name)] = p\n\n    # Let's report by req, pinned in environment will be ==version\n    for req in requirements:\n        vuln_per_req = {}\n        name = canonicalize_name(req.name)\n\n        pkg = found_pkgs.get(name, None)\n\n        if not pkg.version:\n            if not db_full:\n                db_full = fetch_database(session, full=True, db=db_mirror, cached=cached,\n                                         telemetry=telemetry)\n            pkg.refresh_from(db_full)\n\n        if name in vulnerable_packages:\n            # we have a candidate here, build the spec set\n            for specifier in db['vulnerable_packages'][name]:\n                spec_set = SpecifierSet(specifiers=specifier)\n\n                if is_vulnerable(spec_set, req, pkg):\n                    if not db_full:\n                        db_full = fetch_database(session, full=True, db=db_mirror, cached=cached,\n                                                 telemetry=telemetry)\n                    if not pkg.latest_version:\n                        pkg.refresh_from(db_full)\n\n                    for data in get_vulnerabilities(pkg=name, spec=specifier, db=db_full):\n                        try:\n                            vuln_id: str = str(next(filter(lambda i: i.get('type', None) == 'pyup', data.get('ids', []))).get('id', ''))\n                        except StopIteration:\n                            vuln_id: str = ''\n\n                        if vuln_id in vuln_per_req:\n                            vuln_per_req[vuln_id].vulnerable_spec.add(specifier)\n                            continue\n\n                        cve = get_cve_from(data, db_full)\n\n                        ignore_vuln_if_needed(pkg, vuln_id, cve, ignore_vulns, ignore_severity_rules, req)\n\n                        vulnerability = get_vulnerability_from(vuln_id, cve, data, specifier, db_full, name, pkg,\n                                                               ignore_vulns, req)\n\n                        should_add_vuln = not (vulnerability.is_transitive and is_env_scan)\n\n                        if (include_ignored or vulnerability.vulnerability_id not in ignore_vulns) and should_add_vuln:\n                            vuln_per_req[vulnerability.vulnerability_id] = vulnerability\n                            vulnerabilities.append(vulnerability)\n\n    return vulnerabilities, db_full\n\n\ndef precompute_remediations(remediations, packages, vulns, secure_vulns_by_user):\n\n    for vuln in vulns:\n\n        if vuln.ignored and vuln.ignored_reason != IGNORE_UNPINNED_REQ_REASON:\n            secure_vulns_by_user.add(vuln.vulnerability_id)\n            continue\n\n        if vuln.package_name in remediations.keys() and str(vuln.analyzed_requirement.specifier) in remediations[vuln.package_name]:\n            spec = remediations[vuln.package_name][str(vuln.analyzed_requirement.specifier)]\n            spec['vulnerabilities_found'] = spec.get('vulnerabilities_found', 0) + 1\n        else:\n            version = None\n            is_pinned = is_pinned_requirement(vuln.analyzed_requirement.specifier)\n\n            if is_pinned:\n                version = next(iter(vuln.analyzed_requirement.specifier)).version\n\n            if not is_pinned and is_ignore_unpinned_mode(version):\n                # Let's ignore this requirement\n                continue\n\n            vulns_count = 1\n            packages[vuln.package_name] = vuln.pkg\n\n            remediations[vuln.package_name][str(vuln.analyzed_requirement.specifier)] = {\n                'vulnerabilities_found': vulns_count,\n                'version': version,\n                'requirement': vuln.analyzed_requirement,\n                'more_info_url': vuln.pkg.more_info_url}\n\n", "right_context": "\n\ndef compute_sec_ver_for_user(package: Package, secure_vulns_by_user, db_full):\n    versions = package.get_versions(db_full)\n    affected_versions = []\n\n    for vuln in db_full.get('vulnerable_packages', {}).get(package.name, []):\n        vuln_id: str = str(next(filter(lambda i: i.get('type', None) == 'pyup', vuln.get('ids', []))).get('id', ''))\n        if vuln_id and vuln_id not in secure_vulns_by_user:\n            affected_versions += vuln.get('affected_versions', [])\n\n    affected_v = set(affected_versions)\n    sec_ver_for_user = list(versions.difference(affected_v))\n\n    return sorted(sec_ver_for_user, key=lambda ver: parse_version(ver), reverse=True)\n\n\ndef compute_sec_ver(remediations, packages: Dict[str, Package], secure_vulns_by_user, db_full):\n    \"\"\"\n    Compute the secure_versions and the closest_secure_version for each remediation using the affected_versions\n    of each no ignored vulnerability of the same package, there is only a remediation for each package.\n    \"\"\"\n    for pkg_name in remediations.keys():\n        pkg: Package = packages.get(pkg_name, None)\n\n        secure_versions = []\n\n        if pkg:\n            secure_versions = pkg.secure_versions\n\n        analyzed = set(remediations[pkg_name].keys())\n\n        if not is_using_api_key():\n            continue\n\n        for analyzed_requirement in analyzed:\n            rem = remediations[pkg_name][analyzed_requirement]\n            spec = rem.get('requirement').specifier\n            version = rem['version']\n\n            if not secure_vulns_by_user:\n                secure_v = sorted(secure_versions, key=lambda ver: parse_version(ver), reverse=True)\n            else:\n                secure_v = compute_sec_ver_for_user(package=pkg, secure_vulns_by_user=secure_vulns_by_user, db_full=db_full)\n\n            rem['closest_secure_version'] = get_closest_ver(secure_v, version, spec)\n    \n            upgrade = rem['closest_secure_version'].get('upper', None)\n            downgrade = rem['closest_secure_version'].get('lower', None)\n            recommended_version = None\n    \n            if upgrade:\n                recommended_version = upgrade\n            elif downgrade:\n                recommended_version = downgrade\n    \n            rem['recommended_version'] = recommended_version\n            rem['other_recommended_versions'] = [other_v for other_v in secure_v if\n                                                                    other_v != str(recommended_version)]\n\n            # Refresh the URL with the recommended version.\n\n            spec = str(rem['requirement'].specifier)\n            if is_using_api_key():\n                rem['more_info_url'] = \\\n                    build_remediation_info_url(base_url=rem['more_info_url'], version=version,\n                                           spec=spec,\n                                           target_version=recommended_version)\n\n\ndef calculate_remediations(vulns, db_full):\n    remediations = defaultdict(dict)\n    package_metadata = {}\n    secure_vulns_by_user = set()\n\n    if not db_full:\n        return remediations\n\n    precompute_remediations(remediations, package_metadata, vulns, secure_vulns_by_user)\n    compute_sec_ver(remediations, package_metadata, secure_vulns_by_user, db_full)\n\n    return remediations\n\n\ndef should_apply_auto_fix(from_ver: Optional[Version], to_ver, allowed_automatic):\n    if not from_ver:\n        return False\n\n    if 'major' in allowed_automatic:\n        return True\n\n    major_change = to_ver.major - from_ver.major\n    minor_change = to_ver.minor - from_ver.minor\n\n    if 'minor' in allowed_automatic:\n        if major_change != 0:\n            return False\n\n        return True\n\n    if 'patch' in allowed_automatic:\n        if major_change != 0 or minor_change != 0:\n            return False\n\n        return True\n\n    return False\n\n\ndef get_update_type(from_ver: Optional[Version], to_ver: Version):\n\n    if not from_ver or (to_ver.major - from_ver.major) != 0:\n        return 'major'\n\n    if (to_ver.minor - from_ver.minor) != 0:\n        return 'minor'\n\n    return 'patch'\n\n\ndef process_fixes(files, remediations, auto_remediation_limit, output, no_output=True, prompt=False):\n    req_remediations = itertools.chain.from_iterable(rem.values() for pkg_name, rem in remediations.items())\n    requirements = compute_fixes_per_requirements(files, req_remediations, auto_remediation_limit, prompt=prompt)\n    fixes = apply_fixes(requirements, output, no_output, prompt)\n    return fixes\n\n\ndef process_fixes_scan(file_to_fix, to_fix_spec, auto_remediation_limit, output, no_output=True, prompt=False):\n    to_fix_remediations =  []\n    \n    def get_remmediation_from(spec):\n        upper = None\n        lower = None\n        recommended = None\n        \n        try:\n            upper = Version(spec.remediation.closest_secure.upper) if spec.remediation.closest_secure.upper else None\n        except Exception as e:\n            LOG.error(f'Error getting upper remediation version, ignoring', exc_info=True)\n\n        try:\n            lower = Version(spec.remediation.closest_secure.lower) if spec.remediation.closest_secure.lower else None\n        except Exception as e:\n            LOG.error(f'Error getting lower remediation version, ignoring', exc_info=True)\n\n        try:\n            recommended = Version(spec.remediation.recommended)\n        except Exception as e:\n            LOG.error(f'Error getting recommended version for remediation, ignoring', exc_info=True)            \n\n        return {\n            \"vulnerabilities_found\": spec.remediation.vulnerabilities_found,\n            \"version\": next(iter(spec.specifier)).version if spec.is_pinned() else None,\n            \"requirement\": spec,\n            \"more_info_url\": spec.remediation.more_info_url,\n            \"closest_secure_version\": {\n                'upper': upper, \n                'lower': lower\n                },\n            \"recommended_version\": recommended,\n            \"other_recommended_versions\": spec.remediation.other_recommended\n        }\n\n    req_remediations = iter(get_remmediation_from(spec) for spec in to_fix_spec)\n    SUPPORTED_FILE_TYPES = [FileType.REQUIREMENTS_TXT]\n\n    if file_to_fix.file_type in SUPPORTED_FILE_TYPES:\n        files = (open(file_to_fix.location),)\n        requirements = compute_fixes_per_requirements(files, req_remediations, auto_remediation_limit, prompt=prompt)\n    else:\n        requirements = {\n            'files': {str(file_to_fix.location): {'content': None, 'fixes': {'TO_SKIP': [], 'TO_APPLY': [], 'TO_CONFIRM': []}, 'supported': False, 'filename': file_to_fix.location.name}},\n            'dependencies': defaultdict(dict),\n        }\n    \n    fixes = apply_fixes(requirements, output, no_output, prompt, scan_flow=True, auto_remediation_limit=auto_remediation_limit)\n\n    return fixes\n\n\ndef compute_fixes_per_requirements(files, req_remediations, auto_remediation_limit, prompt=False):\n    requirements_files = get_requirements_content(files)\n\n    from dparse.parser import parse, filetypes\n    from packaging.version import Version, InvalidVersion\n\n    requirements = {\n        'files': {},\n        'dependencies': defaultdict(dict),\n    }\n\n    for name, contents in requirements_files.items():\n        dependency_file = parse(contents, path=name, file_type=filetypes.requirements_txt, resolve=True)\n        dependency_files = dependency_file.resolved_files + [dependency_file]\n        empty_spec = SpecifierSet('')\n        default_spec = SpecifierSet('>=0')\n\n        # Support recursive requirements in the multiple requirement files provided\n        for resolved_f in dependency_files:\n            if not resolved_f or isinstance(resolved_f, str):\n                continue\n            file = {'content': resolved_f.content, 'fixes': {'TO_SKIP': [], 'TO_APPLY': [], 'TO_CONFIRM': []}}\n            requirements['files'][resolved_f.path] = file\n\n            for d in resolved_f.dependencies:\n                if d.specs == empty_spec:\n                    d.specs = default_spec\n\n                requirements['dependencies'][d.name][str(d.specs)] = (d, resolved_f.path)\n\n    for remediation in req_remediations:\n        req: SafetyRequirement = remediation.get('requirement')\n        pkg: str = req.name\n\n        dry_fix = Fix(package=pkg, more_info_url=remediation.get('more_info_url', ''),\n                      previous_spec=str(req.specifier),\n                      other_options=remediation.get('other_recommended_versions', []))\n        from_ver: Optional[str] = remediation.get('version', None)\n\n        if pkg not in requirements['dependencies'] or dry_fix.previous_spec not in requirements['dependencies'][pkg]:\n            # Let's attach it to the first file scanned.\n            file = next(iter(requirements['files']))\n            # Let's use the no parsed version.\n            dry_fix.previous_version = from_ver\n            dry_fix.status = 'AUTOMATICALLY_SKIPPED_NOT_FOUND_IN_FILE'\n            dry_fix.applied_at = file\n            requirements['files'][file]['fixes']['TO_SKIP'].append(dry_fix)\n            continue\n\n        dependency, name = requirements['dependencies'][pkg][dry_fix.previous_spec]\n        dry_fix.applied_at = name\n\n        fixes = requirements['files'][name]['fixes']\n\n        to_ver: Version = remediation['recommended_version']\n\n        try:\n            from_ver = parse_version(from_ver)\n        except (InvalidVersion, TypeError):\n\n            if not dry_fix.previous_spec:\n                dry_fix.status = 'AUTOMATICALLY_SKIPPED_INVALID_VERSION'\n                fixes['TO_SKIP'].append(dry_fix)\n                continue\n\n        dry_fix.previous_version = str(from_ver) if from_ver else from_ver\n\n        if remediation['recommended_version'] is None:\n            dry_fix.status = 'AUTOMATICALLY_SKIPPED_NO_RECOMMENDED_VERSION'\n            fixes['TO_SKIP'].append(dry_fix)\n            continue\n\n        dry_fix.updated_version = str(to_ver)\n\n        is_fixed = from_ver == to_ver\n\n        if is_fixed:\n            dry_fix.status = 'AUTOMATICALLY_SKIPPED_ALREADY_FIXED'\n            fixes['TO_SKIP'].append(dry_fix)\n            continue\n\n        update_type = get_update_type(from_ver, to_ver)\n        dry_fix.update_type = update_type\n        dry_fix.dependency = dependency\n\n        auto_fix = should_apply_auto_fix(from_ver, to_ver, auto_remediation_limit)\n\n        TARGET = 'TO_APPLY'\n\n        if auto_fix:\n            dry_fix.status = 'PENDING_TO_APPLY'\n            dry_fix.fix_type = 'AUTOMATIC'\n        elif prompt:\n            TARGET = 'TO_CONFIRM'\n            dry_fix.status = 'PENDING_TO_CONFIRM'\n            dry_fix.fix_type = 'MANUAL'\n        else:\n            TARGET = 'TO_SKIP'\n            dry_fix.status = 'AUTOMATICALLY_SKIPPED_UNABLE_TO_CONFIRM'\n\n        fixes[TARGET].append(dry_fix)\n\n    return requirements\n\n\ndef apply_fixes(requirements, out_type, no_output, prompt, scan_flow=False, auto_remediation_limit=None):\n\n    from dparse.updater import RequirementsTXTUpdater\n\n    skip = []\n    apply = []\n    confirm = []\n\n    brief = []\n\n    if not no_output:\n        style_kwargs = {}\n\n        if not scan_flow:\n            brief.append(('', {}))\n            brief.append((f\"Safety fix running\", style_kwargs))\n        print_service(brief, out_type)\n\n    for name, data in requirements['files'].items():\n        output = [('', {}),\n                  (f\"Analyzing {name}... [{get_fix_opt_used_msg(auto_remediation_limit)} limit]\", {'styling': {'bold': True}, 'start_line_decorator': '->', 'indent': ' '})]\n        \n        r_skip = data['fixes']['TO_SKIP']\n        r_apply = data['fixes']['TO_APPLY']\n        r_confirm = data['fixes']['TO_CONFIRM']\n\n        if data.get('supported', True):\n            new_content = data['content']\n\n            updated: bool = False\n\n            for f in r_apply:\n                new_content = RequirementsTXTUpdater.update(content=new_content, version=f.updated_version,\n                                                            dependency=f.dependency, hashes=get_hashes(f.dependency))\n                f.status = 'APPLIED'\n                updated = True\n                output.append(('', {}))\n                output.append((f'- {get_applied_msg(f, mode=\"auto\")}', {}))\n\n            for f in r_skip:\n                output.append(('', {}))\n                output.append((f'- {get_skipped_msg(f)}', {}))\n\n            if not no_output:\n                print_service(output, out_type)\n\n            if prompt and not no_output:\n                for f in r_confirm:\n                    options = [f\"({index}) =={option}\" for index, option in enumerate(f.other_options)]\n                    input_hint = f'Enter \u201cy\u201d to update to {f.package}=={f.updated_version}, \u201cn\u201d to skip this package upgrade'\n\n                    if len(options) > 0:\n                        input_hint += f', or enter the index from these secure versions to upgrade to that version: {\", \".join(options)}'\n\n                    print_service([('', {})], out_type)\n                    confirmed: str = prompt_service(\n                        (f'- {f.package}{f.previous_spec} requires at least a {f.update_type} version update. Do you want to update {f.package} from {f.previous_spec} to =={f.updated_version}, which is the closest secure version? {input_hint}', {}),\n                        out_type\n                    ).lower()\n\n                    try:\n                        index: int = int(confirmed)\n                        if index <= len(f.other_options):\n                            confirmed = 'y'\n                    except ValueError:\n                        index = -1\n\n                    if confirmed == 'y' or index > -1:\n                        f.status = 'APPLIED'\n                        updated = True\n\n                        if index > -1:\n                            f.updated_version = f.other_options[index]\n\n                        new_content = RequirementsTXTUpdater.update(content=new_content, version=f.updated_version,\n                                                                    dependency=f.dependency,\n                                                                    hashes=get_hashes(f.dependency))\n                        output.append((get_applied_msg(f, mode=\"manual\"), {'indent': ' ' * 5}))\n                    else:\n                        f.status = 'MANUALLY_SKIPPED'\n                        output.append((get_skipped_msg(f), {'indent': ' ' * 5}))\n\n                    if not no_output:\n                        print_service(output, out_type)\n\n            if updated:\n                output.append(('', {}))\n                output.append((f\"Updating {name}...\", {}))\n                with open(name, mode=\"w\") as r_file:\n                    r_file.write(new_content)\n                output.append((f\"Changes applied to {name}.\", {}))\n                count = len(r_apply) + len([1 for fix in r_confirm if fix.status == 'APPLIED'])\n                output.append((f\"{count} package {pluralize('version', count)} {pluralize('has', count)} been updated to secure versions in {Path(name).name}\", {}))\n                output.append((\"Always check for breaking changes after updating packages.\", {}))\n            else:\n                output.append((f\"No fixes to be made in {name}.\", {}))\n                output.append(('', {}))\n        else:\n            not_supported_filename = data.get('filename', name)\n            output.append(\n                (f\"{not_supported_filename} updates not supported: Please update these dependencies using your package manager.\", \n                 {'start_line_decorator': ' -', 'indent': ' '}))\n            output.append(('', {}))\n\n        if not no_output:\n            print_service(output, out_type)\n\n        skip.extend(r_skip)\n        apply.extend(r_apply)\n        confirm.extend(r_confirm)\n\n    # The scan flow will handle the header and divider, because the scan flow can be called multiple times.\n    if not no_output and not scan_flow:\n        divider = f'{\"=\" * 78}' if out_type == 'text' else f'{\"=\" * (get_terminal_size().columns - 2)}'\n        format_text = {'start_line_decorator': '+', 'end_line_decorator': '+', 'indent': ''}\n        print_service([(divider, {})], out_type, format_text=format_text)\n\n    return skip + apply + confirm\n\n\ndef find_vulnerabilities_fixed(vulnerabilities: Dict, fixes) -> List[Vulnerability]:\n    fixed_specs = set(fix.previous_spec for fix in fixes)\n\n    if not fixed_specs:\n        return []\n\n    return [vulnerability for vulnerability in vulnerabilities if\n            str(vulnerability['analyzed_requirement'].specifier) in fixed_specs]\n\n\n@sync_safety_context\ndef review(*, report=None, params=None):\n    SafetyContext().command = 'review'\n    vulnerable = []\n    vulnerabilities = report.get('vulnerabilities', []) + report.get('ignored_vulnerabilities', [])\n    remediations = defaultdict(dict)\n\n    for key, pkg_rem in report.get('remediations', {}).items():\n        analyzed = set(pkg_rem['requirements'].keys())\n\n        for req in analyzed:\n            req_rem = pkg_rem['requirements'][req]\n            recommended = req_rem.get('recommended_version', None)\n            secure_v = req_rem.get('other_recommended_versions', [])\n\n            remediations[key][req] = {'vulnerabilities_found': req_rem.get('vulnerabilities_found', 0),\n                                      'version': req_rem.get('version'),\n                                      'requirement': SafetyRequirement(req_rem['requirement']['raw']),\n                                      'other_recommended_versions': secure_v,\n                                      'recommended_version': parse_version(recommended) if recommended else None,\n                                      # minor isn't supported in review\n                                      'more_info_url': req_rem.get('more_info_url')}\n\n    packages = report.get('scanned_packages', [])\n    pkgs = {}\n\n    for name, values in packages.items():\n        requirements = [SafetyRequirement(r['raw']) for r in values.get('requirements', [])]\n        values.update({'requirements': requirements})\n        pkgs[name] = Package(**values)\n\n    ctx = SafetyContext()\n    found_packages = list(pkgs.values())\n    ctx.packages = found_packages\n    ctx.review = report.get('report_meta', [])\n    ctx.key = ctx.review.get('api_key', False)\n    cvssv2 = None\n    cvssv3 = None\n\n    for vuln in vulnerabilities:\n        vuln['pkg'] = pkgs.get(vuln.get('package_name', None))\n        XVE_ID = vuln.get('CVE', None)  # Trying to get first the CVE ID\n\n        severity = vuln.get('severity', None)\n        if severity and severity.get('source', False):\n            cvssv2 = severity.get('cvssv2', None)\n            cvssv3 = severity.get('cvssv3', None)\n            # Trying to get the PVE ID if it exists, otherwise it will be the same CVE ID of above\n            XVE_ID = severity.get('source', False)\n            vuln['severity'] = Severity(source=XVE_ID, cvssv2=cvssv2, cvssv3=cvssv3)\n        else:\n            vuln['severity'] = None\n\n        ignored_expires = vuln.get('ignored_expires', None)\n\n        if ignored_expires:\n            vuln['ignored_expires'] = validate_expiration_date(ignored_expires)\n\n        vuln['CVE'] = CVE(name=XVE_ID, cvssv2=cvssv2, cvssv3=cvssv3) if XVE_ID else None\n        vuln['analyzed_requirement'] = SafetyRequirement(vuln['analyzed_requirement']['raw'])\n\n        vulnerable.append(Vulnerability(**vuln))\n\n    return vulnerable, remediations, found_packages\n\n\n@sync_safety_context\ndef get_licenses(*, session=None, db_mirror=False, cached=0, telemetry=True):\n    \n    if db_mirror:\n        mirrors = [db_mirror]\n    else:\n        mirrors = API_MIRRORS\n\n    db_name = \"licenses.json\"\n\n    for mirror in mirrors:\n        # mirror can either be a local path or a URL\n        if is_a_remote_mirror(mirror):\n            licenses = fetch_database_url(session, mirror, db_name=db_name, cached=cached,\n                                          telemetry=telemetry)\n        else:\n            licenses = fetch_database_file(mirror, db_name=db_name)\n        if licenses:\n            return licenses\n    raise DatabaseFetchError()\n\n\ndef add_local_notifications(packages: List[Package],\n                            ignore_unpinned_requirements: Optional[bool]) -> List[Dict[str, str]]:\n    announcements = []\n    unpinned_packages: [str] = [f\"{pkg.name}\" for pkg in packages if pkg.has_unpinned_req()]\n\n    if unpinned_packages and ignore_unpinned_requirements is not False:\n        found = len(unpinned_packages)\n        and_msg = ''\n\n        if found >= 2:\n            last = unpinned_packages.pop()\n            and_msg = f' and {last}'\n\n        pkgs: str = f\"{', '.join(unpinned_packages)}{and_msg} {'are' if found > 1 else 'is'}\"\n        doc_msg: str = get_specifier_range_info(style=False, pin_hint=True)\n\n        if ignore_unpinned_requirements is None:\n            msg = f'Warning: {pkgs} unpinned. Safety by default does not ' \\\n                  f'report on potential vulnerabilities in unpinned packages. {doc_msg}'\n        else:\n            msg = f'Warning: {pkgs} unpinned and potential vulnerabilities are ' \\\n                  f'being ignored given `ignore-unpinned-requirements` is True in your config. {doc_msg}'\n\n        announcements.append({'message': msg, 'type': 'warning', 'local': True})\n\n    announcements.extend(SafetyContext().local_announcements)\n\n    return announcements\n\n\ndef get_announcements(session, telemetry=True, with_telemetry=None):\n    LOG.info('Getting announcements')\n\n    announcements = []\n\n    url = f\"{DATA_API_BASE_URL}announcements/\"\n    method = 'post'\n    telemetry_data = with_telemetry if with_telemetry else build_telemetry_data(telemetry=telemetry)\n    data = asdict(telemetry_data)\n    request_kwargs = {'timeout': 3}\n    data_keyword = 'json'\n\n    source = os.environ.get('SAFETY_ANNOUNCEMENTS_URL', None)\n\n    if source:\n        LOG.debug(f'Getting the announcement from a different source: {source}')\n        url = source\n        method = 'get'\n        data = {\n            'telemetry': json.dumps(data)}\n        data_keyword = 'params'\n\n    request_kwargs[data_keyword] = data\n    request_kwargs['url'] = url\n\n    LOG.debug(f'Telemetry data sent: {data}')\n\n    try:\n        request_func = getattr(session, method)\n        r = request_func(**request_kwargs)\n        LOG.debug(r.text)\n    except Exception as e:\n        LOG.info('Unexpected but HANDLED Exception happened getting the announcements: %s', e)\n        return announcements\n\n    if r.status_code == 200:\n        try:\n            announcements = r.json()\n            if 'announcements' in announcements.keys():\n                announcements = announcements.get('announcements', [])\n            else:\n                LOG.info('There is not announcements key in the JSON response, is this a wrong structure?')\n                announcements = []\n\n        except json.JSONDecodeError as e:\n            LOG.info('Unexpected but HANDLED Exception happened decoding the announcement response: %s', e)\n\n    LOG.info('Announcements fetched')\n\n    return announcements\n\n\ndef get_packages(files=False, stdin=False):\n\n    if files:\n        return list(itertools.chain.from_iterable(read_requirements(f, resolve=True) for f in files))\n\n    if stdin:\n        return list(read_requirements(sys.stdin))\n\n    import pkg_resources\n\n    def allowed_version(pkg: str, version: str):\n        try:\n            parse_version(version)\n        except Exception:\n            SafetyContext.local_announcements.append(\n                {'message': f'Version {version} for {pkg} is invalid and is ignored by Safety. Please See PEP 440.',\n                 'type': 'warning', 'local': True})\n            return False\n\n        return True\n\n    w_set = pkg_resources.working_set\n\n    SafetyContext().scanned_full_path.extend(w_set.entry_keys.keys())\n\n    return [\n        Package(name=d.key, version=d.version,\n                requirements=[SafetyRequirement(f'{d.key}=={d.version}', found=d.location)],\n                found=d.location, insecure_versions=[],\n                secure_versions=[], latest_version=None, latest_version_without_known_vulnerabilities=None,\n                more_info_url=None) for d in\n        w_set\n        if d.key not in {\"python\", \"wsgiref\", \"argparse\"} and allowed_version(d.key, d.version)\n    ]\n\n\ndef read_vulnerabilities(fh):\n    try:\n        data = json.load(fh)\n    except json.JSONDecodeError as e:\n        raise MalformedDatabase(reason=e, fetched_from=fh.name)\n    except TypeError as e:\n        raise MalformedDatabase(reason=e, fetched_from=fh.name)\n\n    return data\n\n\ndef get_server_policies(session, policy_file, proxy_dictionary: Dict):\n    if session.api_key:\n        server_policies = fetch_policy(session)\n        server_audit_and_monitor = server_policies[\"audit_and_monitor\"]\n        server_safety_policy = server_policies[\"safety_policy\"]\n    else:\n        server_audit_and_monitor = False\n        server_safety_policy = \"\"\n\n    if server_safety_policy and policy_file:\n        click.secho(\n            \"Warning: both a local policy file '{policy_filename}' and a server sent policy are present. \"\n            \"Continuing with the local policy file.\".format(policy_filename=policy_file['filename']),\n            fg=\"yellow\",\n            file=sys.stderr\n        )\n    elif server_safety_policy:\n        with tempfile.NamedTemporaryFile(prefix='server-safety-policy-') as tmp:\n            tmp.write(server_safety_policy.encode('utf-8'))\n            tmp.seek(0)\n\n            policy_file = SafetyPolicyFile().convert(tmp.name, param=None, ctx=None)\n            LOG.info('Using server side policy file')\n\n    return policy_file, server_audit_and_monitor\n\n\ndef save_report(path: str, default_name: str, report: str):\n    if path:\n        save_at = path\n\n        if os.path.isdir(path):\n            save_at = os.path.join(path, default_name)\n\n        with open(save_at, 'w+') as report_file:\n            report_file.write(report)\n", "import_text": ["dataclasses.asdict", "errno", "itertools", "json", "logging", "os", "pathlib.Path", "random", "sys", "tempfile", "time", "collections.defaultdict", "datetime.datetime", "typing.Dict", "typing.Optional", "typing.List", "click", "requests", "requests.models.PreparedRequest", "packaging.specifiers.SpecifierSet", "packaging.utils.canonicalize_name", "packaging.version.parse", "packaging.version.Version", "pydantic.json.pydantic_encoder", "safety_schemas.models.Ecosystem", "safety_schemas.models.FileType"], "prompt": "\"\"\"\nDescription: This function is used to get the closest version from a list of versions based on a given version and a specification.\n\nArgs:\n    versions (list): A list of versions to search from.\n    version (str): The version to compare with.\n    spec (SpecifierSet): A set of specifications to filter the versions.\n\nReturns:\n    dict: A dictionary containing the upper and lower closest versions.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["packaging.version.parse"], "project_create_time": "2016-10-19T13:22:56+00:00", "project_update_time": "2024-04-16T09:19:45+00:00", "file_create_time": "2016-10-19T13:23:51Z", "file_update_time": "2024-01-20T01:03:46Z", "function_update_time": "2023-01-29T18:05:27Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["packaging.version.parse"], "test_function": [{"file_path": "/safety-3.1.0/safety-3.1.0/tests/test_safety.py", "class_name": "TestSafety", "function_name": "test_get_closest_ver", "code": "\n    def test_get_closest_ver(self):\n        versions = get_closest_ver(versions=['1.2', '1.3', '1.3.1', '1.3.2.dev5'], version='1.3.1',\n                                   spec=SpecifierSet('==1.3.1'))\n        EXPECTED = {'lower': parse('1.3'), 'upper': parse('1.3.2.dev5')}\n        self.assertEqual(versions, EXPECTED)\n\n        versions = get_closest_ver(versions=['1.2', '1.3', '1.3.1'], version='1.3.1', spec=SpecifierSet('==1.3.1'))\n        EXPECTED = {'lower': parse('1.3'), 'upper': None}\n        self.assertEqual(versions, EXPECTED)\n\n        versions = get_closest_ver(versions=['1.2', '1.3', '1.3.1'], version='1.2', spec=SpecifierSet('==1.2'))\n        EXPECTED = {'lower': None, 'upper': parse('1.3')}\n        self.assertEqual(versions, EXPECTED)\n\n        versions = get_closest_ver(versions=[], version='1.2', spec=SpecifierSet('==1.2'))\n        EXPECTED = {'lower': None, 'upper': None}\n        self.assertEqual(versions, EXPECTED)\n\n        versions = get_closest_ver(versions=['1.2', '1.3'], version=None, spec=None)\n        EXPECTED = {'lower': None, 'upper': None}\n        self.assertEqual(versions, EXPECTED)"}]}, {"git_group": "PennyLaneAI", "git_name": "pennylane", "version": "v0.35.1", "language": "Python", "project_name": "pennylane-v0.35.1.zip", "file_path": "/pennylane-v0.35.1/pennylane-0.35.1/pennylane/pauli/grouping/group_observables.py", "file_name": "group_observables.py", "focal_class": "PauliGroupingStrategy", "focal_name": "complement_adj_matrix_for_operator", "focal_parameter": [], "solution": "    def complement_adj_matrix_for_operator(self):\n\n        if self.binary_observables is None:\n            self.binary_observables = self.binary_repr()\n\n        n_qubits = int(np.shape(self.binary_observables)[1] / 2)\n\n        if self.grouping_type == \"qwc\":\n            adj = qwc_complement_adj_matrix(self.binary_observables)\n\n        elif self.grouping_type in frozenset([\"commuting\", \"anticommuting\"]):\n            symplectic_form = np.block(\n                [\n                    [np.zeros((n_qubits, n_qubits)), np.eye(n_qubits)],\n                    [np.eye(n_qubits), np.zeros((n_qubits, n_qubits))],\n                ]\n            )\n            mat_prod = (\n                self.binary_observables @ symplectic_form @ np.transpose(self.binary_observables)\n            )\n\n            if self.grouping_type == \"commuting\":\n                adj = mat_prod % 2\n\n            elif self.grouping_type == \"anticommuting\":\n                adj = (mat_prod + 1) % 2\n                np.fill_diagonal(adj, 0)\n\n        return adj", "function_signature": "def complement_adj_matrix_for_operator(self) :", "left_context": "# Copyright 2018-2021 Xanadu Quantum Technologies Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis module contains the high-level Pauli-word-partitioning functionality used in measurement optimization.\n\"\"\"\n\nfrom copy import copy\n\nimport numpy as np\nimport pennylane as qml\n\nfrom pennylane.ops import Prod, SProd\nfrom pennylane.pauli.utils import (\n    are_identical_pauli_words,\n    binary_to_pauli,\n    observables_to_binary_matrix,\n    qwc_complement_adj_matrix,\n)\nfrom pennylane.wires import Wires\n\nfrom .graph_colouring import largest_first, recursive_largest_first\n\n\nGROUPING_TYPES = frozenset([\"qwc\", \"commuting\", \"anticommuting\"])\nGRAPH_COLOURING_METHODS = {\"lf\": largest_first, \"rlf\": recursive_largest_first}\n\n\nclass PauliGroupingStrategy:  # pylint: disable=too-many-instance-attributes\n    \"\"\"\n    Class for partitioning a list of Pauli words according to some binary symmetric relation.\n\n    Partitions are defined by the binary symmetric relation of interest, e.g., all Pauli words in a\n    partition being mutually commuting. The partitioning is accomplished by formulating the list of\n    Pauli words as a graph where nodes represent Pauli words and edges between nodes denotes that\n    the two corresponding Pauli words satisfy the symmetric binary relation.\n\n    Obtaining the fewest number of partitions such that all Pauli terms within a partition mutually\n    satisfy the binary relation can then be accomplished by finding a partition of the graph nodes\n    such that each partition is a fully connected subgraph (a \"clique\"). The problem of finding the\n    optimal partitioning, i.e., the fewest number of cliques, is the minimum clique cover (MCC)\n    problem. The solution of MCC may be found by graph colouring on the complementary graph. Both\n    MCC and graph colouring are NP-Hard, so heuristic graph colouring algorithms are employed to\n    find approximate solutions in polynomial time.\n\n    Args:\n        observables (list[Observable]): a list of Pauli words to be partitioned according to a\n            grouping strategy\n        grouping_type (str): the binary relation used to define partitions of\n            the Pauli words, can be ``'qwc'`` (qubit-wise commuting), ``'commuting'``, or\n            ``'anticommuting'``.\n        graph_colourer (str): the heuristic algorithm to employ for graph\n            colouring, can be ``'lf'`` (Largest First) or ``'rlf'`` (Recursive\n            Largest First)\n\n    Raises:\n        ValueError: if arguments specified for ``grouping_type`` or\n            ``graph_colourer`` are not recognized\n    \"\"\"\n\n    def __init__(self, observables, grouping_type=\"qwc\", graph_colourer=\"rlf\"):\n        if grouping_type.lower() not in GROUPING_TYPES:\n            raise ValueError(\n                f\"Grouping type must be one of: {GROUPING_TYPES}, instead got {grouping_type}.\"\n            )\n\n        self.grouping_type = grouping_type.lower()\n\n        if graph_colourer.lower() not in GRAPH_COLOURING_METHODS:\n            raise ValueError(\n                f\"Graph colouring method must be one of: {list(GRAPH_COLOURING_METHODS)}, \"\n                f\"instead got {graph_colourer}.\"\n            )\n\n        self.graph_colourer = GRAPH_COLOURING_METHODS[graph_colourer.lower()]\n        self.observables = observables\n        self._wire_map = None\n        self._n_qubits = None\n        self.binary_observables = None\n        self.adj_matrix = None\n        self.grouped_paulis = None\n\n    def binary_repr(self, n_qubits=None, wire_map=None):\n        \"\"\"Converts the list of Pauli words to a binary matrix.\n\n        Args:\n            n_qubits (int): number of qubits to specify dimension of binary vector representation\n            wire_map (dict): dictionary containing all wire labels used in the Pauli word as keys,\n                and unique integer labels as their values\n\n        Returns:\n            array[int]: a column matrix of the Pauli words in binary vector representation\n        \"\"\"\n\n        if wire_map is None:\n            self._wire_map = {\n                wire: c\n                for c, wire in enumerate(\n                    Wires.all_wires([obs.wires for obs in self.observables]).tolist()\n                )\n            }\n\n        else:\n            self._wire_map = wire_map\n\n        self._n_qubits = n_qubits\n\n        return observables_to_binary_matrix(self.observables, n_qubits, self._wire_map)\n", "right_context": "\n    def colour_pauli_graph(self):\n        \"\"\"\n        Runs the graph colouring heuristic algorithm to obtain the partitioned Pauli words.\n\n        Returns:\n            list[list[Observable]]: a list of the obtained groupings. Each grouping is itself a\n            list of Pauli word ``Observable`` instances\n        \"\"\"\n\n        if self.adj_matrix is None:\n            self.adj_matrix = self.complement_adj_matrix_for_operator()\n\n        coloured_binary_paulis = self.graph_colourer(self.binary_observables, self.adj_matrix)\n\n        self.grouped_paulis = [\n            [binary_to_pauli(pauli_word, wire_map=self._wire_map) for pauli_word in grouping]\n            for grouping in coloured_binary_paulis.values()\n        ]\n\n        return self.grouped_paulis\n\n\ndef group_observables(observables, coefficients=None, grouping_type=\"qwc\", method=\"rlf\"):\n    \"\"\"Partitions a list of observables (Pauli operations and tensor products thereof) into\n    groupings according to a binary relation (qubit-wise commuting, fully-commuting, or\n    anticommuting).\n\n    Partitions are found by 1) mapping the list of observables to a graph where vertices represent\n    observables and edges encode the binary relation, then 2) solving minimum clique cover for the\n    graph using graph-colouring heuristic algorithms.\n\n    Args:\n        observables (list[Observable]): a list of Pauli word ``Observable`` instances (Pauli\n            operation instances and :class:`~.Tensor` instances thereof)\n        coefficients (tensor_like): A tensor or list of coefficients. If not specified,\n            output ``partitioned_coeffs`` is not returned.\n        grouping_type (str): The type of binary relation between Pauli words.\n            Can be ``'qwc'``, ``'commuting'``, or ``'anticommuting'``.\n        method (str): the graph coloring heuristic to use in solving minimum clique cover, which\n            can be ``'lf'`` (Largest First) or ``'rlf'`` (Recursive Largest First)\n\n    Returns:\n       tuple:\n\n           * list[list[Observable]]: A list of the obtained groupings. Each grouping\n             is itself a list of Pauli word ``Observable`` instances.\n           * list[tensor_like]: A list of coefficient groupings. Each coefficient\n             grouping is itself a tensor or list of the grouping's corresponding coefficients. This is only\n             returned if coefficients are specified.\n\n    Raises:\n        IndexError: if the input list of coefficients is not of the same length as the input list\n            of Pauli words\n\n    **Example**\n\n    >>> obs = [qml.Y(0), qml.X(0) @ qml.X(1), qml.Z(1)]\n    >>> coeffs = [1.43, 4.21, 0.97]\n    >>> obs_groupings, coeffs_groupings = group_observables(obs, coeffs, 'anticommuting', 'lf')\n    >>> obs_groupings\n    [[Z(1), X(0) @ X(1)],\n     [Y(0)]]\n    >>> coeffs_groupings\n    [[0.97, 4.21], [1.43]]\n    \"\"\"\n\n    if coefficients is not None:\n        if qml.math.shape(coefficients)[0] != len(observables):\n            raise IndexError(\n                \"The coefficients list must be the same length as the observables list.\"\n            )\n\n    pauli_grouping = PauliGroupingStrategy(\n        observables, grouping_type=grouping_type, graph_colourer=method\n    )\n\n    temp_opmath = not qml.operation.active_new_opmath() and any(\n        isinstance(o, (Prod, SProd)) for o in observables\n    )\n    if temp_opmath:\n        qml.operation.enable_new_opmath()\n\n    try:\n        partitioned_paulis = pauli_grouping.colour_pauli_graph()\n    finally:\n        if temp_opmath:\n            qml.operation.disable_new_opmath()\n\n    if coefficients is None:\n        return partitioned_paulis\n\n    partitioned_coeffs = [\n        qml.math.cast_like([0] * len(g), coefficients) for g in partitioned_paulis\n    ]\n\n    observables = copy(observables)\n    # we cannot delete elements from the coefficients tensor, so we\n    # use a proxy list memorising the indices for this logic\n    coeff_indices = list(range(qml.math.shape(coefficients)[0]))\n    for i, partition in enumerate(partitioned_paulis):  # pylint:disable=too-many-nested-blocks\n        indices = []\n        for pauli_word in partition:\n            # find index of this pauli word in remaining original observables,\n            for ind, observable in enumerate(observables):\n                if are_identical_pauli_words(pauli_word, observable):\n                    indices.append(coeff_indices[ind])\n                    observables.pop(ind)\n                    coeff_indices.pop(ind)\n                    break\n\n        # add a tensor of coefficients to the grouped coefficients\n        partitioned_coeffs[i] = qml.math.take(coefficients, indices, axis=0)\n\n    # make sure the output is of the same format as the input\n    # for these two frequent cases\n    if isinstance(coefficients, list):\n        partitioned_coeffs = [list(p) for p in partitioned_coeffs]\n\n    return partitioned_paulis, partitioned_coeffs\n", "import_text": ["copy.copy", "numpy", "pennylane", "pennylane.ops.Prod", "pennylane.ops.SProd", "pennylane.pauli.utils.are_identical_pauli_words", "pennylane.pauli.utils.binary_to_pauli", "pennylane.pauli.utils.observables_to_binary_matrix", "pennylane.pauli.utils.qwc_complement_adj_matrix", "pennylane.wires.Wires"], "prompt": "\"\"\"\nDescription: This function generates the complement adjacency matrix for an operator.\n\nArgs:\n    self (object): The instance of the class that the function is called on.\n\nAttributes:\n    binary_observables (numpy.ndarray): A 2D array representing the binary observables.\n    grouping_type (str): A string indicating the type of grouping. It can be \"qwc\", \"commuting\", or \"anticommuting\".\n\nReturns:\n    numpy.ndarray: The complement adjacency matrix for the operator.\n\nRaises:\n    ValueError: If the grouping type is not one of the expected values.\n\nNotes:\n    This function uses the numpy.eye, numpy.transpose, and numpy.shape functions.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"Constructs the adjacency matrix for the complement of the Pauli graph.\n\n        The adjacency matrix for an undirected graph of N vertices is an N by N symmetric binary\n        matrix, where matrix elements of 1 denote an edge, and matrix elements of 0 denote no edge.\n\n        Returns:\n            array[int]: the square and symmetric adjacency matrix\n        \"\"\"", "function_dependencies": ["numpy.shape", "pennylane.pauli.utils.qwc_complement_adj_matrix", "numpy.block", "numpy.zeros", "numpy.eye", "numpy.transpose", "numpy.fill_diagonal"], "project_create_time": "2018-04-17T16:45:42+00:00", "project_update_time": "2024-04-18T01:03:09+00:00", "file_create_time": "2022-11-01T17:24:29Z", "file_update_time": "2024-02-22T13:41:28Z", "function_update_time": "2022-11-01T17:24:29Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["numpy.eye", "numpy.transpose", "numpy.shape"], "test_function": [{"file_path": "/pennylane-v0.35.1/pennylane-0.35.1/tests/pauli/grouping/test_pauli_group_observables.py", "class_name": "TestPauliGroupingStrategy", "function_name": "test_construct_qwc_complement_adj_matrix_for_operators", "code": "    def test_construct_qwc_complement_adj_matrix_for_operators(self):\n\n        observables = [PauliY(0), PauliZ(0) @ PauliZ(1), PauliY(0) @ PauliX(1)]\n        qwc_complement_adjacency_matrix = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n\n        grouping_instance = PauliGroupingStrategy(observables, \"qwc\")\n        assert (\n            grouping_instance.complement_adj_matrix_for_operator()\n            == qwc_complement_adjacency_matrix\n        ).all()"}, {"file_path": "/pennylane-v0.35.1/pennylane-0.35.1/tests/pauli/grouping/test_pauli_group_observables.py", "class_name": "TestPauliGroupingStrategy", "function_name": "test_construct_commuting_complement_adj_matrix_for_operators", "code": "    def test_construct_commuting_complement_adj_matrix_for_operators(self):\n\n        observables = [PauliY(0), PauliZ(0) @ PauliZ(1), PauliY(0) @ PauliX(1)]\n        commuting_complement_adjacency_matrix = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]])\n\n        grouping_instance = PauliGroupingStrategy(observables, \"commuting\")\n        assert (\n            grouping_instance.complement_adj_matrix_for_operator()\n            == commuting_complement_adjacency_matrix\n        ).all()"}, {"file_path": "/pennylane-v0.35.1/pennylane-0.35.1/tests/pauli/grouping/test_pauli_group_observables.py", "class_name": "TestPauliGroupingStrategy", "function_name": "test_construct_anticommuting_complement_adj_matrix_for_operators", "code": "    def test_construct_anticommuting_complement_adj_matrix_for_operators(self):\n\n        observables = [PauliY(0), PauliZ(0) @ PauliZ(1), PauliY(0) @ PauliX(1)]\n        anticommuting_complement_adjacency_matrix = np.array([[0, 0, 1], [0, 0, 1], [1, 1, 0]])\n\n        grouping_instance = PauliGroupingStrategy(observables, \"anticommuting\")\n        assert (\n            grouping_instance.complement_adj_matrix_for_operator()\n            == anticommuting_complement_adjacency_matrix\n        ).all()"}]}, {"git_group": "getsentry", "git_name": "zeus", "version": "master", "language": "Python", "project_name": "zeus-master.zip", "file_path": "/zeus-master/zeus-master/zeus/artifacts/gotest.py", "file_name": "gotest.py", "focal_class": "GoTestHandler", "focal_name": "get_tests", "focal_parameter": ["fp"], "solution": "\n    def get_tests(self, fp):\n        job = self.job\n\n        results = []\n\n        actionToResult = {\"pass\": Result.passed, \"fail\": Result.failed}\n        last_output = {}\n        for lineno, line in enumerate(fp.readlines()):\n            # Format at https://tip.golang.org/cmd/test2json/\n            try:\n                event = json.loads(line)\n            except Exception:\n                current_app.logger.exception(\n                    \"Failed to parse JSON on line %d\", lineno + 1\n                )\n                return []\n\n            if \"Test\" not in event:\n                continue\n\n            key = (event.get(\"Package\"), event[\"Test\"])\n            output = event.get(\"Output\") or last_output.get(key)\n            if output:\n                last_output[key] = output\n\n            result = actionToResult.get(event[\"Action\"])\n            if result is None:\n                continue\n\n            results.append(\n                TestResult(\n                    job=job,\n                    name=event[\"Test\"],\n                    message=output if result == Result.failed else None,\n                    package=event.get(\"Package\"),\n                    result=result,\n                    duration=int(event[\"Elapsed\"] * 1000),\n                )\n            )\n\n        return results", "function_signature": "def get_tests(self, fp) :", "left_context": "from flask import current_app\nimport json\n\nfrom zeus.constants import Result\nfrom zeus.utils.testresult import TestResult, TestResultManager\n\nfrom .base import ArtifactHandler\n\n\nclass GoTestHandler(ArtifactHandler):\n    supported_types = frozenset([\"application/x-gotest+json\"])\n\n    def process(self, fp):\n        test_list = self.get_tests(fp)\n\n        manager = TestResultManager(self.job)\n        manager.clear()\n        manager.save(test_list)\n\n        return test_list\n", "right_context": "", "import_text": ["flask.current_app", "json", "zeus.constants.Result", "zeus.utils.testresult.TestResult", "zeus.utils.testresult.TestResultManager"], "prompt": "\"\"\"\nDescription: This function reads a file-like object and parses it as JSON lines, returning a list of TestResult objects.\n\nArgs:\n    fp (file-like object): The file-like object to read and parse.\n\nReturns:\n    list: A list of TestResult objects. Each TestResult object represents the result of a test, including its job, name, message, package, result, and duration.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["json.loads", "flask.current_app.logger.exception", "json.loads.get", "zeus.utils.testresult.TestResult"], "project_create_time": "2017-07-03T16:39:35+00:00", "project_update_time": "2024-03-10T04:52:12+00:00", "file_create_time": "2018-01-15T21:54:49Z", "file_update_time": "2018-04-23T16:19:23Z", "function_update_time": "2018-01-15T21:54:49Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["flask.current_app.logger.exception"], "test_function": []}, {"git_group": "mie-lab", "git_name": "trackintel", "version": "v1.3.1", "language": "Python", "project_name": "trackintel-v1.3.1.zip", "file_path": "/trackintel-v1.3.1/trackintel-1.3.1/trackintel/analysis/location_identification.py", "file_name": "location_identification.py", "focal_class": null, "focal_name": "osna_method", "focal_parameter": ["staypoints"], "solution": "def osna_method(staypoints):\n    sp_in = staypoints  # no copy --> used to join back later.\n    sp = sp_in.copy()\n    sp[\"duration\"] = sp[\"finished_at\"] - sp[\"started_at\"]\n    sp[\"mean_time\"] = sp[\"started_at\"] + sp[\"duration\"] / 2\n\n    sp[\"label\"] = sp[\"mean_time\"].apply(_osna_label_timeframes)\n    sp.loc[sp[\"label\"] == \"rest\", \"duration\"] *= 0.739  # weight given in paper\n    sp.loc[sp[\"label\"] == \"leisure\", \"duration\"] *= 0.358  # weight given in paper\n\n    groups_map = {\n        \"rest\": \"home\",\n        \"leisure\": \"home\",\n        \"work\": \"work\",\n    }  # weekends aren't included in analysis!\n    # groupby user, location and label.\n    groups = [\"user_id\", \"location_id\", sp[\"label\"].map(groups_map)]\n\n    sp_agg = sp.groupby(groups)[\"duration\"].sum()\n    if sp_agg.empty:\n        warnings.warn(\"Got empty table in the osna method, check if the dates lie in weekends.\")\n        sp_in[\"purpose\"] = pd.NA\n        return sp_in\n\n    # create a pivot table -> labels \"home\" and \"work\" as columns. (\"user_id\", \"location_id\" still in index.)\n    sp_pivot = sp_agg.unstack()\n    # get index of maximum for columns \"work\" and \"home\"\n    # looks over locations to find maximum for columns\n    # use fillna such that idxmax raises no error on columns with only NaT\n    sp_idxmax = sp_pivot.fillna(pd.Timedelta(0)).groupby([\"user_id\"]).idxmax()\n\n    # preset dtype to avoid upcast (float64 -> object) in pandas (and the corresponding error)\n    sp_pivot[\"purpose\"] = None\n    # assign empty index to idx_work/idx_home to have a default behavior for the intersection later\n    idx_work = idx_home = pd.Index([])\n    if \"work\" in sp_pivot.columns:\n        # first get all index of max entries (of work) that are not NaT\n        idx_work = sp_pivot.loc[sp_idxmax[\"work\"], \"work\"].dropna().index\n        # set them to the corresponding purpose (work)\n        sp_pivot.loc[idx_work, \"purpose\"] = \"work\"\n\n    if \"home\" in sp_pivot.columns:\n        # get all index of max entries (of home) that are not NaT\n        idx_home = sp_pivot.loc[sp_idxmax[\"home\"], \"home\"].dropna().index\n        # set them to the corresponding purpose (home overrides work!)\n        sp_pivot.loc[idx_home, \"purpose\"] = \"home\"\n\n    # if override happened recalculate work\n    overlap = idx_home.intersection(idx_work)\n    if not overlap.empty:\n        # remove overlap -> must take another location as everything is NaT on maximum\n        sp_pivot.loc[overlap, \"work\"] = pd.NaT\n        sp_idxmax = sp_pivot[\"work\"].fillna(pd.Timedelta(0)).groupby([\"user_id\"]).idxmax()\n        idx_work = sp_pivot.loc[sp_idxmax, \"work\"].dropna().index\n        sp_pivot.loc[idx_work, \"purpose\"] = \"work\"\n\n    # now join it back together\n    sel = sp_in.columns != \"purpose\"  # no overlap with older \"purpose\"\n    return pd.merge(\n        sp_in.loc[:, sel],\n        sp_pivot[\"purpose\"],\n        how=\"left\",\n        left_on=[\"user_id\", \"location_id\"],\n        right_index=True,\n    )", "function_signature": "def osna_method(staypoints) :", "left_context": "import warnings\nimport numpy as np\nimport pandas as pd\n\n\ndef location_identifier(staypoints, method=\"FREQ\", pre_filter=True, **pre_filter_kwargs):\n    \"\"\"Assign \"home\" and \"work\" activity label for each user with different methods.\n\n    Parameters\n    ----------\n    staypoints : Staypoints\n        Staypoints with column \"location_id\".\n\n    method : {'FREQ', 'OSNA'}, default \"FREQ\"\n        'FREQ': Generate an activity label per user by assigning the most visited location the label \"home\"\n        and the second most visited location the label \"work\". The remaining locations get no label.\n        'OSNA': Use weekdays data divided in three time frames [\"rest\", \"work\", \"leisure\"]. Finds most popular\n        home location for timeframes \"rest\" and \"leisure\" and most popular \"work\" location for \"work\" timeframe.\n\n    pre_filter : bool, default True\n        Prefiltering the staypoints to exclude locations with not enough data.\n        The filter function can also be accessed via `pre_filter_locations`.\n\n    pre_filter_kwargs : dict\n        Kwargs to hand to `pre_filter_locations` if used. See function for more informations.\n\n    Returns\n    -------\n    sp: Staypoints\n        With additional column `purpose` assigning one of three activity labels {'home', 'work', None}.\n\n    Note\n    ----\n    The methods are adapted from [1]. The original algorithms count the distinct hours at a\n    location as the home location is derived from geo-tagged tweets. We directly sum the time\n    spent at a location as our data model includes that.\n\n    References\n    ----------\n    [1] Chen, Qingqing, and Ate Poorthuis. 2021.\n    \u2018Identifying Home Locations in Human Mobility Data: An Open-Source R Package for Comparison and Reproducibility\u2019.\n    International Journal of Geographical Information Science 0 (0): 1\u201324.\n    https://doi.org/10.1080/13658816.2021.1887489.\n\n    Examples\n    --------\n    >>> from ti.analysis import location_identifier\n    >>> location_identifier(staypoints, pre_filter=True, method=\"FREQ\")\n    \"\"\"\n    sp = staypoints.copy()\n    if \"location_id\" not in sp.columns:\n        raise KeyError(\n            (\n                \"To derive activity labels the Staypoints must have a column \"\n                f\"named 'location_id' but it has [{', '.join(sp.columns)}]\"\n            )\n        )\n    if pre_filter:\n        f = pre_filter_locations(sp, **pre_filter_kwargs)\n    else:\n        f = pd.Series(np.full(len(sp.index), True), index=sp.index)\n\n    if method == \"FREQ\":\n        method_val = freq_method(sp[f], \"home\", \"work\")\n    elif method == \"OSNA\":\n        method_val = osna_method(sp[f])\n    else:\n        raise ValueError(f\"Method {method} does not exist.\")\n\n    sp.loc[f, \"purpose\"] = method_val[\"purpose\"]\n    return sp\n\n\ndef pre_filter_locations(\n    staypoints,\n    agg_level=\"user\",\n    thresh_sp=10,\n    thresh_loc=10,\n    thresh_sp_at_loc=10,\n    thresh_loc_time=\"1h\",\n    thresh_loc_period=\"5h\",\n):\n    \"\"\"Filter locations and user out that have not enough data to do a proper analysis.\n\n    To disable a specific filter parameter set it to zero.\n\n    Parameters\n    ----------\n    staypoints : Staypoints\n        Staypoints with the column \"location_id\".\n\n    agg_level: {\"user\", \"dataset\"}, default \"user\"\n        The level of aggregation when filtering locations. 'user' : locations are filtered per-user;\n        'dataset' : locations are filtered over the whole dataset.\n\n    thresh_sp : int, default 10\n        Minimum staypoints a user must have to be included.\n\n    thresh_loc : int, default 10\n        Minimum locations a user must have to be included.\n\n    thresh_sp_at_loc : int, default 10\n        Minimum number of staypoints at a location must have to be included.\n\n    thresh_loc_time : str or pd.Timedelta, default \"1h\"\n        Minimum sum of durations that was spent at location to be included.\n        If str must be parsable by pd.to_timedelta.\n\n    thresh_loc_period : str or pd.Timedelta, default \"5h\"\n        Minimum timespan of first to last visit at a location to be included.\n        If str must be parsable by pd.to_timedelta.\n\n    Returns\n    -------\n    total_filter: pd.Series\n        Boolean series containing the filter as a mask.\n\n    Examples\n    --------\n    >>> from ti.analysis import pre_filter_locations\n    >>> mask = pre_filter_locations(staypoints)\n    >>> staypoints = staypoints[mask]\n    \"\"\"\n    sp = staypoints.copy()\n    if isinstance(thresh_loc_time, str):\n        thresh_loc_time = pd.to_timedelta(thresh_loc_time)\n    if isinstance(thresh_loc_period, str):\n        thresh_loc_period = pd.to_timedelta(thresh_loc_period)\n\n    # filtering users\n    user = sp.groupby(\"user_id\").nunique()\n    user_sp = user[\"started_at\"] >= thresh_sp  # every staypoint should have a started_at -> count\n    user_loc = user[\"location_id\"] >= thresh_loc\n    user_filter_agg = user_sp & user_loc\n    user_filter_agg.rename(\"user_filter\", inplace=True)  # rename for merging\n    user_filter = pd.merge(sp[\"user_id\"], user_filter_agg, left_on=\"user_id\", right_index=True)[\"user_filter\"]\n\n    # filtering locations\n    sp[\"duration\"] = sp[\"finished_at\"] - sp[\"started_at\"]\n    if agg_level == \"user\":\n        groupby_loc = [\"user_id\", \"location_id\"]\n    elif agg_level == \"dataset\":\n        groupby_loc = [\"location_id\"]\n    else:\n        raise ValueError(f\"Unknown agg_level '{agg_level}' use instead {{'user', 'dataset'}}.\")\n    loc = sp.groupby(groupby_loc).agg({\"started_at\": [\"min\", \"count\"], \"finished_at\": \"max\", \"duration\": \"sum\"})\n    loc.columns = loc.columns.droplevel(0)  # remove possible multi-index\n    loc.rename(columns={\"min\": \"started_at\", \"max\": \"finished_at\", \"sum\": \"duration\"}, inplace=True)\n    # period for maximal time span first visit - last visit.\n    # duration for effective time spent at location summed up.\n    loc[\"period\"] = loc[\"finished_at\"] - loc[\"started_at\"]\n    loc_sp = loc[\"count\"] >= thresh_sp_at_loc\n    loc_time = loc[\"duration\"] >= thresh_loc_time\n    loc_period = loc[\"period\"] >= thresh_loc_period\n    loc_filter_agg = loc_sp & loc_time & loc_period\n    loc_filter_agg.rename(\"loc_filter\", inplace=True)  # rename for merging\n    loc_filter = pd.merge(sp[groupby_loc], loc_filter_agg, how=\"left\", left_on=groupby_loc, right_index=True)\n    loc_filter = loc_filter[\"loc_filter\"]\n\n    total_filter = user_filter & loc_filter\n\n    return total_filter\n\n\ndef freq_method(staypoints, *labels):\n    \"\"\"Generate an activity label per user.\n\n    Assigning the most visited location the label \"home\" and the second most visited location the label \"work\".\n    The remaining locations get no label.\n\n    Labels can also be given as arguments.\n\n    Parameters\n    ----------\n    staypoints : Staypoints\n        Staypoints with the column \"location_id\".\n\n    labels : collection of str, default (\"home\", \"work\")\n        Labels in decreasing time of activity.\n\n    Returns\n    -------\n    sp: Staypoints\n        The input staypoints with additional column \"purpose\".\n\n    Examples\n    --------\n    >>> from ti.analysis import freq_method\n    >>> staypoints = freq_method(staypoints, \"home\", \"work\")\n    \"\"\"\n    sp = staypoints.copy()\n    if not labels:\n        labels = (\"home\", \"work\")\n    for name, group in sp.groupby(\"user_id\"):\n        if \"duration\" not in group.columns:\n            group[\"duration\"] = group[\"finished_at\"] - group[\"started_at\"]\n        # pandas keeps inner order of groups\n        sp.loc[sp[\"user_id\"] == name, \"purpose\"] = _freq_transform(group, *labels)\n    if \"purpose\" not in sp.columns:  # if empty sp\n        sp[\"purpose\"] = None\n    return sp\n\n\ndef _freq_transform(group, *labels):\n    \"\"\"Transform function that assigns the longest (duration) visited locations the labels in order.\n\n    Parameters\n    ----------\n    group : pd.DataFrame\n        Should have columns \"location_id\" and \"duration\".\n\n    Returns\n    -------\n    pd.Series\n        dtype : object\n    \"\"\"\n    group_agg = group.groupby(\"location_id\").agg({\"duration\": \"sum\"})\n    group_agg[\"purpose\"] = _freq_assign(group_agg[\"duration\"], *labels)\n    group_merge = pd.merge(\n        group[\"location_id\"], group_agg[\"purpose\"], how=\"left\", left_on=\"location_id\", right_index=True\n    )\n    return group_merge[\"purpose\"]\n\n\ndef _freq_assign(duration, *labels):\n    \"\"\"Assign k labels to k longest durations the rest is `None`.\n\n    Parameters\n    ----------\n    duration : pd.Series\n\n    Returns\n    -------\n    np.array\n        dtype : object\n    \"\"\"\n    kth = (-duration).argsort()[: len(labels)]  # if inefficient use partial sort.\n    label_array = np.full(len(duration), fill_value=None)\n    labels = labels[: len(kth)]  # if provided with more labels than entries.\n    label_array[kth] = labels\n    return label_array\n\n", "right_context": "\n\ndef _osna_label_timeframes(dt, weekend=[5, 6], start_rest=2, start_work=8, start_leisure=19):\n    \"\"\"Help function to assign \"weekend\", \"rest\", \"work\", \"leisure\".\"\"\"\n    if dt.weekday() in weekend:\n        return \"weekend\"\n    if start_rest <= dt.hour < start_work:\n        return \"rest\"\n    if start_work <= dt.hour < start_leisure:\n        return \"work\"\n    return \"leisure\"\n", "import_text": ["warnings", "numpy", "pandas"], "prompt": "\"\"\"\nDescription: This function is used to determine the purpose of a stay point in a given location for each user.\n\nArgs:\n    staypoints (pandas.DataFrame): A DataFrame containing stay points data. It should have columns 'user_id', 'location_id', 'started_at', 'finished_at'.\n\nReturns:\n    pandas.DataFrame: A DataFrame with the same columns as the input DataFrame plus an additional 'purpose' column indicating the purpose of the stay point.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Find \"home\" location for timeframes \"rest\" and \"leisure\" and \"work\" location for \"work\" timeframe.\n\n    Use weekdays data divided in three time frames [\"rest\", \"work\", \"leisure\"] to generate location labels.\n    \"rest\" + \"leisure\" locations are weighted together. The location with the longest duration is assigned \"home\" label.\n    The longest \"work\" location is assigned \"work\" label.\n\n    Parameters\n    ----------\n    staypoints : Staypoints\n        Staypoints with the column \"location_id\".\n\n    Returns\n    -------\n    Staypoints\n        The input staypoints with additional column \"purpose\".\n\n    Note\n    ----\n    The method is adapted from [1].\n    When \"home\" and \"work\" label overlap, the method selects the \"work\" location by the 2nd highest score.\n    The original algorithm count the distinct hours at a location as the home location is derived from\n    geo-tagged tweets. We directly sum the time spent at a location.\n\n    References\n    ----------\n    [1] Efstathiades, Hariton, Demetris Antoniades, George Pallis, and Marios Dikaiakos. 2015.\n    \u2018Identification of Key Locations Based on Online Social Network Activity\u2019.\n    In https://doi.org/10.1145/2808797.2808877.\n\n    Examples\n    --------\n    >>> from ti.analysis import osna_method\n    >>> staypoints = osna_method(staypoints)\n    \"\"\"", "function_dependencies": ["warnings.warn", "pandas.Timedelta", "pandas.Index", "pandas.Index.intersection", "pandas.merge"], "project_create_time": "2019-01-21T17:13:18+00:00", "project_update_time": "2024-04-15T11:38:01+00:00", "file_create_time": "2021-05-19T20:40:04Z", "file_update_time": "2023-11-01T22:30:00Z", "function_update_time": "2021-09-14T13:36:40Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["pandas.Index"], "test_function": [{"file_path": "/trackintel-v1.3.1/trackintel-1.3.1/tests/analysis/test_location_identification.py", "class_name": "TestOsna_Method", "function_name": "test_overlap", "code": "    def test_overlap(self, example_osna):\n        # add 2 work times to location 0,\n        # location 0 would be the most stayed location for both home and work\n        t = pd.Timestamp(\"2021-05-19 12:00:00\", tz=\"utc\")\n        h = pd.to_timedelta(\"1h\")\n        p = Point(0.0, 0.0)\n        list_dict = [\n            {\"user_id\": 0, \"location_id\": 0, \"started_at\": t, \"finished_at\": t + h, \"geom\": p},\n            {\"user_id\": 0, \"location_id\": 0, \"started_at\": t, \"finished_at\": t + h, \"geom\": p},\n        ]\n        sp = gpd.GeoDataFrame(data=list_dict, geometry=\"geom\", crs=\"EPSG:4326\")\n        sp.index.name = \"id\"\n        sp = pd.concat((example_osna, sp))\n\n        result = osna_method(sp).iloc[:-2]\n        example_osna[\"purpose\"] = None\n        example_osna.loc[example_osna[\"location_id\"] == 0, \"purpose\"] = \"home\"\n        example_osna.loc[example_osna[\"location_id\"] == 1, \"purpose\"] = \"work\"\n        assert_geodataframe_equal(result, example_osna)"}, {"file_path": "/trackintel-v1.3.1/trackintel-1.3.1/tests/analysis/test_location_identification.py", "class_name": "TestOsna_Method", "function_name": "test_only_weekends", "code": "    def test_only_weekends(self, example_osna):\n        weekend = \"2021-05-22\"  # a saturday\n\n        def _insert_weekend(dt, day=weekend):\n            \"\"\"Take datetime and return new datetime with same time but new day.\"\"\"\n            time = dt.time().strftime(\"%H:%M:%S\")\n            new_dt = \" \".join((day, time))\n            return pd.Timestamp(new_dt, tz=dt.tz)\n\n        # replace all days with weekends --> no label in data.\n        example_osna[\"started_at\"] = example_osna[\"started_at\"].apply(_insert_weekend)\n        example_osna[\"finished_at\"] = example_osna[\"finished_at\"].apply(_insert_weekend)\n\n        # check if warning is raised if all points are excluded (weekend)\n        with pytest.warns(UserWarning):\n            result = osna_method(example_osna)\n\n        # activity_label column is all pd.NA\n        example_osna[\"purpose\"] = pd.NA\n        assert_geodataframe_equal(result, example_osna)"}, {"file_path": "/trackintel-v1.3.1/trackintel-1.3.1/tests/analysis/test_location_identification.py", "class_name": "TestOsna_Method", "function_name": "test_leisure_weighting", "code": "    def test_leisure_weighting(self):\n        weight_rest = 0.739\n        weight_leis = 0.358\n        ratio = weight_rest / weight_leis\n        ratio += 0.01  # tip the scale in favour of leisure\n        weekday = \"2021-05-19 \"\n        t_rest = pd.Timestamp(weekday + \"07:00:00\", tz=\"utc\")\n        t_work = pd.Timestamp(weekday + \"18:00:00\", tz=\"utc\")\n        t_leis = pd.Timestamp(weekday + \"01:00:00\", tz=\"utc\")\n        h = pd.Timedelta(\"1h\")\n\n        list_dict = [\n            {\"user_id\": 0, \"location_id\": 0, \"started_at\": t_rest, \"finished_at\": t_rest + h},\n            {\"user_id\": 0, \"location_id\": 1, \"started_at\": t_leis, \"finished_at\": t_leis + ratio * h},\n            {\"user_id\": 0, \"location_id\": 2, \"started_at\": t_work, \"finished_at\": t_work + h},\n        ]\n        p = Point(8.0, 47.0)  # geometry isn't used\n        for d in list_dict:\n            d[\"geom\"] = p\n        sp = gpd.GeoDataFrame(data=list_dict, geometry=\"geom\", crs=\"EPSG:4326\")\n        sp.index.name = \"id\"\n        result = osna_method(sp)\n        sp[\"purpose\"] = None\n        sp.loc[sp[\"location_id\"] == 1, \"purpose\"] = \"home\"\n        sp.loc[sp[\"location_id\"] == 2, \"purpose\"] = \"work\"\n        assert_geodataframe_equal(sp, result)"}, {"file_path": "/trackintel-v1.3.1/trackintel-1.3.1/tests/analysis/test_location_identification.py", "class_name": "TestOsna_Method", "function_name": "test_multiple_users_with_only_one_location", "code": "    def test_multiple_users_with_only_one_location(self):\n        t_leis = pd.Timestamp(\"2021-07-14 01:00:00\", tz=\"utc\")\n        t_work = pd.Timestamp(\"2021-07-14 18:00:00\", tz=\"utc\")\n        h = pd.Timedelta(\"1h\")\n        list_dict = [\n            {\"user_id\": 0, \"location_id\": 0, \"started_at\": t_leis, \"finished_at\": t_leis + h},\n            {\"user_id\": 0, \"location_id\": 1, \"started_at\": t_work, \"finished_at\": t_work + h},\n            {\"user_id\": 1, \"location_id\": 0, \"started_at\": t_leis, \"finished_at\": t_leis + h},\n            {\"user_id\": 2, \"location_id\": 0, \"started_at\": t_work, \"finished_at\": t_work + h},\n        ]\n        sp = pd.DataFrame(list_dict)\n        sp.index.name = \"id\"\n        result = osna_method(sp)\n        sp[\"purpose\"] = [\"home\", \"work\", \"home\", \"work\"]\n        assert_frame_equal(sp, result)"}]}, {"git_group": "datamllab", "git_name": "rlcard", "version": "1.0.7", "language": "Python", "project_name": "rlcard-1.0.7.zip", "file_path": "/rlcard-1.0.7/rlcard-1.0.7/rlcard/games/nolimitholdem/game.py", "file_name": "game.py", "focal_class": "NolimitholdemGame", "focal_name": "step", "focal_parameter": ["action"], "solution": "    def step(self, action):\n\n        if action not in self.get_legal_actions():\n            print(action, self.get_legal_actions())\n            print(self.get_state(self.game_pointer))\n            raise Exception('Action not allowed')\n\n        if self.allow_step_back:\n            # First snapshot the current state\n            r = deepcopy(self.round)\n            b = self.game_pointer\n            r_c = self.round_counter\n            d = deepcopy(self.dealer)\n            p = deepcopy(self.public_cards)\n            ps = deepcopy(self.players)\n            self.history.append((r, b, r_c, d, p, ps))\n\n        # Then we proceed to the next round\n        self.game_pointer = self.round.proceed_round(self.players, action)\n\n        players_in_bypass = [1 if player.status in (PlayerStatus.FOLDED, PlayerStatus.ALLIN) else 0 for player in self.players]\n        if self.num_players - sum(players_in_bypass) == 1:\n            last_player = players_in_bypass.index(0)\n            if self.round.raised[last_player] >= max(self.round.raised):\n                # If the last player has put enough chips, he is also bypassed\n                players_in_bypass[last_player] = 1\n\n        # If a round is over, we deal more public cards\n        if self.round.is_over():\n            # Game pointer goes to the first player not in bypass after the dealer, if there is one\n            self.game_pointer = (self.dealer_id + 1) % self.num_players\n            if sum(players_in_bypass) < self.num_players:\n                while players_in_bypass[self.game_pointer]:\n                    self.game_pointer = (self.game_pointer + 1) % self.num_players\n\n            # For the first round, we deal 3 cards\n            if self.round_counter == 0:\n                self.stage = Stage.FLOP\n                self.public_cards.append(self.dealer.deal_card())\n                self.public_cards.append(self.dealer.deal_card())\n                self.public_cards.append(self.dealer.deal_card())\n                if len(self.players) == np.sum(players_in_bypass):\n                    self.round_counter += 1\n            # For the following rounds, we deal only 1 card\n            if self.round_counter == 1:\n                self.stage = Stage.TURN\n                self.public_cards.append(self.dealer.deal_card())\n                if len(self.players) == np.sum(players_in_bypass):\n                    self.round_counter += 1\n            if self.round_counter == 2:\n                self.stage = Stage.RIVER\n                self.public_cards.append(self.dealer.deal_card())\n                if len(self.players) == np.sum(players_in_bypass):\n                    self.round_counter += 1\n\n            self.round_counter += 1\n            self.round.start_new_round(self.game_pointer)\n\n        state = self.get_state(self.game_pointer)\n\n        return state, self.game_pointer", "function_signature": "def step(self, action) :", "left_context": "from enum import Enum\n\nimport numpy as np\nfrom copy import deepcopy\nfrom rlcard.games.limitholdem import Game\nfrom rlcard.games.limitholdem import PlayerStatus\n\nfrom rlcard.games.nolimitholdem import Dealer\nfrom rlcard.games.nolimitholdem import Player\nfrom rlcard.games.nolimitholdem import Judger\nfrom rlcard.games.nolimitholdem import Round, Action\n\n\nclass Stage(Enum):\n    PREFLOP = 0\n    FLOP = 1\n    TURN = 2\n    RIVER = 3\n    END_HIDDEN = 4\n    SHOWDOWN = 5\n\n\nclass NolimitholdemGame(Game):\n    def __init__(self, allow_step_back=False, num_players=2):\n        \"\"\"Initialize the class no limit holdem Game\"\"\"\n        super().__init__(allow_step_back, num_players)\n\n        self.np_random = np.random.RandomState()\n\n        # small blind and big blind\n        self.small_blind = 1\n        self.big_blind = 2 * self.small_blind\n\n        # config players\n        self.init_chips = [100] * num_players\n\n        # If None, the dealer will be randomly chosen\n        self.dealer_id = None\n\n    def configure(self, game_config):\n        \"\"\"\n        Specify some game specific parameters, such as number of players, initial chips, and dealer id.\n        If dealer_id is None, he will be randomly chosen\n        \"\"\"\n        self.num_players = game_config['game_num_players']\n        # must have num_players length\n        self.init_chips = [game_config['chips_for_each']] * game_config[\"game_num_players\"]\n        self.dealer_id = game_config['dealer_id']\n\n    def init_game(self):\n        \"\"\"\n        Initialize the game of not limit holdem\n\n        This version supports two-player no limit texas holdem\n\n        Returns:\n            (tuple): Tuple containing:\n\n                (dict): The first state of the game\n                (int): Current player's id\n        \"\"\"\n        if self.dealer_id is None:\n            self.dealer_id = self.np_random.randint(0, self.num_players)\n\n        # Initialize a dealer that can deal cards\n        self.dealer = Dealer(self.np_random)\n\n        # Initialize players to play the game\n        self.players = [Player(i, self.init_chips[i], self.np_random) for i in range(self.num_players)]\n\n        # Initialize a judger class which will decide who wins in the end\n        self.judger = Judger(self.np_random)\n\n        # Deal cards to each  player to prepare for the first round\n        for i in range(2 * self.num_players):\n            self.players[i % self.num_players].hand.append(self.dealer.deal_card())\n\n        # Initialize public cards\n        self.public_cards = []\n        self.stage = Stage.PREFLOP\n\n        # Big blind and small blind\n        s = (self.dealer_id + 1) % self.num_players\n        b = (self.dealer_id + 2) % self.num_players\n        self.players[b].bet(chips=self.big_blind)\n        self.players[s].bet(chips=self.small_blind)\n\n        # The player next to the big blind plays the first\n        self.game_pointer = (b + 1) % self.num_players\n\n        # Initialize a bidding round, in the first round, the big blind and the small blind needs to\n        # be passed to the round for processing.\n        self.round = Round(self.num_players, self.big_blind, dealer=self.dealer, np_random=self.np_random)\n\n        self.round.start_new_round(game_pointer=self.game_pointer, raised=[p.in_chips for p in self.players])\n\n        # Count the round. There are 4 rounds in each game.\n        self.round_counter = 0\n\n        # Save the history for stepping back to the last state.\n        self.history = []\n\n        state = self.get_state(self.game_pointer)\n\n        return state, self.game_pointer\n\n    def get_legal_actions(self):\n        \"\"\"\n        Return the legal actions for current player\n\n        Returns:\n            (list): A list of legal actions\n        \"\"\"\n        return self.round.get_nolimit_legal_actions(players=self.players)\n", "right_context": "\n    def get_state(self, player_id):\n        \"\"\"\n        Return player's state\n\n        Args:\n            player_id (int): player id\n\n        Returns:\n            (dict): The state of the player\n        \"\"\"\n        self.dealer.pot = np.sum([player.in_chips for player in self.players])\n\n        chips = [self.players[i].in_chips for i in range(self.num_players)]\n        legal_actions = self.get_legal_actions()\n        state = self.players[player_id].get_state(self.public_cards, chips, legal_actions)\n        state['stakes'] = [self.players[i].remained_chips for i in range(self.num_players)]\n        state['current_player'] = self.game_pointer\n        state['pot'] = self.dealer.pot\n        state['stage'] = self.stage\n        return state\n\n    def step_back(self):\n        \"\"\"\n        Return to the previous state of the game\n\n        Returns:\n            (bool): True if the game steps back successfully\n        \"\"\"\n        if len(self.history) > 0:\n            self.round, self.game_pointer, self.round_counter, self.dealer, self.public_cards, self.players = self.history.pop()\n            self.stage = Stage(self.round_counter)\n            return True\n        return False\n\n    def get_num_players(self):\n        \"\"\"\n        Return the number of players in no limit texas holdem\n\n        Returns:\n            (int): The number of players in the game\n        \"\"\"\n        return self.num_players\n\n    def get_payoffs(self):\n        \"\"\"\n        Return the payoffs of the game\n\n        Returns:\n            (list): Each entry corresponds to the payoff of one player\n        \"\"\"\n        hands = [p.hand + self.public_cards if p.status in (PlayerStatus.ALIVE, PlayerStatus.ALLIN) else None for p in self.players]\n        chips_payoffs = self.judger.judge_game(self.players, hands)\n        return chips_payoffs\n\n    @staticmethod\n    def get_num_actions():\n        \"\"\"\n        Return the number of applicable actions\n\n        Returns:\n            (int): The number of actions. There are 6 actions (call, raise_half_pot, raise_pot, all_in, check and fold)\n        \"\"\"\n        return len(Action)\n", "import_text": ["enum.Enum", "numpy", "copy.deepcopy", "rlcard.games.limitholdem.Game", "rlcard.games.limitholdem.PlayerStatus", "rlcard.games.nolimitholdem.Dealer", "rlcard.games.nolimitholdem.Player", "rlcard.games.nolimitholdem.Judger", "rlcard.games.nolimitholdem.Round", "rlcard.games.nolimitholdem.Action"], "prompt": "\"\"\"\nDescription: This function is for executing a step in a game. It checks if the action is legal, then proceeds to the next round. If a round is over, it deals more public cards and starts a new round.\n\nArgs:\n    self (object): The instance of the class that this function is a part of.\n    action (type): The action to be taken in the game.\n\nReturns:\n    tuple: A tuple containing the current state of the game and the game pointer.\n\nRaises:\n    Exception: If the action is not allowed.\n\nNotes:\n    This function uses the numpy.sum API to sum up the elements of an iterable.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"\n        Get the next state\n\n        Args:\n            action (str): a specific action. (call, raise, fold, or check)\n\n        Returns:\n            (tuple): Tuple containing:\n\n                (dict): next player's state\n                (int): next player id\n        \"\"\"", "function_dependencies": ["copy.deepcopy", "numpy.sum"], "project_create_time": "2019-09-05T12:48:01+00:00", "project_update_time": "2024-04-17T16:43:03+00:00", "file_create_time": "2020-04-19T16:17:07Z", "file_update_time": "2021-09-26T19:14:56Z", "function_update_time": "2020-04-19T16:17:07Z", "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "reference_api": ["numpy.sum"], "test_function": [{"file_path": "/rlcard-1.0.7/rlcard-1.0.7/tests/games/test_nolimitholdem_game.py", "class_name": "TestNolimitholdemMethods", "function_name": "test_step", "code": "\n    def test_step(self):\n        game = Game()\n\n        # test call\n        game.init_game()\n        init_not_raise_num = game.round.not_raise_num\n        game.step(Action.CHECK_CALL)\n        step_not_raise_num = game.round.not_raise_num\n        self.assertEqual(init_not_raise_num + 1, step_not_raise_num)\n\n        # test fold\n        _, player_id = game.init_game()\n        game.step(Action.FOLD)\n\n        self.assertEqual(PlayerStatus.FOLDED, game.players[player_id].status)\n\n        # test check\n        game.init_game()\n        game.step(Action.CHECK_CALL)"}, {"file_path": "/rlcard-1.0.7/rlcard-1.0.7/tests/games/test_nolimitholdem_game.py", "class_name": "TestNolimitholdemMethods", "function_name": "test_step_2", "code": "\n    def test_step_2(self):\n        game = Game()\n\n        # test check\n        game.init_game()\n        self.assertEqual(Stage.PREFLOP, game.stage)\n        game.step(Action.CHECK_CALL)\n        game.step(Action.RAISE_POT)\n        game.step(Action.CHECK_CALL)\n\n        self.assertEqual(Stage.FLOP, game.stage)\n        game.step(Action.CHECK_CALL)\n        game.step(Action.CHECK_CALL)\n\n        self.assertEqual(Stage.TURN, game.stage)\n        game.step(Action.CHECK_CALL)\n        game.step(Action.CHECK_CALL)\n\n        self.assertEqual(Stage.RIVER, game.stage)"}, {"file_path": "/rlcard-1.0.7/rlcard-1.0.7/tests/games/test_nolimitholdem_game.py", "class_name": "TestNolimitholdemMethods", "function_name": "test_step_3_players", "code": "\n    def test_step_3_players(self):\n        game = Game(num_players=3)\n\n        # test check\n        _, first_player_id = game.init_game()\n        self.assertEqual(Stage.PREFLOP, game.stage)\n        game.step(Action.CHECK_CALL)\n        game.step(Action.CHECK_CALL)\n        game.step(Action.RAISE_POT)\n        game.step(Action.FOLD)\n        game.step(Action.CHECK_CALL)\n\n        self.assertEqual(Stage.FLOP, game.stage)\n        self.assertEqual((first_player_id - 2) % 3, game.round.game_pointer)\n        game.step(Action.CHECK_CALL)\n        game.step(Action.RAISE_POT)\n        game.step(Action.CHECK_CALL)\n\n        self.assertEqual(Stage.TURN, game.stage)\n        self.assertEqual((first_player_id - 2) % 3, game.round.game_pointer)\n        game.step(Action.CHECK_CALL)\n        game.step(Action.CHECK_CALL)\n\n        self.assertEqual(Stage.RIVER, game.stage)"}, {"file_path": "/rlcard-1.0.7/rlcard-1.0.7/tests/games/test_nolimitholdem_game.py", "class_name": "TestNolimitholdemMethods", "function_name": "test_all_in_rounds", "code": "\n    def test_all_in_rounds(self):\n        game = Game()\n\n        game.init_game()\n        game.step(Action.CHECK_CALL)\n        game.step(Action.CHECK_CALL)\n        self.assertEqual(game.round_counter, 1)\n\n        game.step(Action.CHECK_CALL)\n        game.step(Action.ALL_IN)\n        self.assertListEqual([Action.FOLD, Action.CHECK_CALL], game.get_legal_actions())\n        game.step(Action.CHECK_CALL)\n        self.assertEqual(game.round_counter, 4)\n        self.assertEqual(200, game.dealer.pot)"}, {"file_path": "/rlcard-1.0.7/rlcard-1.0.7/tests/games/test_nolimitholdem_game.py", "class_name": "TestNolimitholdemMethods", "function_name": "test_raise_pot", "code": "\n    def test_raise_pot(self):\n        game = Game()\n\n        _, player_id = game.init_game()\n        game.step(Action.RAISE_POT)\n        step_raised = game.round.raised[player_id]\n        self.assertEqual(4, step_raised)\n\n        player_id = game.round.game_pointer\n        game.step(Action.RAISE_POT)\n        step_raised = game.round.raised[player_id]\n        self.assertEqual(8, step_raised)\n\n        player_id = game.round.game_pointer\n        game.step(Action.RAISE_POT)\n        step_raised = game.round.raised[player_id]\n        self.assertEqual(16, step_raised)\n\n        game.step(Action.CHECK_CALL)\n        player_id = game.round.game_pointer\n        game.step(Action.RAISE_POT)\n        step_raised = game.round.raised[player_id]\n        self.assertEqual(32, step_raised)"}, {"file_path": "/rlcard-1.0.7/rlcard-1.0.7/tests/games/test_nolimitholdem_game.py", "class_name": "TestNolimitholdemMethods", "function_name": "test_raise_half_pot", "code": "\n    def test_raise_half_pot(self):\n        game = Game()\n\n        _, player_id = game.init_game()\n        self.assertNotIn(Action.RAISE_HALF_POT, game.get_legal_actions()) # Half pot equals call\n        game.step(Action.CHECK_CALL)\n        step_raised = game.round.raised[player_id]\n        self.assertEqual(2, step_raised)\n\n        player_id = game.round.game_pointer\n        game.step(Action.RAISE_HALF_POT)\n        step_raised = game.round.raised[player_id]\n        self.assertEqual(4, step_raised)\n\n        player_id = game.round.game_pointer\n        game.step(Action.RAISE_HALF_POT)\n        step_raised = game.round.raised[player_id]\n        self.assertEqual(5, step_raised)"}]}, {"git_group": "tensorflow", "git_name": "agents", "version": "v0.19.0", "language": "Python", "project_name": "agents-v0.19.0.zip", "file_path": "/agents-v0.19.0/agents-0.19.0/tf_agents/agents/ppo/ppo_actor_network.py", "file_name": "ppo_actor_network.py", "focal_class": "PPOActorNetwork", "focal_name": "create_sequential_actor_net", "focal_parameter": ["fc_layer_units", "action_tensor_spec"], "solution": "  def create_sequential_actor_net(\n      self, fc_layer_units, action_tensor_spec, seed=None\n  ):\n\n    self._seed_stream = self.seed_stream_class(\n        seed=seed, salt='tf_agents_sequential_layers'\n    )\n\n    def _get_seed():\n      seed = self._seed_stream()\n      if seed is not None:\n        seed = seed % sys.maxsize\n      return seed\n\n    def create_dist(loc_and_scale):\n      loc = loc_and_scale['loc']\n      loc = tanh_and_scale_to_spec(loc, action_tensor_spec)\n\n      scale = loc_and_scale['scale']\n      scale = tf.math.softplus(scale)\n\n      return tfp.distributions.MultivariateNormalDiag(\n          loc=loc, scale_diag=scale, validate_args=True\n      )\n\n    def means_layers():\n      # TODO(b/179510447): align these parameters with Schulman 17.\n      return tf.keras.layers.Dense(\n          action_tensor_spec.shape.num_elements(),\n          kernel_initializer=tf.keras.initializers.VarianceScaling(\n              scale=0.1, seed=_get_seed()\n          ),\n          name='means_projection_layer',\n      )\n\n    def std_layers():\n      # TODO(b/179510447): align these parameters with Schulman 17.\n      std_bias_initializer_value = np.log(np.exp(0.35) - 1)\n      return bias_layer.BiasLayer(\n          bias_initializer=tf.constant_initializer(\n              value=std_bias_initializer_value\n          )\n      )\n\n    def no_op_layers():\n      return tf.keras.layers.Lambda(lambda x: x)\n\n    dense = functools.partial(\n        tf.keras.layers.Dense,\n        activation=tf.nn.tanh,\n        kernel_initializer=tf.keras.initializers.Orthogonal(seed=_get_seed()),\n    )\n\n    return sequential.Sequential(\n        [dense(num_units) for num_units in fc_layer_units]\n        + [means_layers()]\n        + [\n            tf.keras.layers.Lambda(\n                lambda x: {'loc': x, 'scale': tf.zeros_like(x)}\n            )\n        ]\n        + [\n            nest_map.NestMap({\n                'loc': no_op_layers(),\n                'scale': std_layers(),\n            })\n        ]\n        +\n        # Create the output distribution from the mean and standard deviation.\n        [tf.keras.layers.Lambda(create_dist)]\n    )", "function_signature": "def create_sequential_actor_net(\n      self, fc_layer_units, action_tensor_spec, seed=None\n  ) :", "left_context": "# coding=utf-8\n# Copyright 2020 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr\"\"\"Sequential Actor Network for PPO.\"\"\"\nimport functools\nimport sys\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\nfrom tf_agents.keras_layers import bias_layer\nfrom tf_agents.networks import nest_map\nfrom tf_agents.networks import sequential\n\n\ndef tanh_and_scale_to_spec(inputs, spec):\n  \"\"\"Maps inputs with arbitrary range to range defined by spec using `tanh`.\"\"\"\n  means = (spec.maximum + spec.minimum) / 2.0\n  magnitudes = (spec.maximum - spec.minimum) / 2.0\n\n  return means + magnitudes * tf.tanh(inputs)\n\n\nclass PPOActorNetwork:\n  \"\"\"Contains the actor network structure.\"\"\"\n\n  def __init__(self, seed_stream_class=tfp.util.SeedStream):\n    self.seed_stream_class = seed_stream_class\n", "right_context": "", "import_text": ["functools", "sys", "numpy", "tensorflow.compat.v2", "tensorflow_probability", "tf_agents.keras_layers.bias_layer", "tf_agents.networks.nest_map", "tf_agents.networks.sequential"], "prompt": "\"\"\"\nDescription: This function creates a sequential actor network for a reinforcement learning agent.\n\nArgs:\n    fc_layer_units (list): A list of integers representing the number of units in each fully connected layer.\n    action_tensor_spec (tf.TensorSpec): The specification of the action tensor.\n    seed (int, optional): The seed for the random number generator. Defaults to None.\n\nReturns:\n    sequential.Sequential: A sequential model representing the actor network.\n\nNotes:\n    This function uses the API of numpy.log and numpy.exp. The numpy.log function is used to initialize the bias of the standard deviation layer,\n    and numpy.exp is used to ensure the standard deviation is positive.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"Helper method for creating the actor network.\"\"\"", "function_dependencies": ["tensorflow.compat.v2.math.softplus", "tensorflow_probability.distributions.MultivariateNormalDiag", "tensorflow.compat.v2.keras.layers.Dense", "tensorflow.compat.v2.keras.initializers.VarianceScaling", "numpy.log", "numpy.exp", "tf_agents.keras_layers.bias_layer.BiasLayer", "tensorflow.compat.v2.constant_initializer", "tensorflow.compat.v2.keras.layers.Lambda", "functools.partial", "tensorflow.compat.v2.keras.initializers.Orthogonal", "tf_agents.networks.sequential.Sequential", "tensorflow.compat.v2.zeros_like", "tf_agents.networks.nest_map.NestMap"], "project_create_time": "2018-11-17T00:29:12+00:00", "project_update_time": "2024-04-16T15:23:04+00:00", "file_create_time": "2021-04-07T23:32:39Z", "file_update_time": "2023-07-21T22:42:05Z", "function_update_time": "2021-04-07T23:32:39Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["numpy.log", "numpy.exp"], "test_function": [{"file_path": "/agents-v0.19.0/agents-0.19.0/tf_agents/agents/ppo/ppo_actor_network_test.py", "class_name": "PpoActorNetworkTest", "function_name": "test_no_mismatched_shape", "code": "\n  def test_no_mismatched_shape(self):\n    if not tf.executing_eagerly():\n      self.skipTest('Skipping test: sequential networks not supported in TF1')\n    observation_tensor_spec = tf.TensorSpec(shape=[1], dtype=tf.float32)\n    action_tensor_spec = tensor_spec.BoundedTensorSpec((8,), tf.float32, -1, 1)\n\n    actor_net_lib = ppo_actor_network.PPOActorNetwork()\n    actor_net_lib.seed_stream_class = DeterministicSeedStream\n    actor_net = actor_net_lib.create_sequential_actor_net(\n        fc_layer_units=(1,), action_tensor_spec=action_tensor_spec, seed=1\n    )\n\n    actor_output_spec = actor_net.create_variables(observation_tensor_spec)\n\n    distribution_utils.assert_specs_are_compatible(\n        actor_output_spec,\n        action_tensor_spec,\n        'actor_network output spec does not match action spec',\n    )"}, {"file_path": "/agents-v0.19.0/agents-0.19.0/tf_agents/agents/ppo/ppo_actor_network_test.py", "class_name": "PpoActorNetworkTest", "function_name": "test_same_actor_net_output", "code": "\n  def test_same_actor_net_output(self):\n    if not tf.executing_eagerly():\n      self.skipTest('Skipping test: sequential networks not supported in TF1')\n    observation_tensor_spec = tf.TensorSpec(shape=[1], dtype=tf.float32)\n    action_tensor_spec = tensor_spec.BoundedTensorSpec((8,), tf.float32, -1, 1)\n\n    actor_net_lib = ppo_actor_network.PPOActorNetwork()\n    actor_net_lib.seed_stream_class = DeterministicSeedStream\n    actor_net_sequential = actor_net_lib.create_sequential_actor_net(\n        fc_layer_units=(1,), action_tensor_spec=action_tensor_spec, seed=1\n    )\n\n    actor_net_actor_dist = actor_distribution_network.ActorDistributionNetwork(\n        observation_tensor_spec,\n        action_tensor_spec,\n        fc_layer_params=(1,),\n        activation_fn=tf.nn.tanh,\n        kernel_initializer=tf.keras.initializers.Orthogonal(seed=1),\n        seed_stream_class=DeterministicSeedStream,\n        seed=1,\n    )\n\n    sample_observation = tf.constant([[1], [2]], dtype=tf.float32)\n    tf.random.set_seed(111)\n    sequential_output_dist, _ = actor_net_sequential(\n        sample_observation, step_type=ts.StepType.MID, network_state=()\n    )\n    tf.random.set_seed(111)\n    actor_dist_output_dist, _ = actor_net_actor_dist(\n        sample_observation, step_type=ts.StepType.MID, network_state=()\n    )\n    self.assertAllEqual(\n        sequential_output_dist.mean(), actor_dist_output_dist.mean()\n    )\n    self.assertAllEqual(\n        sequential_output_dist.stddev(), actor_dist_output_dist.stddev()\n    )"}, {"file_path": "/agents-v0.19.0/agents-0.19.0/tf_agents/agents/ppo/ppo_actor_network_test.py", "class_name": "PpoActorNetworkTest", "function_name": "test_same_policy_same_output", "code": "\n  def test_same_policy_same_output(self):\n    if not tf.executing_eagerly():\n      self.skipTest('Skipping test: sequential networks not supported in TF1')\n    observation_tensor_spec = tf.TensorSpec(shape=[1], dtype=tf.float32)\n    action_tensor_spec = tensor_spec.BoundedTensorSpec((8,), tf.float32, -1, 1)\n\n    value_net = value_network.ValueNetwork(\n        observation_tensor_spec, fc_layer_params=(1,)\n    )\n\n    actor_net_lib = ppo_actor_network.PPOActorNetwork()\n    actor_net_lib.seed_stream_class = DeterministicSeedStream\n    actor_net_sequential = actor_net_lib.create_sequential_actor_net(\n        fc_layer_units=(1,), action_tensor_spec=action_tensor_spec, seed=1\n    )\n    actor_net_actor_dist = actor_distribution_network.ActorDistributionNetwork(\n        observation_tensor_spec,\n        action_tensor_spec,\n        fc_layer_params=(1,),\n        activation_fn=tf.nn.tanh,\n        kernel_initializer=tf.keras.initializers.Orthogonal(seed=1),\n        seed_stream_class=DeterministicSeedStream,\n        seed=1,\n    )\n\n    tf.random.set_seed(111)\n    seq_policy = ppo_policy.PPOPolicy(\n        ts.time_step_spec(observation_tensor_spec),\n        action_tensor_spec,\n        actor_net_sequential,\n        value_net,\n        collect=True,\n    )\n    tf.random.set_seed(111)\n    actor_dist_policy = ppo_policy.PPOPolicy(\n        ts.time_step_spec(observation_tensor_spec),\n        action_tensor_spec,\n        actor_net_actor_dist,\n        value_net,\n        collect=True,\n    )\n\n    sample_timestep = ts.TimeStep(\n        step_type=tf.constant([1, 1], dtype=tf.int32),\n        reward=tf.constant([1, 1], dtype=tf.float32),\n        discount=tf.constant([1, 1], dtype=tf.float32),\n        observation=tf.constant([[1], [2]], dtype=tf.float32),\n    )\n    seq_policy_step = seq_policy._distribution(sample_timestep, policy_state=())\n    act_dist_policy_step = actor_dist_policy._distribution(\n        sample_timestep, policy_state=()\n    )\n\n    seq_scale = seq_policy_step.info['dist_params']['scale_diag']\n    act_dist_scale = act_dist_policy_step.info['dist_params']['scale']\n    self.assertAllEqual(seq_scale, act_dist_scale)\n    self.assertAllEqual(\n        seq_policy_step.info['dist_params']['loc'],\n        act_dist_policy_step.info['dist_params']['loc'],\n    )"}]}, {"git_group": "openstack", "git_name": "tacker", "version": "yoga-eom", "language": "Python", "project_name": "tacker-yoga-eom.zip", "file_path": "/tacker-yoga-eom/tacker-yoga-eom/tacker/tests/utils.py", "file_name": "utils.py", "focal_class": null, "focal_name": "create_csar_with_unique_artifact", "focal_parameter": ["csar_dir"], "solution": "\ndef create_csar_with_unique_artifact(csar_dir):\n    unique_id = uuidutils.generate_uuid()\n    tempfd, tempname = tempfile.mkstemp(suffix=\".zip\",\n                                        dir=os.path.dirname(csar_dir))\n    os.close(tempfd)\n    common_artifact_dir = os.path.join(csar_dir, '../common_artifact')\n    common_dir = os.path.join(csar_dir, '../common')\n    zcsar = zipfile.ZipFile(tempname, 'w')\n\n    artifact_files = []\n    for (dpath, _, fnames) in os.walk(csar_dir):\n        if not fnames:\n            continue\n        for fname in fnames:\n            if fname == 'TOSCA.meta' or fname.endswith('.mf'):\n                src_file = os.path.join(dpath, fname)\n                with open(src_file, 'rb') as f:\n                    artifacts_data = f.read()\n                artifacts_data_split = re.split(b'\\n\\n+', artifacts_data)\n                for data in artifacts_data_split:\n                    if re.findall(b\".?Algorithm:.?\", data) and\\\n                            re.findall(b\".?Hash:.?\", data):\n                        artifact_data_dict = yaml.safe_load(data)\n                        artifact_files.append(\n                            artifact_data_dict['Source']\n                            if 'Source' in artifact_data_dict.keys()\n                            else artifact_data_dict['Name'])\n    artifact_files = list(set(artifact_files))\n\n    for (dpath, _, fnames) in os.walk(common_artifact_dir):\n        if not fnames:\n            continue\n        for fname in fnames:\n            src_file = os.path.join(dpath, fname)\n            dst_file = os.path.relpath(\n                os.path.join(dpath, fname), common_artifact_dir)\n            if fname.endswith('.yaml') or fname.endswith('.yml'):\n                if dst_file not in artifact_files:\n                    with open(src_file, 'rb') as yfile:\n                        data = yaml.safe_load(yfile)\n                        _update_unique_id_in_yaml(data, unique_id)\n                        zcsar.writestr(dst_file, yaml.dump(\n                            data, default_flow_style=False,\n                            allow_unicode=True))\n                else:\n                    zcsar.write(src_file, dst_file)\n            else:\n                zcsar.write(src_file, dst_file)\n\n    for (dpath, _, fnames) in os.walk(csar_dir):\n        if not fnames:\n            continue\n        for fname in fnames:\n            src_file = os.path.join(dpath, fname)\n            dst_file = os.path.relpath(os.path.join(dpath, fname), csar_dir)\n            zcsar.write(src_file, dst_file)\n\n    for (dpath, _, fnames) in os.walk(common_dir):\n        if not fnames:\n            continue\n        for fname in fnames:\n            src_file = os.path.join(dpath, fname)\n            dst_file = os.path.relpath(os.path.join(dpath, fname), common_dir)\n            zcsar.write(src_file, dst_file)\n\n    zcsar.close()\n\n    return tempname", "function_signature": "def create_csar_with_unique_artifact(csar_dir) :", "left_context": "# Copyright 2015 Brocade Communications System, Inc.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nimport base64\nimport http.server\nimport os\nimport re\nimport shutil\nimport threading\n\nfrom oslo_utils import uuidutils\nimport socketserver\nimport tempfile\nimport yaml\nimport zipfile\n\n\ndef read_file(input_file):\n    yaml_file = os.path.abspath(os.path.join(os.path.dirname(__file__),\n                                             'etc/samples/' + str(input_file)))\n    with open(yaml_file, 'r') as f:\n        return f.read()\n\n\ndef _update_unique_id_in_yaml(data, uid):\n    try:\n        prop = data['topology_template']['node_templates']['VNF'][\n            'properties']\n        if (prop.get('descriptor_id', None)):\n            prop['descriptor_id'] = uid\n    except KeyError:\n        # Let's check for 'node_types'\n        pass\n\n    if not data.get('node_types', None):\n        return\n\n    for ntype in data['node_types'].values():\n        if ntype['derived_from'] != 'tosca.nodes.nfv.VNF':\n            continue\n        try:\n            desc_id = ntype['properties']['descriptor_id']\n            if desc_id.get('constraints', None):\n                for constraint in desc_id.get('constraints'):\n                    if constraint.get('valid_values', None):\n                        constraint['valid_values'] = [uid]\n            if desc_id.get('default', None):\n                desc_id['default'] = uid\n        except KeyError:\n            # Let's check next node_type\n            pass\n\n\ndef create_csar_with_unique_vnfd_id(csar_dir):\n    \"\"\"Create CSAR file from a directory structure\n\n    For various tests it is necessary to have a CSAR having unique vnfd id.\n    This function reads a directory structure, updates vnfd id in yaml files\n    and creates a temporary CSAR zip file.\n\n    :returns:\n        - csar_file_name\n        - vnfd_id\n    \"\"\"\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    csar_dir = os.path.join(current_dir, \"../../\", csar_dir)\n    unique_id = uuidutils.generate_uuid()\n    tempfd, tempname = tempfile.mkstemp(suffix=\".zip\",\n        dir=os.path.dirname(csar_dir))\n    os.close(tempfd)\n    common_dir = os.path.join(csar_dir, \"../common/\")\n    zcsar = zipfile.ZipFile(tempname, 'w')\n\n    artifact_files = []\n    for (dpath, _, fnames) in os.walk(csar_dir):\n        if not fnames:\n            continue\n        for fname in fnames:\n            if fname == 'TOSCA.meta' or fname.endswith('.mf'):\n                src_file = os.path.join(dpath, fname)\n                with open(src_file, 'rb') as f:\n                    artifacts_data = f.read()\n                artifacts_data_split = re.split(b'\\n\\n+', artifacts_data)\n                for data in artifacts_data_split:\n                    if re.findall(b'.?Algorithm:.?|.?Hash:.?', data):\n                        artifact_data_dict = yaml.safe_load(data)\n                        artifact_files.append(\n                            artifact_data_dict['Source']\n                            if 'Source' in artifact_data_dict.keys()\n                            else artifact_data_dict['Name'])\n    artifact_files = list(set(artifact_files))\n\n    for (dpath, _, fnames) in os.walk(csar_dir):\n        if not fnames:\n            continue\n        for fname in fnames:\n            src_file = os.path.join(dpath, fname)\n            dst_file = os.path.relpath(os.path.join(dpath, fname), csar_dir)\n            if fname.endswith('.yaml') or fname.endswith('.yml'):\n                if dst_file not in artifact_files:\n                    with open(src_file, 'rb') as yfile:\n                        data = yaml.safe_load(yfile)\n                        _update_unique_id_in_yaml(data, unique_id)\n                        zcsar.writestr(dst_file, yaml.dump(\n                            data, default_flow_style=False,\n                            allow_unicode=True))\n                else:\n                    zcsar.write(src_file, dst_file)\n            else:\n                zcsar.write(src_file, dst_file)\n\n    for (dpath, _, fnames) in os.walk(common_dir):\n        if not fnames:\n            continue\n        if ('test_cnf' in csar_dir and\n                re.search('images|kubernetes|Scripts', dpath)):\n            continue\n        for fname in fnames:\n            src_file = os.path.join(dpath, fname)\n            dst_file = os.path.relpath(os.path.join(dpath, fname), common_dir)\n            zcsar.write(src_file, dst_file)\n\n    zcsar.close()\n    return tempname, unique_id\n\n", "right_context": "\n\ndef copy_csar_files(fake_csar_path, csar_dir_name,\n                    csar_without_tosca_meta=False, read_vnfd_only=False):\n    \"\"\"Copy csar directory to temporary directory\n\n    :param fake_csar_path: a temporary directory in which csar data will be\n                           copied.\n    :param csar_dir_name: the relative path of csar directory with respect to\n                          './tacker/tests/etc/samples/etsi/nfv' directory.\n                          for ex. 'vnfpkgm1'.\n    :param csar_without_tosca_meta: when set to 'True', it will only copy root\n                                    level yaml file and image file.\n    :param read_vnfd_only: when set to 'True', it won't copy the image file\n                           from source directory.\n    \"\"\"\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    sample_vnf_package = os.path.join(current_dir, \"etc/samples/etsi/nfv\",\n                                      csar_dir_name)\n    shutil.copytree(sample_vnf_package, fake_csar_path)\n    common_files_path = os.path.join(current_dir,\n                                     \"etc/samples/etsi/nfv/common/\")\n\n    if not read_vnfd_only:\n        # Copying image file.\n        shutil.copytree(os.path.join(common_files_path, \"Files/\"),\n                        os.path.join(fake_csar_path, \"Files/\"))\n\n    if csar_without_tosca_meta:\n        return\n\n    # Copying common vnfd files.\n    tosca_definition_file_path = os.path.join(common_files_path,\n                                              \"Definitions/\")\n    for (dpath, _, fnames) in os.walk(tosca_definition_file_path):\n        if not fnames:\n            continue\n        for fname in fnames:\n            src_file = os.path.join(dpath, fname)\n            shutil.copy(src_file, os.path.join(fake_csar_path,\n                                               \"Definitions\"))\n\n\ndef copy_artifact_files(fake_csar_path, csar_dir_name,\n                    csar_without_tosca_meta=False, read_vnfd_only=False):\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    sample_vnf_package = os.path.join(current_dir,\n                                      \"etc/samples/etsi/nfv\",\n                                      csar_dir_name)\n    shutil.copytree(sample_vnf_package, fake_csar_path)\n    common_files_path = os.path.join(current_dir,\n                                     \"etc/samples/etsi/nfv/\")\n\n    if not read_vnfd_only:\n        # Copying image file.\n        shutil.copytree(os.path.join(common_files_path, \"Files/\"),\n                        os.path.join(fake_csar_path, \"Files/\"))\n        shutil.copytree(os.path.join(common_files_path, \"Scripts/\"),\n                        os.path.join(fake_csar_path, \"Scripts/\"))\n\n    if csar_without_tosca_meta:\n        return\n\n    # Copying common vnfd files.\n    tosca_definition_file_paths = [\n        os.path.join(common_files_path, \"common_artifact/Definitions/\"),\n        os.path.join(common_files_path, \"common/Definitions/\")\n    ]\n    for tosca_definition_file_path in tosca_definition_file_paths:\n        for (dpath, _, fnames) in os.walk(tosca_definition_file_path):\n            if not fnames:\n                continue\n            for fname in fnames:\n                src_file = os.path.join(dpath, fname)\n                if not os.path.exists(os.path.join(\n                        fake_csar_path, \"Definitions\")):\n                    os.mkdir(os.path.join(fake_csar_path, \"Definitions\"))\n                os.mknod(os.path.join(fake_csar_path, \"Definitions\", fname))\n                shutil.copyfile(src_file, os.path.join(\n                    fake_csar_path, \"Definitions\", fname))\n\n\nclass AuthHandler(http.server.SimpleHTTPRequestHandler):\n    '''Main class to present webpages and authentication.'''\n\n    def do_AUTHHEAD(self):\n        self.send_response(401)\n        self.send_header('WWW-Authenticate', 'Basic realm=\\\"Test\\\"')\n        self.send_header('Content-type', 'text/plain')\n        self.end_headers()\n\n    def do_GET(self):\n        '''Present frontpage with user authentication.'''\n        global key\n        if 'Authorization' not in self.headers:\n            http.server.SimpleHTTPRequestHandler.do_GET(self)\n        elif self.headers.get('Authorization') is None:\n            self.do_AUTHHEAD()\n            self.wfile.write(bytes('no auth header received'))\n        elif self.headers.get('Authorization') == 'Basic ' + base64.b64encode(\n                b\"username:password\").decode(\"utf-8\"):\n            http.server.SimpleHTTPRequestHandler.do_GET(self)\n        else:\n            self.do_AUTHHEAD()\n            self.wfile.write(bytes(self.headers.get('Authorization')))\n            self.wfile.write(bytes('not authenticated'))\n\n\nclass StaticHttpFileHandler(object):\n\n    def __init__(self, static_files_path):\n        if os.path.isabs(static_files_path):\n            web_dir = static_files_path\n        else:\n            web_dir = os.path.join(os.path.dirname(__file__),\n                static_files_path)\n        os.chdir(web_dir)\n        server_address = ('127.0.0.1', 0)\n        self.httpd = socketserver.TCPServer(server_address, AuthHandler)\n        self.port = self.httpd.socket.getsockname()[1]\n\n        thread = threading.Thread(target=self.httpd.serve_forever)\n        thread.daemon = True\n        thread.start()\n\n    def stop(self):\n        self.httpd.shutdown()\n        self.httpd.server_close()\n", "import_text": ["base64", "http.server", "os", "re", "shutil", "threading", "oslo_utils.uuidutils", "socketserver", "tempfile", "yaml", "zipfile"], "prompt": "\"\"\"\nDescription: This function creates a CSAR (Cloud Service Archive) with unique artifacts.\n\nArgs:\n    csar_dir (str): The directory path of the CSAR.\n\nReturns:\n    str: The path of the temporary CSAR file created.\n\"\"\"", "comment": null, "prompt_is_gen_from_api": true, "function_dependencies": ["oslo_utils.uuidutils.generate_uuid", "tempfile.mkstemp", "os.path.dirname", "os.close", "os.path.join", "zipfile.ZipFile", "os.walk", "re.split", "re.findall", "yaml.safe_load", "yaml.safe_load.keys", "os.path.relpath", "zipfile.ZipFile.writestr", "yaml.dump", "zipfile.ZipFile.write", "zipfile.ZipFile.close"], "project_create_time": "2014-06-27T01:11:56+00:00", "project_update_time": "2023-12-19T17:37:02+00:00", "file_create_time": "2015-09-21T21:51:41Z", "file_update_time": "2022-02-17T07:51:06Z", "function_update_time": "2020-07-07T09:57:44Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["yaml.dump"], "test_function": [{"file_path": "/tacker-yoga-eom/tacker-yoga-eom/tacker/tests/unit/common/test_csar_utils.py", "class_name": "TestCSARUtils", "function_name": "test_load_csar_data_in_meta_and_manifest_with_vnf_artifact", "code": "\n    def test_load_csar_data_in_meta_and_manifest_with_vnf_artifact(\n            self, mock_extract_csar_zip_file):\n        file_path = utils.create_csar_with_unique_artifact(\n            './tacker/tests/etc/samples/etsi/nfv/'\n            'sample_vnf_package_csar_in_meta_and_manifest')\n        self.addCleanup(os.remove, file_path)\n        vnf_data, flavours, vnf_artifacts = csar_utils.load_csar_data(\n            self.context, constants.UUID, file_path)\n        self.assertEqual(vnf_data['descriptor_version'], '1.0')\n        self.assertEqual(vnf_data['vnfm_info'], ['Tacker'])\n        self.assertEqual(flavours[0]['flavour_id'], 'simple')\n        self.assertIsNotNone(flavours[0]['sw_images'])\n        self.assertIsNotNone(vnf_artifacts)\n        self.assertIsNotNone(vnf_artifacts[0]['Source'])\n        self.assertIsNotNone(vnf_artifacts[0]['Hash'])\n        for item in vnf_artifacts:\n            flag = item.get('Source').lower().endswith('.img')\n            self.assertEqual(flag, False)"}, {"file_path": "/tacker-yoga-eom/tacker-yoga-eom/tacker/tests/unit/common/test_csar_utils.py", "class_name": "TestCSARUtils", "function_name": "test_load_csar_data_with_single_manifest_with_vnf_artifact", "code": "\n    def test_load_csar_data_with_single_manifest_with_vnf_artifact(\n            self, mock_extract_csar_zip_file):\n        file_path = utils.create_csar_with_unique_artifact(\n            './tacker/tests/etc/samples/etsi/nfv/'\n            'sample_vnf_package_csar_manifest')\n        self.addCleanup(os.remove, file_path)\n        vnf_data, flavours, vnf_artifacts = csar_utils.load_csar_data(\n            self.context, constants.UUID, file_path)\n        self.assertEqual(vnf_data['descriptor_version'], '1.0')\n        self.assertEqual(vnf_data['vnfm_info'], ['Tacker'])\n        self.assertEqual(flavours[0]['flavour_id'], 'simple')\n        self.assertIsNotNone(flavours[0]['sw_images'])\n        self.assertIsNotNone(vnf_artifacts)\n        self.assertIsNotNone(vnf_artifacts[0]['Source'])\n        self.assertIsNotNone(vnf_artifacts[0]['Hash'])"}, {"file_path": "/tacker-yoga-eom/tacker-yoga-eom/tacker/tests/unit/common/test_csar_utils.py", "class_name": "TestCSARUtils", "function_name": "test_load_csar_data_with_single_meta_with_vnf_artifact", "code": "\n    def test_load_csar_data_with_single_meta_with_vnf_artifact(\n            self, mock_extract_csar_zip_file):\n        file_path = utils.create_csar_with_unique_artifact(\n            './tacker/tests/etc/samples/etsi/nfv/'\n            'sample_vnf_package_csar_meta')\n        self.addCleanup(os.remove, file_path)\n        vnf_data, flavours, vnf_artifacts = csar_utils.load_csar_data(\n            self.context, constants.UUID, file_path)\n        self.assertEqual(vnf_data['descriptor_version'], '1.0')\n        self.assertEqual(vnf_data['vnfm_info'], ['Tacker'])\n        self.assertEqual(flavours[0]['flavour_id'], 'simple')\n        self.assertIsNotNone(flavours[0]['sw_images'])\n        self.assertIsNotNone(vnf_artifacts)\n        self.assertIsNotNone(vnf_artifacts[0]['Source'])\n        self.assertIsNotNone(vnf_artifacts[0]['Hash'])"}, {"file_path": "/tacker-yoga-eom/tacker-yoga-eom/tacker/tests/unit/common/test_csar_utils.py", "class_name": "TestCSARUtils", "function_name": "test_load_csar_data_meta_in_manifest_with_vnf_artifact", "code": "\n    def test_load_csar_data_meta_in_manifest_with_vnf_artifact(\n            self, mock_extract_csar_zip_file):\n        file_path = utils.create_csar_with_unique_artifact(\n            './tacker/tests/etc/samples/etsi/nfv/'\n            'sample_vnf_package_csar_meta_in_manifest')\n        self.addCleanup(os.remove, file_path)\n        vnf_data, flavours, vnf_artifacts = csar_utils.load_csar_data(\n            self.context, constants.UUID, file_path)\n        self.assertEqual(vnf_data['descriptor_version'], '1.0')\n        self.assertEqual(vnf_data['vnfm_info'], ['Tacker'])\n        self.assertEqual(flavours[0]['flavour_id'], 'simple')\n        self.assertIsNotNone(flavours[0]['sw_images'])\n        self.assertIsNotNone(vnf_artifacts)\n        self.assertIsNotNone(vnf_artifacts[0]['Source'])\n        self.assertIsNotNone(vnf_artifacts[0]['Hash'])"}, {"file_path": "/tacker-yoga-eom/tacker-yoga-eom/tacker/tests/unit/common/test_csar_utils.py", "class_name": "TestCSARUtils", "function_name": "test_load_csar_data_false_mf_with_vnf_artifact", "code": "\n    def test_load_csar_data_false_mf_with_vnf_artifact(\n            self, mock_extract_csar_zip_file):\n        file_path = utils.create_csar_with_unique_artifact(\n            './tacker/tests/etc/samples/etsi/nfv/'\n            'sample_vnf_package_csar_in_meta_and_manifest_false')\n        self.addCleanup(os.remove, file_path)\n        manifest_path = 'manifest.mf1'\n        exc = self.assertRaises(exceptions.InvalidCSAR,\n                                csar_utils.load_csar_data,\n                                self.context, constants.UUID, file_path)\n        msg = (('The file \"%(manifest)s\" in the CSAR \"%(csar)s\" does not '\n                'contain valid manifest.') %\n               {'manifest': manifest_path, 'csar': file_path})\n        self.assertEqual(msg, exc.format_message())"}, {"file_path": "/tacker-yoga-eom/tacker-yoga-eom/tacker/tests/unit/common/test_csar_utils.py", "class_name": "TestCSARUtils", "function_name": "test_load_csar_data_false_mf_name_with_vnf_artifact", "code": "\n    def test_load_csar_data_false_mf_name_with_vnf_artifact(\n            self, mock_extract_csar_zip_file):\n        file_path = utils.create_csar_with_unique_artifact(\n            './tacker/tests/etc/samples/etsi/nfv/'\n            'sample_vnf_package_csar_in_single_manifest_false_name')\n        self.addCleanup(os.remove, file_path)\n        manifest_path = 'VNF1.mf'\n        exc = self.assertRaises(exceptions.InvalidCSAR,\n                                csar_utils.load_csar_data,\n                                self.context, constants.UUID, file_path)\n        msg = (('The filename \"%(manifest)s\" is an invalid name.'\n                'The name must be the same as the main template '\n                'file name.') %\n               {'manifest': manifest_path})\n        self.assertEqual(msg, exc.format_message())"}, {"file_path": "/tacker-yoga-eom/tacker-yoga-eom/tacker/tests/unit/common/test_csar_utils.py", "class_name": "TestCSARUtils", "function_name": "test_load_csar_data_false_hash_with_vnf_artifact", "code": "\n    def test_load_csar_data_false_hash_with_vnf_artifact(\n            self, mock_extract_csar_zip_file):\n        file_path = utils.create_csar_with_unique_artifact(\n            './tacker/tests/etc/samples/etsi/nfv/'\n            'sample_vnf_package_csar_in_meta_and_manifest_false_hash')\n        self.addCleanup(os.remove, file_path)\n        exc = self.assertRaises(exceptions.InvalidCSAR,\n                                csar_utils.load_csar_data,\n                                self.context, constants.UUID, file_path)\n        hash_code = '27bbdb25d8f4ed6d07d6f6581b86515e8b2f' \\\n                    '0059b236ef7b6f50d6674b34f02'\n        artifact_path = 'Scripts/install.sh'\n        msg = (('The hash \"%(hash)s\" of artifact file '\n                '\"%(artifact)s\" is an invalid value.') %\n               {'hash': hash_code, 'artifact': artifact_path})\n        self.assertEqual(msg, exc.format_message())"}, {"file_path": "/tacker-yoga-eom/tacker-yoga-eom/tacker/tests/unit/common/test_csar_utils.py", "class_name": "TestCSARUtils", "function_name": "test_load_csar_data_missing_key_with_vnf_artifact", "code": "\n    def test_load_csar_data_missing_key_with_vnf_artifact(\n            self, mock_extract_csar_zip_file):\n        file_path = utils.create_csar_with_unique_artifact(\n            './tacker/tests/etc/samples/etsi/nfv/'\n            'sample_vnf_package_csar_in_meta_and_manifest_missing_key')\n        self.addCleanup(os.remove, file_path)\n        exc = self.assertRaises(exceptions.InvalidCSAR,\n                                csar_utils.load_csar_data,\n                                self.context, constants.UUID, file_path)\n        key_name = sorted(['Algorithm'])\n        msg = (('One of the artifact information may not have '\n                'the key(\"%(key)s\")') % {'key': key_name})\n        self.assertEqual(msg, exc.format_message())"}, {"file_path": "/tacker-yoga-eom/tacker-yoga-eom/tacker/tests/unit/common/test_csar_utils.py", "class_name": "TestCSARUtils", "function_name": "test_load_csar_data_missing_value_with_vnf_artifact", "code": "\n    def test_load_csar_data_missing_value_with_vnf_artifact(\n            self, mock_extract_csar_zip_file):\n        file_path = utils.create_csar_with_unique_artifact(\n            './tacker/tests/etc/samples/etsi/nfv/'\n            'sample_vnf_package_csar_in_meta_and_manifest_missing_value')\n        self.addCleanup(os.remove, file_path)\n        exc = self.assertRaises(exceptions.InvalidCSAR,\n                                csar_utils.load_csar_data,\n                                self.context, constants.UUID, file_path)\n        key_name = 'Algorithm'\n        msg = (('One of the artifact information may not have '\n                'the key value(\"%(key)s\")') % {'key': key_name})\n        self.assertEqual(msg, exc.format_message())"}, {"file_path": "/tacker-yoga-eom/tacker-yoga-eom/tacker/tests/unit/common/test_csar_utils.py", "class_name": "TestCSARUtils", "function_name": "test_load_csar_data_false_source_with_vnf_artifact", "code": "\n    def test_load_csar_data_false_source_with_vnf_artifact(\n            self, mock_extract_csar_zip_file):\n        file_path = utils.create_csar_with_unique_artifact(\n            './tacker/tests/etc/samples/etsi/nfv/'\n            'sample_vnf_package_csar_in_meta_and_manifest_false_source')\n        self.addCleanup(os.remove, file_path)\n        exc = self.assertRaises(exceptions.InvalidCSAR,\n                                csar_utils.load_csar_data,\n                                self.context, constants.UUID, file_path)\n        artifact_path = 'Scripts/install.s'\n        msg = (('The path(\"%(artifact_path)s\") of '\n                'artifact Source is an invalid value.') %\n               {'artifact_path': artifact_path})\n        self.assertEqual(msg, exc.format_message())"}, {"file_path": "/tacker-yoga-eom/tacker-yoga-eom/tacker/tests/unit/common/test_csar_utils.py", "class_name": "TestCSARUtils", "function_name": "test_load_csar_data_false_algorithm_with_vnf_artifact", "code": "\n    def test_load_csar_data_false_algorithm_with_vnf_artifact(\n            self, mock_extract_csar_zip_file):\n        file_path = utils.create_csar_with_unique_artifact(\n            './tacker/tests/etc/samples/etsi/nfv/'\n            'sample_vnf_package_csar_in_meta_and_manifest_false_algorithm')\n        self.addCleanup(os.remove, file_path)\n        exc = self.assertRaises(exceptions.InvalidCSAR,\n                                csar_utils.load_csar_data,\n                                self.context, constants.UUID, file_path)\n        algorithm = 'sha-255'\n        artifact_path = 'Scripts/install.sh'\n        msg = (('The algorithm(\"%(algorithm)s\") of '\n                'artifact(\"%(artifact_path)s\") is '\n                'an invalid value.') %\n               {'algorithm': algorithm,\n                'artifact_path': artifact_path})\n        self.assertEqual(msg, exc.format_message())"}]}, {"git_group": "yitu-opensource", "git_name": "MobileNeXt", "version": "master", "language": "Python", "project_name": "MobileNeXt-master.zip", "file_path": "/MobileNeXt-master/MobileNeXt-master/mobile_deployment/tensorflow/slim/models/official/modeling/hyperparams/params_dict.py", "file_name": "params_dict.py", "focal_class": null, "focal_name": "override_params_dict", "focal_parameter": ["params", "dict_or_string_or_yaml_file", "is_strict"], "solution": "def override_params_dict(params, dict_or_string_or_yaml_file, is_strict):\n  if not dict_or_string_or_yaml_file:\n    return params\n  if isinstance(dict_or_string_or_yaml_file, dict):\n    params.override(dict_or_string_or_yaml_file, is_strict)\n  elif isinstance(dict_or_string_or_yaml_file, six.string_types):\n    try:\n      dict_or_string_or_yaml_file = (\n          nested_csv_str_to_json_str(dict_or_string_or_yaml_file))\n    except ValueError:\n      pass\n    params_dict = yaml.load(dict_or_string_or_yaml_file)\n    if isinstance(params_dict, dict):\n      params.override(params_dict, is_strict)\n    else:\n      with tf.io.gfile.GFile(dict_or_string_or_yaml_file) as f:\n        params.override(yaml.load(f), is_strict)\n  else:\n    raise ValueError('Unknown input type to parse.')\n  return params", "function_signature": "def override_params_dict(params, dict_or_string_or_yaml_file, is_strict) :", "left_context": "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"A parameter dictionary class which supports the nest structure.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport re\n\nimport six\nimport tensorflow as tf\nimport yaml\n\n# regex pattern that matches on key-value pairs in a comma-separated\n# key-value pair string. It splits each k-v pair on the = sign, and\n# matches on values that are within single quotes, double quotes, single\n# values (e.g. floats, ints, etc.), and a lists within brackets.\n_PARAM_RE = re.compile(r\"\"\"\n  (?P<name>[a-zA-Z][\\w\\.]*)    # variable name: \"var\" or \"x\"\n  \\s*=\\s*\n  ((?P<val>\\'(.*?)\\'           # single quote\n  |\n  \\\"(.*?)\\\"                    # double quote\n  |\n  [^,\\[]*                      # single value\n  |\n  \\[[^\\]]*\\]))                 # list of values\n  ($|,\\s*)\"\"\", re.VERBOSE)\n\n# pylint: disable=anomalous-backslash-in-string\n_CONST_VALUE_RE = re.compile('(\\d.*|-\\d.*|None)')\n# pylint: enable=anomalous-backslash-in-string\n\n\nclass ParamsDict(object):\n  \"\"\"A hyperparameter container class.\"\"\"\n\n  RESERVED_ATTR = ['_locked', '_restrictions']\n\n  def __init__(self, default_params=None, restrictions=None):\n    \"\"\"Instantiate a ParamsDict.\n\n    Instantiate a ParamsDict given a set of default parameters and a list of\n    restrictions. Upon initialization, it validates itself by checking all the\n    defined restrictions, and raise error if it finds inconsistency.\n\n    Args:\n      default_params: a Python dict or another ParamsDict object including the\n        default parameters to initialize.\n      restrictions: a list of strings, which define a list of restrictions to\n        ensure the consistency of different parameters internally. Each\n        restriction string is defined as a binary relation with a set of\n        operators, including {'==', '!=',  '<', '<=', '>', '>='}.\n    \"\"\"\n    self._locked = False\n    self._restrictions = []\n    if restrictions:\n      self._restrictions = restrictions\n    if default_params is None:\n      default_params = {}\n    self.override(default_params, is_strict=False)\n    self.validate()\n\n  def _set(self, k, v):\n    if isinstance(v, dict):\n      self.__dict__[k] = ParamsDict(v)\n    else:\n      self.__dict__[k] = copy.deepcopy(v)\n\n  def __setattr__(self, k, v):\n    \"\"\"Sets the value of the existing key.\n\n    Note that this does not allow directly defining a new key. Use the\n    `override` method with `is_strict=False` instead.\n\n    Args:\n      k: the key string.\n      v: the value to be used to set the key `k`.\n\n    Raises:\n      KeyError: if k is not defined in the ParamsDict.\n    \"\"\"\n    if k not in ParamsDict.RESERVED_ATTR:\n      if k not in self.__dict__.keys():\n        raise KeyError('The key `%{}` does not exist. '\n                       'To extend the existing keys, use '\n                       '`override` with `is_strict` = True.'.format(k))\n      if self._locked:\n        raise ValueError('The ParamsDict has been locked. '\n                         'No change is allowed.')\n    self._set(k, v)\n\n  def __getattr__(self, k):\n    \"\"\"Gets the value of the existing key.\n\n    Args:\n      k: the key string.\n\n    Returns:\n      the value of the key.\n\n    Raises:\n      AttributeError: if k is not defined in the ParamsDict.\n    \"\"\"\n    if k not in self.__dict__.keys():\n      raise AttributeError('The key `{}` does not exist. '.format(k))\n    return self.__dict__[k]\n\n  def __contains__(self, key):\n    \"\"\"Implements the membership test operator.\"\"\"\n    return key in self.__dict__\n\n  def get(self, key, value=None):\n    \"\"\"Accesses through built-in dictionary get method.\"\"\"\n    return self.__dict__.get(key, value)\n\n  def __delattr__(self, k):\n    \"\"\"Deletes the key and removes its values.\n\n    Args:\n      k: the key string.\n\n    Raises:\n      AttributeError: if k is reserverd or not defined in the ParamsDict.\n      ValueError: if the ParamsDict instance has been locked.\n    \"\"\"\n    if k in ParamsDict.RESERVED_ATTR:\n      raise AttributeError('The key `{}` is reserved. No change is allowes. '\n                           .format(k))\n    if k not in self.__dict__.keys():\n      raise AttributeError('The key `{}` does not exist. '.format(k))\n    if self._locked:\n      raise ValueError('The ParamsDict has been locked. No change is allowed.')\n    del self.__dict__[k]\n\n  def override(self, override_params, is_strict=True):\n    \"\"\"Override the ParamsDict with a set of given params.\n\n    Args:\n      override_params: a dict or a ParamsDict specifying the parameters to\n        be overridden.\n      is_strict: a boolean specifying whether override is strict or not. If\n        True, keys in `override_params` must be present in the ParamsDict.\n        If False, keys in `override_params` can be different from what is\n        currently defined in the ParamsDict. In this case, the ParamsDict will\n        be extended to include the new keys.\n    \"\"\"\n    if self._locked:\n      raise ValueError('The ParamsDict has been locked. No change is allowed.')\n    if isinstance(override_params, ParamsDict):\n      override_params = override_params.as_dict()\n    self._override(override_params, is_strict)  # pylint: disable=protected-access\n\n  def _override(self, override_dict, is_strict=True):\n    \"\"\"The implementation of `override`.\"\"\"\n    for k, v in six.iteritems(override_dict):\n      if k in ParamsDict.RESERVED_ATTR:\n        raise KeyError('The key `%{}` is internally reserved. '\n                       'Can not be overridden.')\n      if k not in self.__dict__.keys():\n        if is_strict:\n          raise KeyError('The key `{}` does not exist. '\n                         'To extend the existing keys, use '\n                         '`override` with `is_strict` = False.'.format(k))\n        else:\n          self._set(k, v)\n      else:\n        if isinstance(v, dict):\n          self.__dict__[k]._override(v, is_strict)  # pylint: disable=protected-access\n        elif isinstance(v, ParamsDict):\n          self.__dict__[k]._override(v.as_dict(), is_strict)  # pylint: disable=protected-access\n        else:\n          self.__dict__[k] = copy.deepcopy(v)\n\n  def lock(self):\n    \"\"\"Makes the ParamsDict immutable.\"\"\"\n    self._locked = True\n\n  def as_dict(self):\n    \"\"\"Returns a dict representation of ParamsDict.\n\n    For the nested ParamsDict, a nested dict will be returned.\n    \"\"\"\n    params_dict = {}\n    for k, v in six.iteritems(self.__dict__):\n      if k not in ParamsDict.RESERVED_ATTR:\n        if isinstance(v, ParamsDict):\n          params_dict[k] = v.as_dict()\n        else:\n          params_dict[k] = copy.deepcopy(v)\n    return params_dict\n\n  def validate(self):\n    \"\"\"Validate the parameters consistency based on the restrictions.\n\n    This method validates the internal consistency using the pre-defined list of\n    restrictions. A restriction is defined as a string which specfiies a binary\n    operation. The supported binary operations are {'==', '!=', '<', '<=', '>',\n    '>='}. Note that the meaning of these operators are consistent with the\n    underlying Python immplementation. Users should make sure the define\n    restrictions on their type make sense.\n\n    For example, for a ParamsDict like the following\n    ```\n    a:\n      a1: 1\n      a2: 2\n    b:\n      bb:\n        bb1: 10\n        bb2: 20\n      ccc:\n        a1: 1\n        a3: 3\n    ```\n    one can define two restrictions like this\n    ['a.a1 == b.ccc.a1', 'a.a2 <= b.bb.bb2']\n\n    What it enforces are:\n     - a.a1 = 1 == b.ccc.a1 = 2\n     - a.a2 = 2 <= b.bb.bb2 = 20\n\n    Raises:\n      KeyError: if any of the following happens\n        (1) any of parameters in any of restrictions is not defined in\n            ParamsDict,\n        (2) any inconsistency violating the restriction is found.\n      ValueError: if the restriction defined in the string is not supported.\n    \"\"\"\n    def _get_kv(dotted_string, params_dict):\n      \"\"\"Get keys and values indicated by dotted_string.\"\"\"\n      if _CONST_VALUE_RE.match(dotted_string) is not None:\n        const_str = dotted_string\n        if const_str == 'None':\n          constant = None\n        else:\n          constant = float(const_str)\n        return None, constant\n      else:\n        tokenized_params = dotted_string.split('.')\n        v = params_dict\n        for t in tokenized_params:\n          v = v[t]\n        return tokenized_params[-1], v\n\n    def _get_kvs(tokens, params_dict):\n      if len(tokens) != 2:\n        raise ValueError('Only support binary relation in restriction.')\n      stripped_tokens = [t.strip() for t in tokens]\n      left_k, left_v = _get_kv(stripped_tokens[0], params_dict)\n      right_k, right_v = _get_kv(stripped_tokens[1], params_dict)\n      return left_k, left_v, right_k, right_v\n\n    params_dict = self.as_dict()\n    for restriction in self._restrictions:\n      if '==' in restriction:\n        tokens = restriction.split('==')\n        _, left_v, _, right_v = _get_kvs(tokens, params_dict)\n        if left_v != right_v:\n          raise KeyError('Found inconsistncy between key `{}` and key `{}`.'\n                         .format(tokens[0], tokens[1]))\n      elif '!=' in restriction:\n        tokens = restriction.split('!=')\n        _, left_v, _, right_v = _get_kvs(tokens, params_dict)\n        if left_v == right_v:\n          raise KeyError('Found inconsistncy between key `{}` and key `{}`.'\n                         .format(tokens[0], tokens[1]))\n      elif '<' in restriction:\n        tokens = restriction.split('<')\n        _, left_v, _, right_v = _get_kvs(tokens, params_dict)\n        if left_v >= right_v:\n          raise KeyError('Found inconsistncy between key `{}` and key `{}`.'\n                         .format(tokens[0], tokens[1]))\n      elif '<=' in restriction:\n        tokens = restriction.split('<=')\n        _, left_v, _, right_v = _get_kvs(tokens, params_dict)\n        if left_v > right_v:\n          raise KeyError('Found inconsistncy between key `{}` and key `{}`.'\n                         .format(tokens[0], tokens[1]))\n      elif '>' in restriction:\n        tokens = restriction.split('>')\n        _, left_v, _, right_v = _get_kvs(tokens, params_dict)\n        if left_v <= right_v:\n          raise KeyError('Found inconsistncy between key `{}` and key `{}`.'\n                         .format(tokens[0], tokens[1]))\n      elif '>=' in restriction:\n        tokens = restriction.split('>=')\n        _, left_v, _, right_v = _get_kvs(tokens, params_dict)\n        if left_v < right_v:\n          raise KeyError('Found inconsistncy between key `{}` and key `{}`.'\n                         .format(tokens[0], tokens[1]))\n      else:\n        raise ValueError('Unsupported relation in restriction.')\n\n\ndef read_yaml_to_params_dict(file_path):\n  \"\"\"Reads a YAML file to a ParamsDict.\"\"\"\n  with tf.io.gfile.GFile(file_path, 'r') as f:\n    params_dict = yaml.load(f)\n    return ParamsDict(params_dict)\n\n\ndef save_params_dict_to_yaml(params, file_path):\n  \"\"\"Saves the input ParamsDict to a YAML file.\"\"\"\n  with tf.io.gfile.GFile(file_path, 'w') as f:\n    def _my_list_rep(dumper, data):\n      # u'tag:yaml.org,2002:seq' is the YAML internal tag for sequence.\n      return dumper.represent_sequence(\n          u'tag:yaml.org,2002:seq', data, flow_style=True)\n    yaml.add_representer(list, _my_list_rep)\n    yaml.dump(params.as_dict(), f, default_flow_style=False)\n\n\ndef nested_csv_str_to_json_str(csv_str):\n  \"\"\"Converts a nested (using '.') comma-separated k=v string to a JSON string.\n\n  Converts a comma-separated string of key/value pairs that supports\n  nesting of keys to a JSON string. Nesting is implemented using\n  '.' between levels for a given key.\n\n  Spacing between commas and = is supported (e.g. there is no difference between\n  \"a=1,b=2\", \"a = 1, b = 2\", or \"a=1, b=2\") but there should be no spaces before\n  keys or after values (e.g. \" a=1,b=2\" and \"a=1,b=2 \" are not supported).\n\n  Note that this will only support values supported by CSV, meaning\n  values such as nested lists (e.g. \"a=[[1,2,3],[4,5,6]]\") are not\n  supported. Strings are supported as well, e.g. \"a='hello'\".\n\n  An example conversion would be:\n\n  \"a=1, b=2, c.a=2, c.b=3, d.a.a=5\"\n\n  to\n\n  \"{ a: 1, b : 2, c: {a : 2, b : 3}, d: {a: {a : 5}}}\"\n\n  Args:\n    csv_str: the comma separated string.\n\n  Returns:\n    the converted JSON string.\n\n  Raises:\n    ValueError: If csv_str is not in a comma separated string or\n      if the string is formatted incorrectly.\n  \"\"\"\n  if not csv_str:\n    return ''\n\n  formatted_entries = []\n  nested_map = collections.defaultdict(list)\n  pos = 0\n  while pos < len(csv_str):\n    m = _PARAM_RE.match(csv_str, pos)\n    if not m:\n      raise ValueError('Malformed hyperparameter value while parsing '\n                       'CSV string: %s' % csv_str[pos:])\n    pos = m.end()\n    # Parse the values.\n    m_dict = m.groupdict()\n    name = m_dict['name']\n    v = m_dict['val']\n\n    # If a GCS path (e.g. gs://...) is provided, wrap this in quotes\n    # as yaml.load would otherwise throw an exception\n    if re.match(r'(?=[^\\\"\\'])(?=[gs://])', v):\n      v = '\\'{}\\''.format(v)\n\n    name_nested = name.split('.')\n    if len(name_nested) > 1:\n      grouping = name_nested[0]\n      value = '.'.join(name_nested[1:]) + '=' + v\n      nested_map[grouping].append(value)\n    else:\n      formatted_entries.append('%s : %s' % (name, v))\n\n  for grouping, value in nested_map.items():\n    value = ','.join(value)\n    value = nested_csv_str_to_json_str(value)\n    formatted_entries.append('%s : %s' % (grouping, value))\n  return '{' + ', '.join(formatted_entries) + '}'\n\n", "right_context": "", "import_text": ["collections", "copy", "re", "six", "tensorflow", "yaml"], "prompt": "\"\"\"\nDescription: This function is used to override parameters in a dictionary.\n\nArgs:\n    params (dict): The dictionary of parameters to be overridden.\n    dict_or_string_or_yaml_file (dict, str, file): The source of the parameters to override. It can be a dictionary, a string, or a YAML file.\n    is_strict (bool): A flag indicating whether to enforce strict parameter overriding.\n\nReturns:\n    dict: The updated dictionary of parameters.\n\"\"\"", "comment": "  \"\"\"Override a given ParamsDict using a dict, JSON/YAML/CSV string or YAML file.\n\n  The logic of the function is outlined below:\n  1. Test that the input is a dict. If not, proceed to 2.\n  2. Tests that the input is a string. If not, raise unknown ValueError\n  2.1. Test if the string is in a CSV format. If so, parse.\n  If not, proceed to 2.2.\n  2.2. Try loading the string as a YAML/JSON. If successful, parse to\n  dict and use it to override. If not, proceed to 2.3.\n  2.3. Try using the string as a file path and load the YAML file.\n\n  Args:\n    params: a ParamsDict object to be overridden.\n    dict_or_string_or_yaml_file: a Python dict, JSON/YAML/CSV string or\n      path to a YAML file specifying the parameters to be overridden.\n    is_strict: a boolean specifying whether override is strict or not.\n\n  Returns:\n    params: the overridden ParamsDict object.\n\n  Raises:\n    ValueError: if failed to override the parameters.\n  \"\"\"", "prompt_is_gen_from_api": true, "function_dependencies": ["yaml.load", "tensorflow.io.gfile.GFile"], "project_create_time": "2020-07-14T06:44:23+00:00", "project_update_time": "2024-03-28T05:41:27+00:00", "file_create_time": "2020-10-18T15:09:02Z", "file_update_time": "2020-10-18T15:09:02Z", "function_update_time": "2020-10-18T15:09:02Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["yaml.load"], "test_function": [{"file_path": "/MobileNeXt-master/MobileNeXt-master/mobile_deployment/tensorflow/slim/models/official/modeling/hyperparams/params_dict_test.py", "class_name": "ParamsDictIOTest", "function_name": "test_override_params_dict_using_json_string", "code": "\n  def test_override_params_dict_using_json_string(self):\n    params = params_dict.ParamsDict({\n        'a': 1, 'b': {'b1': 2, 'b2': [2, 3],},\n        'd': {'d1': {'d2': 'hello'}}, 'e': False})\n    override_json_string = \"{ b: { b2: [3, 4] }, d: { d1: { d2: 'hi' } } }\"\n    params = params_dict.override_params_dict(\n        params, override_json_string, is_strict=True)\n    self.assertEqual(1, params.a)\n    self.assertEqual(2, params.b.b1)\n    self.assertEqual([3, 4], params.b.b2)\n    self.assertEqual('hi', params.d.d1.d2)\n    self.assertEqual(False, params.e)"}, {"file_path": "/MobileNeXt-master/MobileNeXt-master/mobile_deployment/tensorflow/slim/models/official/modeling/hyperparams/params_dict_test.py", "class_name": "ParamsDictIOTest", "function_name": "test_override_params_dict_using_csv_string", "code": "\n  def test_override_params_dict_using_csv_string(self):\n    params = params_dict.ParamsDict({\n        'a': 1, 'b': {'b1': 2, 'b2': [2, 3],},\n        'd': {'d1': {'d2': 'hello'}}, 'e': False})\n    override_csv_string = \"b.b2=[3,4], d.d1.d2='hi, world', e=gs://test\"\n    params = params_dict.override_params_dict(\n        params, override_csv_string, is_strict=True)\n    self.assertEqual(1, params.a)\n    self.assertEqual(2, params.b.b1)\n    self.assertEqual([3, 4], params.b.b2)\n    self.assertEqual('hi, world', params.d.d1.d2)\n    self.assertEqual('gs://test', params.e)"}, {"file_path": "/MobileNeXt-master/MobileNeXt-master/mobile_deployment/tensorflow/slim/models/official/modeling/hyperparams/params_dict_test.py", "class_name": "ParamsDictIOTest", "function_name": "test_override_params_dict_using_yaml_file", "code": "\n  def test_override_params_dict_using_yaml_file(self):\n    params = params_dict.ParamsDict({\n        'a': 1, 'b': 2.5, 'c': [3, 4], 'd': 'hello', 'e': False})\n    override_yaml_file = self.write_temp_file(\n        'params.yaml', r\"\"\"\n        b: 5.2\n        c: [30, 40]\n        \"\"\")\n    params = params_dict.override_params_dict(\n        params, override_yaml_file, is_strict=True)\n    self.assertEqual(1, params.a)\n    self.assertEqual(5.2, params.b)\n    self.assertEqual([30, 40], params.c)\n    self.assertEqual('hello', params.d)\n    self.assertEqual(False, params.e)"}]}, {"git_group": "GoogleCloudPlatform", "git_name": "covid-19-open-data", "version": "main", "language": "Python", "project_name": "covid-19-open-data-main.zip", "file_path": "/covid-19-open-data-main/covid-19-open-data-main/src/lib/cast.py", "file_name": "cast.py", "focal_class": null, "focal_name": "age_group", "focal_parameter": [], "solution": "def age_group(age: int, bin_size: int = 10, age_cutoff: int = 90) -> str:\n    if pandas.isna(age) or age < 0:\n        raise ValueError(f\"Invalid age value: {age}\")\n\n    if age >= age_cutoff:\n        return f\"{age_cutoff}-\"\n\n    bin_idx = age // bin_size\n    lo = int(bin_idx * bin_size)\n    hi = lo + bin_size - 1\n    return f\"{lo}-{hi}\"", "function_signature": "def age_group(age: int, bin_size: int = 10, age_cutoff: int = 90) -> str :", "left_context": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport warnings\nfrom typing import Any, Dict, Callable, Optional\n\nimport pandas\n\n\ndef _clean_numeric(value: Any) -> str:\n    if not isinstance(value, str):\n        value = str(value)\n    if \",\" in value:\n        value = value.replace(\",\", \"\")\n    if value.startswith(\"\u2212\"):\n        value = value.replace(\"\u2212\", \"-\")\n    return value\n\n\ndef isna(value: Any, skip_pandas_nan: bool = False) -> bool:\n    if not skip_pandas_nan and pandas.isna(value):\n        return True\n    if value is None:\n        return True\n    if value != value:\n        # NaN != NaN\n        return True\n    return False\n\n\ndef safe_float_cast(value: Any, skip_pandas_nan: bool = False) -> Optional[float]:\n    if isinstance(value, float):\n        return value\n    if isinstance(value, int):\n        return float(value)\n    if isna(value, skip_pandas_nan=skip_pandas_nan):\n        return None\n    if value == \"\":\n        # Handle common special case to avoid unnecessary try-catch\n        return None\n    try:\n        value = _clean_numeric(value)\n        return float(value)\n    except:\n        return None\n\n\ndef safe_int_cast(value: Any, skip_pandas_nan: bool = False) -> Optional[int]:\n    if isinstance(value, int):\n        return value\n    try:\n        # Converting to float first might seem inefficient, but we want to\n        # implicitly round numbers to the nearest integer\n        value = safe_float_cast(value, skip_pandas_nan=skip_pandas_nan)\n        return int(value)\n    except:\n        return None\n\n\ndef safe_str_cast(value: Any, skip_pandas_nan: bool = False) -> Optional[str]:\n    if isinstance(value, str):\n        return value\n    if isna(value, skip_pandas_nan=skip_pandas_nan):\n        return None\n    try:\n        return str(value)\n    except:\n        return None\n\n\ndef safe_datetime_parse(\n    value: str, date_format: str = None, warn: bool = False\n) -> Optional[datetime.datetime]:\n    try:\n        return (\n            datetime.datetime.fromisoformat(str(value))\n            if not date_format\n            else datetime.datetime.strptime(str(value), date_format)\n        )\n    except ValueError as exc:\n        if warn:\n            warnings.warn(\"Could not parse date {} using format {}\".format(value, date_format))\n        return None\n\n\ndef column_converters(schema: Dict[str, Any]) -> Dict[str, Callable]:\n    converters: Dict[str, Callable] = {}\n    for column, dtype in schema.items():\n        if dtype == \"int\" or dtype == pandas.Int64Dtype():\n            converters[column] = safe_int_cast\n        elif dtype == \"float\":\n            converters[column] = safe_float_cast\n        elif dtype == \"str\":\n            converters[column] = safe_str_cast\n        else:\n            raise ValueError(f\"Unsupported dtype {dtype} for column {column}\")\n    return converters\n\n", "right_context": "\n\ndef numeric_code_as_string(code: Any, digits: int = 0) -> str:\n    \"\"\"\n    Converts a code, which is typically a (potentially null) number, into its integer string\n    representation. This is very convenient to parse things like FIPS codes.\n\n    Arguments:\n        code: The input to cast into a string.\n        digits: The number of digits to force on the output, left-padding with zeroes. If this\n            argument is <= 0 then the output is unpadded.\n    Returns:\n        str: The input cast as a string, or None if it could not be converted into an integer.\n    \"\"\"\n    code = safe_int_cast(code)\n    if code is None:\n        return code\n    else:\n        fmt = f\"%0{digits}d\" if digits > 0 else \"%d\"\n        return fmt % code\n\n\ndef parse_google_sheets_date(date_like: Any) -> Optional[str]:\n    \"\"\" Import a date-like object from Google Sheets represented as number of days since 1900. \"\"\"\n    date_like = safe_int_cast(date_like)\n    if date_like:\n        return (datetime.date(1900, 1, 1) + datetime.timedelta(days=date_like)).isoformat()\n    else:\n        return None\n", "import_text": ["datetime", "warnings", "typing.Any", "typing.Dict", "typing.Callable", "typing.Optional", "pandas"], "prompt": "\"\"\"\nDescription: This function categorizes an age into an age group based on a specified bin size and cutoff age.\n\nArgs:\n    age (int): The age to be categorized.\n    bin_size (int, optional): The size of the age group bins. Defaults to 10.\n    age_cutoff (int, optional): The age above which all ages are grouped together. Defaults to 90.\n\nReturns:\n    str: The age group as a string in the format \"lo-hi\" or \"cutoff-\".\n\nRaises:\n    ValueError: If the age is not a non-negative number or NaN.\n\nNotes:\n    This function uses the pandas.isna function to check if the age is NaN.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Categorical age group given a specific age, codified into a function to enforce consistency.\n    \"\"\"", "function_dependencies": ["pandas.isna"], "project_create_time": "2020-07-23T23:43:51+00:00", "project_update_time": "2024-04-11T02:00:27+00:00", "file_create_time": "2020-07-02T14:36:08Z", "file_update_time": "2020-12-18T11:34:34Z", "function_update_time": "2020-12-18T11:34:34Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["pandas.isna"], "test_function": [{"file_path": "/covid-19-open-data-main/covid-19-open-data-main/src/test/test_cast.py", "class_name": "TestCastFunctions", "function_name": "test_age_group_standard", "code": "\n    def test_age_group_standard(self):\n        self.assertEqual(\"0-9\", age_group(0, bin_size=10, age_cutoff=90))\n        self.assertEqual(\"0-9\", age_group(0.0, bin_size=10, age_cutoff=90))\n        self.assertEqual(\"0-9\", age_group(9, bin_size=10, age_cutoff=90))\n        self.assertEqual(\"10-19\", age_group(10, bin_size=10, age_cutoff=90))\n        self.assertEqual(\"10-19\", age_group(19, bin_size=10, age_cutoff=90))\n        self.assertEqual(\"90-\", age_group(90, bin_size=10, age_cutoff=90))\n        self.assertEqual(\"90-\", age_group(100, bin_size=10, age_cutoff=90))\n        self.assertEqual(\"90-\", age_group(110, bin_size=10, age_cutoff=90))\n        self.assertEqual(\"90-\", age_group(1e9, bin_size=10, age_cutoff=90))\n        self.assertRaises(ValueError, lambda: age_group(-1, bin_size=10, age_cutoff=90))\n        self.assertRaises(ValueError, lambda: age_group(None, bin_size=10, age_cutoff=90))\n        self.assertRaises(ValueError, lambda: age_group(numpy.nan, bin_size=10, age_cutoff=90))"}]}, {"git_group": "DoubleML", "git_name": "doubleml-for-py", "version": "0.7.1", "language": "Python", "project_name": "doubleml-for-py-0.7.1.zip", "file_path": "/doubleml-for-py-0.7.1/doubleml-for-py-0.7.1/doubleml/datasets.py", "file_name": "datasets.py", "focal_class": null, "focal_name": "make_confounded_irm_data", "focal_parameter": [], "solution": "def make_confounded_irm_data(n_obs=500, theta=5.0, cf_y=0.04, cf_d=0.04):\n    c = 0.0  # the confounding strength is only valid for c=0\n    dim_x = 5\n\n    # observed covariates\n    cov_mat = toeplitz([np.power(c, k) for k in range(dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x), cov_mat, size=[n_obs, ])\n\n    z_tilde_1 = np.exp(0.5*x[:, 0])\n    z_tilde_2 = 10 + x[:, 1] / (1 + np.exp(x[:, 0]))\n    z_tilde_3 = (0.6 + x[:, 0] * x[:, 2]/25)**3\n    z_tilde_4 = (20 + x[:, 1] + x[:, 3])**2\n\n    z_tilde = np.column_stack((z_tilde_1, z_tilde_2, z_tilde_3, z_tilde_4, x[:, 4:]))\n    z = (z_tilde - np.mean(z_tilde, axis=0)) / np.std(z_tilde, axis=0)\n\n    # error terms and unobserved confounder\n    var_eps_y = 5\n    eps_y = np.random.normal(loc=0, scale=np.sqrt(var_eps_y), size=n_obs)\n\n    # unobserved confounder\n    a_bounds = (-1, 1)\n    a = np.random.uniform(low=a_bounds[0], high=a_bounds[1], size=n_obs)\n\n    # get the required impact of the confounder on the propensity score\n    possible_coefs = np.arange(0.001, 0.4999, 0.001)\n    gamma_a = possible_coefs[(np.arctanh(2*possible_coefs) / (2*possible_coefs)) - 1 - cf_d/(1 - cf_d) >= 0][0]\n\n    # compute short and long form of riesz representer\n    m_long = 0.5 + gamma_a*a\n    m_short = 0.5 * np.ones_like(m_long)\n\n    u = np.random.uniform(low=0, high=1, size=n_obs)\n    d = 1.0 * (m_long >= u)\n\n    # short and long version of g\n    g_partial_reg = 210 + 27.4*z[:, 0] + 13.7*(z[:, 1] + z[:, 2] + z[:, 3])\n\n    dx = d * (x[:, 4] + 1)\n    d1x = x[:, 4] + 1\n    var_dx = np.var(dx)\n    cov_adx = np.cov(a, dx)[0, 1]\n\n    def f_g(beta_a):\n        g_diff = beta_a * (a - cov_adx / var_dx)\n        y_diff = eps_y + g_diff\n        return np.square(np.mean(np.square(g_diff)) / np.mean(np.square(y_diff)) - cf_y)\n    beta_a = minimize_scalar(f_g).x\n\n    g_short_d0 = g_partial_reg\n    g_short_d1 = (theta + beta_a * cov_adx / var_dx) * d1x + g_partial_reg\n    g_short = d * g_short_d1 + (1.0-d) * g_short_d0\n\n    g_long_d0 = g_partial_reg + beta_a * a\n    g_long_d1 = theta * d1x + g_partial_reg + beta_a * a\n    g_long = d * g_long_d1 + (1.0-d) * g_long_d0\n\n    y0 = g_long_d0 + eps_y\n    y1 = g_long_d1 + eps_y\n\n    y = d * y1 + (1.0-d) * y0\n\n    oracle_values = {'g_long': g_long,\n                     'g_short': g_short,\n                     'm_long': m_long,\n                     'm_short': m_short,\n                     'gamma_a': gamma_a,\n                     'beta_a': beta_a,\n                     'a': a,\n                     'y0': y0,\n                     'y1': y1,\n                     'z': z}\n\n    res_dict = {'x': x,\n                'y': y,\n                'd': d,\n                'oracle_values': oracle_values}\n\n    return res_dict", "function_signature": "def make_confounded_irm_data(n_obs=500, theta=5.0, cf_y=0.04, cf_d=0.04) :", "left_context": "import pandas as pd\nimport numpy as np\n\nfrom scipy.linalg import toeplitz\nfrom scipy.optimize import minimize_scalar\n\nfrom sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\nfrom sklearn.datasets import make_spd_matrix\n\nfrom .double_ml_data import DoubleMLData, DoubleMLClusterData\n\n_array_alias = ['array', 'np.ndarray', 'np.array', np.ndarray]\n_data_frame_alias = ['DataFrame', 'pd.DataFrame', pd.DataFrame]\n_dml_data_alias = ['DoubleMLData', DoubleMLData]\n_dml_cluster_data_alias = ['DoubleMLClusterData', DoubleMLClusterData]\n\n\ndef fetch_401K(return_type='DoubleMLData', polynomial_features=False):\n    \"\"\"\n    Data set on financial wealth and 401(k) plan participation.\n\n    Parameters\n    ----------\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n    polynomial_features :\n        If ``True`` polynomial features are added (see replication files of Chernozhukov et al. (2018)).\n\n    References\n    ----------\n    Abadie, A. (2003), Semiparametric instrumental variable estimation of treatment response models. Journal of\n    Econometrics, 113(2): 231-263.\n\n    Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W. and Robins, J. (2018),\n    Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21: C1-C68.\n    doi:`10.1111/ectj.12097 <https://doi.org/10.1111/ectj.12097>`_.\n    \"\"\"\n    url = 'https://github.com/VC2015/DMLonGitHub/raw/master/sipp1991.dta'\n    raw_data = pd.read_stata(url)\n\n    y_col = 'net_tfa'\n    d_cols = ['e401']\n    x_cols = ['age', 'inc', 'educ', 'fsize', 'marr', 'twoearn', 'db', 'pira', 'hown']\n\n    data = raw_data.copy()\n\n    if polynomial_features:\n        raise NotImplementedError('polynomial_features os not implemented yet for fetch_401K.')\n\n    if return_type in _data_frame_alias + _dml_data_alias:\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, y_col, d_cols, x_cols)\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef fetch_bonus(return_type='DoubleMLData', polynomial_features=False):\n    \"\"\"\n    Data set on the Pennsylvania Reemployment Bonus experiment.\n\n    Parameters\n    ----------\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n    polynomial_features :\n        If ``True`` polynomial features are added (see replication files of Chernozhukov et al. (2018)).\n\n    References\n    ----------\n    Bilias Y. (2000), Sequential Testing of Duration Data: The Case of Pennsylvania 'Reemployment Bonus' Experiment.\n    Journal of Applied Econometrics, 15(6): 575-594.\n\n    Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W. and Robins, J. (2018),\n    Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21: C1-C68.\n    doi:`10.1111/ectj.12097 <https://doi.org/10.1111/ectj.12097>`_.\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/VC2015/DMLonGitHub/master/penn_jae.dat'\n    raw_data = pd.read_csv(url, delim_whitespace=True)\n\n    ind = (raw_data['tg'] == 0) | (raw_data['tg'] == 4)\n    data = raw_data.copy()[ind]\n    data.reset_index(inplace=True)\n    data['tg'].replace(4, 1, inplace=True)\n    data['inuidur1'] = np.log(data['inuidur1'])\n\n    # variable dep as factor (dummy encoding)\n    dummy_enc = OneHotEncoder(drop='first', categories='auto').fit(data.loc[:, ['dep']])\n    xx = dummy_enc.transform(data.loc[:, ['dep']]).toarray()\n    data['dep1'] = xx[:, 0]\n    data['dep2'] = xx[:, 1]\n\n    y_col = 'inuidur1'\n    d_cols = ['tg']\n    x_cols = ['female', 'black', 'othrace',\n              'dep1', 'dep2',\n              'q2', 'q3', 'q4', 'q5', 'q6',\n              'agelt35', 'agegt54', 'durable', 'lusd', 'husd']\n\n    if polynomial_features:\n        poly = PolynomialFeatures(2, include_bias=False)\n        data_transf = poly.fit_transform(data[x_cols])\n        x_cols = list(poly.get_feature_names_out(x_cols))\n\n        data_transf = pd.DataFrame(data_transf, columns=x_cols)\n        data = pd.concat((data[[y_col] + d_cols], data_transf),\n                         axis=1, sort=False)\n\n    if return_type in _data_frame_alias + _dml_data_alias:\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, y_col, d_cols, x_cols)\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef _g(x):\n    return np.power(np.sin(x), 2)\n\n\ndef _m(x, nu=0., gamma=1.):\n    return 0.5/np.pi*(np.sinh(gamma))/(np.cosh(gamma)-np.cos(x-nu))\n\n\ndef make_plr_CCDDHNR2018(n_obs=500, dim_x=20, alpha=0.5, return_type='DoubleMLData', **kwargs):\n    \"\"\"\n    Generates data from a partially linear regression model used in Chernozhukov et al. (2018) for Figure 1.\n    The data generating process is defined as\n\n    .. math::\n\n        d_i &= m_0(x_i) + s_1 v_i, & &v_i \\\\sim \\\\mathcal{N}(0,1),\n\n        y_i &= \\\\alpha d_i + g_0(x_i) + s_2 \\\\zeta_i, & &\\\\zeta_i \\\\sim \\\\mathcal{N}(0,1),\n\n\n    with covariates :math:`x_i \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`, where  :math:`\\\\Sigma` is a matrix with entries\n    :math:`\\\\Sigma_{kj} = 0.7^{|j-k|}`.\n    The nuisance functions are given by\n\n    .. math::\n\n        m_0(x_i) &= a_0 x_{i,1} + a_1 \\\\frac{\\\\exp(x_{i,3})}{1+\\\\exp(x_{i,3})},\n\n        g_0(x_i) &= b_0 \\\\frac{\\\\exp(x_{i,1})}{1+\\\\exp(x_{i,1})} + b_1 x_{i,3}.\n\n    Parameters\n    ----------\n    n_obs :\n        The number of observations to simulate.\n    dim_x :\n        The number of covariates.\n    alpha :\n        The value of the causal parameter.\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s ``(x, y, d)``.\n    **kwargs\n        Additional keyword arguments to set non-default values for the parameters\n        :math:`a_0=1`, :math:`a_1=0.25`, :math:`s_1=1`, :math:`b_0=1`, :math:`b_1=0.25` or :math:`s_2=1`.\n\n    References\n    ----------\n    Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W. and Robins, J. (2018),\n    Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21: C1-C68.\n    doi:`10.1111/ectj.12097 <https://doi.org/10.1111/ectj.12097>`_.\n    \"\"\"\n    a_0 = kwargs.get('a_0', 1.)\n    a_1 = kwargs.get('a_1', 0.25)\n    s_1 = kwargs.get('s_1', 1.)\n\n    b_0 = kwargs.get('b_0', 1.)\n    b_1 = kwargs.get('b_1', 0.25)\n    s_2 = kwargs.get('s_2', 1.)\n\n    cov_mat = toeplitz([np.power(0.7, k) for k in range(dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x), cov_mat, size=[n_obs, ])\n\n    d = a_0 * x[:, 0] + a_1 * np.divide(np.exp(x[:, 2]), 1 + np.exp(x[:, 2])) \\\n        + s_1 * np.random.standard_normal(size=[n_obs, ])\n    y = alpha * d + b_0 * np.divide(np.exp(x[:, 0]), 1 + np.exp(x[:, 0])) \\\n        + b_1 * x[:, 2] + s_2 * np.random.standard_normal(size=[n_obs, ])\n\n    if return_type in _array_alias:\n        return x, y, d\n    elif return_type in _data_frame_alias + _dml_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_x)]\n        data = pd.DataFrame(np.column_stack((x, y, d)),\n                            columns=x_cols + ['y', 'd'])\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, 'y', 'd', x_cols)\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef make_plr_turrell2018(n_obs=100, dim_x=20, theta=0.5, return_type='DoubleMLData', **kwargs):\n    \"\"\"\n    Generates data from a partially linear regression model used in a blog article by Turrell (2018).\n    The data generating process is defined as\n\n    .. math::\n\n        d_i &= m_0(x_i' b) + v_i, & &v_i \\\\sim \\\\mathcal{N}(0,1),\n\n        y_i &= \\\\theta d_i + g_0(x_i' b) + u_i, & &u_i \\\\sim \\\\mathcal{N}(0,1),\n\n\n    with covariates :math:`x_i \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`, where  :math:`\\\\Sigma` is a random symmetric,\n    positive-definite matrix generated with :py:meth:`sklearn.datasets.make_spd_matrix`.\n    :math:`b` is a vector with entries :math:`b_j=\\\\frac{1}{j}` and the nuisance functions are given by\n\n    .. math::\n\n        m_0(x_i) &= \\\\frac{1}{2 \\\\pi} \\\\frac{\\\\sinh(\\\\gamma)}{\\\\cosh(\\\\gamma) - \\\\cos(x_i-\\\\nu)},\n\n        g_0(x_i) &= \\\\sin(x_i)^2.\n\n    Parameters\n    ----------\n    n_obs :\n        The number of observations to simulate.\n    dim_x :\n        The number of covariates.\n    theta :\n        The value of the causal parameter.\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s ``(x, y, d)``.\n    **kwargs\n        Additional keyword arguments to set non-default values for the parameters\n        :math:`\\\\nu=0`, or :math:`\\\\gamma=1`.\n\n    References\n    ----------\n    Turrell, A. (2018), Econometrics in Python part I - Double machine learning, Markov Wanderer: A blog on economics,\n    science, coding and data. `https://aeturrell.com/blog/posts/econometrics-in-python-parti-ml/\n    <https://aeturrell.com/blog/posts/econometrics-in-python-parti-ml/>`_.\n    \"\"\"\n    nu = kwargs.get('nu', 0.)\n    gamma = kwargs.get('gamma', 1.)\n\n    b = [1 / k for k in range(1, dim_x + 1)]\n    sigma = make_spd_matrix(dim_x)\n\n    x = np.random.multivariate_normal(np.zeros(dim_x), sigma, size=[n_obs, ])\n    G = _g(np.dot(x, b))\n    M = _m(np.dot(x, b), nu=nu, gamma=gamma)\n    d = M + np.random.standard_normal(size=[n_obs, ])\n    y = np.dot(theta, d) + G + np.random.standard_normal(size=[n_obs, ])\n\n    if return_type in _array_alias:\n        return x, y, d\n    elif return_type in _data_frame_alias + _dml_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_x)]\n        data = pd.DataFrame(np.column_stack((x, y, d)),\n                            columns=x_cols + ['y', 'd'])\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, 'y', 'd', x_cols)\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef make_irm_data(n_obs=500, dim_x=20, theta=0, R2_d=0.5, R2_y=0.5, return_type='DoubleMLData'):\n    \"\"\"\n    Generates data from a interactive regression (IRM) model.\n    The data generating process is defined as\n\n    .. math::\n\n        d_i &= 1\\\\left\\\\lbrace \\\\frac{\\\\exp(c_d x_i' \\\\beta)}{1+\\\\exp(c_d x_i' \\\\beta)} > v_i \\\\right\\\\rbrace, & &v_i\n        \\\\sim \\\\mathcal{U}(0,1),\n\n        y_i &= \\\\theta d_i + c_y x_i' \\\\beta d_i + \\\\zeta_i, & &\\\\zeta_i \\\\sim \\\\mathcal{N}(0,1),\n\n    with covariates :math:`x_i \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`, where  :math:`\\\\Sigma` is a matrix with entries\n    :math:`\\\\Sigma_{kj} = 0.5^{|j-k|}`.\n    :math:`\\\\beta` is a `dim_x`-vector with entries :math:`\\\\beta_j=\\\\frac{1}{j^2}` and the constants :math:`c_y` and\n    :math:`c_d` are given by\n\n    .. math::\n\n        c_y = \\\\sqrt{\\\\frac{R_y^2}{(1-R_y^2) \\\\beta' \\\\Sigma \\\\beta}}, \\\\qquad c_d =\n        \\\\sqrt{\\\\frac{(\\\\pi^2 /3) R_d^2}{(1-R_d^2) \\\\beta' \\\\Sigma \\\\beta}}.\n\n    The data generating process is inspired by a process used in the simulation experiment (see Appendix P) of Belloni\n    et al. (2017).\n\n    Parameters\n    ----------\n    n_obs :\n        The number of observations to simulate.\n    dim_x :\n        The number of covariates.\n    theta :\n        The value of the causal parameter.\n    R2_d :\n        The value of the parameter :math:`R_d^2`.\n    R2_y :\n        The value of the parameter :math:`R_y^2`.\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s ``(x, y, d)``.\n\n    References\n    ----------\n    Belloni, A., Chernozhukov, V., Fern\u00e1ndez\u2010Val, I. and Hansen, C. (2017). Program Evaluation and Causal Inference With\n    High\u2010Dimensional Data. Econometrica, 85: 233-298.\n    \"\"\"\n    # inspired by https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA12723, see suplement\n    v = np.random.uniform(size=[n_obs, ])\n    zeta = np.random.standard_normal(size=[n_obs, ])\n\n    cov_mat = toeplitz([np.power(0.5, k) for k in range(dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x), cov_mat, size=[n_obs, ])\n\n    beta = [1 / (k**2) for k in range(1, dim_x + 1)]\n    b_sigma_b = np.dot(np.dot(cov_mat, beta), beta)\n    c_y = np.sqrt(R2_y/((1-R2_y) * b_sigma_b))\n    c_d = np.sqrt(np.pi**2 / 3. * R2_d/((1-R2_d) * b_sigma_b))\n\n    xx = np.exp(np.dot(x, np.multiply(beta, c_d)))\n    d = 1. * ((xx/(1+xx)) > v)\n\n    y = d * theta + d * np.dot(x, np.multiply(beta, c_y)) + zeta\n\n    if return_type in _array_alias:\n        return x, y, d\n    elif return_type in _data_frame_alias + _dml_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_x)]\n        data = pd.DataFrame(np.column_stack((x, y, d)),\n                            columns=x_cols + ['y', 'd'])\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, 'y', 'd', x_cols)\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef make_iivm_data(n_obs=500, dim_x=20, theta=1., alpha_x=0.2, return_type='DoubleMLData'):\n    \"\"\"\n    Generates data from a interactive IV regression (IIVM) model.\n    The data generating process is defined as\n\n    .. math::\n\n        d_i &= 1\\\\left\\\\lbrace \\\\alpha_x Z + v_i > 0 \\\\right\\\\rbrace,\n\n        y_i &= \\\\theta d_i + x_i' \\\\beta + u_i,\n\n    with :math:`Z \\\\sim \\\\text{Bernoulli}(0.5)` and\n\n    .. math::\n\n        \\\\left(\\\\begin{matrix} u_i \\\\\\\\ v_i \\\\end{matrix} \\\\right) \\\\sim\n        \\\\mathcal{N}\\\\left(0, \\\\left(\\\\begin{matrix} 1 & 0.3 \\\\\\\\ 0.3 & 1 \\\\end{matrix} \\\\right) \\\\right).\n\n    The covariates :math:`x_i \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`, where  :math:`\\\\Sigma` is a matrix with entries\n    :math:`\\\\Sigma_{kj} = 0.5^{|j-k|}` and :math:`\\\\beta` is a `dim_x`-vector with entries\n    :math:`\\\\beta_j=\\\\frac{1}{j^2}`.\n\n    The data generating process is inspired by a process used in the simulation experiment of Farbmacher, Gruber and\n    Klaassen (2020).\n\n    Parameters\n    ----------\n    n_obs :\n        The number of observations to simulate.\n    dim_x :\n        The number of covariates.\n    theta :\n        The value of the causal parameter.\n    alpha_x :\n        The value of the parameter :math:`\\\\alpha_x`.\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s ``(x, y, d, z)``.\n\n    References\n    ----------\n    Farbmacher, H., Guber, R. and Klaa\u00dfen, S. (2020). Instrument Validity Tests with Causal Forests. MEA Discussion\n    Paper No. 13-2020. Available at SSRN: http://dx.doi.org/10.2139/ssrn.3619201.\n    \"\"\"\n    # inspired by https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3619201\n    xx = np.random.multivariate_normal(np.zeros(2),\n                                       np.array([[1., 0.3], [0.3, 1.]]),\n                                       size=[n_obs, ])\n    u = xx[:, 0]\n    v = xx[:, 1]\n\n    cov_mat = toeplitz([np.power(0.5, k) for k in range(dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x), cov_mat, size=[n_obs, ])\n\n    beta = [1 / (k**2) for k in range(1, dim_x + 1)]\n\n    z = np.random.binomial(p=0.5, n=1, size=[n_obs, ])\n    d = 1. * (alpha_x * z + v > 0)\n\n    y = d * theta + np.dot(x, beta) + u\n\n    if return_type in _array_alias:\n        return x, y, d, z\n    elif return_type in _data_frame_alias + _dml_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_x)]\n        data = pd.DataFrame(np.column_stack((x, y, d, z)),\n                            columns=x_cols + ['y', 'd', 'z'])\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, 'y', 'd', x_cols, 'z')\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef _make_pliv_data(n_obs=100, dim_x=20, theta=0.5, gamma_z=0.4, return_type='DoubleMLData'):\n    b = [1/k for k in range(1, dim_x+1)]\n    sigma = make_spd_matrix(dim_x)\n\n    x = np.random.multivariate_normal(np.zeros(dim_x), sigma, size=[n_obs, ])\n    G = _g(np.dot(x, b))\n    # instrument\n    z = _m(np.dot(x, b)) + np.random.standard_normal(size=[n_obs, ])\n    # treatment\n    M = _m(gamma_z * z + np.dot(x, b))\n    d = M + np.random.standard_normal(size=[n_obs, ])\n    y = np.dot(theta, d) + G + np.random.standard_normal(size=[n_obs, ])\n\n    if return_type in _array_alias:\n        return x, y, d, z\n    elif return_type in _data_frame_alias + _dml_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_x)]\n        data = pd.DataFrame(np.column_stack((x, y, d, z)),\n                            columns=x_cols + ['y', 'd', 'z'])\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, 'y', 'd', x_cols, 'z')\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef make_pliv_CHS2015(n_obs, alpha=1., dim_x=200, dim_z=150, return_type='DoubleMLData'):\n    \"\"\"\n    Generates data from a partially linear IV regression model used in Chernozhukov, Hansen and Spindler (2015).\n    The data generating process is defined as\n\n    .. math::\n\n        z_i &= \\\\Pi x_i + \\\\zeta_i,\n\n        d_i &= x_i' \\\\gamma + z_i' \\\\delta + u_i,\n\n        y_i &= \\\\alpha d_i + x_i' \\\\beta + \\\\varepsilon_i,\n\n    with\n\n    .. math::\n\n        \\\\left(\\\\begin{matrix} \\\\varepsilon_i \\\\\\\\ u_i \\\\\\\\ \\\\zeta_i \\\\\\\\ x_i \\\\end{matrix} \\\\right) \\\\sim\n        \\\\mathcal{N}\\\\left(0, \\\\left(\\\\begin{matrix} 1 & 0.6 & 0 & 0 \\\\\\\\ 0.6 & 1 & 0 & 0 \\\\\\\\\n        0 & 0 & 0.25 I_{p_n^z} & 0 \\\\\\\\ 0 & 0 & 0 & \\\\Sigma \\\\end{matrix} \\\\right) \\\\right)\n\n    where  :math:`\\\\Sigma` is a :math:`p_n^x \\\\times p_n^x` matrix with entries\n    :math:`\\\\Sigma_{kj} = 0.5^{|j-k|}` and :math:`I_{p_n^z}` is the :math:`p_n^z \\\\times p_n^z` identity matrix.\n    :math:`\\\\beta = \\\\gamma` is a :math:`p_n^x`-vector with entries :math:`\\\\beta_j=\\\\frac{1}{j^2}`,\n    :math:`\\\\delta` is a :math:`p_n^z`-vector with entries :math:`\\\\delta_j=\\\\frac{1}{j^2}`\n    and :math:`\\\\Pi = (I_{p_n^z}, 0_{p_n^z \\\\times (p_n^x - p_n^z)})`.\n\n    Parameters\n    ----------\n    n_obs :\n        The number of observations to simulate.\n    alpha :\n        The value of the causal parameter.\n    dim_x :\n        The number of covariates.\n    dim_z :\n        The number of instruments.\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s ``(x, y, d, z)``.\n\n    References\n    ----------\n    Chernozhukov, V., Hansen, C. and Spindler, M. (2015), Post-Selection and Post-Regularization Inference in Linear\n    Models with Many Controls and Instruments. American Economic Review: Papers and Proceedings, 105 (5): 486-90.\n    \"\"\"\n    assert dim_x >= dim_z\n    # see https://assets.aeaweb.org/asset-server/articles-attachments/aer/app/10505/P2015_1022_app.pdf\n    xx = np.random.multivariate_normal(np.zeros(2),\n                                       np.array([[1., 0.6], [0.6, 1.]]),\n                                       size=[n_obs, ])\n    epsilon = xx[:, 0]\n    u = xx[:, 1]\n\n    sigma = toeplitz([np.power(0.5, k) for k in range(0, dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x),\n                                      sigma,\n                                      size=[n_obs, ])\n\n    I_z = np.eye(dim_z)\n    xi = np.random.multivariate_normal(np.zeros(dim_z),\n                                       0.25*I_z,\n                                       size=[n_obs, ])\n\n    beta = [1 / (k**2) for k in range(1, dim_x + 1)]\n    gamma = beta\n    delta = [1 / (k**2) for k in range(1, dim_z + 1)]\n    Pi = np.hstack((I_z, np.zeros((dim_z, dim_x-dim_z))))\n\n    z = np.dot(x, np.transpose(Pi)) + xi\n    d = np.dot(x, gamma) + np.dot(z, delta) + u\n    y = alpha * d + np.dot(x, beta) + epsilon\n\n    if return_type in _array_alias:\n        return x, y, d, z\n    elif return_type in _data_frame_alias + _dml_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_x)]\n        z_cols = [f'Z{i + 1}' for i in np.arange(dim_z)]\n        data = pd.DataFrame(np.column_stack((x, y, d, z)),\n                            columns=x_cols + ['y', 'd'] + z_cols)\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLData(data, 'y', 'd', x_cols, z_cols)\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef make_pliv_multiway_cluster_CKMS2021(N=25, M=25, dim_X=100, theta=1., return_type='DoubleMLClusterData', **kwargs):\n    \"\"\"\n    Generates data from a partially linear IV regression model with multiway cluster sample used in Chiang et al.\n    (2021). The data generating process is defined as\n\n    .. math::\n\n        Z_{ij} &= X_{ij}' \\\\xi_0 + V_{ij},\n\n        D_{ij} &= Z_{ij}' \\\\pi_{10} + X_{ij}' \\\\pi_{20} + v_{ij},\n\n        Y_{ij} &= D_{ij} \\\\theta + X_{ij}' \\\\zeta_0 + \\\\varepsilon_{ij},\n\n    with\n\n    .. math::\n\n        X_{ij} &= (1 - \\\\omega_1^X - \\\\omega_2^X) \\\\alpha_{ij}^X\n        + \\\\omega_1^X \\\\alpha_{i}^X + \\\\omega_2^X \\\\alpha_{j}^X,\n\n        \\\\varepsilon_{ij} &= (1 - \\\\omega_1^\\\\varepsilon - \\\\omega_2^\\\\varepsilon) \\\\alpha_{ij}^\\\\varepsilon\n        + \\\\omega_1^\\\\varepsilon \\\\alpha_{i}^\\\\varepsilon + \\\\omega_2^\\\\varepsilon \\\\alpha_{j}^\\\\varepsilon,\n\n        v_{ij} &= (1 - \\\\omega_1^v - \\\\omega_2^v) \\\\alpha_{ij}^v\n        + \\\\omega_1^v \\\\alpha_{i}^v + \\\\omega_2^v \\\\alpha_{j}^v,\n\n        V_{ij} &= (1 - \\\\omega_1^V - \\\\omega_2^V) \\\\alpha_{ij}^V\n        + \\\\omega_1^V \\\\alpha_{i}^V + \\\\omega_2^V \\\\alpha_{j}^V,\n\n    and :math:`\\\\alpha_{ij}^X, \\\\alpha_{i}^X, \\\\alpha_{j}^X \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`\n    where  :math:`\\\\Sigma` is a :math:`p_x \\\\times p_x` matrix with entries\n    :math:`\\\\Sigma_{kj} = s_X^{|j-k|}`.\n    Further\n\n    .. math::\n\n        \\\\left(\\\\begin{matrix} \\\\alpha_{ij}^\\\\varepsilon \\\\\\\\ \\\\alpha_{ij}^v \\\\end{matrix}\\\\right),\n        \\\\left(\\\\begin{matrix} \\\\alpha_{i}^\\\\varepsilon \\\\\\\\ \\\\alpha_{i}^v \\\\end{matrix}\\\\right),\n        \\\\left(\\\\begin{matrix} \\\\alpha_{j}^\\\\varepsilon \\\\\\\\ \\\\alpha_{j}^v \\\\end{matrix}\\\\right)\n        \\\\sim \\\\mathcal{N}\\\\left(0, \\\\left(\\\\begin{matrix} 1 & s_{\\\\varepsilon v} \\\\\\\\\n        s_{\\\\varepsilon v} & 1 \\\\end{matrix} \\\\right) \\\\right)\n\n\n    and :math:`\\\\alpha_{ij}^V, \\\\alpha_{i}^V, \\\\alpha_{j}^V \\\\sim \\\\mathcal{N}(0, 1)`.\n\n    Parameters\n    ----------\n    N :\n        The number of observations (first dimension).\n    M :\n        The number of observations (second dimension).\n    dim_X :\n        The number of covariates.\n    theta :\n        The value of the causal parameter.\n    return_type :\n        If ``'DoubleMLClusterData'`` or ``DoubleMLClusterData``, returns a ``DoubleMLClusterData`` object where\n        ``DoubleMLClusterData.data`` is a ``pd.DataFrame``.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s\n        ``(x, y, d, cluster_vars, z)``.\n    **kwargs\n        Additional keyword arguments to set non-default values for the parameters\n        :math:`\\\\pi_{10}=1.0`, :math:`\\\\omega_X = \\\\omega_{\\\\varepsilon} = \\\\omega_V = \\\\omega_v = (0.25, 0.25)`,\n        :math:`s_X = s_{\\\\varepsilon v} = 0.25`,\n        or the :math:`p_x`-vectors :math:`\\\\zeta_0 = \\\\pi_{20} = \\\\xi_0` with default entries\n        :math:`(\\\\zeta_{0})_j = 0.5^j`.\n\n    References\n    ----------\n    Chiang, H. D., Kato K., Ma, Y. and Sasaki, Y. (2021), Multiway Cluster Robust Double/Debiased Machine Learning,\n    Journal of Business & Economic Statistics,\n    doi: `10.1080/07350015.2021.1895815 <https://doi.org/10.1080/07350015.2021.1895815>`_,\n    arXiv:`1909.03489 <https://arxiv.org/abs/1909.03489>`_.\n    \"\"\"\n    # additional parameters specifiable via kwargs\n    pi_10 = kwargs.get('pi_10', 1.0)\n\n    xx = np.arange(1, dim_X + 1)\n    zeta_0 = kwargs.get('zeta_0', np.power(0.5, xx))\n    pi_20 = kwargs.get('pi_20', np.power(0.5, xx))\n    xi_0 = kwargs.get('xi_0', np.power(0.5, xx))\n\n    omega_X = kwargs.get('omega_X', np.array([0.25, 0.25]))\n    omega_epsilon = kwargs.get('omega_epsilon', np.array([0.25, 0.25]))\n    omega_v = kwargs.get('omega_v', np.array([0.25, 0.25]))\n    omega_V = kwargs.get('omega_V', np.array([0.25, 0.25]))\n\n    s_X = kwargs.get('s_X', 0.25)\n    s_epsilon_v = kwargs.get('s_epsilon_v', 0.25)\n\n    # use np.tile() and np.repeat() for repeating vectors in different styles, i.e.,\n    # np.tile([v1, v2, v3], 2) [v1, v2, v3, v1, v2, v3]\n    # np.repeat([v1, v2, v3], 2) [v1, v1, v2, v2, v3, v3]\n\n    alpha_V = np.random.normal(size=(N * M))\n    alpha_V_i = np.repeat(np.random.normal(size=N), M)\n    alpha_V_j = np.tile(np.random.normal(size=M), N)\n\n    cov_mat = np.array([[1, s_epsilon_v], [s_epsilon_v, 1]])\n    alpha_eps_v = np.random.multivariate_normal(np.zeros(2), cov_mat, size=[N * M, ])\n    alpha_eps = alpha_eps_v[:, 0]\n    alpha_v = alpha_eps_v[:, 1]\n\n    alpha_eps_v_i = np.random.multivariate_normal(np.zeros(2), cov_mat, size=[N, ])\n    alpha_eps_i = np.repeat(alpha_eps_v_i[:, 0], M)\n    alpha_v_i = np.repeat(alpha_eps_v_i[:, 1], M)\n\n    alpha_eps_v_j = np.random.multivariate_normal(np.zeros(2), cov_mat, size=[M, ])\n    alpha_eps_j = np.tile(alpha_eps_v_j[:, 0], N)\n    alpha_v_j = np.tile(alpha_eps_v_j[:, 1], N)\n\n    cov_mat = toeplitz([np.power(s_X, k) for k in range(dim_X)])\n    alpha_X = np.random.multivariate_normal(np.zeros(dim_X), cov_mat, size=[N * M, ])\n    alpha_X_i = np.repeat(np.random.multivariate_normal(np.zeros(dim_X), cov_mat, size=[N, ]),\n                          M, axis=0)\n    alpha_X_j = np.tile(np.random.multivariate_normal(np.zeros(dim_X), cov_mat, size=[M, ]),\n                        (N, 1))\n\n    # generate variables\n    x = (1 - omega_X[0] - omega_X[1]) * alpha_X \\\n        + omega_X[0] * alpha_X_i + omega_X[1] * alpha_X_j\n\n    eps = (1 - omega_epsilon[0] - omega_epsilon[1]) * alpha_eps \\\n        + omega_epsilon[0] * alpha_eps_i + omega_epsilon[1] * alpha_eps_j\n\n    v = (1 - omega_v[0] - omega_v[1]) * alpha_v \\\n        + omega_v[0] * alpha_v_i + omega_v[1] * alpha_v_j\n\n    V = (1 - omega_V[0] - omega_V[1]) * alpha_V \\\n        + omega_V[0] * alpha_V_i + omega_V[1] * alpha_V_j\n\n    z = np.matmul(x, xi_0) + V\n    d = z * pi_10 + np.matmul(x, pi_20) + v\n    y = d * theta + np.matmul(x, zeta_0) + eps\n\n    cluster_cols = ['cluster_var_i', 'cluster_var_j']\n    cluster_vars = pd.MultiIndex.from_product([range(N), range(M)]).to_frame(name=cluster_cols).reset_index(drop=True)\n\n    if return_type in _array_alias:\n        return x, y, d, cluster_vars.values, z\n    elif return_type in _data_frame_alias + _dml_cluster_data_alias:\n        x_cols = [f'X{i + 1}' for i in np.arange(dim_X)]\n        data = pd.concat((cluster_vars,\n                          pd.DataFrame(np.column_stack((x, y, d, z)), columns=x_cols + ['Y', 'D', 'Z'])),\n                         axis=1)\n        if return_type in _data_frame_alias:\n            return data\n        else:\n            return DoubleMLClusterData(data, 'Y', 'D', cluster_cols, x_cols, 'Z')\n    else:\n        raise ValueError('Invalid return_type.')\n\n\ndef make_did_SZ2020(n_obs=500, dgp_type=1, cross_sectional_data=False, return_type='DoubleMLData', **kwargs):\n    \"\"\"\n    Generates data from a difference-in-differences model used in Sant'Anna and Zhao (2020).\n    The data generating process is defined as follows. For a generic :math:`W=(W_1, W_2, W_3, W_4)^T`, let\n\n    .. math::\n\n        f_{reg}(W) &= 210 + 27.4 \\\\cdot W_1 +13.7 \\\\cdot (W_2 + W_3 + W_4),\n\n        f_{ps}(W) &= 0.75 \\\\cdot (-W_1 + 0.5 \\\\cdot W_2 -0.25 \\\\cdot W_3 - 0.1 \\\\cdot W_4).\n\n\n    Let :math:`X= (X_1, X_2, X_3, X_4)^T \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`, where  :math:`\\\\Sigma` is a matrix with entries\n    :math:`\\\\Sigma_{kj} = c^{|j-k|}`. The default value is  :math:`c = 0`, corresponding to the identity matrix.\n    Further, define :math:`Z_j = (\\\\tilde{Z_j} - \\\\mathbb{E}[\\\\tilde{Z}_j]) / \\\\sqrt{\\\\text{Var}(\\\\tilde{Z}_j)}`,\n    where :math:`\\\\tilde{Z}_1 = \\\\exp(0.5 \\\\cdot X_1)`, :math:`\\\\tilde{Z}_2 = 10 + X_2/(1 + \\\\exp(X_1))`,\n    :math:`\\\\tilde{Z}_3 = (0.6 + X_1 \\\\cdot X_3 / 25)^3` and :math:`\\\\tilde{Z}_4 = (20 + X_2 + X_4)^2`.\n    At first define\n\n    .. math::\n\n        Y_0(0) &= f_{reg}(W_{reg}) + \\\\nu(W_{reg}, D) + \\\\varepsilon_0,\n\n        Y_1(d) &= 2 \\\\cdot f_{reg}(W_{reg}) + \\\\nu(W_{reg}, D) + \\\\varepsilon_1(d),\n\n        p(W_{ps}) &= \\\\frac{\\\\exp(f_{ps}(W_{ps}))}{1 + \\\\exp(f_{ps}(W_{ps}))},\n\n        D &= 1\\\\{p(W_{ps}) \\\\ge U\\\\},\n\n    where :math:`\\\\varepsilon_0, \\\\varepsilon_1(d), d=0, 1` are independent standard normal random variables,\n    :math:`U \\\\sim \\\\mathcal{U}[0, 1]` is a independent standard uniform\n    and :math:`\\\\nu(W_{reg}, D)\\\\sim \\\\mathcal{N}(D \\\\cdot f_{reg}(W_{reg}),1)`.\n    The different data generating processes are defined via\n\n    .. math::\n\n        DGP1:\\\\quad W_{reg} &= Z \\\\quad W_{ps} = Z\n\n        DGP2:\\\\quad W_{reg} &= Z \\\\quad W_{ps} = X\n\n        DGP3:\\\\quad W_{reg} &= X \\\\quad W_{ps} = Z\n\n        DGP4:\\\\quad W_{reg} &= X \\\\quad W_{ps} = X\n\n        DGP5:\\\\quad W_{reg} &= Z \\\\quad W_{ps} = 0\n\n        DGP6:\\\\quad W_{reg} &= X \\\\quad W_{ps} = 0,\n\n    such that the last two settings correspond to an experimental setting with treatment probability\n    of :math:`P(D=1) = \\\\frac{1}{2}.`\n    For the panel data the outcome is already defined as the difference :math:`Y = Y_1(D) - Y_0(0)`.\n    For cross-sectional data the flag ``cross_sectional_data`` has to be set to ``True``.\n    Then the outcome will be defined to be\n\n    .. math::\n\n        Y = T \\\\cdot Y_1(D) + (1-T) \\\\cdot Y_0(0),\n\n    where :math:`T = 1\\\\{U_T\\\\le \\\\lambda_T \\\\}` with :math:`U_T\\\\sim \\\\mathcal{U}[0, 1]` and :math:`\\\\lambda_T=0.5`.\n    The true average treatment effect on the treated is zero for all data generating processes.\n\n    Parameters\n    ----------\n    n_obs :\n        The number of observations to simulate.\n    dgp_type :\n        The DGP to be used. Default value is ``1`` (integer).\n    cross_sectional_data :\n        Indicates whether the setting is uses cross-sectional or panel data. Default value is ``False``.\n    return_type :\n        If ``'DoubleMLData'`` or ``DoubleMLData``, returns a ``DoubleMLData`` object.\n\n        If ``'DataFrame'``, ``'pd.DataFrame'`` or ``pd.DataFrame``, returns a ``pd.DataFrame``.\n\n        If ``'array'``, ``'np.ndarray'``, ``'np.array'`` or ``np.ndarray``, returns ``np.ndarray``'s ``(x, y, d)``\n        or ``(x, y, d, t)``.\n    **kwargs\n        Additional keyword arguments to set non-default values for the parameter\n        :math:`xi=0.75`, :math:`c=0.0` and :math:`\\\\lambda_T=0.5`.\n\n    References\n    ----------\n    Sant\u2019Anna, P. H. and Zhao, J. (2020),\n    Doubly robust difference-in-differences estimators. Journal of Econometrics, 219(1), 101-122.\n    doi:`10.1016/j.jeconom.2020.06.003 <https://doi.org/10.1016/j.jeconom.2020.06.003>`_.\n    \"\"\"\n    xi = kwargs.get('xi', 0.75)\n    c = kwargs.get('c', 0.0)\n    lambda_t = kwargs.get('lambda_t', 0.5)\n\n    def f_reg(w):\n        res = 210 + 27.4*w[:, 0] + 13.7*(w[:, 1] + w[:, 2] + w[:, 3])\n        return res\n\n    def f_ps(w, xi):\n        res = xi*(-w[:, 0] + 0.5*w[:, 1] - 0.25*w[:, 2] - 0.1*w[:, 3])\n        return res\n\n    dim_x = 4\n    cov_mat = toeplitz([np.power(c, k) for k in range(dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x), cov_mat, size=[n_obs, ])\n\n    z_tilde_1 = np.exp(0.5*x[:, 0])\n    z_tilde_2 = 10 + x[:, 1] / (1 + np.exp(x[:, 0]))\n    z_tilde_3 = (0.6 + x[:, 0]*x[:, 2]/25)**3\n    z_tilde_4 = (20 + x[:, 1] + x[:, 3])**2\n\n    z_tilde = np.column_stack((z_tilde_1, z_tilde_2, z_tilde_3, z_tilde_4))\n    z = (z_tilde - np.mean(z_tilde, axis=0)) / np.std(z_tilde, axis=0)\n\n    # error terms\n    epsilon_0 = np.random.normal(loc=0, scale=1, size=n_obs)\n    epsilon_1 = np.random.normal(loc=0, scale=1, size=[n_obs, 2])\n\n    if dgp_type == 1:\n        features_ps = z\n        features_reg = z\n    elif dgp_type == 2:\n        features_ps = x\n        features_reg = z\n    elif dgp_type == 3:\n        features_ps = z\n        features_reg = x\n    elif dgp_type == 4:\n        features_ps = x\n        features_reg = x\n    elif dgp_type == 5:\n        features_ps = None\n        features_reg = z\n    elif dgp_type == 6:\n        features_ps = None\n        features_reg = x\n    else:\n        raise ValueError('The dgp_type is not valid.')\n\n    # treatment and propensities\n    is_experimental = (dgp_type == 5) or (dgp_type == 6)\n    if is_experimental:\n        # Set D to be experimental\n        p = 0.5 * np.ones(n_obs)\n    else:\n        p = np.exp(f_ps(features_ps, xi)) / (1 + np.exp(f_ps(features_ps, xi)))\n    u = np.random.uniform(low=0, high=1, size=n_obs)\n    d = 1.0 * (p >= u)\n\n    # potential outcomes\n    nu = np.random.normal(loc=d*f_reg(features_reg), scale=1, size=n_obs)\n    y0 = f_reg(features_reg) + nu + epsilon_0\n    y1_d0 = 2 * f_reg(features_reg) + nu + epsilon_1[:, 0]\n    y1_d1 = 2 * f_reg(features_reg) + nu + epsilon_1[:, 1]\n    y1 = d * y1_d1 + (1-d) * y1_d0\n\n    if not cross_sectional_data:\n        y = y1 - y0\n\n        if return_type in _array_alias:\n            return z, y, d\n        elif return_type in _data_frame_alias + _dml_data_alias:\n            z_cols = [f'Z{i + 1}' for i in np.arange(dim_x)]\n            data = pd.DataFrame(np.column_stack((z, y, d)),\n                                columns=z_cols + ['y', 'd'])\n            if return_type in _data_frame_alias:\n                return data\n            else:\n                return DoubleMLData(data, 'y', 'd', z_cols)\n        else:\n            raise ValueError('Invalid return_type.')\n\n    else:\n        u_t = np.random.uniform(low=0, high=1, size=n_obs)\n        t = 1.0 * (u_t <= lambda_t)\n        y = t * y1 + (1-t)*y0\n\n        if return_type in _array_alias:\n            return z, y, d, t\n        elif return_type in _data_frame_alias + _dml_data_alias:\n            z_cols = [f'Z{i + 1}' for i in np.arange(dim_x)]\n            data = pd.DataFrame(np.column_stack((z, y, d, t)),\n                                columns=z_cols + ['y', 'd', 't'])\n            if return_type in _data_frame_alias:\n                return data\n            else:\n                return DoubleMLData(data, 'y', 'd', z_cols, t_col='t')\n        else:\n            raise ValueError('Invalid return_type.')\n\n", "right_context": "\n\ndef make_confounded_plr_data(n_obs=500, theta=5.0, cf_y=0.04, cf_d=0.04, **kwargs):\n    \"\"\"\n    Generates counfounded data from an partially linear regression model.\n\n    The data generating process is defined as follows (similar to the Monte Carlo simulation used\n    in Sant'Anna and Zhao (2020)). Let :math:`X= (X_1, X_2, X_3, X_4, X_5)^T \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`,\n    where  :math:`\\\\Sigma` is a matrix with entries\n    :math:`\\\\Sigma_{kj} = c^{|j-k|}`. The default value is  :math:`c = 0`, corresponding to the identity matrix.\n    Further, define :math:`Z_j = (\\\\tilde{Z_j} - \\\\mathbb{E}[\\\\tilde{Z}_j]) / \\\\sqrt{\\\\text{Var}(\\\\tilde{Z}_j)}`,\n    where\n\n    .. math::\n\n        \\\\tilde{Z}_1 &= \\\\exp(0.5 \\\\cdot X_1)\n\n        \\\\tilde{Z}_2 &= 10 + X_2/(1 + \\\\exp(X_1))\n\n        \\\\tilde{Z}_3 &= (0.6 + X_1 \\\\cdot X_3 / 25)^3\n\n        \\\\tilde{Z}_4 &= (20 + X_2 + X_4)^2.\n\n    Additionally, generate a confounder :math:`A \\\\sim \\\\mathcal{U}[-1, 1]`.\n    At first, define the treatment as\n\n    .. math::\n\n        D = -Z_1 + 0.5 \\\\cdot Z_2 - 0.25 \\\\cdot Z_3 - 0.1 \\\\cdot Z_4 + \\\\gamma_A \\\\cdot A + \\\\varepsilon_D\n\n    and with :math:`\\\\varepsilon \\\\sim \\\\mathcal{N}(0,1)`.\n    Since :math:`A` is independent of :math:`X`, the long and short form of the treatment regression are given as\n\n    .. math::\n\n        E[D|X,A] = -Z_1 + 0.5 \\\\cdot Z_2 - 0.25 \\\\cdot Z_3 - 0.1 \\\\cdot Z_4 + \\\\gamma_A \\\\cdot A\n\n        E[D|X] = -Z_1 + 0.5 \\\\cdot Z_2 - 0.25 \\\\cdot Z_3 - 0.1 \\\\cdot Z_4.\n\n    Further, generate the outcome of interest :math:`Y` as\n\n    .. math::\n\n        Y &= \\\\theta \\\\cdot D + g(Z) + \\\\beta_A \\\\cdot A + \\\\varepsilon\n\n        g(Z) &= 210 + 27.4 \\\\cdot Z_1 +13.7 \\\\cdot (Z_2 + Z_3 + Z_4)\n\n    where :math:`\\\\varepsilon \\\\sim \\\\mathcal{N}(0,5)`.\n    This implies an average treatment effect of :math:`\\\\theta`. Additionally, the long and short forms of\n    the conditional expectation take the following forms\n\n    .. math::\n\n        \\\\mathbb{E}[Y|D, X, A] &= \\\\theta \\\\cdot D + g(Z) + \\\\beta_A \\\\cdot A\n\n        \\\\mathbb{E}[Y|D, X] &= (\\\\theta + \\\\gamma_A\\\\beta_A \\\\frac{\\\\mathrm{Var}(A)}{\\\\mathrm{Var}(D)}) \\\\cdot D + g(Z).\n\n    Consequently, the strength of confounding is determined via :math:`\\\\gamma_A` and :math:`\\\\beta_A`.\n    Both are chosen to obtain the desired confounding of the outcome and Riesz Representer (in sample).\n\n    The observed data is given as :math:`W = (Y, D, X)`.\n    Further, orcale values of the confounder :math:`A`, the transformed covariated :math:`Z`, the effect :math:`\\\\theta`,\n    the coefficients :math:`\\\\gamma_a`, :math:`\\\\beta_a`, the long and short forms of the main regression and\n    the propensity score are returned in a dictionary.\n\n    Parameters\n    ----------\n    n_obs : int\n        The number of observations to simulate.\n        Default is ``500``.\n    theta : float or int\n        Average treatment effect.\n        Default is ``5.0``.\n    cf_y : float\n        Percentage of the residual variation of the outcome explained by latent/confounding variable.\n        Default is ``0.04``.\n    cf_d : float\n        Percentage gains in the variation of the Riesz Representer generated by latent/confounding variable.\n        Default is ``0.04``.\n\n    Returns\n    -------\n    res_dict : dictionary\n       Dictionary with entries ``x``, ``y``, ``d`` and ``oracle_values``.\n\n    References\n    ----------\n    Sant\u2019Anna, P. H. and Zhao, J. (2020),\n    Doubly robust difference-in-differences estimators. Journal of Econometrics, 219(1), 101-122.\n    doi:`10.1016/j.jeconom.2020.06.003 <https://doi.org/10.1016/j.jeconom.2020.06.003>`_.\n    \"\"\"\n    c = kwargs.get('c', 0.0)\n    dim_x = kwargs.get('dim_x', 4)\n\n    # observed covariates\n    cov_mat = toeplitz([np.power(c, k) for k in range(dim_x)])\n    x = np.random.multivariate_normal(np.zeros(dim_x), cov_mat, size=[n_obs, ])\n\n    z_tilde_1 = np.exp(0.5*x[:, 0])\n    z_tilde_2 = 10 + x[:, 1] / (1 + np.exp(x[:, 0]))\n    z_tilde_3 = (0.6 + x[:, 0] * x[:, 2]/25)**3\n    z_tilde_4 = (20 + x[:, 1] + x[:, 3])**2\n\n    z_tilde = np.column_stack((z_tilde_1, z_tilde_2, z_tilde_3, z_tilde_4, x[:, 4:]))\n    z = (z_tilde - np.mean(z_tilde, axis=0)) / np.std(z_tilde, axis=0)\n\n    # error terms\n    var_eps_y = 5\n    eps_y = np.random.normal(loc=0, scale=np.sqrt(var_eps_y), size=n_obs)\n    var_eps_d = 1\n    eps_d = np.random.normal(loc=0, scale=np.sqrt(var_eps_d), size=n_obs)\n\n    # unobserved confounder\n    a_bounds = (-1, 1)\n    a = np.random.uniform(low=a_bounds[0], high=a_bounds[1], size=n_obs)\n    var_a = np.square(a_bounds[1] - a_bounds[0]) / 12\n\n    # get the required impact of the confounder on the propensity score\n    m_short = -z[:, 0] + 0.5*z[:, 1] - 0.25*z[:, 2] - 0.1*z[:, 3]\n\n    def f_m(gamma_a):\n        rr_long = eps_d / var_eps_d\n        rr_short = (gamma_a * a + eps_d) / (gamma_a**2 * var_a + var_eps_d)\n        C2_D = (np.mean(np.square(rr_long)) - np.mean(np.square(rr_short))) / np.mean(np.square(rr_short))\n        return np.square(C2_D / (1 + C2_D) - cf_d)\n\n    gamma_a = minimize_scalar(f_m).x\n    m_long = m_short + gamma_a*a\n    d = m_long + eps_d\n\n    # short and long version of g\n    g_partial_reg = 210 + 27.4*z[:, 0] + 13.7*(z[:, 1] + z[:, 2] + z[:, 3])\n\n    var_d = np.var(d)\n\n    def f_g(beta_a):\n        g_diff = beta_a * (a - gamma_a * (var_a/var_d) * d)\n        y_diff = eps_y + g_diff\n        return np.square(np.mean(np.square(g_diff)) / np.mean(np.square(y_diff)) - cf_y)\n\n    beta_a = minimize_scalar(f_g).x\n\n    g_long = theta*d + g_partial_reg + beta_a*a\n    g_short = (theta + gamma_a*beta_a * var_a / var_d)*d + g_partial_reg\n\n    y = g_long + eps_y\n\n    oracle_values = {'g_long': g_long,\n                     'g_short': g_short,\n                     'm_long': m_long,\n                     'm_short': m_short,\n                     'theta': theta,\n                     'gamma_a': gamma_a,\n                     'beta_a': beta_a,\n                     'a': a,\n                     'z': z}\n\n    res_dict = {'x': x,\n                'y': y,\n                'd': d,\n                'oracle_values': oracle_values}\n\n    return res_dict\n\n\ndef make_heterogeneous_data(n_obs=200, p=30, support_size=5, n_x=1, binary_treatment=False):\n    \"\"\"\n    Creates a simple synthetic example for heterogeneous treatment effects.\n    The data generating process is based on the Monte Carlo simulation from Oprescu et al. (2019).\n\n    The data is generated as\n\n    .. math::\n\n        Y_i & = \\\\theta_0(X_i)D_i + \\\\langle X_i,\\\\gamma_0\\\\rangle + \\\\epsilon_i\n\n        D_i & = \\\\langle X_i,\\\\beta_0\\\\rangle + \\\\eta_i,\n\n    where :math:`X_i\\\\sim\\\\mathcal{U}[0,1]^{p}` and :math:`\\\\epsilon_i,\\\\eta_i\n    \\\\sim\\\\mathcal{U}[-1,1]`.\n    If the treatment is set to be binary, the treatment is generated as\n\n    .. math::\n        D_i = 1\\\\{\\\\langle X_i,\\\\beta_0\\\\rangle \\\\ge \\\\eta_i\\\\}.\n\n    The coefficient vectors :math:`\\\\gamma_0` and :math:`\\\\beta_0` both have small random (identical) support\n    which values are drawn independently from :math:`\\\\mathcal{U}[0,1]` and :math:`\\\\mathcal{U}[0,0.3]`.\n    Further, :math:`\\\\theta_0(x)` defines the conditional treatment effect, which is defined differently depending\n    on the dimension of :math:`x`.\n\n    If the heterogeneity is univariate the conditional treatment effect takes the following form\n\n    .. math::\n            \\\\theta_0(x) = \\\\exp(2x_0) + 3\\\\sin(4x_0),\n\n    whereas for the two-dimensional case the conditional treatment effect is defined as\n\n    .. math::\n        \\\\theta_0(x) = \\\\exp(2x_0) + 3\\\\sin(4x_1).\n\n    Parameters\n    ----------\n    n_obs : int\n        Number of observations to simulate.\n        Default is ``200``.\n\n    p : int\n        Dimension of covariates.\n        Default is ``30``.\n\n    support_size : int\n        Number of relevant (confounding) covariates.\n        Default is ``5``.\n\n    n_x : int\n        Dimension of the heterogeneity. Can be either ``1`` or ``2``.\n        Default is ``1``.\n\n    binary_treatment : bool\n        Indicates whether the treatment is binary.\n        Default is ``False``.\n\n    Returns\n    -------\n    res_dict : dictionary\n       Dictionary with entries ``data``, ``effects``, ``treatment_effect``.\n\n    \"\"\"\n    # simple input checks\n    assert n_x in [1, 2], 'n_x must be either 1 or 2.'\n    assert support_size <= p, 'support_size must be smaller than p.'\n    assert isinstance(binary_treatment, bool), 'binary_treatment must be a boolean.'\n\n    # define treatment effects\n    if n_x == 1:\n        def treatment_effect(x):\n            return np.exp(2 * x[:, 0]) + 3 * np.sin(4 * x[:, 0])\n    else:\n        assert n_x == 2\n\n        # redefine treatment effect\n        def treatment_effect(x):\n            return np.exp(2 * x[:, 0]) + 3 * np.sin(4 * x[:, 1])\n\n    # Outcome support and coefficients\n    support_y = np.random.choice(np.arange(p), size=support_size, replace=False)\n    coefs_y = np.random.uniform(0, 1, size=support_size)\n    # treatment support and coefficients\n    support_d = support_y\n    coefs_d = np.random.uniform(0, 0.3, size=support_size)\n\n    # noise\n    epsilon = np.random.uniform(-1, 1, size=n_obs)\n    eta = np.random.uniform(-1, 1, size=n_obs)\n\n    # Generate controls, covariates, treatments and outcomes\n    x = np.random.uniform(0, 1, size=(n_obs, p))\n    # Heterogeneous treatment effects\n    te = treatment_effect(x)\n    if binary_treatment:\n        d = 1.0 * (np.dot(x[:, support_d], coefs_d) >= eta)\n    else:\n        d = np.dot(x[:, support_d], coefs_d) + eta\n    y = te * d + np.dot(x[:, support_y], coefs_y) + epsilon\n\n    # Now we build the dataset\n    y_df = pd.DataFrame({'y': y})\n    d_df = pd.DataFrame({'d': d})\n    x_df = pd.DataFrame(\n        data=x,\n        index=np.arange(x.shape[0]),\n        columns=[f'X_{i}' for i in range(x.shape[1])]\n    )\n\n    data = pd.concat([y_df, d_df, x_df], axis=1)\n    res_dict = {\n        'data': data,\n        'effects': te,\n        'treatment_effect': treatment_effect}\n    return res_dict\n", "import_text": ["pandas", "numpy", "scipy.linalg.toeplitz", "scipy.optimize.minimize_scalar", "sklearn.preprocessing.PolynomialFeatures", "sklearn.preprocessing.OneHotEncoder", "sklearn.datasets.make_spd_matrix"], "prompt": "\"\"\"\nDescription: This function generates confounded Inverse Reinforcement Learning (IRL) data.\n\nArgs:\n    n_obs (int): The number of observations to generate. Default is 500.\n    theta (float): The true effect size. Default is 5.0.\n    cf_y (float): The desired correlation between the outcome and the confounder. Default is 0.04.\n    cf_d (float): The desired correlation between the treatment and the confounder. Default is 0.04.\n\nReturns:\n    dict: A dictionary containing the generated data and oracle values. The dictionary keys are:\n        - 'x': The observed covariates.\n        - 'y': The outcome variable.\n        - 'd': The treatment variable.\n        - 'oracle_values': A dictionary containing the oracle values for the confounded IRM data. The dictionary keys are:\n            - 'g_long': The long form of the riesz representer.\n            - 'g_short': The short form of the riesz representer.\n            - 'm_long': The long form of the propensity score.\n            - 'm_short': The short form of the propensity score.\n            - 'gamma_a': The required impact of the confounder on the propensity score.\n            - 'beta_a': The coefficient of the confounder in the outcome model.\n            - 'a': The unobserved confounder.\n            - 'y0': The potential outcome under the control group.\n            - 'y1': The potential outcome under the treatment group.\n            - 'z': The observed covariates.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Generates counfounded data from an interactive regression model.\n\n    The data generating process is defined as follows (similar to the Monte Carlo simulation used\n    in Sant'Anna and Zhao (2020)).\n\n    Let :math:`X= (X_1, X_2, X_3, X_4, X_5)^T \\\\sim \\\\mathcal{N}(0, \\\\Sigma)`, where  :math:`\\\\Sigma` corresponds\n    to the identity matrix.\n    Further, define :math:`Z_j = (\\\\tilde{Z_j} - \\\\mathbb{E}[\\\\tilde{Z}_j]) / \\\\sqrt{\\\\text{Var}(\\\\tilde{Z}_j)}`,\n    where\n\n    .. math::\n\n        \\\\tilde{Z}_1 &= \\\\exp(0.5 \\\\cdot X_1)\n\n        \\\\tilde{Z}_2 &= 10 + X_2/(1 + \\\\exp(X_1))\n\n        \\\\tilde{Z}_3 &= (0.6 + X_1 \\\\cdot X_3 / 25)^3\n\n        \\\\tilde{Z}_4 &= (20 + X_2 + X_4)^2\n\n        \\\\tilde{Z}_5 &= X_5.\n\n    Additionally, generate a confounder :math:`A \\\\sim \\\\mathcal{U}[-1, 1]`.\n    At first, define the propensity score as\n\n    .. math::\n\n        m(X, A) = P(D=1|X,A) = 0.5 + \\\\gamma_A \\\\cdot A\n\n    and generate the treatment :math:`D = 1\\\\{m(X, A) \\\\ge U\\\\}` with :math:`U \\\\sim \\\\mathcal{U}[0, 1]`.\n    Since :math:`A` is independent of :math:`X`, the short form of the propensity score is given as\n\n    .. math::\n\n        P(D=1|X) = 0.5.\n\n    Further, generate the outcome of interest :math:`Y` as\n\n    .. math::\n\n        Y &= \\\\theta \\\\cdot D (Z_5 + 1) + g(Z) + \\\\beta_A \\\\cdot A + \\\\varepsilon\n\n        g(Z) &= 210 + 27.4 \\\\cdot Z_1 +13.7 \\\\cdot (Z_2 + Z_3 + Z_4)\n\n    where :math:`\\\\varepsilon \\\\sim \\\\mathcal{N}(0,5)`.\n    This implies an average treatment effect of :math:`\\\\theta`. Additionally, the long and short forms of\n    the conditional expectation take the following forms\n\n    .. math::\n\n        \\\\mathbb{E}[Y|D, X, A] &= \\\\theta \\\\cdot D (Z_5 + 1) + g(Z) + \\\\beta_A \\\\cdot A\n\n        \\\\mathbb{E}[Y|D, X] &= (\\\\theta + \\\\beta_A \\\\frac{\\\\mathrm{Cov}(A, D(Z_5 + 1))}{\\\\mathrm{Var}(D(Z_5 + 1))})\n            \\\\cdot D (Z_5 + 1) + g(Z).\n\n    Consequently, the strength of confounding is determined via :math:`\\\\gamma_A` and :math:`\\\\beta_A`.\n    Both are chosen to obtain the desired confounding of the outcome and Riesz Representer (in sample).\n\n    The observed data is given as :math:`W = (Y, D, X)`.\n    Further, orcale values of the confounder :math:`A`, the transformed covariated :math:`Z`,\n    the potential outcomes of :math:`Y`, the coefficients :math:`\\\\gamma_a`, :math:`\\\\beta_a`, the\n    long and short forms of the main regression and the propensity score\n    are returned in a dictionary.\n\n    Parameters\n    ----------\n    n_obs : int\n        The number of observations to simulate.\n        Default is ``500``.\n    theta : float or int\n        Average treatment effect.\n        Default is ``5.0``.\n    cf_y : float\n        Percentage of the residual variation of the outcome explained by latent/confounding variable.\n        Default is ``0.04``.\n    cf_d : float\n        Percentage gains in the variation of the Riesz Representer generated by latent/confounding variable.\n        Default is ``0.04``.\n\n    Returns\n    -------\n    res_dict : dictionary\n       Dictionary with entries ``x``, ``y``, ``d`` and ``oracle_values``.\n\n    References\n    ----------\n    Sant\u2019Anna, P. H. and Zhao, J. (2020),\n    Doubly robust difference-in-differences estimators. Journal of Econometrics, 219(1), 101-122.\n    doi:`10.1016/j.jeconom.2020.06.003 <https://doi.org/10.1016/j.jeconom.2020.06.003>`_.\n    \"\"\"", "function_dependencies": ["scipy.linalg.toeplitz", "numpy.power", "numpy.random.multivariate_normal", "numpy.zeros", "numpy.exp", "numpy.column_stack", "numpy.mean", "numpy.std", "numpy.random.normal", "numpy.sqrt", "numpy.random.uniform", "numpy.arange", "numpy.arctanh", "numpy.ones_like", "numpy.var", "numpy.cov", "numpy.square", "scipy.optimize.minimize_scalar"], "project_create_time": "2020-09-09T13:27:25+00:00", "project_update_time": "2024-04-14T19:25:09+00:00", "file_create_time": "2020-09-09T14:06:05Z", "file_update_time": "2023-12-04T13:44:45Z", "function_update_time": "2023-05-31T11:44:06Z", "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "reference_api": ["numpy.square"], "test_function": [{"file_path": "/doubleml-for-py-0.7.1/doubleml-for-py-0.7.1/doubleml/tests/test_datasets.py", "class_name": null, "function_name": "test_make_confounded_irm_data_return_types", "code": "\ndef test_make_confounded_irm_data_return_types():\n    np.random.seed(3141)\n    res = make_confounded_irm_data()\n    assert isinstance(res, dict)\n    assert isinstance(res['x'], np.ndarray)\n    assert isinstance(res['y'], np.ndarray)\n    assert isinstance(res['d'], np.ndarray)\n\n    assert isinstance(res['oracle_values'], dict)\n    assert isinstance(res['oracle_values']['g_long'], np.ndarray)\n    assert isinstance(res['oracle_values']['g_short'], np.ndarray)\n    assert isinstance(res['oracle_values']['m_long'], np.ndarray)\n    assert isinstance(res['oracle_values']['m_short'], np.ndarray)\n    assert isinstance(res['oracle_values']['gamma_a'], float)\n    assert isinstance(res['oracle_values']['beta_a'], float)\n    assert isinstance(res['oracle_values']['a'], np.ndarray)\n    assert isinstance(res['oracle_values']['y0'], np.ndarray)\n    assert isinstance(res['oracle_values']['y1'], np.ndarray)\n    assert isinstance(res['oracle_values']['z'], np.ndarray)"}]}, {"git_group": "Axelrod-Python", "git_name": "Axelrod", "version": "v4.13.0", "language": "Python", "project_name": "Axelrod-v4.13.0.zip", "file_path": "/Axelrod-v4.13.0/Axelrod-4.13.0/axelrod/fingerprint.py", "file_name": "fingerprint.py", "focal_class": "AshlockFingerprint", "focal_name": "plot", "focal_parameter": [], "solution": "    def plot(\n        self,\n        cmap: str = \"seismic\",\n        interpolation: str = \"none\",\n        title: str = None,\n        colorbar: bool = True,\n        labels: bool = True,\n    ) -> plt.Figure:\n        size = int((1 / self.step) // 1) + 1\n        plotting_data = _reshape_data(self.data, self.points, size)\n        fig, ax = plt.subplots()\n        cax = ax.imshow(plotting_data, cmap=cmap, interpolation=interpolation)\n\n        if colorbar:\n            max_score = max(self.data.values())\n            min_score = min(self.data.values())\n            ticks = [min_score, (max_score + min_score) / 2, max_score]\n            fig.colorbar(cax, ticks=ticks)\n\n        plt.xlabel(\"$x$\")\n        plt.ylabel(\"$y$\", rotation=0)\n        ax.tick_params(axis=\"both\", which=\"both\", length=0)\n        plt.xticks([0, len(plotting_data) - 1], [\"0\", \"1\"])\n        plt.yticks([0, len(plotting_data) - 1], [\"1\", \"0\"])\n\n        if not labels:\n            plt.axis(\"off\")\n\n        if title is not None:\n            plt.title(title)\n        return fig", "function_signature": "def plot(\n        self,\n        cmap: str = \"seismic\",\n        interpolation: str = \"none\",\n        title: str = None,\n        colorbar: bool = True,\n        labels: bool = True,\n    ) -> plt.Figure :", "left_context": "import os\nfrom collections import namedtuple\nfrom tempfile import mkstemp\nfrom typing import Any, List, Union\n\nimport axelrod as axl\nimport dask.dataframe as dd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tqdm\nfrom axelrod import Player\nfrom axelrod.interaction_utils import (\n    compute_final_score_per_turn,\n    read_interactions_from_file,\n)\nfrom axelrod.strategy_transformers import DualTransformer, JossAnnTransformer\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nPoint = namedtuple(\"Point\", \"x y\")\n\n\ndef _create_points(step: float, progress_bar: bool = True) -> List[Point]:\n    \"\"\"Creates a set of Points over the unit square.\n\n    A Point has coordinates (x, y). This function constructs points that are\n    separated by a step equal to `step`. The points are over the unit\n    square which implies that the number created will be (1/`step` + 1)^2.\n\n    Parameters\n    ----------\n    step : float\n        The separation between each Point. Smaller steps will produce more\n        Points with coordinates that will be closer together.\n    progress_bar : bool\n        Whether or not to create a progress bar which will be updated\n\n    Returns\n    ----------\n    points : list\n        of Point objects with coordinates (x, y)\n    \"\"\"\n    num = int((1 / step) // 1) + 1\n\n    if progress_bar:\n        p_bar = tqdm.tqdm(total=num**2, desc=\"Generating points\")\n\n    points = []\n    for x in np.linspace(0, 1, num):\n        for y in np.linspace(0, 1, num):\n            points.append(Point(x, y))\n\n            if progress_bar:\n                p_bar.update()\n\n    if progress_bar:\n        p_bar.close()\n\n    return points\n\n\ndef _create_jossann(point: Point, probe: Any) -> Player:\n    \"\"\"Creates a JossAnn probe player that matches the Point.\n\n    If the coordinates of point sums to more than 1 the parameters are\n    flipped and subtracted from 1 to give meaningful probabilities. We also\n    use the Dual of the probe. This is outlined further in [Ashlock2010]_.\n\n    Parameters\n    ----------\n    point : Point\n    probe : class or instance\n        A class that must be descended from axelrod.Player or an instance of\n        axelrod.Player.\n\n    Returns\n    ----------\n    joss_ann: Joss-AnnTitForTat object\n        `JossAnnTransformer` with parameters that correspond to `point`.\n    \"\"\"\n    x, y = point\n\n    if isinstance(probe, axl.Player):\n        probe_class = probe.__class__\n        init_kwargs = probe.init_kwargs\n    else:\n        probe_class = probe\n        init_kwargs = {}\n\n    if x + y >= 1:\n        joss_ann = DualTransformer()(\n            JossAnnTransformer((1 - x, 1 - y))(probe_class)\n        )(**init_kwargs)\n    else:\n        joss_ann = JossAnnTransformer((x, y))(probe_class)(**init_kwargs)\n    return joss_ann\n\n\ndef _create_probes(\n    probe: Union[type, Player], points: list, progress_bar: bool = True\n) -> List[Player]:\n    \"\"\"Creates a set of probe strategies over the unit square.\n\n    Constructs probe strategies that correspond to points with coordinates\n    (x, y). The probes are created using the `JossAnnTransformer`.\n\n    Parameters\n    ----------\n    probe : class or instance\n        A class that must be descended from axelrod.Player or an instance of\n        axelrod.Player.\n    points : list\n        of Point objects with coordinates (x, y)\n    progress_bar : bool\n        Whether or not to create a progress bar which will be updated\n\n    Returns\n    ----------\n    probes : list\n        A list of `JossAnnTransformer` players with parameters that\n        correspond to point.\n    \"\"\"\n    if progress_bar:\n        points = tqdm.tqdm(points, desc=\"Generating probes\")\n    probes = [_create_jossann(point, probe) for point in points]\n    return probes\n\n\ndef _create_edges(points: List[Point], progress_bar: bool = True) -> list:\n    \"\"\"Creates a set of edges for a spatial tournament.\n\n    Constructs edges that correspond to `points`. All edges begin at 0, and\n    connect to the index + 1 of the probe.\n\n    Parameters\n    ----------\n    points : list\n        of Point objects with coordinates (x, y)\n    progress_bar : bool\n        Whether or not to create a progress bar which will be updated\n\n    Returns\n    ----------\n    edges : list of tuples\n        A list containing tuples of length 2. All tuples will have 0 as the\n        first element. The second element is the index of the\n        corresponding probe (+1 to allow for including the Strategy).\n    \"\"\"\n    if progress_bar:\n        points = tqdm.tqdm(points, desc=\"Generating network edges\")\n    edges = [(0, index + 1) for index, point in enumerate(points)]\n    return edges\n\n\ndef _generate_data(interactions: dict, points: list, edges: list) -> dict:\n    \"\"\"Generates useful data from a spatial tournament.\n\n    Matches interactions from `results` to their corresponding Point in\n    `probe_points`.\n\n    Parameters\n    ----------\n    interactions : dict\n        A dictionary mapping edges to the corresponding interactions of\n        those players.\n    points : list\n        of Point objects with coordinates (x, y).\n    edges : list of tuples\n        A list containing tuples of length 2. All tuples will have either 0\n        or 1 as the first element. The second element is the index of the\n        corresponding probe (+1 to allow for including the Strategy).\n\n    Returns\n    ----------\n    point_scores : dict\n        A dictionary where the keys are Points of the form (x, y) and\n        the values are the mean score for the corresponding interactions.\n    \"\"\"\n    edge_scores = [\n        np.mean(\n            [\n                compute_final_score_per_turn(scores)[0]\n                for scores in interactions[edge]\n            ]\n        )\n        for edge in edges\n    ]\n    point_scores = dict(zip(points, edge_scores))\n    return point_scores\n\n\ndef _reshape_data(data: dict, points: list, size: int) -> np.ndarray:\n    \"\"\"Shape the data so that it can be plotted easily.\n\n    Parameters\n    ----------\n    data : dictionary\n        A dictionary where the keys are Points of the form (x, y) and\n        the values are the mean score for the corresponding interactions.\n\n    points : list\n        of Point objects with coordinates (x, y).\n\n    size : int\n        The number of Points in every row/column.\n\n    Returns\n    ----------\n    plotting_data : list\n        2-D numpy array of the scores, correctly shaped to ensure that the\n        score corresponding to Point (0, 0) is in the left hand corner ie.\n        the standard origin.\n    \"\"\"\n    ordered_data = [data[point] for point in points]\n    shaped_data = np.reshape(ordered_data, (size, size), order=\"F\")\n    plotting_data = np.flipud(shaped_data)\n    return plotting_data\n\n\nclass AshlockFingerprint(object):\n    def __init__(\n        self,\n        strategy: Union[type, Player],\n        probe: Union[type, Player] = axl.TitForTat,\n    ) -> None:\n        \"\"\"\n        Parameters\n        ----------\n        strategy : class or instance\n            A class that must be descended from axelrod.Player or an instance of\n            axelrod.Player.\n        probe : class or instance\n            A class that must be descended from axelrod.Player or an instance of\n            axelrod.Player.\n            Default: Tit For Tat\n        \"\"\"\n        self.strategy = strategy\n        self.probe = probe\n\n    def _construct_tournament_elements(\n        self, step: float, progress_bar: bool = True\n    ) -> tuple:\n        \"\"\"Build the elements required for a spatial tournament\n\n        Parameters\n        ----------\n        step : float\n            The separation between each Point. Smaller steps will\n            produce more Points that will be closer together.\n        progress_bar : bool\n            Whether or not to create a progress bar which will be updated\n\n        Returns\n        ----------\n        edges : list of tuples\n            A list containing tuples of length 2. All tuples will have either 0\n            or 1 as the first element. The second element is the index of the\n            corresponding probe (+1 to allow for including the Strategy).\n\n        tournament_players : list\n            A list containing instances of axelrod.Player. The first item is the\n            original player, the rest are the probes.\n\n        \"\"\"\n        self.points = _create_points(step, progress_bar=progress_bar)\n        edges = _create_edges(self.points, progress_bar=progress_bar)\n        probe_players = _create_probes(\n            self.probe, self.points, progress_bar=progress_bar\n        )\n\n        if isinstance(self.strategy, axl.Player):\n            tournament_players = [self.strategy.clone()] + probe_players\n        else:\n            tournament_players = [self.strategy()] + probe_players\n\n        return edges, tournament_players\n\n    def fingerprint(\n        self,\n        turns: int = 50,\n        repetitions: int = 10,\n        step: float = 0.01,\n        processes: int = None,\n        filename: str = None,\n        progress_bar: bool = True,\n        seed: int = None,\n    ) -> dict:\n        \"\"\"Build and play the spatial tournament.\n\n        Creates the probes and their edges then builds a spatial tournament.\n        When the coordinates of the probe sum to more than 1, the flip_plays of the\n        probe is taken instead and then the Joss-Ann Transformer is applied. If\n        the coordinates sum to less than 1 (or equal), then only the Joss-Ann is\n        applied, a flip_plays is not required.\n\n        Parameters\n        ----------\n        turns : int, optional\n            The number of turns per match\n        repetitions : int, optional\n            The number of times the round robin should be repeated\n        step : float, optional\n            The separation between each Point. Smaller steps will\n            produce more Points that will be closer together.\n        processes : int, optional\n            The number of processes to be used for parallel processing\n        filename: str, optional\n            The name of the file for self.spatial_tournament's interactions.\n            if None, will auto-generate a filename.\n        progress_bar : bool\n            Whether or not to create a progress bar which will be updated\n        seed : int, optional\n            Random seed for reproducibility\n\n        Returns\n        ----------\n        self.data : dict\n            A dictionary where the keys are coordinates of the form (x, y) and\n            the values are the mean score for the corresponding interactions.\n        \"\"\"\n\n        temp_file_descriptor = None\n        if filename is None:\n            temp_file_descriptor, filename = mkstemp()  # type: ignore\n\n        edges, tourn_players = self._construct_tournament_elements(\n            step, progress_bar=progress_bar\n        )\n\n        self.step = step\n        self.spatial_tournament = axl.Tournament(\n            tourn_players,\n            turns=turns,\n            repetitions=repetitions,\n            edges=edges,\n            seed=seed,\n        )\n        self.spatial_tournament.play(\n            build_results=False,\n            filename=filename,\n            processes=processes,\n            progress_bar=progress_bar,\n        )\n\n        self.interactions = read_interactions_from_file(\n            filename, progress_bar=progress_bar\n        )\n\n        if temp_file_descriptor is not None:\n            assert filename is not None\n            os.close(temp_file_descriptor)\n            os.remove(filename)\n\n        self.data = _generate_data(self.interactions, self.points, edges)\n        return self.data\n", "right_context": "\n\nclass TransitiveFingerprint(object):\n    def __init__(self, strategy, opponents=None, number_of_opponents=50):\n        \"\"\"\n        Parameters\n        ----------\n        strategy : class or instance\n            A class that must be descended from axelrod.Player or an instance of\n            axelrod.Player.\n        opponents : list of instances\n            A list that contains a list of opponents\n            Default: A spectrum of Random  players\n        number_of_opponents: int\n            The number of Random opponents\n            Default: 50\n        \"\"\"\n        self.strategy = strategy\n\n        if opponents is None:\n            self.opponents = [\n                axl.Random(p) for p in np.linspace(0, 1, number_of_opponents)\n            ]\n        else:\n            self.opponents = opponents\n\n    def fingerprint(\n        self,\n        turns: int = 50,\n        repetitions: int = 1000,\n        noise: float = None,\n        processes: int = None,\n        filename: str = None,\n        progress_bar: bool = True,\n        seed: int = None,\n    ) -> np.ndarray:\n        \"\"\"Creates a spatial tournament to run the necessary matches to obtain\n        fingerprint data.\n\n          Creates the opponents and their edges then builds a spatial tournament.\n\n        Parameters\n        ----------\n        turns : int, optional\n            The number of turns per match\n        repetitions : int, optional\n            The number of times the round robin should be repeated\n        noise : float, optional\n            The probability that a player's intended action should be flipped\n        processes : int, optional\n            The number of processes to be used for parallel processing\n        filename: str, optional\n            The name of the file for spatial tournament's interactions.\n            if None, a filename will be generated.\n        progress_bar : bool\n            Whether or not to create a progress bar which will be updated\n\n        Returns\n        ----------\n        self.data : np.array\n            A numpy array containing the mean cooperation rate against each\n            opponent in each turn. The ith row corresponds to the ith opponent\n            and the jth column the jth turn.\n        \"\"\"\n\n        if isinstance(self.strategy, axl.Player):\n            players = [self.strategy] + self.opponents\n        else:\n            players = [self.strategy()] + self.opponents\n\n        temp_file_descriptor = None\n        if filename is None:\n            temp_file_descriptor, filename = mkstemp()  # type: ignore\n\n        edges = [(0, k + 1) for k in range(len(self.opponents))]\n        tournament = axl.Tournament(\n            players=players,\n            edges=edges,\n            turns=turns,\n            noise=noise,\n            repetitions=repetitions,\n            seed=seed,\n        )\n        tournament.play(\n            filename=filename,\n            build_results=False,\n            progress_bar=progress_bar,\n            processes=processes,\n        )\n\n        self.data = self.analyse_cooperation_ratio(filename)\n\n        if temp_file_descriptor is not None:\n            assert filename is not None\n            os.close(temp_file_descriptor)\n            os.remove(filename)\n\n        return self.data\n\n    @staticmethod\n    def analyse_cooperation_ratio(filename):\n        \"\"\"Generates the data used from the tournament\n\n        Return an M by N array where M is the number of opponents and N is the\n        number of turns.\n\n        Parameters\n        ----------\n        filename : str\n            The filename of the interactions\n\n        Returns\n        ----------\n        self.data : np.array\n            A numpy array containing the mean cooperation rate against each\n            opponent in each turn. The ith row corresponds to the ith opponent\n            and the jth column the jth turn.\n        \"\"\"\n        did_c = np.vectorize(\n            lambda actions: [int(action == \"C\") for action in actions]\n        )\n\n        cooperation_rates = {}\n        df = dd.read_csv(filename)\n        # We ignore the actions of all opponents. So we filter the dataframe to\n        # only include the results of the player with index `0`.\n        df = df[df[\"Player index\"] == 0][[\"Opponent index\", \"Actions\"]]\n\n        for _, row in df.iterrows():\n            opponent_index, player_history = (\n                row[\"Opponent index\"],\n                row[\"Actions\"],\n            )\n            if opponent_index in cooperation_rates:\n                cooperation_rates[opponent_index].append(did_c(player_history))\n            else:\n                cooperation_rates[opponent_index] = [did_c(player_history)]\n\n        for index, rates in cooperation_rates.items():\n            cooperation_rates[index] = np.mean(rates, axis=0)\n\n        return np.array(\n            [cooperation_rates[index] for index in sorted(cooperation_rates)]\n        )\n\n    def plot(\n        self,\n        cmap: str = \"viridis\",\n        interpolation: str = \"none\",\n        title: str = None,\n        colorbar: bool = True,\n        labels: bool = True,\n        display_names: bool = False,\n        ax: plt.Figure = None,\n    ) -> plt.Figure:\n        \"\"\"Plot the results of the spatial tournament.\n        Parameters\n        ----------\n        cmap : str, optional\n            A matplotlib colour map, full list can be found at\n            http://matplotlib.org/examples/color/colormaps_reference.html\n        interpolation : str, optional\n            A matplotlib interpolation, full list can be found at\n            http://matplotlib.org/examples/images_contours_and_fields/interpolation_methods.html\n        title : str, optional\n            A title for the plot\n        colorbar : bool, optional\n            Choose whether the colorbar should be included or not\n        labels : bool, optional\n            Choose whether the axis labels and ticks should be included\n        display_names : bool, optional\n            Choose whether to display the names of the strategies\n        ax: matplotlib axis\n            Allows the plot to be written to a given matplotlib axis.\n            Default is None.\n        Returns\n        ----------\n        figure : matplotlib figure\n            A heat plot of the results of the spatial tournament\n        \"\"\"\n        if ax is None:\n            fig, ax = plt.subplots()\n        else:\n            ax = ax\n\n        fig = ax.get_figure()\n        mat = ax.imshow(self.data, cmap=cmap, interpolation=interpolation)\n\n        width = len(self.data) / 2\n        height = width\n        fig.set_size_inches(width, height)\n\n        plt.xlabel(\"turns\")\n        ax.tick_params(axis=\"both\", which=\"both\", length=0)\n\n        if display_names:\n            plt.yticks(\n                range(len(self.opponents)),\n                [str(player) for player in self.opponents],\n            )\n        else:\n            plt.yticks([0, len(self.opponents) - 1], [0, 1])\n            plt.ylabel(\"Probability of cooperation\")\n\n        if not labels:\n            plt.axis(\"off\")\n\n        if title is not None:\n            plt.title(title)\n\n        if colorbar:\n            max_score = 0\n            min_score = 1\n            ticks = [min_score, 1 / 2, max_score]\n\n            divider = make_axes_locatable(ax)\n            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\n            cbar = fig.colorbar(mat, cax=cax, ticks=ticks)\n\n        plt.tight_layout()\n        return fig\n", "import_text": ["os", "collections.namedtuple", "tempfile.mkstemp", "typing.Any", "typing.List", "typing.Union", "axelrod", "dask.dataframe", "matplotlib.pyplot", "numpy", "tqdm", "axelrod.Player", "axelrod.interaction_utils.compute_final_score_per_turn", "axelrod.interaction_utils.read_interactions_from_file", "axelrod.strategy_transformers.DualTransformer", "axelrod.strategy_transformers.JossAnnTransformer", "mpl_toolkits.axes_grid1.make_axes_locatable"], "prompt": "\"\"\"\nDescription: This function plots a heatmap of the data using matplotlib.\n\nArgs:\n    cmap (str): The colormap that will be used in the plot. Default is \"seismic\".\n    interpolation (str): The interpolation method that will be used in the plot. Default is \"none\".\n    title (str): The title of the plot. Default is None.\n    colorbar (bool): Whether to include a colorbar in the plot. Default is True.\n    labels (bool): Whether to include labels in the plot. Default is True.\n\nReturns:\n    plt.Figure: The figure object containing the plot.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "        \"\"\"Plot the results of the spatial tournament.\n        Parameters\n        ----------\n        cmap : str, optional\n            A matplotlib colour map, full list can be found at\n            http://matplotlib.org/examples/color/colormaps_reference.html\n        interpolation : str, optional\n            A matplotlib interpolation, full list can be found at\n            http://matplotlib.org/examples/images_contours_and_fields/interpolation_methods.html\n        title : str, optional\n            A title for the plot\n        colorbar : bool, optional\n            Choose whether the colorbar should be included or not\n        labels : bool, optional\n            Choose whether the axis labels and ticks should be included\n        display_names : bool, optional\n            Choose whether to display the names of the strategies\n        ax: matplotlib axis\n            Allows the plot to be written to a given matplotlib axis.\n            Default is None.\n        Returns\n        ----------\n        figure : matplotlib figure\n            A heat plot of the results of the spatial tournament\n        \"\"\"", "function_dependencies": ["matplotlib.pyplot.subplots", "matplotlib.pyplot.xlabel", "matplotlib.pyplot.ylabel", "matplotlib.pyplot.xticks", "matplotlib.pyplot.yticks", "matplotlib.pyplot.axis", "matplotlib.pyplot.title"], "project_create_time": "2015-02-18T09:37:17+00:00", "project_update_time": "2024-04-17T09:01:56+00:00", "file_create_time": "2016-12-07T10:15:02Z", "file_update_time": "2022-03-14T10:46:06Z", "function_update_time": "2016-12-07T10:15:02Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["matplotlib.pyplot.subplots", "matplotlib.pyplot.ylabel", "matplotlib.pyplot.yticks"], "test_function": [{"file_path": "/Axelrod-v4.13.0/Axelrod-4.13.0/axelrod/tests/unit/test_fingerprint.py", "class_name": "TestFingerprint", "function_name": "test_plot_figure", "code": "\n    def test_plot_figure(self):\n        af = AshlockFingerprint(axl.WinStayLoseShift, axl.TitForTat)\n        af.fingerprint(turns=10, repetitions=2, step=0.25, progress_bar=False)\n        p = af.plot()\n        self.assertIsInstance(p, matplotlib.pyplot.Figure)\n        q = af.plot(cmap=\"jet\")\n        self.assertIsInstance(q, matplotlib.pyplot.Figure)\n        r = af.plot(interpolation=\"bicubic\")\n        self.assertIsInstance(r, matplotlib.pyplot.Figure)\n        t = af.plot(title=\"Title\")\n        self.assertIsInstance(t, matplotlib.pyplot.Figure)\n        u = af.plot(colorbar=False)\n        self.assertIsInstance(u, matplotlib.pyplot.Figure)\n        v = af.plot(labels=False)\n        self.assertIsInstance(v, matplotlib.pyplot.Figure)"}]}, {"git_group": "aesara-devs", "git_name": "aesara", "version": "rel-2.9.3", "language": "Python", "project_name": "aesara-rel-2.9.3.zip", "file_path": "/aesara-rel-2.9.3/aesara-rel-2.9.3/aesara/tensor/subtensor.py", "file_name": "subtensor.py", "focal_class": "AdvancedSubtensor1", "focal_name": "perform", "focal_parameter": ["node", "inp", "out_"], "solution": "\n    def perform(self, node, inp, out_):\n        x, i = inp\n        (out,) = out_\n        # Copy always implied by numpy advanced indexing semantic.\n        if out[0] is not None and out[0].shape == (len(i),) + x.shape[1:]:\n            o = out[0]\n        else:\n            o = None\n\n        # If i.dtype is more precise than numpy.intp (int32 on 32-bit machines,\n        # int64 on 64-bit machines), numpy may raise the following error:\n        # TypeError: array cannot be safely cast to required type.\n        # We need to check if values in i can fit in numpy.intp, because\n        # if they don't, that should be an error (no array can have that\n        # many elements on a 32-bit arch).\n        if i.dtype != np.intp:\n            i_ = _asarray(i, dtype=np.intp)\n            if not np.can_cast(i.dtype, np.intp):\n                # Check if there was actually an incorrect conversion\n                if np.any(i != i_):\n                    raise IndexError(\n                        \"index contains values that are bigger \"\n                        \"than the maximum array size on this system.\",\n                        i,\n                    )\n            i = i_\n\n        out[0] = x.take(i, axis=0, out=o)", "function_signature": "def perform(self, node, inp, out_) :", "left_context": "import logging\nimport sys\nfrom itertools import chain, groupby\nfrom textwrap import dedent\nfrom typing import Callable, Iterable, List, Optional, Tuple, Union\n\nimport numpy as np\n\nimport aesara\nfrom aesara import scalar as aes\nfrom aesara.configdefaults import config\nfrom aesara.gradient import DisconnectedType\nfrom aesara.graph.basic import Apply, Constant, Variable\nfrom aesara.graph.op import Op\nfrom aesara.graph.type import Type\nfrom aesara.graph.utils import MethodNotDefined\nfrom aesara.link.c.op import COp\nfrom aesara.link.c.params_type import ParamsType\nfrom aesara.misc.safe_asarray import _asarray\nfrom aesara.printing import Printer, pprint, set_precedence\nfrom aesara.scalar.basic import ScalarConstant\nfrom aesara.tensor import _get_vector_length, as_tensor_variable, get_vector_length\nfrom aesara.tensor.basic import alloc, get_scalar_constant_value\nfrom aesara.tensor.elemwise import DimShuffle\nfrom aesara.tensor.exceptions import (\n    AdvancedIndexingError,\n    NotScalarConstantError,\n    ShapeError,\n)\nfrom aesara.tensor.math import clip\nfrom aesara.tensor.shape import Reshape, specify_broadcastable\nfrom aesara.tensor.type import (\n    TensorType,\n    bscalar,\n    complex_dtypes,\n    cscalar,\n    discrete_dtypes,\n    dscalar,\n    fscalar,\n    integer_dtypes,\n    iscalar,\n    lscalar,\n    tensor,\n    ubscalar,\n    uiscalar,\n    ulscalar,\n    uwscalar,\n    wscalar,\n    zscalar,\n)\nfrom aesara.tensor.type_other import NoneConst, NoneTypeT, SliceType, make_slice\n\n\n_logger = logging.getLogger(\"aesara.tensor.subtensor\")\n\ninvalid_scal_types = (aes.float64, aes.float32, aes.float16)\nscal_types = (\n    aes.int64,\n    aes.int32,\n    aes.int16,\n    aes.int8,\n    aes.uint64,\n    aes.uint32,\n    aes.uint16,\n    aes.uint8,\n)\ntensor_types = (\n    lscalar,\n    iscalar,\n    wscalar,\n    bscalar,\n    ulscalar,\n    uiscalar,\n    uwscalar,\n    ubscalar,\n)\ninvalid_tensor_types = (\n    fscalar,\n    dscalar,\n    cscalar,\n    zscalar,\n)\n\n\ndef indices_from_subtensor(\n    op_indices: Iterable[ScalarConstant],\n    idx_list: Optional[List[Union[Type, slice, Variable]]],\n) -> Tuple[Union[slice, Variable]]:\n    \"\"\"Recreate the index tuple from which a ``*Subtensor**`` ``Op`` was created.\n\n    Parameters\n    ==========\n    op_indices\n        The flattened indices obtained from ``x.inputs``, when ``x`` is a\n        ``*Subtensor*`` node.\n    idx_list\n        The values describing the types of each dimension's index.  This is\n        obtained from ``op.idx_list``, when ``op`` is a ``*Subtensor*``\n        ``Op``.\n\n    Example\n    =======\n        array, *op_indices = subtensor_node.inputs\n        idx_list = getattr(subtensor_node.op, \"idx_list\", None)\n        indices = indices_from_subtensor(op_indices, idx_list)\n\n    \"\"\"\n\n    def convert_indices(indices, entry):\n        \"\"\"Reconstruct ``*Subtensor*`` index input parameter entries.\"\"\"\n        if indices and isinstance(entry, Type):\n            rval = indices.pop(0)\n            return rval\n        elif isinstance(entry, slice):\n            return slice(\n                convert_indices(indices, entry.start),\n                convert_indices(indices, entry.stop),\n                convert_indices(indices, entry.step),\n            )\n        else:\n            return entry\n\n    op_indices = list(op_indices)\n\n    return (\n        tuple(convert_indices(op_indices, idx) for idx in idx_list)\n        if idx_list\n        else tuple(op_indices)\n    )\n\n\ndef as_index_constant(\n    a: Optional[Union[slice, int, np.integer, Variable]]\n) -> Optional[Union[Variable, slice]]:\n    r\"\"\"Convert Python literals to Aesara constants--when possible--in `Subtensor` arguments.\n\n    This will leave `Variable`\\s untouched.\n    \"\"\"\n    if a is None:\n        return a\n    elif isinstance(a, slice):\n        return slice(\n            as_index_constant(a.start),\n            as_index_constant(a.stop),\n            as_index_constant(a.step),\n        )\n    elif isinstance(a, (int, np.integer)):\n        return aes.ScalarConstant(aes.int64, a)\n    elif not isinstance(a, Variable):\n        return as_tensor_variable(a)\n    else:\n        return a\n\n\ndef as_index_literal(\n    idx: Optional[Union[Variable, slice]]\n) -> Optional[Union[int, slice]]:\n    \"\"\"Convert a symbolic index element to its Python equivalent.\n\n    This is like the inverse of `as_index_constant`\n\n    Raises\n    ------\n    NotScalarConstantError\n    \"\"\"\n    if idx == np.newaxis or isinstance(getattr(idx, \"type\", None), NoneTypeT):\n        return np.newaxis\n\n    if isinstance(idx, Constant):\n        return idx.data.item() if isinstance(idx, np.ndarray) else idx.data\n\n    if isinstance(getattr(idx, \"type\", None), SliceType):\n        idx = slice(*idx.owner.inputs)\n\n    if isinstance(idx, slice):\n        return slice(\n            as_index_literal(idx.start),\n            as_index_literal(idx.stop),\n            as_index_literal(idx.step),\n        )\n\n    raise NotScalarConstantError()\n\n\ndef get_idx_list(inputs, idx_list):\n    return indices_from_subtensor(inputs[1:], idx_list)\n\n\ndef get_canonical_form_slice(\n    theslice: Union[slice, Variable], length: Variable\n) -> Tuple[Variable, int]:\n    \"\"\"Convert slices to canonical form.\n\n    Given a slice [start:stop:step] transform it into a canonical form\n    that respects the conventions imposed by python and numpy.\n\n    In a canonical form a slice is represented by a canonical form slice,\n    in which 0 <= start <= stop <= length and step > 0, and a flag which says\n    if the resulting set of numbers needs to be reversed or not.\n\n    \"\"\"\n    from aesara.tensor import ge, lt, sgn, switch\n\n    if not isinstance(theslice, slice):\n        try:\n            value = as_index_literal(theslice)\n        except NotScalarConstantError:\n            value = theslice\n\n        value = switch(lt(value, 0), (value + length), value)\n\n        return value, 1\n\n    def analyze(x):\n        try:\n            x_constant = as_index_literal(x)\n            is_constant = True\n        except NotScalarConstantError:\n            x_constant = x\n            is_constant = False\n        return x_constant, is_constant\n\n    start, is_start_constant = analyze(theslice.start)\n    stop, is_stop_constant = analyze(theslice.stop)\n    step, is_step_constant = analyze(theslice.step)\n    length, is_length_constant = analyze(length)\n\n    if step is None:\n        step = 1\n        is_step_constant = True\n\n    # First handle the easier and common case where `step` is 1 and\n    # either `start` or `stop` is a range boundary. More specializations\n    # could be added later. This makes the resulting graph smaller than\n    # in the generic case below.\n    if step == 1:\n        is_start_0 = (\n            start is None\n            or start == 0\n            or (\n                is_start_constant\n                and is_length_constant\n                and start < 0\n                and start + length <= 0\n            )\n        )\n        is_stop_length = (\n            stop is None\n            or stop in [length, sys.maxsize]\n            or (is_stop_constant and is_length_constant and stop >= length)\n        )\n        if is_start_0:\n            # 0:stop:1\n            if is_stop_length:\n                # Full slice.\n                return slice(0, length, 1), 1\n            if is_stop_constant and stop >= 0:\n                return (slice(0, switch(lt(stop, length), stop, length), 1), 1)\n            stop_plus_len = stop + length\n            stop = switch(\n                lt(stop, 0),\n                # stop < 0\n                switch(\n                    lt(stop_plus_len, 0),\n                    # stop + len < 0\n                    0,\n                    # stop + len >= 0\n                    stop_plus_len,\n                ),\n                # stop >= 0: use min(stop, length)\n                switch(lt(stop, length), stop, length),\n            )\n            return slice(0, stop, 1), 1\n        elif is_stop_length:\n            # start:length:1\n            if is_start_constant and start >= 0:\n                return slice(switch(lt(start, length), start, length), length, 1), 1\n            start_plus_len = start + length\n            start = switch(\n                lt(start, 0),\n                # start < 0\n                switch(\n                    lt(start_plus_len, 0),\n                    # start + len < 0\n                    0,\n                    # start + len >= 0\n                    start_plus_len,\n                ),\n                # start >= 0: use min(start, length)\n                switch(lt(start, length), start, length),\n            )\n            return slice(start, length, 1), 1\n\n    # This is the generic case.\n\n    if is_step_constant:\n        # When we know the sign of `step`, the graph can be made simpler.\n        assert step != 0\n        if step > 0:\n\n            def switch_neg_step(a, b):\n                return b\n\n            abs_step = step\n            sgn_step = 1\n        else:\n\n            def switch_neg_step(a, b):\n                return a\n\n            abs_step = -step\n            sgn_step = -1\n    else:\n        is_step_neg = lt(step, 0)\n\n        def switch_neg_step(a, b):\n            return switch(is_step_neg, a, b)\n\n        abs_step = abs(step)\n        sgn_step = sgn(step)\n\n    defstart = switch_neg_step(length - 1, 0)\n    defstop = switch_neg_step(-1, length)\n    if start is None:\n        start = defstart\n    else:\n        start = switch(lt(start, 0), start + length, start)\n        start = switch(lt(start, 0), switch_neg_step(-1, 0), start)\n        start = switch(ge(start, length), switch_neg_step(length - 1, length), start)\n    if stop is None or stop == sys.maxsize:\n        # The special \"maxsize\" case is probably not needed here,\n        # as slices containing maxsize are not generated by\n        # __getslice__ anymore.\n        stop = defstop\n    else:\n        stop = switch(lt(stop, 0), stop + length, stop)\n        stop = switch(lt(stop, 0), -1, stop)\n        stop = switch(ge(stop, length), length, stop)\n\n    nw_stop = switch_neg_step(start + 1, stop)\n    slice_len = (start - stop - 1) // abs_step + 1\n    slice_len = switch(lt(slice_len, 0), 0, slice_len)\n    neg_start = nw_stop - (slice_len - 1) * abs_step - 1\n    neg_start = switch(lt(neg_start, 0), (nw_stop - 1), neg_start)\n    nw_start = switch_neg_step(neg_start, start)\n    nw_start = switch(lt(nw_start, 0), 0, nw_start)\n    nw_stop = switch(lt(nw_stop, 0), 0, nw_stop)\n    # Ensure start <= stop.\n    nw_start = switch(lt(nw_start, nw_stop), nw_start, nw_stop)\n\n    nw_step = abs_step\n    if step != 1:\n        reverse = sgn_step\n        return slice(nw_start, nw_stop, nw_step), reverse\n    else:\n        return slice(nw_start, nw_stop, nw_step), 1\n\n\ndef range_len(slc):\n    \"\"\"Length of a `range` object.\n\n    Adapted from CPython.\n\n    \"\"\"\n    from aesara.tensor import and_, gt, lt, switch\n\n    start, stop, step = tuple(\n        as_index_constant(a) for a in [slc.start, slc.stop, slc.step]\n    )\n    return switch(\n        and_(gt(step, 0), lt(start, stop)),\n        1 + (stop - 1 - start) // step,\n        switch(\n            and_(lt(step, 0), gt(start, stop)),\n            1 + (start - 1 - stop) // (-step),\n            aes.ScalarConstant(aes.int64, 0),\n        ),\n    )\n\n\ndef slice_len(slc, n):\n    \"\"\"Compute the length of a slice for an array of a given length.\n\n    We're essentially computing `len(range(*slc.indices(n)))`.\n\n    \"\"\"\n    # TODO: Do we need to do this or should we expect `slc` to\n    # already be canonicalized?\n    canon_slc, _ = get_canonical_form_slice(slc, n)\n    return range_len(canon_slc)\n\n\ndef is_basic_idx(idx):\n    \"\"\"Determine if an index is of the NumPy basic type.\n\n    XXX: This only checks a single index, so an integer is *not* considered a\n    basic index, because--depending on the other indices its used with--an\n    integer can indicate advanced indexing.\n\n    \"\"\"\n    return isinstance(idx, (slice, type(None))) or isinstance(\n        getattr(idx, \"type\", None), (SliceType, NoneTypeT)\n    )\n\n\ndef basic_shape(shape, indices):\n    r\"\"\"Computes the shape resulting from basic NumPy indexing.\n\n    Basic indices are either ``slice``\\s or ``None``\\s.  ``Ellipsis`` are not\n    supported here; convert them to ``slice``\\s first.\n\n    Parameters\n    ----------\n    shape: Tuple[int]\n        The shape of the array being indexed\n    indices: Sequence[Or[slice, NoneType]]\n        A sequence of basic indices used to index an array.\n\n    \"\"\"\n    res_shape = ()\n    for idx, n in zip(indices, shape):\n        if isinstance(idx, slice):\n            res_shape += (slice_len(idx, n),)\n        elif isinstance(getattr(idx, \"type\", None), SliceType):\n            if idx.owner:\n                idx_inputs = idx.owner.inputs\n            else:\n                idx_inputs = (None,)\n            res_shape += (slice_len(slice(*idx_inputs), n),)\n        elif idx is None:\n            res_shape += (aes.ScalarConstant(aes.int64, 1),)\n        elif isinstance(getattr(idx, \"type\", None), NoneTypeT):\n            res_shape += (aes.ScalarConstant(aes.int64, 1),)\n        else:\n            raise ValueError(f\"Invalid index type: {idx}\")\n    return res_shape\n\n\ndef group_indices(indices):\n    \"\"\"Group indices sequentially by whether or not they're basic or advanced.\n\n    Returns\n    -------\n    Tuple[Boolean, List[Tuple[Integer, Any]]]\n        The boolean indicates whether or not the group is a set of basic\n        indices.  The list contains the contiguous set of indices paired with their\n        corresponding dimension number in the array being indexed.\n    \"\"\"\n    idx_groups = []\n    dim_num = -1\n    for basic, grp_indices in groupby(indices, key=is_basic_idx):\n        enum_grp_indices = []\n        for idx in grp_indices:\n            # We \"zip\" the dimension number to each index, which means we can't\n            # count indices that add new axes\n            if (idx is not None) and not isinstance(\n                getattr(idx, \"type\", None), NoneTypeT\n            ):\n                dim_num += 1\n\n            enum_grp_indices.append((dim_num, idx))\n\n        idx_groups.append((basic, enum_grp_indices))\n\n    return idx_groups\n\n\ndef indexed_result_shape(array_shape, indices, indices_are_shapes=False):\n    \"\"\"Compute the symbolic shape resulting from `a[indices]` for `a.shape == array_shape`.\n\n    This function uses NumPy's basic and advanced indexing logic.  It can also\n    handle combinations of advanced and basic indices.\n\n    Parameters\n    ----------\n    array_shape: Tuple[Variable]\n        Shape of the array being indexed.\n    indices: Sequence[Union[TensorVariable, Tuple[Union[None, slice, Variable]]]]\n        Either the indices themselves or the shapes of each index--depending\n        on the value of `indices_are_shapes`.\n    indices_are_shapes: bool (Optional)\n        Indicates whether or not the `indices` contains shape tuples instead of\n        the actual index arrays.  If you use this approach, make sure that the\n        broadcastable dimensions are (scalar) constants with the value `1`, or `1`\n        exactly.\n    \"\"\"\n    res_shape = ()\n\n    remaining_dims = range(aesara.tensor.basic.get_vector_length(array_shape))\n    idx_groups = group_indices(indices)\n\n    if len(idx_groups) > 2 or len(idx_groups) > 1 and not idx_groups[0][0]:\n        # Bring adv. index groups to the front and merge each group\n        idx_groups = sorted(idx_groups, key=lambda x: x[0])\n        idx_groups = groupby(\n            chain.from_iterable(d_idx for _, d_idx in idx_groups),\n            key=lambda x: is_basic_idx(x[1]),\n        )\n\n    for basic, grp_dim_indices in idx_groups:\n        dim_nums, grp_indices = zip(*grp_dim_indices)\n        remaining_dims = tuple(dim for dim in remaining_dims if dim not in dim_nums)\n\n        if basic:\n            grp_shapes = tuple(array_shape[dim] for dim in dim_nums)\n            res_shape += basic_shape(grp_shapes, grp_indices)\n        else:\n            from aesara.tensor.extra_ops import broadcast_shape\n\n            res_shape += broadcast_shape(\n                *grp_indices, arrays_are_shapes=indices_are_shapes\n            )\n\n    res_shape += tuple(array_shape[dim] for dim in remaining_dims)\n\n    return res_shape\n\n\ndef get_slice_elements(idxs: List, cond: Callable) -> List:\n    \"\"\"Extract slice elements conditional on a given predicate function.\n\n    Parameters\n    ----------\n    idxs : a list of indices or slices.\n    cond : a callable that returns a bool\n\n    Returns\n    -------\n    list\n        idxs, with the slices flattened out into a list.\n        If cond is true for an entry, does not flatten it.\n\n    \"\"\"\n    ret = []\n\n    def helper(entry):\n        if cond(entry):\n            ret.append(entry)\n        elif isinstance(entry, slice):\n            helper(entry.start)\n            helper(entry.stop)\n            helper(entry.step)\n\n    for idx in idxs:\n        helper(idx)\n\n    return ret\n\n\ndef index_vars_to_types(entry, slice_ok=True):\n    r\"\"\"Change references to `Variable`s into references to `Type`s.\n\n    The `Subtensor.idx_list` field is unique to each `Subtensor` instance.  It\n    is not unique to each `Apply` node, so it should not refer to specific\n    `Variable`s.\n\n    TODO WRITEME: This function also accepts an `entry` already being a `Type`;\n    when would that happen?\n\n    \"\"\"\n    if (\n        isinstance(entry, (np.ndarray, Variable))\n        and hasattr(entry, \"dtype\")\n        and entry.dtype == \"bool\"\n    ):\n        raise AdvancedIndexingError(\"Invalid index type or slice for Subtensor\")\n\n    if isinstance(entry, Variable) and (\n        entry.type in invalid_scal_types or entry.type in invalid_tensor_types\n    ):\n        raise TypeError(\"Expected an integer\")\n\n    if isinstance(entry, Variable) and entry.type in scal_types:\n        return entry.type\n    elif isinstance(entry, Type) and entry in scal_types:\n        return entry\n\n    if (\n        isinstance(entry, Variable)\n        and entry.type in tensor_types\n        and all(entry.type.broadcastable)\n    ):\n        return aes.get_scalar_type(entry.type.dtype)\n    elif isinstance(entry, Type) and entry in tensor_types and all(entry.broadcastable):\n        return aes.get_scalar_type(entry.dtype)\n    elif slice_ok and isinstance(entry, slice):\n        a = entry.start\n        b = entry.stop\n        c = entry.step\n\n        if a is not None:\n            slice_a = index_vars_to_types(a, False)\n        else:\n            slice_a = None\n\n        if b is not None and b != sys.maxsize:\n            # The special \"maxsize\" case is probably not needed here,\n            # as slices containing maxsize are not generated by\n            # __getslice__ anymore.\n            slice_b = index_vars_to_types(b, False)\n        else:\n            slice_b = None\n\n        if c is not None:\n            slice_c = index_vars_to_types(c, False)\n        else:\n            slice_c = None\n\n        return slice(slice_a, slice_b, slice_c)\n    elif isinstance(entry, (int, np.integer)):\n        raise TypeError()\n    else:\n        raise AdvancedIndexingError(\"Invalid index type or slice for Subtensor\")\n\n\ndef get_constant_idx(\n    idx_list, inputs, allow_partial=False, only_process_constants=False, elemwise=True\n):\n    r\"\"\"Return an `idx_list` with its constant inputs replaced by their Python scalar equivalents.\n\n    May raise `NotScalarConstantError` if the indices contain non-constant entries.\n\n    If `allow_partial` is ``True``, then entries that are not constant will\n    stay as their input variable rather than raising an exception.\n\n    ``None`` entries are always left as-is.\n\n    Parameters\n    ----------\n    only_process_constants\n        If ``True``, we only attempt to obtain the value of an index/slice if\n        it's directly constant and don't try to dig through `DimShuffle`\\s,\n        fills, `Alloc`\\s, and other to figure out its value.\n\n    Examples\n    --------\n    Example usage where `v` and `a` are appropriately typed Aesara variables :\n    >>> b = a[v, 1:3]\n    >>> b.owner.op.idx_list\n    (ScalarType(int64), slice(ScalarType(int64), ScalarType(int64), None))\n    >>> get_constant_idx(b.owner.op.idx_list, b.owner.inputs, allow_partial=True)\n    [v, slice(1, 3, None)]\n    >>> get_constant_idx(b.owner.op.idx_list, b.owner.inputs)\n    NotScalarConstantError: v\n\n    \"\"\"\n    real_idx = get_idx_list(inputs, idx_list)\n\n    # TODO: Combine this with `as_index_literal`\n    def conv(val):\n        if val is None:\n            return None\n        elif isinstance(val, slice):\n            return slice(conv(val.start), conv(val.stop), conv(val.step))\n        else:\n            try:\n                return get_scalar_constant_value(\n                    val,\n                    only_process_constants=only_process_constants,\n                    elemwise=elemwise,\n                )\n            except NotScalarConstantError:\n                if allow_partial:\n                    return val\n                else:\n                    raise\n\n    return list(map(conv, real_idx))\n\n\ndef as_nontensor_scalar(a: Variable) -> aes.ScalarVariable:\n    \"\"\"Convert a value to a `ScalarType` variable.\"\"\"\n    # Since aes.as_scalar does not know about tensor types (it would\n    # create a circular import) , this method converts either a\n    # TensorVariable or a ScalarVariable to a scalar.\n    if isinstance(a, Variable) and isinstance(a.type, TensorType):\n        return aesara.tensor.scalar_from_tensor(a)\n    else:\n        return aes.as_scalar(a)\n\n\nclass Subtensor(COp):\n    \"\"\"Basic NumPy indexing operator.\"\"\"\n\n    check_input = False\n    view_map = {0: [0]}\n    _f16_ok = True\n    __props__ = (\"idx_list\",)\n\n    def __init__(self, idx_list):\n        # TODO: Provide the type of `self.idx_list`\n        self.idx_list = tuple(map(index_vars_to_types, idx_list))\n\n    def make_node(self, x, *inputs):\n        \"\"\"\n        Parameters\n        ----------\n        x\n            The tensor to take a subtensor of.\n        inputs\n            A list of aesara Scalars.\n\n        \"\"\"\n        x = as_tensor_variable(x)\n        inputs = tuple(as_nontensor_scalar(a) for a in inputs)\n\n        idx_list = list(self.idx_list)\n        if len(idx_list) > x.type.ndim:\n            raise IndexError(\"too many indices for array\")\n\n        input_types = get_slice_elements(\n            idx_list, lambda entry: isinstance(entry, Type)\n        )\n\n        assert len(inputs) == len(input_types)\n\n        for input, expected_type in zip(inputs, input_types):\n            if not expected_type.is_super(input.type):\n                raise TypeError(\n                    f\"Incompatible types for Subtensor template. Expected {input.type}, got {expected_type}.\"\n                )\n\n        # infer the broadcasting pattern\n        padded = get_constant_idx(\n            self.idx_list, (None,) + inputs, allow_partial=True\n        ) + [slice(None, None, None)] * (x.type.ndim - len(idx_list))\n\n        out_shape = []\n        for i, (p, s) in enumerate(zip(padded, x.type.shape)):\n            if isinstance(p, slice):\n                if s == 1:\n                    start = p.start\n                    try:\n                        start = get_scalar_constant_value(start)\n                    except NotScalarConstantError:\n                        pass\n                    if start is None or start == 0:\n                        start = p.start\n                        if start is None:\n                            start = 0\n                        if p.stop is None or (\n                            isinstance(p.stop, (int, np.integer, np.ndarray))\n                            and p.stop > start\n                        ):\n                            out_shape.append(1)\n                            continue\n\n                out_shape.append(None)\n\n        return Apply(\n            self,\n            (x,) + inputs,\n            [tensor(dtype=x.type.dtype, shape=out_shape)],\n        )\n\n    def perform(self, node, inputs, out_):\n        (out,) = out_\n        x = inputs[0]\n\n        cdata = get_idx_list(inputs, self.idx_list)\n        if len(cdata) == 1:\n            cdata = cdata[0]\n\n        out[0] = np.asarray(x.__getitem__(cdata))\n\n    def infer_shape(self, fgraph, node, shapes):\n        xshp = shapes[0]\n        assert len(xshp) == node.inputs[0].ndim\n        outshp = []\n        actual_idx_list = list(get_idx_list(node.inputs, self.idx_list))\n        padded = actual_idx_list + [slice(None, None, None)] * (\n            len(xshp) - len(self.idx_list)\n        )\n        i = 0\n        for idx, xl in zip(padded, xshp):\n            if isinstance(idx, slice):\n                # If it is the default (None, None, None) slice, or a variant,\n                # the shape will be xl\n                if (\n                    (idx.start in [None, 0])\n                    and (idx.stop in [None, sys.maxsize])\n                    and (idx.step is None or idx.step == 1)\n                ):\n                    outshp.append(xl)\n                else:\n                    cnf = get_canonical_form_slice(idx, xl)[0]\n                    if cnf.step == 1:\n                        length = cnf.stop - cnf.start\n                    else:\n                        length = (cnf.stop - cnf.start - 1) // cnf.step + 1\n                    outshp.append(length)\n                i += 1\n            else:\n                # That dimension is dropped\n                pass\n        assert i == node.outputs[0].ndim\n        assert len(outshp) == node.outputs[0].ndim\n        return [outshp]\n\n    def grad(self, inputs, grads):\n        (gz,) = grads\n        x = inputs[0]\n        rest = inputs[1:]\n        if x.dtype in discrete_dtypes:\n            first = x.zeros_like().astype(config.floatX)\n        else:\n            # For best optimization, we let this as an inc.\n            # This allow the opt local_IncSubtensor_serialize to apply first.\n            # We have an optimization that will convert this to a\n            # set subtensor here at:\n            # aesara/tensor/opt.py:local_incsubtensor_of_zeros_to_setsubtensor()\n            first = IncSubtensor(self.idx_list)(x.zeros_like(), gz, *rest)\n        return [first] + [DisconnectedType()()] * len(rest)\n\n    def connection_pattern(self, node):\n        rval = [[True]]\n\n        for ipt in node.inputs[1:]:\n            rval.append([False])\n\n        return rval\n\n    def __hash__(self):\n        msg = []\n        for entry in self.idx_list:\n            if isinstance(entry, slice):\n                msg += [(entry.start, entry.stop, entry.step)]\n            else:\n                msg += [entry]\n\n        idx_list = tuple(msg)\n        # backport\n        # idx_list = tuple((entry.start, entry.stop, entry.step)\n        #                 if isinstance(entry, slice)\n        #                 else entry\n        #                 for entry in self.idx_list)\n        return hash(idx_list)\n\n    @staticmethod\n    def str_from_slice(entry):\n        msg = []\n        for x in [entry.start, entry.stop, entry.step]:\n            if x is None:\n                msg.append(\"\")\n            else:\n                msg.append(str(x))\n        return \":\".join(msg)\n\n    def __str__(self):\n        indices = []\n        for entry in self.idx_list:\n            if isinstance(entry, slice):\n                indices.append(self.str_from_slice(entry))\n            else:\n                indices.append(str(entry))\n        return f\"{self.__class__.__name__}{{{', '.join(indices)}}}\"\n\n    @staticmethod\n    def default_helper_c_code_args():\n        \"\"\"\n        Returns a dictionary of default arguments to helper_c_code.\n\n        \"\"\"\n\n        return {\"c_prefix\": \"PyArray\", \"strides_mul\": 1}\n\n    @staticmethod\n    def helper_c_code(\n        node,\n        name,\n        inputs,\n        outputs,\n        sub,\n        idx_list,\n        view_ndim,\n        c_prefix=None,\n        strides_mul=None,\n    ):\n        \"\"\"\n        The parameters c_prefix are there to allow reusing this\n        function on PyArray object.\n\n        This fct take as input the x.\n\n        \"\"\"\n\n        default_args = Subtensor.default_helper_c_code_args()\n\n        if strides_mul is None:\n            strides_mul = default_args[\"strides_mul\"]\n\n        if c_prefix is None:\n            c_prefix = default_args[\"c_prefix\"]\n\n        #\n        # two arrays are created in C code:\n        # is_slice: len == ndim, 0 means int, 1 means slice\n        # subtensor_spec: len = n_ints + 3 * n_slices\n        #\n        fail = sub[\"fail\"]\n        init_cmds = []  # initialization for subtensor_spec\n        is_slice = []\n        # TODO: change that, it might lead to unexpected results,\n        # see assembla-#767\n        NONE_CODE = sys.maxsize - 1\n\n        pos = [0, 1]  # annoying version of global variable for init_entry\n\n        def inc_spec_pos(amt):\n            pos[0] += amt\n\n        def inc_input_pos(amt):\n            pos[1] += amt\n\n        def spec_pos():\n            return pos[0]\n\n        def input_pos():\n            return pos[1]\n\n        def init_entry(entry, depth=0):\n            if isinstance(entry, (np.integer, int)):\n                init_cmds.append(\"subtensor_spec[%i] = %i;\" % (spec_pos(), entry))\n                inc_spec_pos(1)\n                if depth == 0:\n                    is_slice.append(0)\n            elif isinstance(entry, Type):\n                init_cmds.append(\n                    \"subtensor_spec[%i] = %s;\" % (spec_pos(), inputs[input_pos()])\n                )\n                inc_spec_pos(1)\n                inc_input_pos(1)\n                if depth == 0:\n                    is_slice.append(0)\n            elif entry is None:\n                init_cmds.append(\"subtensor_spec[%i] = %i;\" % (spec_pos(), NONE_CODE))\n                inc_spec_pos(1)\n                if depth == 0:\n                    is_slice.append(0)\n            elif depth == 0 and isinstance(entry, slice):\n                init_entry(entry.start, depth + 1)\n                init_entry(entry.stop, depth + 1)\n                init_entry(entry.step, depth + 1)\n                is_slice.append(1)\n            else:\n                assert 0, entry\n\n        for entry in idx_list:\n            init_entry(entry)\n        # make sure we used all inputs\n        assert input_pos() == len(inputs), input_pos()\n        assert len(is_slice) <= node.inputs[0].ndim, node.inputs[0].ndim\n\n        len_is_slice = len(is_slice)\n\n        len_subtensor_spec = spec_pos()\n        subensor_spec = f\"npy_intp subtensor_spec[{len_subtensor_spec}];\"\n        if len_subtensor_spec == 0:\n            subensor_spec = \"npy_intp * subtensor_spec = NULL;\"\n\n        if is_slice:\n            is_slice_init = (\n                \"int is_slice[] = {\" + \",\".join([str(s) for s in is_slice]) + \"};\"\n            )\n        else:\n            is_slice_init = \"int* is_slice = NULL;\"\n        subtensor_init = \"\\n\".join(init_cmds)\n\n        (x,) = inputs[:1]\n        (z,) = outputs\n\n        if view_ndim:\n            rval = f\"\"\"\n        // Argument of the view\n        npy_intp xview_dims[{view_ndim}];\n        npy_intp xview_strides[{view_ndim}];\n\n        \"\"\"\n        else:\n            rval = \"\"\"\n        // Argument of the view\n        npy_intp* xview_dims = NULL;\n        npy_intp* xview_strides = NULL;\n\n        \"\"\"\n\n        rval += (\n            \"\"\"\n        // One more argument of the view\n        npy_intp xview_offset = 0;\n\n        // The subtensor is created by iterating over the dimensions\n        // and updating stride, shape, and data pointers\n\n        %(is_slice_init)s\n        %(subensor_spec)s\n        %(subtensor_init)s;\n        int spec_pos = 0; //position in subtensor_spec\n        int inner_ii = 0; // the current dimension of zview\n        int outer_ii = 0; // current dimension of z\n\n\n        for (; outer_ii < %(len_is_slice)s; ++outer_ii)\n        {\n            if (is_slice[outer_ii])\n            {\n                npy_intp length = %(c_prefix)s_DIMS(%(x)s)[outer_ii];\n                npy_intp slicelength;\n                npy_intp start = subtensor_spec[spec_pos+0];\n                npy_intp stop  = subtensor_spec[spec_pos+1];\n                npy_intp step  = subtensor_spec[spec_pos+2];\n                if (step == %(NONE_CODE)s) step = 1;\n\n                npy_intp defstart = step < 0 ? length-1 : 0;\n                npy_intp defstop = step < 0 ? -1 : length;\n\n                // logic adapted from\n                // PySlice_GetIndicesEx in python source\n                if (!step)\n                {\n                    PyErr_Format(PyExc_ValueError,\n                                 \"slice step cannot be zero\");\n                    %(fail)s;\n                }\n\n                if (start == %(NONE_CODE)s)\n                {\n                    start = defstart;\n                }\n                else\n                {\n                    if (start < 0) start += length;\n                    if (start < 0) start = (step < 0) ? -1 : 0;\n                    if (start >= length)\n                        start = (step < 0) ? length - 1 : length;\n                }\n\n                if (stop == %(NONE_CODE)s)\n                {\n                    stop = defstop;\n                }\n                else\n                {\n                    if (stop < 0) stop += length;\n                    if (stop < 0) stop = (step < 0) ? -1 : 0;\n                    if (stop >= length)\n                        stop = (step < 0) ? length - 1 : length;\n                }\n\n                if ((step < 0 && stop >= start)\n                    || (step > 0 && start >= stop)) {\n                    slicelength = 0;\n                }\n                else if (step < 0) {\n                    slicelength = (stop-start+1)/step+1;\n                }\n                else {\n                    slicelength = (stop-start-1)/step+1;\n                }\n\n                if (0){\n                    fprintf(stdout, \"start %%zi\\\\n\", start);\n                    fprintf(stdout, \"stop %%zi\\\\n\", stop);\n                    fprintf(stdout, \"step %%zi\\\\n\", step);\n                    fprintf(stdout, \"length %%zi\\\\n\", length);\n                    fprintf(stdout, \"slicelength %%zi\\\\n\", slicelength);\n                }\n\n                assert (slicelength <= length);\n\n                xview_offset += (npy_intp)%(c_prefix)s_STRIDES(%(x)s)[outer_ii]\n                    * start * %(strides_mul)s;\n                xview_dims[inner_ii] = slicelength;\n                xview_strides[inner_ii] = (npy_intp)%(c_prefix)s_STRIDES(%(x)s)[outer_ii] * step;\n\n                inner_ii += 1;\n                spec_pos += 3;\n            }\n            else // tuple coord `outer_ii` is an int\n            {\n                int idx = subtensor_spec[spec_pos];\n                if (idx < 0) idx += %(c_prefix)s_DIMS(%(x)s)[outer_ii];\n                if (idx >= 0)\n                {\n                    if (idx < %(c_prefix)s_DIMS(%(x)s)[outer_ii])\n                    {\n                        xview_offset += (npy_intp)%(c_prefix)s_STRIDES(%(x)s)[outer_ii] * idx *\n                               %(strides_mul)s;\n                    }\n                    else\n                    {\n                        PyErr_Format(PyExc_IndexError,\"index out of bounds\");\n                        %(fail)s;\n                    }\n                }\n                else\n                {\n                    PyErr_Format(PyExc_IndexError,\"index out of bounds\");\n                    %(fail)s;\n                }\n\n                spec_pos += 1;\n            }\n        }\n        assert (inner_ii <= %(view_ndim)s);\n        while (inner_ii < %(view_ndim)s)\n        {\n            assert (outer_ii < %(c_prefix)s_NDIM(%(x)s));\n            xview_dims[inner_ii] = %(c_prefix)s_DIMS(%(x)s)[outer_ii];\n            xview_strides[inner_ii] = %(c_prefix)s_STRIDES(%(x)s)[outer_ii];\n\n            inner_ii += 1;\n            outer_ii += 1;\n        }\n        \"\"\"\n            % locals()\n        )\n        # print rval\n        return rval\n\n    @staticmethod\n    def helper_c_code_cache_version():\n        return (9,)\n\n    def c_code(self, node, name, inputs, outputs, sub):  # DEBUG\n        if not isinstance(node.inputs[0].type, TensorType):\n            raise NotImplementedError()\n\n        x = inputs[0]\n        (z,) = outputs\n        ndim = node.inputs[0].ndim\n        view_ndim = node.outputs[0].ndim\n        fail = sub[\"fail\"]\n\n        decl = \"PyArrayObject * xview = NULL;\"\n\n        checkNDim = (\n            \"\"\"\n        if (PyArray_NDIM(%(x)s) != %(ndim)s){\n            PyErr_SetString(PyExc_ValueError,\n                                     \"Expected %(ndim)s dimensions input\"\n                                        );\n            %(fail)s\n        }\n        \"\"\"\n            % locals()\n        )\n\n        get_xview = self.helper_c_code(\n            node, name, inputs, outputs, sub, self.idx_list, view_ndim\n        )\n        build_view = (\n            \"\"\"\n        //TODO: give this Op a second output so that this view can be cached\n        //TODO: alternatively, fix the memory leak on failure\n        Py_INCREF(PyArray_DESCR(%(x)s));\n        xview = (PyArrayObject*)PyArray_NewFromDescr(\n                &PyArray_Type,\n                PyArray_DESCR(%(x)s),\n                %(view_ndim)s,\n                xview_dims,\n                xview_strides,\n                PyArray_BYTES(%(x)s) + xview_offset,\n                PyArray_FLAGS(%(x)s),\n                NULL);\n        assert (PyArray_NDIM(xview) == %(view_ndim)s);\n        if (!xview)\n        {\n            %(fail)s;\n        }\n        \"\"\"\n            % locals()\n        )\n\n        finish_view = f\"\"\"\n        Py_XDECREF({z});\n        Py_INCREF(py_{x});\n        PyArray_SetBaseObject(xview, py_{x});\n        assert(py_{x} == (PyObject*){x});\n        {z} = xview;\n        \"\"\"\n\n        return decl + checkNDim + \"{\" + get_xview + build_view + finish_view + \"}\"\n\n    def c_code_cache_version(self):\n        hv = self.helper_c_code_cache_version()\n        # If `helper_c_code_cache_version` is not versioned we do not want to\n        # have a versioned version of this op's C code.\n        if len(hv) == 0:\n            return ()\n        return (4, hv)\n\n    def R_op(self, inputs, eval_points):\n        # Subtensor is not differentiable wrt to its indices, therefore we\n        # do not even need to consider the eval_points provided for those\n        # (they should be defaulted to zeros_like by the global R_op)\n        if eval_points[0] is None:\n            return [None]\n        return self(eval_points[0], *inputs[1:], return_list=True)\n\n\nclass SubtensorPrinter(Printer):\n    def process(self, r, pstate):\n        return self._process(r.owner.op.idx_list, r.owner.inputs, pstate)\n\n    def _process(self, idxs, op_inputs, pstate):\n        inputs = list(op_inputs)\n        input = inputs.pop(0)\n        sidxs = []\n        getattr(pstate, \"precedence\", None)\n        for entry in idxs:\n            if isinstance(entry, aes.ScalarType):\n                with set_precedence(pstate):\n                    sidxs.append(pstate.pprinter.process(inputs.pop()))\n            elif isinstance(entry, slice):\n                if entry.start is None or entry.start == 0:\n                    msg1 = \"\"\n                else:\n                    msg1 = entry.start\n\n                if entry.stop is None or entry.stop == sys.maxsize:\n                    msg2 = \"\"\n                else:\n                    msg2 = entry.stop\n\n                if entry.step is None:\n                    msg3 = \"\"\n                else:\n                    msg3 = f\":{entry.step}\"\n\n                sidxs.append(f\"{msg1}:{msg2}{msg3}\")\n\n        with set_precedence(pstate, 1000):\n            sub = pstate.pprinter.process(input, pstate)\n\n        return f\"{sub}[{', '.join(sidxs)}]\"\n\n\npprint.assign(Subtensor, SubtensorPrinter())\n\n\ndef set_subtensor(x, y, inplace=False, tolerate_inplace_aliasing=False):\n    \"\"\"\n    Return x with the given subtensor overwritten by y.\n\n    Parameters\n    ----------\n    x\n        Symbolic variable for the lvalue of = operation.\n    y\n        Symbolic variable for the rvalue of = operation.\n    tolerate_inplace_aliasing\n        See inc_subtensor for documentation.\n\n    Examples\n    --------\n    To replicate the numpy expression \"r[10:] = 5\", type\n\n    >>> r = ivector()\n    >>> new_r = set_subtensor(r[10:], 5)\n\n    \"\"\"\n    return inc_subtensor(\n        x,\n        y,\n        inplace,\n        set_instead_of_inc=True,\n        tolerate_inplace_aliasing=tolerate_inplace_aliasing,\n    )\n\n\ndef inc_subtensor(\n    x,\n    y,\n    inplace=False,\n    set_instead_of_inc=False,\n    tolerate_inplace_aliasing=False,\n    ignore_duplicates=False,\n):\n    \"\"\"Update the value of an indexed array by a given amount.\n\n    This is equivalent to ``x[indices] += y`` or ``np.add.at(x, indices, y)``,\n    depending on the value of `ignore_duplicates`.\n\n    Parameters\n    ----------\n    x\n        The symbolic result of a Subtensor operation.\n    y\n        The amount by which to increment the array.\n    inplace\n        Don't use. Aesara will do in-place operations itself, when possible.\n    set_instead_of_inc\n        If True, do a set_subtensor instead.\n    tolerate_inplace_aliasing:\n        Allow `x` and `y` to be views of a single underlying array even while\n        working in-place. For correct results, `x` and `y` must not be overlapping\n        views; if they overlap, the result of this `Op` will generally be\n        incorrect. This value has no effect if ``inplace=False``.\n    ignore_duplicates\n        This determines whether or not ``x[indices] += y`` is used or\n        ``np.add.at(x, indices, y)``.  When the special duplicates handling of\n        ``np.add.at`` isn't required, setting this option to ``True``\n        (i.e. using ``x[indices] += y``) can resulting in faster compiled\n        graphs.\n\n    Examples\n    --------\n    To replicate the expression ``r[10:] += 5``:\n\n    ..code-block:: python\n\n        r = ivector()\n        new_r = inc_subtensor(r[10:], 5)\n\n    To replicate the expression ``r[[0, 1, 0]] += 5``:\n\n    ..code-block:: python\n\n        r = ivector()\n        new_r = inc_subtensor(r[10:], 5, ignore_duplicates=True)\n\n    \"\"\"\n    # First of all, y cannot have a higher dimension than x,\n    # nor have non-broadcastable dimensions where x is broadcastable.\n\n    x = as_tensor_variable(x)\n    y = as_tensor_variable(y)\n\n    if y.ndim > x.ndim:\n        raise TypeError(\n            f\"Trying to increment a {int(x.ndim)}-dimensional \"\n            f\"subtensor with a {int(y.ndim)}-dimensional value.\"\n        )\n\n    dim_offset = x.ndim - y.ndim\n    for dim in range(y.ndim):\n        if x.broadcastable[dim + dim_offset] and not y.broadcastable[dim]:\n            # It is acceptable to try to increment a subtensor with a\n            # broadcastable dim with a tensor that is not broadcastable\n            # on that dimension. However, its length must then be 1.\n            # We insert a SpecifyShape Op to make sure it is the case.\n            y = specify_broadcastable(y, dim)\n\n    if not x.owner:\n        raise TypeError(\"x must be the result of a subtensor operation\")\n\n    # retrieve idx_list from x.owner\n    if isinstance(x.owner.op, Subtensor):\n        if tolerate_inplace_aliasing:\n            destroyhandler_tolerate_aliased = [[0, 1]]\n        else:\n            destroyhandler_tolerate_aliased = []\n        the_op = IncSubtensor(\n            x.owner.op.idx_list,\n            inplace,\n            set_instead_of_inc,\n            destroyhandler_tolerate_aliased=destroyhandler_tolerate_aliased,\n        )\n        real_x = x.owner.inputs[0]\n        real_idxargs = x.owner.inputs[1:]\n        return the_op(real_x, y, *real_idxargs)\n    elif isinstance(x.owner.op, AdvancedSubtensor1):\n        real_x = x.owner.inputs[0]\n        ilist = x.owner.inputs[1]\n        if ignore_duplicates:\n            the_op = AdvancedIncSubtensor(\n                inplace, set_instead_of_inc=set_instead_of_inc, ignore_duplicates=True\n            )\n        else:\n            the_op = AdvancedIncSubtensor1(\n                inplace, set_instead_of_inc=set_instead_of_inc\n            )\n        return the_op(real_x, y, ilist)\n    elif isinstance(x.owner.op, AdvancedSubtensor):\n        real_x = x.owner.inputs[0]\n        ilist = x.owner.inputs[1:]\n        the_op = AdvancedIncSubtensor(\n            inplace,\n            set_instead_of_inc=set_instead_of_inc,\n            ignore_duplicates=ignore_duplicates,\n        )\n        return the_op(real_x, y, *ilist)\n    elif isinstance(x.owner.op, DimShuffle):\n        inner_x = x.owner.inputs[0]\n        # In the dimshuffle case, there are in fact two dimshuffles:\n        # one to make the indexed dimension the last one,\n        # and one to put it back where it was. So, in the case where we have\n        # inc_subtensor(x[:,i], y), the graph is actually\n        # inc_subtensor((x.T)[i].T, y).\n        # We could get all the way to x, and then get rid of the dimshuffles\n        # completely, but the problem is that advanced_inc_subtensor1 can only\n        # work on the first (outer-most, left-most) dimension of x,\n        # just like advanced_subtensor1.\n        # So we call advanced_inc_subtensor1(x.T, i, y.T) (as we also need to\n        # transpose y if it is not a scalar or a vector), but then we need to\n        # return something that has the same shape as x, not as x.T (inner_x).\n        # So re-apply the outer dimshuffle on the new inc_subtensor,\n        # and return advanced_inc_subtensor1(x.T, i, y.T).T.\n\n        # Get the dimshuffle pattern to apply to y.\n        x_order = x.owner.op.new_order\n        y_order = [\"x\"] * x.ndim\n        for i, v in enumerate(x_order):\n            if v != \"x\" and (v - dim_offset) >= 0:\n                y_order[v - dim_offset] = i\n\n        inner_incsubtensor = inc_subtensor(\n            inner_x,\n            y.dimshuffle(y_order),\n            inplace=inplace,\n            set_instead_of_inc=set_instead_of_inc,\n            tolerate_inplace_aliasing=tolerate_inplace_aliasing,\n            ignore_duplicates=ignore_duplicates,\n        )\n        # The broadcastable pattern of inner_x may not be the same as\n        # the one of x, so we have to build a new dimshuffle here,\n        # instead of reusing x.owner.op().\n        return inner_incsubtensor.dimshuffle(x.owner.op.new_order)\n\n    elif isinstance(x.owner.op, Reshape):\n        # This case happens when the indices are not arranged as a vector, but\n        # as a higher-dimensional array. This is handled by the subtensor\n        # by flattening this list, taking the subtensor, then reshaping the\n        # result.\n        inner_x = x.owner.inputs[0]\n        # Try to apply inc_subtensor on inner_x.\n        # If it works, there is no need to reshape, as the inc_subtensor\n        # will have the same shape as inner_x, which is what we want.\n        # We also explicitly duplicate y to its broadcasted shape\n        # before we partially flatten it to inner_x dimension. This is\n        # not strictly needed in all cases, but it is easier this way.\n        if y.ndim > 0:\n            # This if is needed to prevent some useless warning about\n            # old code bug.\n            expanded_y = alloc(y, *[x.shape[i] for i in range(x.ndim)])\n            flattened_y = expanded_y.reshape(inner_x.shape)\n        else:\n            flattened_y = y\n\n        inner_incsubtensor = inc_subtensor(\n            inner_x,\n            flattened_y,\n            inplace=inplace,\n            set_instead_of_inc=set_instead_of_inc,\n            tolerate_inplace_aliasing=tolerate_inplace_aliasing,\n            ignore_duplicates=ignore_duplicates,\n        )\n        return inner_incsubtensor\n    else:\n        raise TypeError(\"x must be the result of a subtensor operation\")\n\n\nclass IncSubtensor(COp):\n    \"\"\"\n    Increment a subtensor.\n\n    This is like numpy's\n\n        x[i,j,k] += y\n\n    It is used internally to implement the gradient on SubTensor.\n\n    Parameters\n    ----------\n    set_instead_of_inc\n        If True set the subtensor to the value instead of incrementing it by\n        that value.\n\n    \"\"\"\n\n    check_input = False\n    __props__ = (\"idx_list\", \"inplace\", \"set_instead_of_inc\")\n\n    def __init__(\n        self,\n        idx_list,\n        inplace=False,\n        set_instead_of_inc=False,\n        destroyhandler_tolerate_aliased=None,\n    ):\n        if destroyhandler_tolerate_aliased is None:\n            destroyhandler_tolerate_aliased = []\n        self.idx_list = list(map(index_vars_to_types, idx_list))\n        self.inplace = inplace\n        if inplace:\n            self.destroy_map = {0: [0]}\n        self.destroyhandler_tolerate_aliased = list(destroyhandler_tolerate_aliased)\n        self.set_instead_of_inc = set_instead_of_inc\n\n    def __hash__(self):\n        idx_list = tuple(\n            (entry.start, entry.stop, entry.step) if isinstance(entry, slice) else entry\n            for entry in self.idx_list\n        )\n        return hash((type(self), idx_list, self.inplace, self.set_instead_of_inc))\n\n    def __str__(self):\n        indices = []\n        for entry in self.idx_list:\n            if isinstance(entry, slice):\n                indices.append(Subtensor.str_from_slice(entry))\n            else:\n                indices.append(str(entry))\n        if self.inplace:\n            msg = \"Inplace\"\n        else:\n            msg = \"\"\n        if not self.set_instead_of_inc:\n            msg += \"Inc\"\n        else:\n            msg += \"Set\"\n        return f\"{self.__class__.__name__}{{{msg};{', '.join(indices)}}}\"\n\n    def make_node(self, x, y, *inputs):\n        \"\"\"\n        Parameters\n        ----------\n        x\n            The tensor to increment.\n        y\n            The value to increment by.\n        inputs: TODO WRITEME\n\n        \"\"\"\n        x, y = map(as_tensor_variable, [x, y])\n        if y.ndim > x.ndim:\n            raise ValueError(\n                f\"Trying to increment a {int(x.ndim)}-dimensional \"\n                f\"subtensor with a {int(y.ndim)}-dimensional value.\"\n            )\n        inputs = tuple(map(as_nontensor_scalar, inputs))\n\n        idx_list = list(self.idx_list)\n        if len(idx_list) > x.type.ndim:\n            raise IndexError(\"too many indices for array\")\n\n        input_types = get_slice_elements(\n            idx_list, lambda entry: isinstance(entry, Type)\n        )\n        if len(inputs) != len(input_types):\n            raise IndexError(\n                \"Not enough inputs to fill in the Subtensor template.\", inputs, idx_list\n            )\n        for input, expected_type in zip(inputs, input_types):\n            if not expected_type.is_super(input.type):\n                raise TypeError(\n                    f\"Wrong type for Subtensor template. Expected {input.type}, got {expected_type}.\"\n                )\n\n        return Apply(self, (x, y) + inputs, [x.type()])\n\n    def decl_view(self):\n        return \"PyArrayObject * zview = NULL;\"\n\n    def perform(self, node, inputs, out_):\n        (out,) = out_\n        x, y = inputs[:2]\n        indices = list(reversed(inputs[2:]))\n\n        def _convert(entry):\n            if isinstance(entry, Type):\n                return indices.pop()\n            elif isinstance(entry, slice):\n                return slice(\n                    _convert(entry.start), _convert(entry.stop), _convert(entry.step)\n                )\n            else:\n                return entry\n\n        cdata = tuple(map(_convert, self.idx_list))\n        if len(cdata) == 1:\n            cdata = cdata[0]\n        if not self.inplace:\n            x = x.copy()\n        sub_x = x.__getitem__(cdata)\n        if sub_x.shape:\n            # we've sliced out an N-D tensor with N > 0\n            if not self.set_instead_of_inc:\n                sub_x += y\n            else:\n                # sub_x += -sub_x + y\n                x.__setitem__(cdata, y)\n        else:\n            # scalar case\n            if not self.set_instead_of_inc:\n                x.__setitem__(cdata, sub_x + y)\n            else:\n                x.__setitem__(cdata, y)\n        out[0] = x\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        # This method delegates much of the work to helper\n        # methods. This method implements the main logic\n        # but subclasses may override the helper methods\n        # to change the particulars.\n\n        self.do_type_checking(node)\n\n        if self.inplace:  # convert bool to int\n            inplace = 1\n        else:\n            inplace = 0\n        x = inputs[0]\n        y = inputs[1]\n        (z,) = outputs\n        if self.set_instead_of_inc:  # convert bool to int\n            op_is_set = 1\n        else:\n            op_is_set = 0\n        fail = sub[\"fail\"]\n        view_ndim = node.inputs[0].ndim - sum(\n            not isinstance(idx, slice) for idx in self.idx_list\n        )\n\n        copy_of_x = self.copy_of_x(x)\n\n        copy_input_if_necessary = (\n            \"\"\"\n        if (%(inplace)s)\n        {\n            if (%(x)s != %(z)s)\n            {\n                Py_XDECREF(%(z)s);\n                Py_INCREF(%(x)s);\n                %(z)s = %(x)s;\n            }\n        }\n        else\n        {\n            Py_XDECREF(%(z)s);\n            %(z)s = %(copy_of_x)s;\n            if (!%(z)s) {\n                // Exception already set\n                %(fail)s\n            }\n        }\n        \"\"\"\n            % locals()\n        )\n\n        # get info needed to make zview: a view of %(z)s\n        helper_args = self.get_helper_c_code_args()\n\n        get_zview = Subtensor.helper_c_code(\n            node=node,\n            name=name,\n            inputs=outputs[:1] + inputs[2:],\n            outputs=outputs,\n            sub=sub,\n            idx_list=self.idx_list,\n            view_ndim=view_ndim,\n            **helper_args,\n        )\n\n        # Make a view on the output, as we will write into it.\n        alloc_zview = self.make_view_array(z, view_ndim)\n\n        build_view = (\n            \"\"\"\n        //TODO: give this Op a second output so that this view can be cached\n        //TODO: alternatively, fix the memory leak on failure\n        %(alloc_zview)s;\n        if (!zview)\n        {\n            %(fail)s;\n        }\n        \"\"\"\n            % locals()\n        )\n\n        copy_into = self.copy_into(\"zview\", y)\n\n        add_to_zview = self.add_to_zview(name, y, fail)\n\n        make_modification = (\n            \"\"\"\n        if (%(op_is_set)s)\n        {\n            if (%(copy_into)s) // does broadcasting\n            {\n                Py_DECREF(zview);\n                %(fail)s;\n            }\n        }\n        else\n        {\n            %(add_to_zview)s\n        }\n        \"\"\"\n            % locals()\n        )\n        return (\n            self.decl_view()\n            + copy_input_if_necessary\n            + \"{\"\n            + get_zview\n            + build_view\n            + make_modification\n            + \"Py_DECREF(zview);\"\n            + \"}\"\n        )\n\n    def do_type_checking(self, node):\n        \"\"\"\n        Should raise NotImplementedError if c_code does not support\n        the types involved in this node.\n\n        \"\"\"\n\n        if not isinstance(node.inputs[0].type, TensorType):\n            raise NotImplementedError()\n\n    def c_code_cache_version(self):\n        hv = Subtensor.helper_c_code_cache_version()\n        if hv:\n            return (3, hv)\n        else:\n            return ()\n\n    def copy_of_x(self, x):\n        \"\"\"\n        Parameters\n        ----------\n        x\n            A string giving the name of a C variable pointing to an array.\n\n        Returns\n        -------\n        object\n            C code expression to make a copy of x.\n\n        Base class uses PyArrayObject *, subclasses may override for\n        different types of arrays.\n\n        \"\"\"\n        # Parameters of PyArrary_FromAny are:\n        # array\n        # dtype: we pass NULL to say any dtype is acceptable, so the existing\n        #        dtype will be copied\n        # min_depth: we pass 0 to have this parameter ignored\n        # max_depth: we pass 0 to have this parameter ignored\n        # requirements: here we pass NPY_ARRAY_ENSURECOPY to force a copy\n        # context: this is almost always NULL, I'm not sure what it's used for\n        return f\"\"\"(PyArrayObject*)PyArray_FromAny(py_{x}, NULL, 0, 0,\n                NPY_ARRAY_ENSURECOPY, NULL)\"\"\"\n\n    def make_view_array(self, x, view_ndim):\n        \"\"\"\n        Parameters\n        ----------\n        x\n            A string identifying an array to be viewed.\n        view_ndim\n            A string specifying the number of dimensions to have in the view.\n\n        This doesn't need to actually set up the view with the right indexing;\n        we'll do that manually later.\n\n        \"\"\"\n\n        return (\n            \"\"\"Py_INCREF(PyArray_DESCR(%(x)s));\n        zview = (PyArrayObject*)PyArray_NewFromDescr(\n                &PyArray_Type,\n                PyArray_DESCR(%(x)s),\n                %(view_ndim)s,\n                xview_dims, //PyArray_DIMS(%(x)s),\n                xview_strides, //PyArray_STRIDES(%(x)s),\n                PyArray_BYTES(%(x)s) + xview_offset, //PyArray_DATA(%(x)s),\n                PyArray_FLAGS(%(x)s),\n                NULL);\n        \"\"\"\n            % locals()\n        )\n\n    def get_helper_c_code_args(self):\n        \"\"\"\n        Return a dictionary of arguments to pass to helper_c_code.\n\n        \"\"\"\n        return Subtensor.default_helper_c_code_args()\n\n    def copy_into(self, view, source):\n        \"\"\"\n        Parameters\n        ----------\n        view : string\n            C code expression for an array.\n        source : string\n            C code expression for an array.\n\n        Returns\n        -------\n        object\n            C code expression to copy source into view, and 0 on success.\n\n        \"\"\"\n        return f\"\"\"PyArray_CopyInto({view}, {source})\"\"\"\n\n    def add_to_zview(self, name, x, fail):\n        \"\"\"\n        Return C code to add x to zview. Should DECREF zview if the\n        add fails.\n\n        \"\"\"\n\n        return (\n            \"\"\"\n            PyArrayObject * add_rval = (PyArrayObject*)PyNumber_InPlaceAdd(\n                    (PyObject*)zview, py_%(x)s);\n            if (add_rval)\n            {\n                assert (PyArray_Check((PyObject*)add_rval));\n                assert (PyArray_DATA(add_rval) == PyArray_DATA(zview));\n                Py_DECREF(add_rval);\n            }\n            else\n            {\n                Py_DECREF(zview);\n                %(fail)s;\n            }\"\"\"\n            % locals()\n        )\n\n    def infer_shape(self, fgraph, node, shapes):\n        return [shapes[0]]\n\n    def R_op(self, inputs, eval_points):\n        if eval_points[0] is None or eval_points[1] is None:\n            return [None]\n        # Again we ignore eval points for indices because incsubtensor is\n        # not differentiable wrt to those\n        return self(eval_points[0], eval_points[1], *inputs[2:], return_list=True)\n\n    def connection_pattern(self, node):\n        rval = [[True], [True]]\n\n        for ipt in node.inputs[2:]:\n            rval.append([False])\n\n        return rval\n\n    def grad(self, inputs, grads):\n        (g_output,) = grads\n        x, y = inputs[:2]\n        idx_list = inputs[2:]\n\n        if x.dtype in discrete_dtypes:\n            # The output dtype is the same as x\n            gx = x.zeros_like(dtype=config.floatX)\n            if y.dtype in discrete_dtypes:\n                gy = y.zeros_like(dtype=config.floatX)\n            else:\n                gy = y.zeros_like()\n        elif x.dtype in complex_dtypes:\n            raise NotImplementedError(\"No support for complex grad yet\")\n        else:\n            if self.set_instead_of_inc:\n                gx = set_subtensor(\n                    Subtensor(idx_list=self.idx_list)(g_output, *idx_list),\n                    aesara.tensor.zeros_like(y),\n                )\n            else:\n                gx = g_output\n            gy = Subtensor(idx_list=self.idx_list)(g_output, *idx_list)\n            gy = _sum_grad_over_bcasted_dims(y, gy)\n\n        return [gx, gy] + [DisconnectedType()()] * len(idx_list)\n\n\nclass IncSubtensorPrinter(SubtensorPrinter):\n    def process(self, r, pstate):\n        x, y, *idx_args = r.owner.inputs\n\n        res = self._process(r.owner.op.idx_list, [x] + idx_args, pstate)\n\n        with set_precedence(pstate, 1000):\n            y_str = pstate.pprinter.process(r.owner.inputs[1], pstate)\n\n        if r.owner.op.set_instead_of_inc:\n            res = f\"set_subtensor({res}, {y_str})\"\n        else:\n            res = f\"inc_subtensor({res}, {y_str})\"\n        return res\n\n\npprint.assign(IncSubtensor, IncSubtensorPrinter())\n\n\ndef _sum_grad_over_bcasted_dims(x, gx):\n    \"\"\"\n    Sum of gx over dimensions to reproduce x.broadcastable.\n\n    This is useful to sum gradients over certain dimensions when\n    x has been broadcasted, and we need to sum the gradient contributions\n    over all duplications.\n\n    \"\"\"\n    if gx.broadcastable != x.broadcastable:\n        x_dim_added = gx.ndim - x.ndim\n        x_broad = (True,) * x_dim_added + x.broadcastable\n        assert sum(gx.broadcastable) < sum(x_broad)\n        axis_to_sum = []\n        for i in range(gx.ndim):\n            if gx.broadcastable[i] is False and x_broad[i] is True:\n                axis_to_sum.append(i)\n            elif gx.broadcastable[i] is True and x_broad[i] is False:\n                # This means that Aesara was able to infer that\n                # gx.shape[i] is 1, so x.shape[i] is 1, but we\n                # didn't know it. It is fine.\n                pass\n            else:\n                assert gx.broadcastable[i] == x_broad[i]\n        gx = gx.sum(axis=axis_to_sum, keepdims=True)\n        if gx.ndim != x.ndim:\n            assert gx.ndim > x.ndim\n            for i in range(x_dim_added):\n                assert gx.broadcastable[i]\n            gx = gx.dimshuffle(*list(range(x_dim_added, gx.ndim)))\n        assert gx.broadcastable == x.broadcastable\n    return gx\n\n\nclass AdvancedSubtensor1(COp):\n    \"\"\"\n    Implement x[ilist] where ilist is a vector of integers.\n\n    \"\"\"\n\n    # sparse_grad doesn't go in here since it only affects the output\n    # of the grad() method.\n    __props__ = ()\n    _f16_ok = True\n    check_input = False\n\n    def __init__(self, sparse_grad=False):\n        self.sparse_grad = sparse_grad\n\n    def make_node(self, x, ilist):\n        x_ = as_tensor_variable(x)\n        ilist_ = as_tensor_variable(ilist)\n        if ilist_.type.dtype not in integer_dtypes:\n            raise TypeError(\"index must be integers\")\n        if ilist_.type.ndim != 1:\n            raise TypeError(\"index must be vector\")\n        if x_.type.ndim == 0:\n            raise TypeError(\"cannot index into a scalar\")\n        out_shape = (ilist_.type.shape[0],) + x_.type.shape[1:]\n        out_shape = tuple(1 if s == 1 else None for s in out_shape)\n        return Apply(self, [x_, ilist_], [TensorType(dtype=x.dtype, shape=out_shape)()])\n", "right_context": "\n    def connection_pattern(self, node):\n        rval = [[True]]\n\n        for ipt in node.inputs[1:]:\n            rval.append([False])\n\n        return rval\n\n    def grad(self, inputs, grads):\n        x, ilist = inputs\n        (gz,) = grads\n        assert len(inputs) == 2\n        if self.sparse_grad:\n            if x.type.ndim != 2:\n                raise TypeError(\n                    \"AdvancedSubtensor1: you can't take the sparse grad\"\n                    \" from a tensor with ndim != 2. ndim is \" + str(x.type.ndim)\n                )\n\n            rval1 = [aesara.sparse.construct_sparse_from_list(x, gz, ilist)]\n        else:\n            if x.dtype in discrete_dtypes:\n                # The output dtype is the same as x\n                gx = x.zeros_like(dtype=config.floatX)\n            elif x.dtype in complex_dtypes:\n                raise NotImplementedError(\"No support for complex grad yet\")\n            else:\n                gx = x.zeros_like()\n            rval1 = [advanced_inc_subtensor1(gx, gz, ilist)]\n        return rval1 + [DisconnectedType()()] * (len(inputs) - 1)\n\n    def R_op(self, inputs, eval_points):\n        if eval_points[0] is None:\n            return [None]\n        return self.make_node(eval_points[0], *inputs[1:]).outputs\n\n    def infer_shape(self, fgraph, node, ishapes):\n        x, ilist = ishapes\n        return [ilist + x[1:]]\n\n    def c_support_code(self, **kwargs):\n        # In some versions of numpy, NPY_MIN_INTP is defined as MIN_LONG,\n        # which is not defined. It should be NPY_MIN_LONG instead in that case.\n        return dedent(\n            \"\"\"\\\n                #ifndef MIN_LONG\n                #define MIN_LONG NPY_MIN_LONG\n                #endif\"\"\"\n        )\n\n    def c_code(self, node, name, input_names, output_names, sub):\n        if self.__class__ is not AdvancedSubtensor1:\n            raise MethodNotDefined(\n                \"c_code defined for AdvancedSubtensor1,\" \" not for child class\",\n                type(self),\n            )\n        a_name, i_name = input_names[0], input_names[1]\n        output_name = output_names[0]\n        fail = sub[\"fail\"]\n        return (\n            \"\"\"\n            PyArrayObject *indices;\n            int i_type = PyArray_TYPE(%(i_name)s);\n            if (i_type != NPY_INTP) {\n                // Cast %(i_name)s to NPY_INTP (expected by PyArray_TakeFrom),\n                // if all values fit.\n                if (!PyArray_CanCastSafely(i_type, NPY_INTP) &&\n                    PyArray_SIZE(%(i_name)s) > 0) {\n                    npy_int64 min_val, max_val;\n                    PyObject* py_min_val = PyArray_Min(%(i_name)s, NPY_MAXDIMS,\n                                                       NULL);\n                    if (py_min_val == NULL) {\n                        %(fail)s;\n                    }\n                    min_val = PyLong_AsLongLong(py_min_val);\n                    Py_DECREF(py_min_val);\n                    if (min_val == -1 && PyErr_Occurred()) {\n                        %(fail)s;\n                    }\n                    PyObject* py_max_val = PyArray_Max(%(i_name)s, NPY_MAXDIMS,\n                                                       NULL);\n                    if (py_max_val == NULL) {\n                        %(fail)s;\n                    }\n                    max_val = PyLong_AsLongLong(py_max_val);\n                    Py_DECREF(py_max_val);\n                    if (max_val == -1 && PyErr_Occurred()) {\n                        %(fail)s;\n                    }\n                    if (min_val < NPY_MIN_INTP || max_val > NPY_MAX_INTP) {\n                        PyErr_SetString(PyExc_IndexError,\n                                     \"Index contains values \"\n                                     \"that are bigger than the maximum array \"\n                                     \"size on this system.\");\n                        %(fail)s;\n                    }\n                }\n                indices = (PyArrayObject*) PyArray_Cast(%(i_name)s, NPY_INTP);\n                if (indices == NULL) {\n                    %(fail)s;\n                }\n            }\n            else {\n                 indices = %(i_name)s;\n                 Py_INCREF(indices);\n            }\n            if (%(output_name)s != NULL) {\n                npy_intp nd, i, *shape;\n                nd = PyArray_NDIM(%(a_name)s) + PyArray_NDIM(indices) - 1;\n                if (PyArray_NDIM(%(output_name)s) != nd) {\n                    Py_CLEAR(%(output_name)s);\n                }\n                else {\n                    shape = PyArray_DIMS(%(output_name)s);\n                    for (i = 0; i < PyArray_NDIM(indices); i++) {\n                        if (shape[i] != PyArray_DIMS(indices)[i]) {\n                            Py_CLEAR(%(output_name)s);\n                            break;\n                        }\n                    }\n                    if (%(output_name)s != NULL) {\n                        for (; i < nd; i++) {\n                            if (shape[i] != PyArray_DIMS(%(a_name)s)[\n                                                i-PyArray_NDIM(indices)+1]) {\n                                Py_CLEAR(%(output_name)s);\n                                break;\n                            }\n                        }\n                    }\n                }\n            }\n            %(output_name)s = (PyArrayObject*)PyArray_TakeFrom(\n                        %(a_name)s, (PyObject*)indices, 0, %(output_name)s, NPY_RAISE);\n            Py_DECREF(indices);\n            if (%(output_name)s == NULL) %(fail)s;\n        \"\"\"\n            % locals()\n        )\n\n    def c_code_cache_version(self):\n        return (0, 1, 2)\n\n\nadvanced_subtensor1 = AdvancedSubtensor1()\n\n\nclass AdvancedIncSubtensor1(COp):\n    \"\"\"\n    Increments a subtensor using advanced slicing (list of index).\n\n    \"\"\"\n\n    __props__ = (\"inplace\", \"set_instead_of_inc\")\n    check_input = False\n    params_type = ParamsType(inplace=aes.bool, set_instead_of_inc=aes.bool)\n\n    def __init__(self, inplace=False, set_instead_of_inc=False):\n        self.inplace = bool(inplace)\n        self.set_instead_of_inc = bool(set_instead_of_inc)\n        if inplace:\n            self.destroy_map = {0: [0]}\n\n    def clone_inplace(self):\n        return self.__class__(inplace=True, set_instead_of_inc=self.set_instead_of_inc)\n\n    def __str__(self):\n        if self.inplace:\n            msg = \"inplace\"\n        else:\n            msg = \"no_inplace\"\n        if self.set_instead_of_inc:\n            msg += \",set\"\n        else:\n            msg += \",inc\"\n\n        return self.__class__.__name__ + \"{%s}\" % msg\n\n    def make_node(self, x, y, ilist):\n        x_ = as_tensor_variable(x)\n        y_ = as_tensor_variable(y)\n        ilist_ = as_tensor_variable(ilist)\n\n        if ilist_.type.dtype not in integer_dtypes:\n            raise TypeError(\"index must be integers\")\n        if ilist_.type.ndim != 1:\n            raise TypeError(\"index must be vector\")\n        if x_.type.ndim == 0:\n            raise TypeError(\"cannot index into a scalar\")\n        if y_.type.ndim > x_.type.ndim:\n            if self.set_instead_of_inc:\n                opname = \"set\"\n            else:\n                opname = \"increment\"\n            raise TypeError(\n                \"cannot %s x subtensor with ndim=%s by y with ndim=%s.\"\n                % (opname, x_.type.ndim, y_.type.ndim)\n            )\n\n        return Apply(self, [x_, y_, ilist_], [x_.type()])\n\n    def copy_of_x(self, x):\n        \"\"\"\n        Parameters\n        ----------\n        x : string\n            Gives the name of a C variable pointing to an array.\n\n        Returns\n        -------\n        object\n            C code expression to make a copy of x.\n\n        Base class uses PyArrayObject *, subclasses may override for\n        different types of arrays.\n\n        \"\"\"\n        # Parameters of PyArrary_FromAny are:\n        # array\n        # dtype: we pass NULL to say any dtype is acceptable, so the existing\n        #        dtype will be copied\n        # min_depth: we pass 0 to have this parameter ignored\n        # max_depth: we pass 0 to have this parameter ignored\n        # requirements: here we pass NPY_ARRAY_ENSURECOPY to force a copy\n        # context: this is almost always NULL, I'm not sure what it's used for\n        return f\"\"\"(PyArrayObject*)PyArray_FromAny(py_{x}, NULL, 0, 0,\n                NPY_ARRAY_ENSURECOPY, NULL)\"\"\"\n\n    def c_support_code(self, **kwargs):\n        types = [\n            \"npy_\" + t\n            for t in [\n                \"int8\",\n                \"int16\",\n                \"int32\",\n                \"int64\",\n                \"uint8\",\n                \"uint16\",\n                \"uint32\",\n                \"uint64\",\n                \"float16\",\n                \"float32\",\n                \"float64\",\n            ]\n        ]\n\n        complex_types = [\"npy_\" + t for t in (\"complex32\", \"complex64\", \"complex128\")]\n\n        inplace_map_template = \"\"\"\n        #if defined(%(typen)s)\n        static void %(type)s_inplace_add(PyArrayMapIterObject *mit,\n                                        PyArrayIterObject *it, int inc_or_set)\n        {\n            int index = mit->size;\n            while (index--) {\n                %(op)s\n\n                PyArray_MapIterNext(mit);\n                PyArray_ITER_NEXT(it);\n            }\n        }\n        #endif\n        \"\"\"\n\n        floatadd = (\n            \"((%(type)s*)mit->dataptr)[0] = \"\n            \"(inc_or_set ? ((%(type)s*)mit->dataptr)[0] : 0)\"\n            \" + ((%(type)s*)it->dataptr)[0];\"\n        )\n        complexadd = \"\"\"\n        ((%(type)s*)mit->dataptr)[0].real =\n            (inc_or_set ? ((%(type)s*)mit->dataptr)[0].real : 0)\n            + ((%(type)s*)it->dataptr)[0].real;\n        ((%(type)s*)mit->dataptr)[0].imag =\n            (inc_or_set ? ((%(type)s*)mit->dataptr)[0].imag : 0)\n            + ((%(type)s*)it->dataptr)[0].imag;\n        \"\"\"\n\n        fns = \"\".join(\n            [\n                inplace_map_template\n                % {\"type\": t, \"typen\": t.upper(), \"op\": floatadd % {\"type\": t}}\n                for t in types\n            ]\n            + [\n                inplace_map_template\n                % {\"type\": t, \"typen\": t.upper(), \"op\": complexadd % {\"type\": t}}\n                for t in complex_types\n            ]\n        )\n\n        def gen_binop(type, typen):\n            return f\"\"\"\n    #if defined({typen})\n    {type}_inplace_add,\n    #endif\n    \"\"\"\n\n        fn_array = (\n            \"static inplace_map_binop addition_funcs[] = {\"\n            + \"\".join(\n                [gen_binop(type=t, typen=t.upper()) for t in types + complex_types]\n            )\n            + \"NULL};\\n\"\n        )\n\n        def gen_num(typen):\n            return f\"\"\"\n    #if defined({typen})\n    {typen},\n    #endif\n    \"\"\"\n\n        type_number_array = (\n            \"static int type_numbers[] = {\"\n            + \"\".join([gen_num(typen=t.upper()) for t in types + complex_types])\n            + \"-1000};\"\n        )\n\n        code = (\n            \"\"\"\n            typedef void (*inplace_map_binop)(PyArrayMapIterObject *,\n                                            PyArrayIterObject *, int inc_or_set);\n            \"\"\"\n            + fns\n            + fn_array\n            + type_number_array\n            + \"\"\"\n    static int\n    map_increment(PyArrayMapIterObject *mit, PyArrayObject *op,\n                inplace_map_binop add_inplace, int inc_or_set)\n    {\n        PyArrayObject *arr = NULL;\n        PyArrayIterObject *it;\n        PyArray_Descr *descr;\n        if (mit->ait == NULL) {\n            return -1;\n        }\n        descr = PyArray_DESCR(mit->ait->ao);\n        Py_INCREF(descr);\n        arr = (PyArrayObject *)PyArray_FromAny((PyObject *)op, descr,\n                                    0, 0, NPY_ARRAY_FORCECAST, NULL);\n        if (arr == NULL) {\n            return -1;\n        }\n        if ((mit->subspace != NULL) && (mit->consec)) {\n            PyArray_MapIterSwapAxes(mit, (PyArrayObject **)&arr, 0);\n            if (arr == NULL) {\n                return -1;\n            }\n        }\n        it = (PyArrayIterObject*)\n                PyArray_BroadcastToShape((PyObject*)arr, mit->dimensions, mit->nd);\n        if (it  == NULL) {\n            Py_DECREF(arr);\n            return -1;\n        }\n\n        (*add_inplace)(mit, it, inc_or_set);\n\n        Py_DECREF(arr);\n        Py_DECREF(it);\n        return 0;\n    }\n\n\n    static int\n    inplace_increment(PyArrayObject *a, PyObject *index, PyArrayObject *inc,\n                    int inc_or_set)\n    {\n        inplace_map_binop add_inplace = NULL;\n        int type_number = -1;\n        int i = 0;\n        PyArrayMapIterObject * mit;\n\n        if (PyArray_FailUnlessWriteable(a, \"input/output array\") < 0) {\n            return -1;\n        }\n\n        if (PyArray_NDIM(a) == 0) {\n            PyErr_SetString(PyExc_IndexError, \"0-d arrays can't be indexed.\");\n            return -1;\n        }\n        type_number = PyArray_TYPE(a);\n\n        while (type_numbers[i] >= 0 && addition_funcs[i] != NULL){\n            if (type_number == type_numbers[i]) {\n                add_inplace = addition_funcs[i];\n                break;\n            }\n            i++ ;\n        }\n\n        if (add_inplace == NULL) {\n            PyErr_SetString(PyExc_TypeError, \"unsupported type for a\");\n            return -1;\n        }\n        mit = (PyArrayMapIterObject *) PyArray_MapIterArray(a, index);\n        if (mit == NULL) {\n            goto fail;\n        }\n        if (map_increment(mit, inc, add_inplace, inc_or_set) != 0) {\n            goto fail;\n        }\n\n        Py_DECREF(mit);\n\n        Py_INCREF(Py_None);\n        return 0;\n\n    fail:\n        Py_XDECREF(mit);\n\n        return -1;\n    }\n    \"\"\"\n        )\n\n        return code\n\n    def c_code(self, node, name, input_names, output_names, sub):\n        numpy_ver = [int(n) for n in np.__version__.split(\".\")[:2]]\n        if bool(numpy_ver < [1, 8]):\n            raise NotImplementedError\n        x, y, idx = input_names\n        out = output_names[0]\n        copy_of_x = self.copy_of_x(x)\n\n        return \"\"\"\n        PyObject* rval = NULL;\n        if (%(params)s->inplace)\n        {\n            if (%(x)s != %(out)s)\n            {\n                Py_XDECREF(%(out)s);\n                Py_INCREF(%(x)s);\n                %(out)s = %(x)s;\n            }\n        }\n        else\n        {\n            Py_XDECREF(%(out)s);\n            %(out)s = %(copy_of_x)s;\n            if (!%(out)s) {\n                // Exception already set\n                %(fail)s\n            }\n        }\n        if (inplace_increment(%(out)s, (PyObject *)%(idx)s, %(y)s, (1 - %(params)s->set_instead_of_inc))) {\n            %(fail)s;\n        }\n        Py_XDECREF(rval);\n        \"\"\" % dict(\n            x=x,\n            y=y,\n            idx=idx,\n            out=out,\n            copy_of_x=copy_of_x,\n            params=sub[\"params\"],\n            fail=sub[\"fail\"],\n        )\n\n    def c_code_cache_version(self):\n        return (8,)\n\n    def perform(self, node, inp, out_, params):\n        x, y, idx = inp\n        (out,) = out_\n        if not self.inplace:\n            x = x.copy()\n\n        if self.set_instead_of_inc:\n            x[idx] = y\n        else:\n            # In Numpy, `x[idx] += y` doesn't work if the same index is present\n            # many times: it does it only once.\n            np.add.at(x, idx, y)\n\n        out[0] = x\n\n    def infer_shape(self, fgraph, node, ishapes):\n        x, y, ilist = ishapes\n        return [x]\n\n    def R_op(self, inputs, eval_points):\n        if None in eval_points[:2]:\n            return [None]\n        return self.make_node(eval_points[0], eval_points[1], *inputs[2:]).outputs\n\n    def connection_pattern(self, node):\n        rval = [[True], [True], [False]]\n        return rval\n\n    def grad(self, inputs, grads):\n        (g_output,) = grads\n        x, y, idx_list = inputs\n        if x.dtype in discrete_dtypes:\n            # The output dtype is the same as x\n            gx = x.zeros_like(dtype=config.floatX)\n            if y.dtype in discrete_dtypes:\n                gy = y.zeros_like(dtype=config.floatX)\n            else:\n                gy = y.zeros_like()\n        elif x.dtype in complex_dtypes:\n            raise NotImplementedError(\"No support for complex grad yet\")\n        else:\n            if self.set_instead_of_inc:\n                gx = advanced_set_subtensor1(g_output, y.zeros_like(), idx_list)\n            else:\n                gx = g_output\n            gy = advanced_subtensor1(g_output, idx_list)\n            gy = _sum_grad_over_bcasted_dims(y, gy)\n\n        return [gx, gy] + [DisconnectedType()()]\n\n\nadvanced_inc_subtensor1 = AdvancedIncSubtensor1()\nadvanced_set_subtensor1 = AdvancedIncSubtensor1(set_instead_of_inc=True)\n\n\ndef as_index_variable(idx):\n    if idx is None:\n        return NoneConst.clone()\n    if isinstance(idx, slice):\n        return make_slice(idx)\n    if isinstance(idx, Variable) and isinstance(idx.type, SliceType):\n        return idx\n    if isinstance(idx, Variable) and isinstance(idx.type, NoneTypeT):\n        return idx\n    idx = as_tensor_variable(idx)\n    if idx.type.dtype not in discrete_dtypes:\n        raise TypeError(\"index must be integers or a boolean mask\")\n    return idx\n\n\ndef check_advanced_indexing_dimensions(input, idx_list):\n    \"\"\"\n    This function checks if the index list in idx_list is correct.\n    If there are any boolean masks, we check if the mask has the\n    same shape as the input. This is enforced in NumPy 0.13.0 and\n    newer, but not by earlier versions. If the size is not the same,\n    this method raises an IndexError.\n    \"\"\"\n    dim_seen = 0\n    for index in idx_list:\n        if index is np.newaxis:\n            # skip, does not count as an input dimension\n            pass\n        elif isinstance(index, np.ndarray) and index.dtype == \"bool\":\n            for i in range(index.ndim):\n                if index.shape[i] != input.shape[dim_seen + i]:\n                    raise IndexError(\n                        \"boolean index did not match indexed array \"\n                        f\"along dimension {int(dim_seen + i)}; dimension is \"\n                        f\"{int(input.shape[dim_seen + i])} but \"\n                        f\"corresponding boolean dimension is {index.shape[i]}\"\n                    )\n            dim_seen += index.ndim\n        else:\n            dim_seen += 1\n\n\nclass AdvancedSubtensor(Op):\n    \"\"\"Implements NumPy's advanced indexing.\"\"\"\n\n    __props__ = ()\n\n    def make_node(self, x, *index):\n        x = as_tensor_variable(x)\n        index = tuple(map(as_index_variable, index))\n\n        # We create a fake symbolic shape tuple and identify the broadcast\n        # dimensions from the shape result of this entire subtensor operation.\n        with config.change_flags(compute_test_value=\"off\"):\n            fake_shape = tuple(\n                tensor(dtype=\"int64\", shape=()) if s != 1 else 1 for s in x.type.shape\n            )\n\n            fake_index = tuple(\n                chain.from_iterable(\n                    aesara.tensor.basic.nonzero(idx)\n                    if getattr(idx, \"ndim\", 0) > 0\n                    and getattr(idx, \"dtype\", None) == \"bool\"\n                    else (idx,)\n                    for idx in index\n                )\n            )\n\n            out_shape = tuple(\n                i.value if isinstance(i, Constant) else None\n                for i in indexed_result_shape(fake_shape, fake_index)\n            )\n\n        return Apply(\n            self,\n            (x,) + index,\n            [tensor(dtype=x.type.dtype, shape=out_shape)],\n        )\n\n    def R_op(self, inputs, eval_points):\n        if eval_points[0] is None:\n            return [None]\n        return self.make_node(eval_points[0], *inputs[1:]).outputs\n\n    def infer_shape(self, fgraph, node, ishapes):\n        indices = node.inputs[1:]\n        index_shapes = list(ishapes[1:])\n        for i, idx in enumerate(indices):\n            if (\n                isinstance(idx, (np.bool_, bool))\n                or getattr(idx, \"dtype\", None) == \"bool\"\n            ):\n                raise ShapeError(\n                    \"Shape inference for boolean indices is not implemented\"\n                )\n            # The `ishapes` entries for `SliceType`s will be None, and\n            # we need to give `indexed_result_shape` the actual slices.\n            if isinstance(getattr(idx, \"type\", None), SliceType):\n                index_shapes[i] = idx\n\n        res_shape = indexed_result_shape(\n            ishapes[0], index_shapes, indices_are_shapes=True\n        )\n        assert node.outputs[0].ndim == len(res_shape)\n        return [[s for s in res_shape]]\n\n    def perform(self, node, inputs, out_):\n        (out,) = out_\n        check_advanced_indexing_dimensions(inputs[0], inputs[1:])\n        rval = inputs[0].__getitem__(tuple(inputs[1:]))\n        # When there are no arrays, we are not actually doing advanced\n        # indexing, so __getitem__ will not return a copy.\n        # Since no view_map is set, we need to copy the returned value\n        if not any(\n            isinstance(v.type, TensorType) and v.ndim > 0 for v in node.inputs[1:]\n        ):\n            rval = rval.copy()\n        out[0] = rval\n\n    def connection_pattern(self, node):\n        rval = [[True]]\n\n        for ipt in node.inputs[1:]:\n            rval.append([False])\n\n        return rval\n\n    def grad(self, inputs, grads):\n        (gz,) = grads\n        x = inputs[0]\n        if x.dtype in discrete_dtypes:\n            # The output dtype is the same as x\n            gx = x.zeros_like(dtype=config.floatX)\n        elif x.dtype in complex_dtypes:\n            raise NotImplementedError(\"No support for complex grad yet\")\n        else:\n            gx = x.zeros_like()\n        rest = inputs[1:]\n        return [advanced_inc_subtensor(gx, gz, *rest)] + [DisconnectedType()()] * len(\n            rest\n        )\n\n\nadvanced_subtensor = AdvancedSubtensor()\n\n\nclass AdvancedIncSubtensor(Op):\n    \"\"\"Increments a subtensor using advanced indexing.\"\"\"\n\n    __props__ = (\"inplace\", \"set_instead_of_inc\", \"ignore_duplicates\")\n\n    def __init__(\n        self, inplace=False, set_instead_of_inc=False, ignore_duplicates=False\n    ):\n        self.set_instead_of_inc = set_instead_of_inc\n        self.inplace = inplace\n        if inplace:\n            self.destroy_map = {0: [0]}\n        self.ignore_duplicates = ignore_duplicates\n\n    def __str__(self):\n        return \"{}{{{}, {}}}\".format(\n            self.__class__.__name__,\n            \"inplace=\" + str(self.inplace),\n            \" set_instead_of_inc=\" + str(self.set_instead_of_inc),\n        )\n\n    def make_node(self, x, y, *inputs):\n        x = as_tensor_variable(x)\n        y = as_tensor_variable(y)\n\n        new_inputs = []\n        for inp in inputs:\n            if isinstance(inp, (list, tuple)):\n                inp = as_tensor_variable(inp)\n            new_inputs.append(inp)\n        return Apply(\n            self,\n            (x, y) + tuple(new_inputs),\n            [\n                tensor(\n                    dtype=x.type.dtype,\n                    shape=tuple(1 if s == 1 else None for s in x.type.shape),\n                )\n            ],\n        )\n\n    def perform(self, node, inputs, out_):\n        x, y, *indices = inputs\n\n        check_advanced_indexing_dimensions(x, indices)\n\n        (out,) = out_\n        if not self.inplace:\n            out[0] = x.copy()\n        else:\n            out[0] = x\n\n        if self.set_instead_of_inc:\n            out[0][tuple(indices)] = y\n        elif self.ignore_duplicates:\n            out[0][tuple(indices)] += y\n        else:\n            np.add.at(out[0], tuple(indices), y)\n\n    def infer_shape(self, fgraph, node, ishapes):\n        return [ishapes[0]]\n\n    def connection_pattern(self, node):\n        rval = [[True], [True]]\n\n        for ipt in node.inputs[2:]:\n            rval.append([False])\n\n        return rval\n\n    def R_op(self, inputs, eval_points):\n        if None in eval_points[:2]:\n            return [None]\n        return self.make_node(eval_points[0], eval_points[1], *inputs[2:]).outputs\n\n    def grad(self, inpt, output_gradients):\n        x, y = inpt[:2]\n        idxs = inpt[2:]\n        (outgrad,) = output_gradients\n        if x.dtype in discrete_dtypes:\n            # The output dtype is the same as x\n            gx = x.zeros_like(dtype=config.floatX)\n            if y.dtype in discrete_dtypes:\n                gy = y.zeros_like(dtype=config.floatX)\n            else:\n                gy = y.zeros_like()\n        elif x.dtype in complex_dtypes:\n            raise NotImplementedError(\"No support for complex grad yet\")\n        else:\n            if self.set_instead_of_inc:\n                gx = advanced_set_subtensor(outgrad, y.zeros_like(), *idxs)\n            else:\n                gx = outgrad\n            gy = advanced_subtensor(outgrad, *idxs)\n            # Make sure to sum gy over the dimensions of y that have been\n            # added or broadcasted\n            gy = _sum_grad_over_bcasted_dims(y, gy)\n        return [gx, gy] + [DisconnectedType()() for _ in idxs]\n\n\nadvanced_inc_subtensor = AdvancedIncSubtensor()\nadvanced_set_subtensor = AdvancedIncSubtensor(set_instead_of_inc=True)\nadvanced_inc_subtensor_nodup = AdvancedIncSubtensor(ignore_duplicates=True)\nadvanced_set_subtensor_nodup = AdvancedIncSubtensor(\n    set_instead_of_inc=True, ignore_duplicates=True\n)\n\n\ndef take(a, indices, axis=None, mode=\"raise\"):\n    \"\"\"Take elements from an array along an axis.\n\n    When axis is not None, this function does the same thing as \"fancy\"\n    indexing (indexing arrays using arrays); however, it can be easier to use\n    if you need elements along a given axis. A call such as\n    ``np.take(arr, indices, axis=3)`` is equivalent to\n    ``arr[:,:,:,indices,...]``.\n\n    See `np.take`\n\n    Parameters\n    ----------\n    a : TensorVariable\n        The source array.\n    indices : TensorVariable, ndarray, list, tuple\n        The indices of the values to extract.\n    axis : int, optional\n        The axis over which to select values. By default, the flattened\n        input array is used.\n\n    \"\"\"\n    a = as_tensor_variable(a)\n    indices = as_tensor_variable(indices)\n\n    if not isinstance(axis, (int, type(None))):\n        raise TypeError(\"`axis` must be an integer or None\")\n\n    if axis is None:\n        return advanced_subtensor(a.flatten(), indices)\n    elif axis < 0:\n        axis += a.ndim\n\n    if mode == \"clip\":\n        indices = clip(indices, 0, a.shape[axis] - 1)\n    elif mode == \"wrap\":\n        indices = indices % a.shape[axis]\n\n    full_indices = (slice(None),) * axis + (indices,)\n\n    return a[full_indices]\n\n\n@_get_vector_length.register(Subtensor)\ndef _get_vector_length_Subtensor(op, var):\n    # If we take a slice, we know how many elements it will result in\n    # TODO: We can cover more `*Subtensor` cases.\n    try:\n        indices = aesara.tensor.subtensor.get_idx_list(\n            var.owner.inputs, var.owner.op.idx_list\n        )\n        start = (\n            None\n            if indices[0].start is None\n            else get_scalar_constant_value(indices[0].start)\n        )\n        stop = (\n            None\n            if indices[0].stop is None\n            else get_scalar_constant_value(indices[0].stop)\n        )\n        step = (\n            None\n            if indices[0].step is None\n            else get_scalar_constant_value(indices[0].step)\n        )\n\n        if start == stop:\n            return 0\n\n        arg_len = get_vector_length(var.owner.inputs[0])\n        return len(range(*slice(start, stop, step).indices(arg_len)))\n    except (ValueError, NotScalarConstantError):\n        raise ValueError(f\"Length of {var} cannot be determined\")\n\n\n__all__ = [\n    \"take\",\n    \"inc_subtensor\",\n    \"set_subtensor\",\n]\n", "import_text": ["logging", "sys", "itertools.chain", "itertools.groupby", "textwrap.dedent", "typing.Callable", "typing.Iterable", "typing.List", "typing.Optional", "typing.Tuple", "typing.Union", "numpy", "aesara", "aesara.scalar", "aesara.configdefaults.config", "aesara.gradient.DisconnectedType", "aesara.graph.basic.Apply", "aesara.graph.basic.Constant", "aesara.graph.basic.Variable", "aesara.graph.op.Op", "aesara.graph.type.Type", "aesara.graph.utils.MethodNotDefined", "aesara.link.c.op.COp", "aesara.link.c.params_type.ParamsType", "aesara.misc.safe_asarray._asarray", "aesara.printing.Printer", "aesara.printing.pprint", "aesara.printing.set_precedence", "aesara.scalar.basic.ScalarConstant", "aesara.tensor._get_vector_length", "aesara.tensor.as_tensor_variable", "aesara.tensor.get_vector_length", "aesara.tensor.basic.alloc", "aesara.tensor.basic.get_scalar_constant_value", "aesara.tensor.elemwise.DimShuffle", "aesara.tensor.exceptions.AdvancedIndexingError", "aesara.tensor.exceptions.NotScalarConstantError", "aesara.tensor.exceptions.ShapeError", "aesara.tensor.math.clip", "aesara.tensor.shape.Reshape", "aesara.tensor.shape.specify_broadcastable", "aesara.tensor.type.TensorType", "aesara.tensor.type.bscalar", "aesara.tensor.type.complex_dtypes", "aesara.tensor.type.cscalar", "aesara.tensor.type.discrete_dtypes", "aesara.tensor.type.dscalar", "aesara.tensor.type.fscalar", "aesara.tensor.type.integer_dtypes", "aesara.tensor.type.iscalar", "aesara.tensor.type.lscalar", "aesara.tensor.type.tensor", "aesara.tensor.type.ubscalar", "aesara.tensor.type.uiscalar", "aesara.tensor.type.ulscalar", "aesara.tensor.type.uwscalar", "aesara.tensor.type.wscalar", "aesara.tensor.type.zscalar", "aesara.tensor.type_other.NoneConst", "aesara.tensor.type_other.NoneTypeT", "aesara.tensor.type_other.SliceType", "aesara.tensor.type_other.make_slice"], "prompt": "\"\"\"\nDescription: This function performs an operation on a given node using input and output arrays.\n\nArgs:\n    node (type): The node on which the operation is to be performed.\n    inp (tuple): A tuple containing the input array and the indices array.\n    out_ (tuple): A tuple containing the output array.\n\nReturns:\n    None: The function does not return any value, it modifies the output array in-place.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": null, "function_dependencies": ["aesara.misc.safe_asarray._asarray", "numpy.can_cast", "numpy.any"], "project_create_time": "2019-11-12T14:02:08+00:00", "project_update_time": "2024-04-17T04:06:16+00:00", "file_create_time": "2021-07-13T02:59:19Z", "file_update_time": "2023-03-21T00:29:25Z", "function_update_time": "2022-08-27T01:27:43Z", "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "reference_api": ["numpy.any"], "test_function": [{"file_path": "/aesara-rel-2.9.3/aesara-rel-2.9.3/tests/tensor/test_subtensor.py", "class_name": "TestSubtensor", "function_name": "test_ok_list", "code": "\n    def test_ok_list(self):\n        for data, idx in [\n            (random(4), [1, 0]),\n            (random(4, 5), [2, 3, -1]),\n            (random(4, 2, 3), [0, 3]),\n            (random(4, 2, 3), [3, 3, 1, 1, 2, 2, 0, 0]),\n            (random(4, 2, 3), [3, 3, 1, 1, 2, 2, 0, 0, -1, -2, -3, -4]),\n            # Test 4 dims as gpu code use another algo\n            # in that case This new algo is not as much\n            # optimized for that case.\n            (random(4, 4, 2, 3), [3, 3, 1, 1, 2, 2, 0, 0, -1, -2, -3, -4]),\n            # Test with TensorConstant index.\n            (random(4, 2, 3), at.constant([3, 3, 1, 1, 2, 2, 0, 0])),\n        ]:\n            data = np.asarray(data, dtype=self.dtype)\n            n = self.shared(data)\n            t = n[idx]\n\n            val = self.eval_output_and_check(t, op_type=AdvancedSubtensor1)\n            if isinstance(idx, list):\n                good = data[idx]\n            else:\n                good = data[idx.data]\n            assert val.ndim == data.ndim\n            assert np.allclose(val, good), (val, good)\n\n            # Test reuse of output memory\n            if type(AdvancedSubtensor1) == AdvancedSubtensor1:\n                op = AdvancedSubtensor1()\n                # When idx is a TensorConstant.\n                if hasattr(idx, \"data\"):\n                    idx = idx.data\n                test_out = [[None]]\n                op.perform(None, [data, idx], test_out)\n                out1 = test_out[0][0]\n                op.perform(None, [data, idx], test_out)\n                out2 = test_out[0][0]\n                assert out1 is out2\n\n            # test the grad\n            gn = aesara.grad(t.sum(), n)\n            g = self.function([], gn, op=AdvancedIncSubtensor1)\n            utt.verify_grad(\n                lambda m: m[[1, 3]],\n                [np.random.random((5, 5)).astype(self.dtype)],\n                mode=self.mode,\n            )\n            g()\n            utt.verify_grad(lambda m: m[idx], [data], mode=self.mode)"}]}, {"git_group": "opendilab", "git_name": "DI-engine", "version": "v0.5.1", "language": "Python", "project_name": "DI-engine-v0.5.1.zip", "file_path": "/DI-engine-v0.5.1/DI-engine-0.5.1/ding/utils/file_helper.py", "file_name": "file_helper.py", "focal_class": null, "focal_name": "save_file", "focal_parameter": [], "solution": "def save_file(path: str, data: object, fs_type: Union[None, str] = None, use_lock: bool = False) -> None:\n    if fs_type is None:\n        if path.lower().startswith('s3'):\n            fs_type = 'ceph'\n        elif get_mc_package() is not None:\n            fs_type = 'mc'\n        else:\n            fs_type = 'normal'\n    assert fs_type in ['normal', 'ceph', 'mc']\n    if fs_type == 'ceph':\n        save_file_ceph(path, data)\n    elif fs_type == 'normal':\n        if use_lock:\n            with get_file_lock(path, 'write'):\n                torch.save(data, path)\n        else:\n            torch.save(data, path)\n    elif fs_type == 'mc':\n        torch.save(data, path)\n        read_from_mc(path, flush=True)", "function_signature": "def save_file(path: str, data: object, fs_type: Union[None, str] = None, use_lock: bool = False) -> None :", "left_context": "import io\nfrom ditk import logging\nimport os\nimport pickle\nimport time\nfrom functools import lru_cache\nfrom typing import Union\n\nimport torch\n\nfrom .import_helper import try_import_ceph, try_import_redis, try_import_rediscluster, try_import_mc\nfrom .lock_helper import get_file_lock\n\n_memcached = None\n_redis_cluster = None\n\nif os.environ.get('DI_STORE', 'off').lower() == 'on':\n    print('Enable DI-store')\n    from di_store import Client\n\n    di_store_config_path = os.environ.get(\"DI_STORE_CONFIG_PATH\", './di_store.yaml')\n    di_store_client = Client(di_store_config_path)\n\n    def save_to_di_store(data):\n        return di_store_client.put(data)\n\n    def read_from_di_store(object_ref):\n        data = di_store_client.get(object_ref)\n        di_store_client.delete(object_ref)\n        return data\nelse:\n    save_to_di_store = read_from_di_store = None\n\n\n@lru_cache()\ndef get_ceph_package():\n    return try_import_ceph()\n\n\n@lru_cache()\ndef get_redis_package():\n    return try_import_redis()\n\n\n@lru_cache()\ndef get_rediscluster_package():\n    return try_import_rediscluster()\n\n\n@lru_cache()\ndef get_mc_package():\n    return try_import_mc()\n\n\ndef read_from_ceph(path: str) -> object:\n    \"\"\"\n    Overview:\n        Read file from ceph\n    Arguments:\n        - path (:obj:`str`): File path in ceph, start with ``\"s3://\"``\n    Returns:\n        - (:obj:`data`): Deserialized data\n    \"\"\"\n    value = get_ceph_package().Get(path)\n    if not value:\n        raise FileNotFoundError(\"File({}) doesn't exist in ceph\".format(path))\n\n    return pickle.loads(value)\n\n\n@lru_cache()\ndef _get_redis(host='localhost', port=6379):\n    \"\"\"\n    Overview:\n        Ensures redis usage\n    Arguments:\n        - host (:obj:`str`): Host string\n        - port (:obj:`int`): Port number\n    Returns:\n        - (:obj:`Redis(object)`): Redis object with given ``host``, ``port``, and ``db=0``\n    \"\"\"\n    return get_redis_package().StrictRedis(host=host, port=port, db=0)\n\n\ndef read_from_redis(path: str) -> object:\n    \"\"\"\n    Overview:\n        Read file from redis\n    Arguments:\n        - path (:obj:`str`): Dile path in redis, could be a string key\n    Returns:\n        - (:obj:`data`): Deserialized data\n    \"\"\"\n    return pickle.loads(_get_redis().get(path))\n\n\ndef _ensure_rediscluster(startup_nodes=[{\"host\": \"127.0.0.1\", \"port\": \"7000\"}]):\n    \"\"\"\n    Overview:\n        Ensures redis usage\n    Arguments:\n        - List of startup nodes (:obj:`dict`) of\n            - host (:obj:`str`): Host string\n            - port (:obj:`int`): Port number\n    Returns:\n        - (:obj:`RedisCluster(object)`): RedisCluster object with given ``host``, ``port``, \\\n            and ``False`` for ``decode_responses`` in default.\n    \"\"\"\n    global _redis_cluster\n    if _redis_cluster is None:\n        _redis_cluster = get_rediscluster_package().RedisCluster(startup_nodes=startup_nodes, decode_responses=False)\n    return\n\n\ndef read_from_rediscluster(path: str) -> object:\n    \"\"\"\n    Overview:\n        Read file from rediscluster\n    Arguments:\n        - path (:obj:`str`): Dile path in rediscluster, could be a string key\n    Returns:\n        - (:obj:`data`): Deserialized data\n    \"\"\"\n    _ensure_rediscluster()\n    value_bytes = _redis_cluster.get(path)\n    value = pickle.loads(value_bytes)\n    return value\n\n\ndef read_from_file(path: str) -> object:\n    \"\"\"\n    Overview:\n        Read file from local file system\n    Arguments:\n        - path (:obj:`str`): File path in local file system\n    Returns:\n        - (:obj:`data`): Deserialized data\n    \"\"\"\n    with open(path, \"rb\") as f:\n        value = pickle.load(f)\n\n    return value\n\n\ndef _ensure_memcached():\n    \"\"\"\n    Overview:\n        Ensures memcache usage\n    Returns:\n        - (:obj:`MemcachedClient instance`): MemcachedClient's class instance built with current \\\n            memcached_client's ``server_list.conf`` and ``client.conf`` files\n    \"\"\"\n    global _memcached\n    if _memcached is None:\n        server_list_config_file = \"/mnt/lustre/share/memcached_client/server_list.conf\"\n        client_config_file = \"/mnt/lustre/share/memcached_client/client.conf\"\n        _memcached = get_mc_package().MemcachedClient.GetInstance(server_list_config_file, client_config_file)\n    return\n\n\ndef read_from_mc(path: str, flush=False) -> object:\n    \"\"\"\n    Overview:\n        Read file from memcache, file must be saved by `torch.save()`\n    Arguments:\n        - path (:obj:`str`): File path in local system\n    Returns:\n        - (:obj:`data`): Deserialized data\n    \"\"\"\n    _ensure_memcached()\n    while True:\n        try:\n            value = get_mc_package().pyvector()\n            if flush:\n                _memcached.Get(path, value, get_mc_package().MC_READ_THROUGH)\n                return\n            else:\n                _memcached.Get(path, value)\n            value_buf = get_mc_package().ConvertBuffer(value)\n            value_str = io.BytesIO(value_buf)\n            value_str = torch.load(value_str, map_location='cpu')\n            return value_str\n        except Exception:\n            print('read mc failed, retry...')\n            time.sleep(0.01)\n\n\ndef read_from_path(path: str):\n    \"\"\"\n    Overview:\n        Read file from ceph\n    Arguments:\n        - path (:obj:`str`): File path in ceph, start with ``\"s3://\"``, or use local file system\n    Returns:\n        - (:obj:`data`): Deserialized data\n    \"\"\"\n    if get_ceph_package() is None:\n        logging.info(\n            \"You do not have ceph installed! Loading local file!\"\n            \" If you are not testing locally, something is wrong!\"\n        )\n        return read_from_file(path)\n    else:\n        return read_from_ceph(path)\n\n\ndef save_file_ceph(path, data):\n    \"\"\"\n    Overview:\n        Save pickle dumped data file to ceph\n    Arguments:\n        - path (:obj:`str`): File path in ceph, start with ``\"s3://\"``, use file system when not\n        - data (:obj:`Any`): Could be dict, list or tensor etc.\n    \"\"\"\n    data = pickle.dumps(data)\n    save_path = os.path.dirname(path)\n    file_name = os.path.basename(path)\n    ceph = get_ceph_package()\n    if ceph is not None:\n        if hasattr(ceph, 'save_from_string'):\n            ceph.save_from_string(save_path, file_name, data)\n        elif hasattr(ceph, 'put'):\n            ceph.put(os.path.join(save_path, file_name), data)\n        else:\n            raise RuntimeError('ceph can not save file, check your ceph installation')\n    else:\n        size = len(data)\n        if save_path == 'do_not_save':\n            logging.info(\n                \"You do not have ceph installed! ignored file {} of size {}!\".format(file_name, size) +\n                \" If you are not testing locally, something is wrong!\"\n            )\n            return\n        p = os.path.join(save_path, file_name)\n        with open(p, 'wb') as f:\n            logging.info(\n                \"You do not have ceph installed! Saving as local file at {} of size {}!\".format(p, size) +\n                \" If you are not testing locally, something is wrong!\"\n            )\n            f.write(data)\n\n\ndef save_file_redis(path, data):\n    \"\"\"\n    Overview:\n        Save pickle dumped data file to redis\n    Arguments:\n        - path (:obj:`str`): File path (could be a string key) in redis\n        - data (:obj:`Any`): Could be dict, list or tensor etc.\n    \"\"\"\n    _get_redis().set(path, pickle.dumps(data))\n\n\ndef save_file_rediscluster(path, data):\n    \"\"\"\n    Overview:\n        Save pickle dumped data file to rediscluster\n    Arguments:\n        - path (:obj:`str`): File path (could be a string key) in redis\n        - data (:obj:`Any`): Could be dict, list or tensor etc.\n    \"\"\"\n    _ensure_rediscluster()\n    data = pickle.dumps(data)\n    _redis_cluster.set(path, data)\n    return\n\n\ndef read_file(path: str, fs_type: Union[None, str] = None, use_lock: bool = False) -> object:\n    \"\"\"\n    Overview:\n        Read file from path\n    Arguments:\n        - path (:obj:`str`): The path of file to read\n        - fs_type (:obj:`str` or :obj:`None`): The file system type, support ``{'normal', 'ceph'}``\n        - use_lock (:obj:`bool`): Whether ``use_lock`` is in local normal file system\n    \"\"\"\n    if fs_type is None:\n        if path.lower().startswith('s3'):\n            fs_type = 'ceph'\n        elif get_mc_package() is not None:\n            fs_type = 'mc'\n        else:\n            fs_type = 'normal'\n    assert fs_type in ['normal', 'ceph', 'mc']\n    if fs_type == 'ceph':\n        data = read_from_path(path)\n    elif fs_type == 'normal':\n        if use_lock:\n            with get_file_lock(path, 'read'):\n                data = torch.load(path, map_location='cpu')\n        else:\n            data = torch.load(path, map_location='cpu')\n    elif fs_type == 'mc':\n        data = read_from_mc(path)\n    return data\n\n", "right_context": "\n\ndef remove_file(path: str, fs_type: Union[None, str] = None) -> None:\n    \"\"\"\n    Overview:\n        Remove file\n    Arguments:\n        - path (:obj:`str`): The path of file you want to remove\n        - fs_type (:obj:`str` or :obj:`None`): The file system type, support ``{'normal', 'ceph'}``\n    \"\"\"\n    if fs_type is None:\n        fs_type = 'ceph' if path.lower().startswith('s3') else 'normal'\n    assert fs_type in ['normal', 'ceph']\n    if fs_type == 'ceph':\n        os.popen(\"aws s3 rm --recursive {}\".format(path))\n    elif fs_type == 'normal':\n        os.popen(\"rm -rf {}\".format(path))\n", "import_text": ["io", "ditk.logging", "os", "pickle", "time", "functools.lru_cache", "typing.Union", "torch"], "prompt": "\"\"\"\nDescription: This function saves a file to a specified path. The file can be saved to different file systems depending on the 'fs_type' parameter.\n\nArgs:\n    path (str): The path where the file should be saved.\n    data (object): The data to be saved.\n    fs_type (Union[None, str]): The type of file system to use. If None, the function will automatically determine the file system type based on the path.\n    use_lock (bool): If True, the function will use a file lock to prevent concurrent writes to the file.\n\nReturns:\n    None: This function does not return any value.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Overview:\n        Save data to file of path\n    Arguments:\n        - path (:obj:`str`): The path of file to save to\n        - data (:obj:`object`): The data to save\n        - fs_type (:obj:`str` or :obj:`None`): The file system type, support ``{'normal', 'ceph'}``\n        - use_lock (:obj:`bool`): Whether ``use_lock`` is in local normal file system\n    \"\"\"", "function_dependencies": ["torch.save"], "project_create_time": "2021-07-04T07:11:05+00:00", "project_update_time": "2024-04-18T01:32:59+00:00", "file_create_time": "2021-07-08T05:56:22Z", "file_update_time": "2024-01-23T06:04:24Z", "function_update_time": "2022-05-13T13:17:42Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["torch.save"], "test_function": [{"file_path": "/DI-engine-v0.5.1/DI-engine-0.5.1/ding/utils/tests/test_file_helper.py", "class_name": null, "function_name": "test_normal_file", "code": "\ndef test_normal_file():\n    data1 = {'a': [random.randint(0, 100) for i in range(100)]}\n    save_file('./f', data1)\n    data2 = read_file(\"./f\")\n    assert (data2 == data1)\n    with open(\"./f1\", \"wb\") as f1:\n        pickle.dump(data1, f1)\n    data3 = read_from_file(\"./f1\")\n    assert (data3 == data1)\n    data4 = read_from_path(\"./f1\")\n    assert (data4 == data1)\n    save_file_ceph(\"./f2\", data1)\n    assert (data1 == read_from_file(\"./f2\"))\n    # test lock\n    save_file('./f3', data1, use_lock=True)\n    data_read = read_file('./f3', use_lock=True)\n    assert isinstance(data_read, dict)\n\n    remove_file(\"./f\")\n    remove_file(\"./f1\")\n    remove_file(\"./f2\")\n    remove_file(\"./f3\")\n    remove_file('./f.lock')\n    remove_file('./f2.lock')\n    remove_file('./f3.lock')\n    remove_file('./name.txt')"}]}, {"git_group": "catboost", "git_name": "catboost", "version": "v1.2.5", "language": "Python", "project_name": "catboost-v1.2.5.zip", "file_path": "/catboost-v1.2.5/catboost-1.2.5/contrib/python/scipy/py2/scipy/signal/_peak_finding.py", "file_name": "_peak_finding.py", "focal_class": null, "focal_name": "find_peaks_cwt", "focal_parameter": ["vector", "widths"], "solution": "def find_peaks_cwt(vector, widths, wavelet=None, max_distances=None,\n                   gap_thresh=None, min_length=None, min_snr=1, noise_perc=10):\n    widths = np.asarray(widths)\n\n    if gap_thresh is None:\n        gap_thresh = np.ceil(widths[0])\n    if max_distances is None:\n        max_distances = widths / 4.0\n    if wavelet is None:\n        wavelet = ricker\n\n    cwt_dat = cwt(vector, wavelet, widths)\n    ridge_lines = _identify_ridge_lines(cwt_dat, max_distances, gap_thresh)\n    filtered = _filter_ridge_lines(cwt_dat, ridge_lines, min_length=min_length,\n                                   min_snr=min_snr, noise_perc=noise_perc)\n    max_locs = np.asarray([x[1][0] for x in filtered])\n    max_locs.sort()\n\n    return max_locs", "function_signature": "def find_peaks_cwt(vector, widths, wavelet=None, max_distances=None,\n                   gap_thresh=None, min_length=None, min_snr=1, noise_perc=10) :", "left_context": "\"\"\"\nFunctions for identifying peaks in signals.\n\"\"\"\nfrom __future__ import division, print_function, absolute_import\n\nimport math\nimport numpy as np\n\nfrom scipy._lib.six import xrange\nfrom scipy.signal.wavelets import cwt, ricker\nfrom scipy.stats import scoreatpercentile\n\nfrom ._peak_finding_utils import (\n    _local_maxima_1d,\n    _select_by_peak_distance,\n    _peak_prominences,\n    _peak_widths\n)\n\n\n__all__ = ['argrelmin', 'argrelmax', 'argrelextrema', 'peak_prominences',\n           'peak_widths', 'find_peaks', 'find_peaks_cwt']\n\n\ndef _boolrelextrema(data, comparator, axis=0, order=1, mode='clip'):\n    \"\"\"\n    Calculate the relative extrema of `data`.\n\n    Relative extrema are calculated by finding locations where\n    ``comparator(data[n], data[n+1:n+order+1])`` is True.\n\n    Parameters\n    ----------\n    data : ndarray\n        Array in which to find the relative extrema.\n    comparator : callable\n        Function to use to compare two data points.\n        Should take two arrays as arguments.\n    axis : int, optional\n        Axis over which to select from `data`.  Default is 0.\n    order : int, optional\n        How many points on each side to use for the comparison\n        to consider ``comparator(n,n+x)`` to be True.\n    mode : str, optional\n        How the edges of the vector are treated.  'wrap' (wrap around) or\n        'clip' (treat overflow as the same as the last (or first) element).\n        Default 'clip'.  See numpy.take\n\n    Returns\n    -------\n    extrema : ndarray\n        Boolean array of the same shape as `data` that is True at an extrema,\n        False otherwise.\n\n    See also\n    --------\n    argrelmax, argrelmin\n\n    Examples\n    --------\n    >>> testdata = np.array([1,2,3,2,1])\n    >>> _boolrelextrema(testdata, np.greater, axis=0)\n    array([False, False,  True, False, False], dtype=bool)\n\n    \"\"\"\n    if((int(order) != order) or (order < 1)):\n        raise ValueError('Order must be an int >= 1')\n\n    datalen = data.shape[axis]\n    locs = np.arange(0, datalen)\n\n    results = np.ones(data.shape, dtype=bool)\n    main = data.take(locs, axis=axis, mode=mode)\n    for shift in xrange(1, order + 1):\n        plus = data.take(locs + shift, axis=axis, mode=mode)\n        minus = data.take(locs - shift, axis=axis, mode=mode)\n        results &= comparator(main, plus)\n        results &= comparator(main, minus)\n        if(~results.any()):\n            return results\n    return results\n\n\ndef argrelmin(data, axis=0, order=1, mode='clip'):\n    \"\"\"\n    Calculate the relative minima of `data`.\n\n    Parameters\n    ----------\n    data : ndarray\n        Array in which to find the relative minima.\n    axis : int, optional\n        Axis over which to select from `data`.  Default is 0.\n    order : int, optional\n        How many points on each side to use for the comparison\n        to consider ``comparator(n, n+x)`` to be True.\n    mode : str, optional\n        How the edges of the vector are treated.\n        Available options are 'wrap' (wrap around) or 'clip' (treat overflow\n        as the same as the last (or first) element).\n        Default 'clip'. See numpy.take\n\n    Returns\n    -------\n    extrema : tuple of ndarrays\n        Indices of the minima in arrays of integers.  ``extrema[k]`` is\n        the array of indices of axis `k` of `data`.  Note that the\n        return value is a tuple even when `data` is one-dimensional.\n\n    See Also\n    --------\n    argrelextrema, argrelmax, find_peaks\n\n    Notes\n    -----\n    This function uses `argrelextrema` with np.less as comparator. Therefore it\n    requires a strict inequality on both sides of a value to consider it a\n    minimum. This means flat minima (more than one sample wide) are not detected.\n    In case of one-dimensional `data` `find_peaks` can be used to detect all\n    local minima, including flat ones, by calling it with negated `data`.\n\n    .. versionadded:: 0.11.0\n\n    Examples\n    --------\n    >>> from scipy.signal import argrelmin\n    >>> x = np.array([2, 1, 2, 3, 2, 0, 1, 0])\n    >>> argrelmin(x)\n    (array([1, 5]),)\n    >>> y = np.array([[1, 2, 1, 2],\n    ...               [2, 2, 0, 0],\n    ...               [5, 3, 4, 4]])\n    ...\n    >>> argrelmin(y, axis=1)\n    (array([0, 2]), array([2, 1]))\n\n    \"\"\"\n    return argrelextrema(data, np.less, axis, order, mode)\n\n\ndef argrelmax(data, axis=0, order=1, mode='clip'):\n    \"\"\"\n    Calculate the relative maxima of `data`.\n\n    Parameters\n    ----------\n    data : ndarray\n        Array in which to find the relative maxima.\n    axis : int, optional\n        Axis over which to select from `data`.  Default is 0.\n    order : int, optional\n        How many points on each side to use for the comparison\n        to consider ``comparator(n, n+x)`` to be True.\n    mode : str, optional\n        How the edges of the vector are treated.\n        Available options are 'wrap' (wrap around) or 'clip' (treat overflow\n        as the same as the last (or first) element).\n        Default 'clip'.  See `numpy.take`.\n\n    Returns\n    -------\n    extrema : tuple of ndarrays\n        Indices of the maxima in arrays of integers.  ``extrema[k]`` is\n        the array of indices of axis `k` of `data`.  Note that the\n        return value is a tuple even when `data` is one-dimensional.\n\n    See Also\n    --------\n    argrelextrema, argrelmin, find_peaks\n\n    Notes\n    -----\n    This function uses `argrelextrema` with np.greater as comparator. Therefore\n    it  requires a strict inequality on both sides of a value to consider it a\n    maximum. This means flat maxima (more than one sample wide) are not detected.\n    In case of one-dimensional `data` `find_peaks` can be used to detect all\n    local maxima, including flat ones.\n\n    .. versionadded:: 0.11.0\n\n    Examples\n    --------\n    >>> from scipy.signal import argrelmax\n    >>> x = np.array([2, 1, 2, 3, 2, 0, 1, 0])\n    >>> argrelmax(x)\n    (array([3, 6]),)\n    >>> y = np.array([[1, 2, 1, 2],\n    ...               [2, 2, 0, 0],\n    ...               [5, 3, 4, 4]])\n    ...\n    >>> argrelmax(y, axis=1)\n    (array([0]), array([1]))\n    \"\"\"\n    return argrelextrema(data, np.greater, axis, order, mode)\n\n\ndef argrelextrema(data, comparator, axis=0, order=1, mode='clip'):\n    \"\"\"\n    Calculate the relative extrema of `data`.\n\n    Parameters\n    ----------\n    data : ndarray\n        Array in which to find the relative extrema.\n    comparator : callable\n        Function to use to compare two data points.\n        Should take two arrays as arguments.\n    axis : int, optional\n        Axis over which to select from `data`.  Default is 0.\n    order : int, optional\n        How many points on each side to use for the comparison\n        to consider ``comparator(n, n+x)`` to be True.\n    mode : str, optional\n        How the edges of the vector are treated.  'wrap' (wrap around) or\n        'clip' (treat overflow as the same as the last (or first) element).\n        Default is 'clip'.  See `numpy.take`.\n\n    Returns\n    -------\n    extrema : tuple of ndarrays\n        Indices of the maxima in arrays of integers.  ``extrema[k]`` is\n        the array of indices of axis `k` of `data`.  Note that the\n        return value is a tuple even when `data` is one-dimensional.\n\n    See Also\n    --------\n    argrelmin, argrelmax\n\n    Notes\n    -----\n\n    .. versionadded:: 0.11.0\n\n    Examples\n    --------\n    >>> from scipy.signal import argrelextrema\n    >>> x = np.array([2, 1, 2, 3, 2, 0, 1, 0])\n    >>> argrelextrema(x, np.greater)\n    (array([3, 6]),)\n    >>> y = np.array([[1, 2, 1, 2],\n    ...               [2, 2, 0, 0],\n    ...               [5, 3, 4, 4]])\n    ...\n    >>> argrelextrema(y, np.less, axis=1)\n    (array([0, 2]), array([2, 1]))\n\n    \"\"\"\n    results = _boolrelextrema(data, comparator,\n                              axis, order, mode)\n    return np.nonzero(results)\n\n\ndef _arg_x_as_expected(value):\n    \"\"\"Ensure argument `x` is a 1D C-contiguous array of dtype('float64').\n\n    Used in `find_peaks`, `peak_prominences` and `peak_widths` to make `x`\n    compatible with the signature of the wrapped Cython functions.\n\n    Returns\n    -------\n    value : ndarray\n        A one-dimensional C-contiguous array with dtype('float64').\n    \"\"\"\n    value = np.asarray(value, order='C', dtype=np.float64)\n    if value.ndim != 1:\n        raise ValueError('`x` must be a 1D array')\n    return value\n\n\ndef _arg_peaks_as_expected(value):\n    \"\"\"Ensure argument `peaks` is a 1D C-contiguous array of dtype('intp').\n\n    Used in `peak_prominences` and `peak_widths` to make `peaks` compatible\n    with the signature of the wrapped Cython functions.\n\n    Returns\n    -------\n    value : ndarray\n        A one-dimensional C-contiguous array with dtype('intp').\n    \"\"\"\n    value = np.asarray(value)\n    if value.size == 0:\n        # Empty arrays default to np.float64 but are valid input\n        value = np.array([], dtype=np.intp)\n    try:\n        # Safely convert to C-contiguous array of type np.intp\n        value = value.astype(np.intp, order='C', casting='safe',\n                             subok=False, copy=False)\n    except TypeError:\n        raise TypeError(\"cannot safely cast `peaks` to dtype('intp')\")\n    if value.ndim != 1:\n        raise ValueError('`peaks` must be a 1D array')\n    return value\n\n\ndef _arg_wlen_as_expected(value):\n    \"\"\"Ensure argument `wlen` is of type `np.intp` and larger than 1.\n\n    Used in `peak_prominences` and `peak_widths`.\n\n    Returns\n    -------\n    value : np.intp\n        The original `value` rounded up to an integer or -1 if `value` was\n        None.\n    \"\"\"\n    if value is None:\n        # _peak_prominences expects an intp; -1 signals that no value was\n        # supplied by the user\n        value = -1\n    elif 1 < value:\n        # Round up to a positive integer\n        if not np.can_cast(value, np.intp, \"safe\"):\n            value = math.ceil(value)\n        value = np.intp(value)\n    else:\n        raise ValueError('`wlen` must be larger than 1, was {}'\n                         .format(value))\n    return value\n\n\ndef peak_prominences(x, peaks, wlen=None):\n    \"\"\"\n    Calculate the prominence of each peak in a signal.\n\n    The prominence of a peak measures how much a peak stands out from the\n    surrounding baseline of the signal and is defined as the vertical distance\n    between the peak and its lowest contour line.\n\n    Parameters\n    ----------\n    x : sequence\n        A signal with peaks.\n    peaks : sequence\n        Indices of peaks in `x`.\n    wlen : int, optional\n        A window length in samples that optionally limits the evaluated area for\n        each peak to a subset of `x`. The peak is always placed in the middle of\n        the window therefore the given length is rounded up to the next odd\n        integer. This parameter can speed up the calculation (see Notes).\n\n    Returns\n    -------\n    prominences : ndarray\n        The calculated prominences for each peak in `peaks`.\n    left_bases, right_bases : ndarray\n        The peaks' bases as indices in `x` to the left and right of each peak.\n        The higher base of each pair is a peak's lowest contour line.\n\n    Raises\n    ------\n    ValueError\n        If a value in `peaks` is an invalid index for `x`.\n\n    Warns\n    -----\n    PeakPropertyWarning\n        For indices in `peaks` that don't point to valid local maxima in `x`\n        the returned prominence will be 0 and this warning is raised. This\n        also happens if `wlen` is smaller than the plateau size of a peak.\n\n    Warnings\n    --------\n    This function may return unexpected results for data containing NaNs. To\n    avoid this, NaNs should either be removed or replaced.\n\n    See Also\n    --------\n    find_peaks\n        Find peaks inside a signal based on peak properties.\n    peak_widths\n        Calculate the width of peaks.\n\n    Notes\n    -----\n    Strategy to compute a peak's prominence:\n\n    1. Extend a horizontal line from the current peak to the left and right\n       until the line either reaches the window border (see `wlen`) or\n       intersects the signal again at the slope of a higher peak. An\n       intersection with a peak of the same height is ignored.\n    2. On each side find the minimal signal value within the interval defined\n       above. These points are the peak's bases.\n    3. The higher one of the two bases marks the peak's lowest contour line. The\n       prominence can then be calculated as the vertical difference between the\n       peaks height itself and its lowest contour line.\n\n    Searching for the peak's bases can be slow for large `x` with periodic\n    behavior because large chunks or even the full signal need to be evaluated\n    for the first algorithmic step. This evaluation area can be limited with the\n    parameter `wlen` which restricts the algorithm to a window around the\n    current peak and can shorten the calculation time if the window length is\n    short in relation to `x`.\n    However this may stop the algorithm from finding the true global contour\n    line if the peak's true bases are outside this window. Instead a higher\n    contour line is found within the restricted window leading to a smaller\n    calculated prominence. In practice this is only relevant for the highest set\n    of peaks in `x`. This behavior may even be used intentionally to calculate\n    \"local\" prominences.\n\n    .. versionadded:: 1.1.0\n\n    References\n    ----------\n    .. [1] Wikipedia Article for Topographic Prominence:\n       https://en.wikipedia.org/wiki/Topographic_prominence\n\n    Examples\n    --------\n    >>> from scipy.signal import find_peaks, peak_prominences\n    >>> import matplotlib.pyplot as plt\n\n    Create a test signal with two overlayed harmonics\n\n    >>> x = np.linspace(0, 6 * np.pi, 1000)\n    >>> x = np.sin(x) + 0.6 * np.sin(2.6 * x)\n\n    Find all peaks and calculate prominences\n\n    >>> peaks, _ = find_peaks(x)\n    >>> prominences = peak_prominences(x, peaks)[0]\n    >>> prominences\n    array([1.24159486, 0.47840168, 0.28470524, 3.10716793, 0.284603  ,\n           0.47822491, 2.48340261, 0.47822491])\n\n    Calculate the height of each peak's contour line and plot the results\n\n    >>> contour_heights = x[peaks] - prominences\n    >>> plt.plot(x)\n    >>> plt.plot(peaks, x[peaks], \"x\")\n    >>> plt.vlines(x=peaks, ymin=contour_heights, ymax=x[peaks])\n    >>> plt.show()\n\n    Let's evaluate a second example that demonstrates several edge cases for\n    one peak at index 5.\n\n    >>> x = np.array([0, 1, 0, 3, 1, 3, 0, 4, 0])\n    >>> peaks = np.array([5])\n    >>> plt.plot(x)\n    >>> plt.plot(peaks, x[peaks], \"x\")\n    >>> plt.show()\n    >>> peak_prominences(x, peaks)  # -> (prominences, left_bases, right_bases)\n    (array([3.]), array([2]), array([6]))\n\n    Note how the peak at index 3 of the same height is not considered as a\n    border while searching for the left base. Instead two minima at 0 and 2\n    are found in which case the one closer to the evaluated peak is always\n    chosen. On the right side however the base must be placed at 6 because the\n    higher peak represents the right border to the evaluated area.\n\n    >>> peak_prominences(x, peaks, wlen=3.1)\n    (array([2.]), array([4]), array([6]))\n\n    Here we restricted the algorithm to a window from 3 to 7 (the length is 5\n    samples because `wlen` was rounded up to the next odd integer). Thus the\n    only two candidates in the evaluated area are the two neighbouring samples\n    and a smaller prominence is calculated.\n    \"\"\"\n    x = _arg_x_as_expected(x)\n    peaks = _arg_peaks_as_expected(peaks)\n    wlen = _arg_wlen_as_expected(wlen)\n    return _peak_prominences(x, peaks, wlen)\n\n\ndef peak_widths(x, peaks, rel_height=0.5, prominence_data=None, wlen=None):\n    \"\"\"\n    Calculate the width of each peak in a signal.\n\n    This function calculates the width of a peak in samples at a relative\n    distance to the peak's height and prominence.\n\n    Parameters\n    ----------\n    x : sequence\n        A signal with peaks.\n    peaks : sequence\n        Indices of peaks in `x`.\n    rel_height : float, optional\n        Chooses the relative height at which the peak width is measured as a\n        percentage of its prominence. 1.0 calculates the width of the peak at\n        its lowest contour line while 0.5 evaluates at half the prominence\n        height. Must be at least 0. See notes for further explanation.\n    prominence_data : tuple, optional\n        A tuple of three arrays matching the output of `peak_prominences` when\n        called with the same arguments `x` and `peaks`. This data is calculated\n        internally if not provided.\n    wlen : int, optional\n        A window length in samples passed to `peak_prominences` as an optional\n        argument for internal calculation of `prominence_data`. This argument\n        is ignored if `prominence_data` is given.\n\n    Returns\n    -------\n    widths : ndarray\n        The widths for each peak in samples.\n    width_heights : ndarray\n        The height of the contour lines at which the `widths` where evaluated.\n    left_ips, right_ips : ndarray\n        Interpolated positions of left and right intersection points of a\n        horizontal line at the respective evaluation height.\n\n    Raises\n    ------\n    ValueError\n        If `prominence_data` is supplied but doesn't satisfy the condition\n        ``0 <= left_base <= peak <= right_base < x.shape[0]`` for each peak,\n        has the wrong dtype, is not C-contiguous or does not have the same\n        shape.\n\n    Warns\n    -----\n    PeakPropertyWarning\n        Raised if any calculated width is 0. This may stem from the supplied\n        `prominence_data` or if `rel_height` is set to 0.\n\n    Warnings\n    --------\n    This function may return unexpected results for data containing NaNs. To\n    avoid this, NaNs should either be removed or replaced.\n\n    See Also\n    --------\n    find_peaks\n        Find peaks inside a signal based on peak properties.\n    peak_prominences\n        Calculate the prominence of peaks.\n\n    Notes\n    -----\n    The basic algorithm to calculate a peak's width is as follows:\n\n    * Calculate the evaluation height :math:`h_{eval}` with the formula\n      :math:`h_{eval} = h_{Peak} - P \\\\cdot R`, where :math:`h_{Peak}` is the\n      height of the peak itself, :math:`P` is the peak's prominence and\n      :math:`R` a positive ratio specified with the argument `rel_height`.\n    * Draw a horizontal line at the evaluation height to both sides, starting at\n      the peak's current vertical position until the lines either intersect a\n      slope, the signal border or cross the vertical position of the peak's\n      base (see `peak_prominences` for an definition). For the first case,\n      intersection with the signal, the true intersection point is estimated\n      with linear interpolation.\n    * Calculate the width as the horizontal distance between the chosen\n      endpoints on both sides. As a consequence of this the maximal possible\n      width for each peak is the horizontal distance between its bases.\n\n    As shown above to calculate a peak's width its prominence and bases must be\n    known. You can supply these yourself with the argument `prominence_data`.\n    Otherwise they are internally calculated (see `peak_prominences`).\n\n    .. versionadded:: 1.1.0\n\n    Examples\n    --------\n    >>> from scipy.signal import chirp, find_peaks, peak_widths\n    >>> import matplotlib.pyplot as plt\n\n    Create a test signal with two overlayed harmonics\n\n    >>> x = np.linspace(0, 6 * np.pi, 1000)\n    >>> x = np.sin(x) + 0.6 * np.sin(2.6 * x)\n\n    Find all peaks and calculate their widths at the relative height of 0.5\n    (contour line at half the prominence height) and 1 (at the lowest contour\n    line at full prominence height).\n\n    >>> peaks, _ = find_peaks(x)\n    >>> results_half = peak_widths(x, peaks, rel_height=0.5)\n    >>> results_half[0]  # widths\n    array([ 64.25172825,  41.29465463,  35.46943289, 104.71586081,\n            35.46729324,  41.30429622, 181.93835853,  45.37078546])\n    >>> results_full = peak_widths(x, peaks, rel_height=1)\n    >>> results_full[0]  # widths\n    array([181.9396084 ,  72.99284945,  61.28657872, 373.84622694,\n        61.78404617,  72.48822812, 253.09161876,  79.36860878])\n\n    Plot signal, peaks and contour lines at which the widths where calculated\n\n    >>> plt.plot(x)\n    >>> plt.plot(peaks, x[peaks], \"x\")\n    >>> plt.hlines(*results_half[1:], color=\"C2\")\n    >>> plt.hlines(*results_full[1:], color=\"C3\")\n    >>> plt.show()\n    \"\"\"\n    x = _arg_x_as_expected(x)\n    peaks = _arg_peaks_as_expected(peaks)\n    if prominence_data is None:\n        # Calculate prominence if not supplied and use wlen if supplied.\n        wlen = _arg_wlen_as_expected(wlen)\n        prominence_data = _peak_prominences(x, peaks, wlen)\n    return _peak_widths(x, peaks, rel_height, *prominence_data)\n\n\ndef _unpack_condition_args(interval, x, peaks):\n    \"\"\"\n    Parse condition arguments for `find_peaks`.\n\n    Parameters\n    ----------\n    interval : number or ndarray or sequence\n        Either a number or ndarray or a 2-element sequence of the former. The\n        first value is always interpreted as `imin` and the second, if supplied,\n        as `imax`.\n    x : ndarray\n        The signal with `peaks`.\n    peaks : ndarray\n        An array with indices used to reduce `imin` and / or `imax` if those are\n        arrays.\n\n    Returns\n    -------\n    imin, imax : number or ndarray or None\n        Minimal and maximal value in `argument`.\n\n    Raises\n    ------\n    ValueError :\n        If interval border is given as array and its size does not match the size\n        of `x`.\n\n    Notes\n    -----\n\n    .. versionadded:: 1.1.0\n    \"\"\"\n    try:\n        imin, imax = interval\n    except (TypeError, ValueError):\n        imin, imax = (interval, None)\n\n    # Reduce arrays if arrays\n    if isinstance(imin, np.ndarray):\n        if imin.size != x.size:\n            raise ValueError('array size of lower interval border must match x')\n        imin = imin[peaks]\n    if isinstance(imax, np.ndarray):\n        if imax.size != x.size:\n            raise ValueError('array size of upper interval border must match x')\n        imax = imax[peaks]\n\n    return imin, imax\n\n\ndef _select_by_property(peak_properties, pmin, pmax):\n    \"\"\"\n    Evaluate where the generic property of peaks confirms to an interval.\n\n    Parameters\n    ----------\n    peak_properties : ndarray\n        An array with properties for each peak.\n    pmin : None or number or ndarray\n        Lower interval boundary for `peak_properties`. ``None`` is interpreted as\n        an open border.\n    pmax : None or number or ndarray\n        Upper interval boundary for `peak_properties`. ``None`` is interpreted as\n        an open border.\n\n    Returns\n    -------\n    keep : bool\n        A boolean mask evaluating to true where `peak_properties` confirms to the\n        interval.\n\n    See Also\n    --------\n    find_peaks\n\n    Notes\n    -----\n\n    .. versionadded:: 1.1.0\n    \"\"\"\n    keep = np.ones(peak_properties.size, dtype=bool)\n    if pmin is not None:\n        keep &= (pmin <= peak_properties)\n    if pmax is not None:\n        keep &= (peak_properties <= pmax)\n    return keep\n\n\ndef _select_by_peak_threshold(x, peaks, tmin, tmax):\n    \"\"\"\n    Evaluate which peaks fulfill the threshold condition.\n\n    Parameters\n    ----------\n    x : ndarray\n        A one-dimensional array which is indexable by `peaks`.\n    peaks : ndarray\n        Indices of peaks in `x`.\n    tmin, tmax : scalar or ndarray or None\n         Minimal and / or maximal required thresholds. If supplied as ndarrays\n         their size must match `peaks`. ``None`` is interpreted as an open\n         border.\n\n    Returns\n    -------\n    keep : bool\n        A boolean mask evaluating to true where `peaks` fulfill the threshold\n        condition.\n    left_thresholds, right_thresholds : ndarray\n        Array matching `peak` containing the thresholds of each peak on\n        both sides.\n\n    Notes\n    -----\n\n    .. versionadded:: 1.1.0\n    \"\"\"\n    # Stack thresholds on both sides to make min / max operations easier:\n    # tmin is compared with the smaller, and tmax with the greater thresold to\n    # each peak's side\n    stacked_thresholds = np.vstack([x[peaks] - x[peaks - 1],\n                                    x[peaks] - x[peaks + 1]])\n    keep = np.ones(peaks.size, dtype=bool)\n    if tmin is not None:\n        min_thresholds = np.min(stacked_thresholds, axis=0)\n        keep &= (tmin <= min_thresholds)\n    if tmax is not None:\n        max_thresholds = np.max(stacked_thresholds, axis=0)\n        keep &= (max_thresholds <= tmax)\n\n    return keep, stacked_thresholds[0], stacked_thresholds[1]\n\n\ndef find_peaks(x, height=None, threshold=None, distance=None,\n               prominence=None, width=None, wlen=None, rel_height=0.5,\n               plateau_size=None):\n    \"\"\"\n    Find peaks inside a signal based on peak properties.\n\n    This function takes a one-dimensional array and finds all local maxima by\n    simple comparison of neighbouring values. Optionally, a subset of these\n    peaks can be selected by specifying conditions for a peak's properties.\n\n    Parameters\n    ----------\n    x : sequence\n        A signal with peaks.\n    height : number or ndarray or sequence, optional\n        Required height of peaks. Either a number, ``None``, an array matching\n        `x` or a 2-element sequence of the former. The first element is\n        always interpreted as the  minimal and the second, if supplied, as the\n        maximal required height.\n    threshold : number or ndarray or sequence, optional\n        Required threshold of peaks, the vertical distance to its neighbouring\n        samples. Either a number, ``None``, an array matching `x` or a\n        2-element sequence of the former. The first element is always\n        interpreted as the  minimal and the second, if supplied, as the maximal\n        required threshold.\n    distance : number, optional\n        Required minimal horizontal distance (>= 1) in samples between\n        neighbouring peaks. The removal order is explained in the notes section.\n    prominence : number or ndarray or sequence, optional\n        Required prominence of peaks. Either a number, ``None``, an array\n        matching `x` or a 2-element sequence of the former. The first\n        element is always interpreted as the  minimal and the second, if\n        supplied, as the maximal required prominence.\n    width : number or ndarray or sequence, optional\n        Required width of peaks in samples. Either a number, ``None``, an array\n        matching `x` or a 2-element sequence of the former. The first\n        element is always interpreted as the  minimal and the second, if\n        supplied, as the maximal required prominence.\n    wlen : int, optional\n        Used for calculation of the peaks prominences, thus it is only used if\n        one of the arguments `prominence` or `width` is given. See argument\n        `wlen` in `peak_prominences` for a full description of its effects.\n    rel_height : float, optional\n        Used for calculation of the peaks width, thus it is only used if `width`\n        is given. See argument  `rel_height` in `peak_widths` for a full\n        description of its effects.\n    plateau_size : number or ndarray or sequence, optional\n        Required size of the flat top of peaks in samples. Either a number,\n        ``None``, an array matching `x` or a 2-element sequence of the former.\n        The first element is always interpreted as the minimal and the second,\n        if supplied as the maximal required plateau size.\n\n        .. versionadded:: 1.2.0\n\n    Returns\n    -------\n    peaks : ndarray\n        Indices of peaks in `x` that satisfy all given conditions.\n    properties : dict\n        A dictionary containing properties of the returned peaks which were\n        calculated as intermediate results during evaluation of the specified\n        conditions:\n\n        * 'peak_heights'\n              If `height` is given, the height of each peak in `x`.\n        * 'left_thresholds', 'right_thresholds'\n              If `threshold` is given, these keys contain a peaks vertical\n              distance to its neighbouring samples.\n        * 'prominences', 'right_bases', 'left_bases'\n              If `prominence` is given, these keys are accessible. See\n              `peak_prominences` for a description of their content.\n        * 'width_heights', 'left_ips', 'right_ips'\n              If `width` is given, these keys are accessible. See `peak_widths`\n              for a description of their content.\n        * 'plateau_sizes', left_edges', 'right_edges'\n              If `plateau_size` is given, these keys are accessible and contain\n              the indices of a peak's edges (edges are still part of the\n              plateau) and the calculated plateau sizes.\n\n              .. versionadded:: 1.2.0\n\n        To calculate and return properties without excluding peaks, provide the\n        open interval ``(None, None)`` as a value to the appropriate argument\n        (excluding `distance`).\n\n    Warns\n    -----\n    PeakPropertyWarning\n        Raised if a peak's properties have unexpected values (see\n        `peak_prominences` and `peak_widths`).\n\n    Warnings\n    --------\n    This function may return unexpected results for data containing NaNs. To\n    avoid this, NaNs should either be removed or replaced.\n\n    See Also\n    --------\n    find_peaks_cwt\n        Find peaks using the wavelet transformation.\n    peak_prominences\n        Directly calculate the prominence of peaks.\n    peak_widths\n        Directly calculate the width of peaks.\n\n    Notes\n    -----\n    In the context of this function, a peak or local maximum is defined as any\n    sample whose two direct neighbours have a smaller amplitude. For flat peaks\n    (more than one sample of equal amplitude wide) the index of the middle\n    sample is returned (rounded down in case the number of samples is even).\n    For noisy signals the peak locations can be off because the noise might\n    change the position of local maxima. In those cases consider smoothing the\n    signal before searching for peaks or use other peak finding and fitting\n    methods (like `find_peaks_cwt`).\n\n    Some additional comments on specifying conditions:\n\n    * Almost all conditions (excluding `distance`) can be given as half-open or\n      closed intervals, e.g ``1`` or ``(1, None)`` defines the half-open\n      interval :math:`[1, \\\\infty]` while ``(None, 1)`` defines the interval\n      :math:`[-\\\\infty, 1]`. The open interval ``(None, None)`` can be specified\n      as well, which returns the matching properties without exclusion of peaks.\n    * The border is always included in the interval used to select valid peaks.\n    * For several conditions the interval borders can be specified with\n      arrays matching `x` in shape which enables dynamic constrains based on\n      the sample position.\n    * The conditions are evaluated in the following order: `plateau_size`,\n      `height`, `threshold`, `distance`, `prominence`, `width`. In most cases\n      this order is the fastest one because faster operations are applied first\n      to reduce the number of peaks that need to be evaluated later.\n    * Satisfying the distance condition is accomplished by iterating over all\n      peaks in descending order based on their height and removing all lower\n      peaks that are too close.\n    * Use `wlen` to reduce the time it takes to evaluate the conditions for\n      `prominence` or `width` if `x` is large or has many local maxima\n      (see `peak_prominences`).\n\n    .. versionadded:: 1.1.0\n\n    Examples\n    --------\n    To demonstrate this function's usage we use a signal `x` supplied with\n    SciPy (see `scipy.misc.electrocardiogram`). Let's find all peaks (local\n    maxima) in `x` whose amplitude lies above 0.\n\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.misc import electrocardiogram\n    >>> from scipy.signal import find_peaks\n    >>> x = electrocardiogram()[2000:4000]\n    >>> peaks, _ = find_peaks(x, height=0)\n    >>> plt.plot(x)\n    >>> plt.plot(peaks, x[peaks], \"x\")\n    >>> plt.plot(np.zeros_like(x), \"--\", color=\"gray\")\n    >>> plt.show()\n\n    We can select peaks below 0 with ``height=(None, 0)`` or use arrays matching\n    `x` in size to reflect a changing condition for different parts of the\n    signal.\n\n    >>> border = np.sin(np.linspace(0, 3 * np.pi, x.size))\n    >>> peaks, _ = find_peaks(x, height=(-border, border))\n    >>> plt.plot(x)\n    >>> plt.plot(-border, \"--\", color=\"gray\")\n    >>> plt.plot(border, \":\", color=\"gray\")\n    >>> plt.plot(peaks, x[peaks], \"x\")\n    >>> plt.show()\n\n    Another useful condition for periodic signals can be given with the\n    `distance` argument. In this case we can easily select the positions of\n    QRS complexes within the electrocardiogram (ECG) by demanding a distance of\n    at least 150 samples.\n\n    >>> peaks, _ = find_peaks(x, distance=150)\n    >>> np.diff(peaks)\n    array([186, 180, 177, 171, 177, 169, 167, 164, 158, 162, 172])\n    >>> plt.plot(x)\n    >>> plt.plot(peaks, x[peaks], \"x\")\n    >>> plt.show()\n\n    Especially for noisy signals peaks can be easily grouped by their\n    prominence (see `peak_prominences`). E.g. we can select all peaks except\n    for the mentioned QRS complexes by limiting the allowed prominenence to 0.6.\n\n    >>> peaks, properties = find_peaks(x, prominence=(None, 0.6))\n    >>> properties[\"prominences\"].max()\n    0.5049999999999999\n    >>> plt.plot(x)\n    >>> plt.plot(peaks, x[peaks], \"x\")\n    >>> plt.show()\n\n    And finally let's examine a different section of the ECG which contains\n    beat forms of different shape. To select only the atypical heart beats we\n    combine two conditions: a minimal prominence of 1 and width of at least 20\n    samples.\n\n    >>> x = electrocardiogram()[17000:18000]\n    >>> peaks, properties = find_peaks(x, prominence=1, width=20)\n    >>> properties[\"prominences\"], properties[\"widths\"]\n    (array([1.495, 2.3  ]), array([36.93773946, 39.32723577]))\n    >>> plt.plot(x)\n    >>> plt.plot(peaks, x[peaks], \"x\")\n    >>> plt.vlines(x=peaks, ymin=x[peaks] - properties[\"prominences\"],\n    ...            ymax = x[peaks], color = \"C1\")\n    >>> plt.hlines(y=properties[\"width_heights\"], xmin=properties[\"left_ips\"],\n    ...            xmax=properties[\"right_ips\"], color = \"C1\")\n    >>> plt.show()\n    \"\"\"\n    # _argmaxima1d expects array of dtype 'float64'\n    x = _arg_x_as_expected(x)\n    if distance is not None and distance < 1:\n        raise ValueError('`distance` must be greater or equal to 1')\n\n    peaks, left_edges, right_edges = _local_maxima_1d(x)\n    properties = {}\n\n    if plateau_size is not None:\n        # Evaluate plateau size\n        plateau_sizes = right_edges - left_edges + 1\n        pmin, pmax = _unpack_condition_args(plateau_size, x, peaks)\n        keep = _select_by_property(plateau_sizes, pmin, pmax)\n        peaks = peaks[keep]\n        properties[\"plateau_sizes\"] = plateau_sizes\n        properties[\"left_edges\"] = left_edges\n        properties[\"right_edges\"] = right_edges\n        properties = {key: array[keep] for key, array in properties.items()}\n\n    if height is not None:\n        # Evaluate height condition\n        peak_heights = x[peaks]\n        hmin, hmax = _unpack_condition_args(height, x, peaks)\n        keep = _select_by_property(peak_heights, hmin, hmax)\n        peaks = peaks[keep]\n        properties[\"peak_heights\"] = peak_heights\n        properties = {key: array[keep] for key, array in properties.items()}\n\n    if threshold is not None:\n        # Evaluate threshold condition\n        tmin, tmax = _unpack_condition_args(threshold, x, peaks)\n        keep, left_thresholds, right_thresholds = _select_by_peak_threshold(\n            x, peaks, tmin, tmax)\n        peaks = peaks[keep]\n        properties[\"left_thresholds\"] = left_thresholds\n        properties[\"right_thresholds\"] = right_thresholds\n        properties = {key: array[keep] for key, array in properties.items()}\n\n    if distance is not None:\n        # Evaluate distance condition\n        keep = _select_by_peak_distance(peaks, x[peaks], distance)\n        peaks = peaks[keep]\n        properties = {key: array[keep] for key, array in properties.items()}\n\n    if prominence is not None or width is not None:\n        # Calculate prominence (required for both conditions)\n        wlen = _arg_wlen_as_expected(wlen)\n        properties.update(zip(\n            ['prominences', 'left_bases', 'right_bases'],\n            _peak_prominences(x, peaks, wlen=wlen)\n        ))\n\n    if prominence is not None:\n        # Evaluate prominence condition\n        pmin, pmax = _unpack_condition_args(prominence, x, peaks)\n        keep = _select_by_property(properties['prominences'], pmin, pmax)\n        peaks = peaks[keep]\n        properties = {key: array[keep] for key, array in properties.items()}\n\n    if width is not None:\n        # Calculate widths\n        properties.update(zip(\n            ['widths', 'width_heights', 'left_ips', 'right_ips'],\n            _peak_widths(x, peaks, rel_height, properties['prominences'],\n                         properties['left_bases'], properties['right_bases'])\n        ))\n        # Evaluate width condition\n        wmin, wmax = _unpack_condition_args(width, x, peaks)\n        keep = _select_by_property(properties['widths'], wmin, wmax)\n        peaks = peaks[keep]\n        properties = {key: array[keep] for key, array in properties.items()}\n\n    return peaks, properties\n\n\ndef _identify_ridge_lines(matr, max_distances, gap_thresh):\n    \"\"\"\n    Identify ridges in the 2-D matrix.\n\n    Expect that the width of the wavelet feature increases with increasing row\n    number.\n\n    Parameters\n    ----------\n    matr : 2-D ndarray\n        Matrix in which to identify ridge lines.\n    max_distances : 1-D sequence\n        At each row, a ridge line is only connected\n        if the relative max at row[n] is within\n        `max_distances`[n] from the relative max at row[n+1].\n    gap_thresh : int\n        If a relative maximum is not found within `max_distances`,\n        there will be a gap. A ridge line is discontinued if\n        there are more than `gap_thresh` points without connecting\n        a new relative maximum.\n\n    Returns\n    -------\n    ridge_lines : tuple\n        Tuple of 2 1-D sequences. `ridge_lines`[ii][0] are the rows of the\n        ii-th ridge-line, `ridge_lines`[ii][1] are the columns. Empty if none\n        found.  Each ridge-line will be sorted by row (increasing), but the\n        order of the ridge lines is not specified.\n\n    References\n    ----------\n    Bioinformatics (2006) 22 (17): 2059-2065.\n    :doi:`10.1093/bioinformatics/btl355`\n    http://bioinformatics.oxfordjournals.org/content/22/17/2059.long\n\n    Examples\n    --------\n    >>> data = np.random.rand(5,5)\n    >>> ridge_lines = _identify_ridge_lines(data, 1, 1)\n\n    Notes\n    -----\n    This function is intended to be used in conjunction with `cwt`\n    as part of `find_peaks_cwt`.\n\n    \"\"\"\n    if(len(max_distances) < matr.shape[0]):\n        raise ValueError('Max_distances must have at least as many rows '\n                         'as matr')\n\n    all_max_cols = _boolrelextrema(matr, np.greater, axis=1, order=1)\n    # Highest row for which there are any relative maxima\n    has_relmax = np.nonzero(all_max_cols.any(axis=1))[0]\n    if(len(has_relmax) == 0):\n        return []\n    start_row = has_relmax[-1]\n    # Each ridge line is a 3-tuple:\n    # rows, cols,Gap number\n    ridge_lines = [[[start_row],\n                   [col],\n                   0] for col in np.nonzero(all_max_cols[start_row])[0]]\n    final_lines = []\n    rows = np.arange(start_row - 1, -1, -1)\n    cols = np.arange(0, matr.shape[1])\n    for row in rows:\n        this_max_cols = cols[all_max_cols[row]]\n\n        # Increment gap number of each line,\n        # set it to zero later if appropriate\n        for line in ridge_lines:\n            line[2] += 1\n\n        # XXX These should always be all_max_cols[row]\n        # But the order might be different. Might be an efficiency gain\n        # to make sure the order is the same and avoid this iteration\n        prev_ridge_cols = np.array([line[1][-1] for line in ridge_lines])\n        # Look through every relative maximum found at current row\n        # Attempt to connect them with existing ridge lines.\n        for ind, col in enumerate(this_max_cols):\n            # If there is a previous ridge line within\n            # the max_distance to connect to, do so.\n            # Otherwise start a new one.\n            line = None\n            if(len(prev_ridge_cols) > 0):\n                diffs = np.abs(col - prev_ridge_cols)\n                closest = np.argmin(diffs)\n                if diffs[closest] <= max_distances[row]:\n                    line = ridge_lines[closest]\n            if(line is not None):\n                # Found a point close enough, extend current ridge line\n                line[1].append(col)\n                line[0].append(row)\n                line[2] = 0\n            else:\n                new_line = [[row],\n                            [col],\n                            0]\n                ridge_lines.append(new_line)\n\n        # Remove the ridge lines with gap_number too high\n        # XXX Modifying a list while iterating over it.\n        # Should be safe, since we iterate backwards, but\n        # still tacky.\n        for ind in xrange(len(ridge_lines) - 1, -1, -1):\n            line = ridge_lines[ind]\n            if line[2] > gap_thresh:\n                final_lines.append(line)\n                del ridge_lines[ind]\n\n    out_lines = []\n    for line in (final_lines + ridge_lines):\n        sortargs = np.array(np.argsort(line[0]))\n        rows, cols = np.zeros_like(sortargs), np.zeros_like(sortargs)\n        rows[sortargs] = line[0]\n        cols[sortargs] = line[1]\n        out_lines.append([rows, cols])\n\n    return out_lines\n\n\ndef _filter_ridge_lines(cwt, ridge_lines, window_size=None, min_length=None,\n                        min_snr=1, noise_perc=10):\n    \"\"\"\n    Filter ridge lines according to prescribed criteria. Intended\n    to be used for finding relative maxima.\n\n    Parameters\n    ----------\n    cwt : 2-D ndarray\n        Continuous wavelet transform from which the `ridge_lines` were defined.\n    ridge_lines : 1-D sequence\n        Each element should contain 2 sequences, the rows and columns\n        of the ridge line (respectively).\n    window_size : int, optional\n        Size of window to use to calculate noise floor.\n        Default is ``cwt.shape[1] / 20``.\n    min_length : int, optional\n        Minimum length a ridge line needs to be acceptable.\n        Default is ``cwt.shape[0] / 4``, ie 1/4-th the number of widths.\n    min_snr : float, optional\n        Minimum SNR ratio. Default 1. The signal is the value of\n        the cwt matrix at the shortest length scale (``cwt[0, loc]``), the\n        noise is the `noise_perc`th percentile of datapoints contained within a\n        window of `window_size` around ``cwt[0, loc]``.\n    noise_perc : float, optional\n        When calculating the noise floor, percentile of data points\n        examined below which to consider noise. Calculated using\n        scipy.stats.scoreatpercentile.\n\n    References\n    ----------\n    Bioinformatics (2006) 22 (17): 2059-2065. :doi:`10.1093/bioinformatics/btl355`\n    http://bioinformatics.oxfordjournals.org/content/22/17/2059.long\n\n    \"\"\"\n    num_points = cwt.shape[1]\n    if min_length is None:\n        min_length = np.ceil(cwt.shape[0] / 4)\n    if window_size is None:\n        window_size = np.ceil(num_points / 20)\n\n    window_size = int(window_size)\n    hf_window, odd = divmod(window_size, 2)\n\n    # Filter based on SNR\n    row_one = cwt[0, :]\n    noises = np.zeros_like(row_one)\n    for ind, val in enumerate(row_one):\n        window_start = max(ind - hf_window, 0)\n        window_end = min(ind + hf_window + odd, num_points)\n        noises[ind] = scoreatpercentile(row_one[window_start:window_end],\n                                        per=noise_perc)\n\n    def filt_func(line):\n        if len(line[0]) < min_length:\n            return False\n        snr = abs(cwt[line[0][0], line[1][0]] / noises[line[1][0]])\n        if snr < min_snr:\n            return False\n        return True\n\n    return list(filter(filt_func, ridge_lines))\n\n", "right_context": "", "import_text": ["math", "numpy", "scipy._lib.six.xrange", "scipy.signal.wavelets.cwt", "scipy.signal.wavelets.ricker", "scipy.stats.scoreatpercentile"], "prompt": "\"\"\"\nDescription: This function is used to find peaks in a 1-D array using continuous wavelet transform.\n\nArgs:\n    vector (numpy.ndarray): The 1-D array in which to find peaks.\n    widths (numpy.ndarray): Widths to use for calculating the CWT matrix.\n    wavelet (function, optional): Wavelet function, defaults to the Ricker wavelet.\n    max_distances (numpy.ndarray, optional): At what distances to search for the peaks. Defaults to widths / 4.0.\n    gap_thresh (float, optional): Minimum distance between adjacent peaks. Defaults to np.ceil(widths[0]).\n    min_length (int, optional): Minimum length of a peak.\n    min_snr (float, optional): Minimum SNR ratio. Defaults to 1.\n    noise_perc (float, optional): Percentage of the mean noise level to use for threshold estimation. Defaults to 10.\n\nReturns:\n    numpy.ndarray: Sorted indices of the peaks in the input array.\n\"\"\"", "prompt_is_gen_from_api": true, "comment": "    \"\"\"\n    Find peaks in a 1-D array with wavelet transformation.\n\n    The general approach is to smooth `vector` by convolving it with\n    `wavelet(width)` for each width in `widths`. Relative maxima which\n    appear at enough length scales, and with sufficiently high SNR, are\n    accepted.\n\n    Parameters\n    ----------\n    vector : ndarray\n        1-D array in which to find the peaks.\n    widths : sequence\n        1-D array of widths to use for calculating the CWT matrix. In general,\n        this range should cover the expected width of peaks of interest.\n    wavelet : callable, optional\n        Should take two parameters and return a 1-D array to convolve\n        with `vector`. The first parameter determines the number of points\n        of the returned wavelet array, the second parameter is the scale\n        (`width`) of the wavelet. Should be normalized and symmetric.\n        Default is the ricker wavelet.\n    max_distances : ndarray, optional\n        At each row, a ridge line is only connected if the relative max at\n        row[n] is within ``max_distances[n]`` from the relative max at\n        ``row[n+1]``.  Default value is ``widths/4``.\n    gap_thresh : float, optional\n        If a relative maximum is not found within `max_distances`,\n        there will be a gap. A ridge line is discontinued if there are more\n        than `gap_thresh` points without connecting a new relative maximum.\n        Default is the first value of the widths array i.e. widths[0].\n    min_length : int, optional\n        Minimum length a ridge line needs to be acceptable.\n        Default is ``cwt.shape[0] / 4``, ie 1/4-th the number of widths.\n    min_snr : float, optional\n        Minimum SNR ratio. Default 1. The signal is the value of\n        the cwt matrix at the shortest length scale (``cwt[0, loc]``), the\n        noise is the `noise_perc`th percentile of datapoints contained within a\n        window of `window_size` around ``cwt[0, loc]``.\n    noise_perc : float, optional\n        When calculating the noise floor, percentile of data points\n        examined below which to consider noise. Calculated using\n        `stats.scoreatpercentile`.  Default is 10.\n\n    Returns\n    -------\n    peaks_indices : ndarray\n        Indices of the locations in the `vector` where peaks were found.\n        The list is sorted.\n\n    See Also\n    --------\n    cwt\n        Continuous wavelet transform.\n    find_peaks\n        Find peaks inside a signal based on peak properties.\n\n    Notes\n    -----\n    This approach was designed for finding sharp peaks among noisy data,\n    however with proper parameter selection it should function well for\n    different peak shapes.\n\n    The algorithm is as follows:\n     1. Perform a continuous wavelet transform on `vector`, for the supplied\n        `widths`. This is a convolution of `vector` with `wavelet(width)` for\n        each width in `widths`. See `cwt`\n     2. Identify \"ridge lines\" in the cwt matrix. These are relative maxima\n        at each row, connected across adjacent rows. See identify_ridge_lines\n     3. Filter the ridge_lines using filter_ridge_lines.\n\n    .. versionadded:: 0.11.0\n\n    References\n    ----------\n    .. [1] Bioinformatics (2006) 22 (17): 2059-2065.\n        :doi:`10.1093/bioinformatics/btl355`\n        http://bioinformatics.oxfordjournals.org/content/22/17/2059.long\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> xs = np.arange(0, np.pi, 0.05)\n    >>> data = np.sin(xs)\n    >>> peakind = signal.find_peaks_cwt(data, np.arange(1,10))\n    >>> peakind, xs[peakind], data[peakind]\n    ([32], array([ 1.6]), array([ 0.9995736]))\n\n    \"\"\"", "function_dependencies": ["numpy.asarray", "numpy.ceil", "scipy.signal.wavelets.cwt", "numpy.asarray.sort"], "project_create_time": "2017-07-18T05:29:04+00:00", "project_update_time": "2024-04-17T19:09:27+00:00", "file_create_time": "2021-12-13T16:11:19Z", "file_update_time": "2023-01-16T14:06:57Z", "function_update_time": "2021-12-13T16:11:19Z", "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "reference_api": ["numpy.ceil"], "test_function": [{"file_path": "/catboost-v1.2.5/catboost-1.2.5/contrib/python/scipy/py2/scipy/signal/tests/test_peak_finding.py", "class_name": "TestFindPeaksCwt", "function_name": "test_find_peaks_exact", "code": "    def test_find_peaks_exact(self):\n        sigmas = [5.0, 3.0, 10.0, 20.0, 10.0, 50.0]\n        num_points = 500\n        test_data, act_locs = _gen_gaussians_even(sigmas, num_points)\n        widths = np.arange(0.1, max(sigmas))\n        found_locs = find_peaks_cwt(test_data, widths, gap_thresh=2, min_snr=0,\n                                         min_length=None)\n        np.testing.assert_array_equal(found_locs, act_locs,\n                        \"Found maximum locations did not equal those expected\")"}, {"file_path": "/catboost-v1.2.5/catboost-1.2.5/contrib/python/scipy/py2/scipy/signal/tests/test_peak_finding.py", "class_name": "TestFindPeaksCwt", "function_name": "test_find_peaks_withnoise", "code": "    def test_find_peaks_withnoise(self):\n        sigmas = [5.0, 3.0, 10.0, 20.0, 10.0, 50.0]\n        num_points = 500\n        test_data, act_locs = _gen_gaussians_even(sigmas, num_points)\n        widths = np.arange(0.1, max(sigmas))\n        noise_amp = 0.07\n        np.random.seed(18181911)\n        test_data += (np.random.rand(num_points) - 0.5)*(2*noise_amp)\n        found_locs = find_peaks_cwt(test_data, widths, min_length=15,\n                                         gap_thresh=1, min_snr=noise_amp / 5)\n\n        np.testing.assert_equal(len(found_locs), len(act_locs), 'Different number' +\n                                'of peaks found than expected')\n        diffs = np.abs(found_locs - act_locs)\n        max_diffs = np.array(sigmas) / 5\n        np.testing.assert_array_less(diffs, max_diffs, 'Maximum location differed' +\n                                     'by more than %s' % (max_diffs))"}, {"file_path": "/catboost-v1.2.5/catboost-1.2.5/contrib/python/scipy/py2/scipy/signal/tests/test_peak_finding.py", "class_name": "TestFindPeaksCwt", "function_name": "test_find_peaks_nopeak", "code": "    def test_find_peaks_nopeak(self):\n        noise_amp = 1.0\n        num_points = 100\n        np.random.seed(181819141)\n        test_data = (np.random.rand(num_points) - 0.5)*(2*noise_amp)\n        widths = np.arange(10, 50)\n        found_locs = find_peaks_cwt(test_data, widths, min_snr=5, noise_perc=30)\n        np.testing.assert_equal(len(found_locs), 0)"}, {"file_path": "/catboost-v1.2.5/catboost-1.2.5/contrib/python/scipy/py3/scipy/signal/tests/test_peak_finding.py", "class_name": "TestFindPeaksCwt", "function_name": "test_find_peaks_exact", "code": "    def test_find_peaks_exact(self):\n        sigmas = [5.0, 3.0, 10.0, 20.0, 10.0, 50.0]\n        num_points = 500\n        test_data, act_locs = _gen_gaussians_even(sigmas, num_points)\n        widths = np.arange(0.1, max(sigmas))\n        found_locs = find_peaks_cwt(test_data, widths, gap_thresh=2, min_snr=0,\n                                         min_length=None)\n        np.testing.assert_array_equal(found_locs, act_locs,\n                        \"Found maximum locations did not equal those expected\")"}, {"file_path": "/catboost-v1.2.5/catboost-1.2.5/contrib/python/scipy/py3/scipy/signal/tests/test_peak_finding.py", "class_name": "TestFindPeaksCwt", "function_name": "test_find_peaks_withnoise", "code": "    def test_find_peaks_withnoise(self):\n        sigmas = [5.0, 3.0, 10.0, 20.0, 10.0, 50.0]\n        num_points = 500\n        test_data, act_locs = _gen_gaussians_even(sigmas, num_points)\n        widths = np.arange(0.1, max(sigmas))\n        noise_amp = 0.07\n        np.random.seed(18181911)\n        test_data += (np.random.rand(num_points) - 0.5)*(2*noise_amp)\n        found_locs = find_peaks_cwt(test_data, widths, min_length=15,\n                                         gap_thresh=1, min_snr=noise_amp / 5)\n\n        np.testing.assert_equal(len(found_locs), len(act_locs), 'Different number' +\n                                'of peaks found than expected')\n        diffs = np.abs(found_locs - act_locs)\n        max_diffs = np.array(sigmas) / 5\n        np.testing.assert_array_less(diffs, max_diffs, 'Maximum location differed' +\n                                     'by more than %s' % (max_diffs))"}, {"file_path": "/catboost-v1.2.5/catboost-1.2.5/contrib/python/scipy/py3/scipy/signal/tests/test_peak_finding.py", "class_name": "TestFindPeaksCwt", "function_name": "test_find_peaks_nopeak", "code": "    def test_find_peaks_nopeak(self):\n        noise_amp = 1.0\n        num_points = 100\n        np.random.seed(181819141)\n        test_data = (np.random.rand(num_points) - 0.5)*(2*noise_amp)\n        widths = np.arange(10, 50)\n        found_locs = find_peaks_cwt(test_data, widths, min_snr=5, noise_perc=30)\n        np.testing.assert_equal(len(found_locs), 0)"}, {"file_path": "/catboost-v1.2.5/catboost-1.2.5/contrib/python/scipy/py3/scipy/signal/tests/test_peak_finding.py", "class_name": "TestFindPeaksCwt", "function_name": "test_find_peaks_window_size", "code": "    def test_find_peaks_window_size(self):\n        sigmas = [2.0, 2.0]\n        num_points = 1000\n        test_data, act_locs = _gen_gaussians_even(sigmas, num_points)\n        widths = np.arange(0.1, max(sigmas), 0.2)\n        noise_amp = 0.05\n        np.random.seed(18181911)\n        test_data += (np.random.rand(num_points) - 0.5)*(2*noise_amp)\n\n        # Possibly contrived negative region to throw off peak finding\n        # when window_size is too large\n        test_data[250:320] -= 1\n\n        found_locs = find_peaks_cwt(test_data, widths, gap_thresh=2, min_snr=3,\n                                    min_length=None, window_size=None)\n        with pytest.raises(AssertionError):\n            assert found_locs.size == act_locs.size\n\n        found_locs = find_peaks_cwt(test_data, widths, gap_thresh=2, min_snr=3,\n                                    min_length=None, window_size=20)\n        assert found_locs.size == act_locs.size"}]}]
